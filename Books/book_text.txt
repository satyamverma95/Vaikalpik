introduction to probability
for
data science
stanley h. chan
purdue university
copyright ¬©2021 stanley h. chan
this book is published by michigan publishing under an agreement with the author. it is
made available free of charge in electronic form to any student or instructor interested in
the subject matter.
published in the united states of america by
michigan publishing
manufactured in the united states of america
isbn 978-1-60785-746-4 (hardcover)
isbn 978-1-60785-747-1 (electronic)
iito vivian, joanna, and cynthia chan
and ye shall know the truth, and the truth shall make you free.
john 8:32
iiiivpreface
this book is an introductory textbook in undergraduate probability. it has a mission: to spell
out the motivation ,intuition , and implication of the probabilistic tools we use in science
and engineering. from over half a decade of teaching the course, i have distilled what i
believe to be the core of probabilistic methods. i put the book in the context of data science
to emphasize the inseparability between data (computing) and probability (theory) in our
time.
probability is one of the most interesting subjects in electrical engineering and com-
puter science. it bridges our favorite engineering principles to the practical reality, a world
that is full of uncertainty. however, because probability is such a mature subject, the under-
graduate textbooks alone might fill several rows of shelves in a library. when the literature
is so rich, the challenge becomes how one can pierce through to the insight while diving into
the details. for example, many of you have used a normal random variable before, but have
you ever wondered where the ‚Äúbell shape‚Äù comes from? every probability class will teach
you about flipping a coin, but how can ‚Äúflipping a coin‚Äù ever be useful in machine learning
today? data scientists use the poisson random variables to model the internet traffic, but
where does the gorgeous poisson equation come from? this book is designed to fill these
gaps with knowledge that is essential to all data science students.
this leads to the three goals of the book. (i) motivation: in the ocean of mathematical
definitions, theorems, and equations, why should we spend our time on this particular topic
but not another? (ii) intuition: when going through the derivations, is there a geometric
interpretation or physics beyond those equations? (iii) implication: after we have learned a
topic, what new problems can we solve?
the book‚Äôs intended audience is undergraduate juniors/seniors and first-year gradu-
ate students majoring in electrical engineering and computer science. the prerequisites are
standard undergraduate linear algebra and calculus, except for the section about charac-
teristic functions, where fourier transforms are needed. an undergraduate course in signals
and systems would suffice, even taken concurrently while studying this book.
the length of the book is suitable for a two-semester course. instructors are encouraged
to use the set of chapters that best fits their classes. for example, a basic probability course
can use chapters 1-5 as its backbone. chapter 6 on sample statistics is suitable for students
who wish to gain theoretical insights into probabilistic convergence. chapter 7 on regression
and chapter 8 on estimation best suit students who want to pursue machine learning and
signal processing. chapter 9 discusses confidence intervals and hypothesis testing, which are
critical to modern data analysis. chapter 10 introduces random processes. my approach for
random processes is more tailored to information processing and communication systems,
which are usually more relevant to electrical engineering students.
additional teaching resources can be found on the book‚Äôs website, where you can
vfind lecture videos and homework videos. throughout the book you will see many ‚Äúpractice
exercises‚Äù, which are easy problems with worked-out solutions. they can be skipped without
loss to the flow of the book.
acknowledgements: if i could thank only one person, it must be professor fawwaz
ulaby of the university of michigan. professor ulaby has been the source of support in
all aspects, from the book‚Äôs layout to technical content, proofreading, and marketing. the
book would not have been published without the help of professor ulaby. i am deeply
moved by professor ulaby‚Äôs vision that education should be made accessible to all students.
with textbook prices rocketing up, the eecs free textbook initiative launched by professor
ulaby is the most direct response to the publishers, teachers, parents, and students. thank
you, fawwaz, for your unbounded support ‚Äî technically, mentally, and financially. thank
you also for recommending richard carnes. the meticulous details richard offered have
significantly improved the fluency of the book. thank you, richard.
i thank my colleagues at purdue who had shared many thoughts with me when i
taught the course (in alphabetical order): professors mark bell, mary comer, saul gelfand,
amy reibman, and chih-chun wang. my teaching assistant i-fan lin was instrumental in
the early development of this book. to the graduate students of my lab (yiheng chi, nick
chimitt, kent gauen, abhiram gnanasambandam, guanzhe hong, chengxi li, zhiyuan
mao, xiangyu qu, and yash sanghvi): thank you! it would have been impossible to finish
the book without your participation. a few students i taught volunteered to help edit
the book: benjamin gottfried, harrison hsueh, dawoon jung, antonio kincaid, deepak
ravikumar, krister ulvog, peace umoru, zhijing yao. i would like to thank my ph.d.
advisor professor truong nguyen for encouraging me to write the book.
finally, i would like to thank my wife vivian and my daughters, joanna and cynthia,
for their love, patience, and support.
stanley h. chan, west lafayette, indiana
may, 2021
companion website:
https://probability4datascience.com/
vicontents
1 mathematical background 1
1.1 infinite series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.1.1 geometric series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.1.2 binomial series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.2 approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.2.1 taylor approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1.2.2 exponential series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.2.3 logarithmic approximation . . . . . . . . . . . . . . . . . . . . . . . . 13
1.3 integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
1.3.1 odd and even functions . . . . . . . . . . . . . . . . . . . . . . . . . . 15
1.3.2 fundamental theorem of calculus . . . . . . . . . . . . . . . . . . . . 17
1.4 linear algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
1.4.1 why do we need linear algebra in data science? . . . . . . . . . . . . . 20
1.4.2 everything you need to know about linear algebra . . . . . . . . . . . 21
1.4.3 inner products and norms . . . . . . . . . . . . . . . . . . . . . . . . . 24
1.4.4 matrix calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
1.5 basic combinatorics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
1.5.1 birthday paradox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
1.5.2 permutation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
1.5.3 combination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
1.6 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
1.7 reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
1.8 problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
2 probability 43
2.1 set theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
2.1.1 why study set theory? . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
2.1.2 basic concepts of a set . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
2.1.3 subsets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
2.1.4 empty set and universal set . . . . . . . . . . . . . . . . . . . . . . . . 48
2.1.5 union . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
2.1.6 intersection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
2.1.7 complement and difference . . . . . . . . . . . . . . . . . . . . . . . . 52
2.1.8 disjoint and partition . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
2.1.9 set operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
2.1.10 closing remarks about set theory . . . . . . . . . . . . . . . . . . . . . 57
viicontents
2.2 probability space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
2.2.1 sample space œâ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
2.2.2 event space f. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
2.2.3 probability law p. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
2.2.4 measure zero sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
2.2.5 summary of the probability space . . . . . . . . . . . . . . . . . . . . 74
2.3 axioms of probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
2.3.1 why these three probability axioms? . . . . . . . . . . . . . . . . . . . 75
2.3.2 axioms through the lens of measure . . . . . . . . . . . . . . . . . . . 76
2.3.3 corollaries derived from the axioms . . . . . . . . . . . . . . . . . . . 77
2.4 conditional probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
2.4.1 definition of conditional probability . . . . . . . . . . . . . . . . . . . 81
2.4.2 independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
2.4.3 bayes‚Äô theorem and the law of total probability . . . . . . . . . . . . . 89
2.4.4 the three prisoners problem . . . . . . . . . . . . . . . . . . . . . . . 92
2.5 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
2.6 references . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
2.7 problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
3 discrete random variables 103
3.1 random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
3.1.1 a motivating example . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
3.1.2 definition of a random variable . . . . . . . . . . . . . . . . . . . . . . 105
3.1.3 probability measure on random variables . . . . . . . . . . . . . . . . 107
3.2 probability mass function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
3.2.1 definition of probability mass function . . . . . . . . . . . . . . . . . . 110
3.2.2 pmf and probability measure . . . . . . . . . . . . . . . . . . . . . . . 110
3.2.3 normalization property . . . . . . . . . . . . . . . . . . . . . . . . . . 112
3.2.4 pmf versus histogram . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
3.2.5 estimating histograms from real data . . . . . . . . . . . . . . . . . . 117
3.3 cumulative distribution functions (discrete) . . . . . . . . . . . . . . . . . . 121
3.3.1 definition of the cumulative distribution function . . . . . . . . . . . . 121
3.3.2 properties of the cdf . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
3.3.3 converting between pmf and cdf . . . . . . . . . . . . . . . . . . . 124
3.4 expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
3.4.1 definition of expectation . . . . . . . . . . . . . . . . . . . . . . . . . . 125
3.4.2 existence of expectation . . . . . . . . . . . . . . . . . . . . . . . . . . 130
3.4.3 properties of expectation . . . . . . . . . . . . . . . . . . . . . . . . . 130
3.4.4 moments and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
3.5 common discrete random variables . . . . . . . . . . . . . . . . . . . . . . . 136
3.5.1 bernoulli random variable . . . . . . . . . . . . . . . . . . . . . . . . . 137
3.5.2 binomial random variable . . . . . . . . . . . . . . . . . . . . . . . . . 143
3.5.3 geometric random variable . . . . . . . . . . . . . . . . . . . . . . . . 149
3.5.4 poisson random variable . . . . . . . . . . . . . . . . . . . . . . . . . . 152
3.6 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
3.7 references . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
3.8 problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
viiicontents
4 continuous random variables 171
4.1 probability density function . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
4.1.1 some intuitions about probability density functions . . . . . . . . . . . 172
4.1.2 more in-depth discussion about pdfs . . . . . . . . . . . . . . . . . . 174
4.1.3 connecting with the pmf . . . . . . . . . . . . . . . . . . . . . . . . . 178
4.2 expectation, moment, and variance . . . . . . . . . . . . . . . . . . . . . . . 180
4.2.1 definition and properties . . . . . . . . . . . . . . . . . . . . . . . . . 180
4.2.2 existence of expectation . . . . . . . . . . . . . . . . . . . . . . . . . . 183
4.2.3 moment and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
4.3 cumulative distribution function . . . . . . . . . . . . . . . . . . . . . . . . 185
4.3.1 cdf for continuous random variables . . . . . . . . . . . . . . . . . . 186
4.3.2 properties of cdf . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
4.3.3 retrieving pdf from cdf . . . . . . . . . . . . . . . . . . . . . . . . 193
4.3.4 cdf: unifying discrete and continuous random variables . . . . . . . 194
4.4 median, mode, and mean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
4.4.1 median . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
4.4.2 mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
4.4.3 mean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
4.5 uniform and exponential random variables . . . . . . . . . . . . . . . . . . . 201
4.5.1 uniform random variables . . . . . . . . . . . . . . . . . . . . . . . . . 202
4.5.2 exponential random variables . . . . . . . . . . . . . . . . . . . . . . . 205
4.5.3 origin of exponential random variables . . . . . . . . . . . . . . . . . . 207
4.5.4 applications of exponential random variables . . . . . . . . . . . . . . 209
4.6 gaussian random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
4.6.1 definition of a gaussian random variable . . . . . . . . . . . . . . . . 211
4.6.2 standard gaussian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
4.6.3 skewness and kurtosis . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
4.6.4 origin of gaussian random variables . . . . . . . . . . . . . . . . . . 220
4.7 functions of random variables . . . . . . . . . . . . . . . . . . . . . . . . . . 223
4.7.1 general principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
4.7.2 examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
4.8 generating random numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
4.8.1 general principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
4.8.2 examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
4.9 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
4.10 reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
4.11 problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
5 joint distributions 241
5.1 joint pmf and joint pdf . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
5.1.1 probability measure in 2d . . . . . . . . . . . . . . . . . . . . . . . . . 244
5.1.2 discrete random variables . . . . . . . . . . . . . . . . . . . . . . . . . 245
5.1.3 continuous random variables . . . . . . . . . . . . . . . . . . . . . . . 247
5.1.4 normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
5.1.5 marginal pmf and marginal pdf . . . . . . . . . . . . . . . . . . . . 250
5.1.6 independent random variables . . . . . . . . . . . . . . . . . . . . . . 251
5.1.7 joint cdf . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
5.2 joint expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
ixcontents
5.2.1 definition and interpretation . . . . . . . . . . . . . . . . . . . . . . . 257
5.2.2 covariance and correlation coefficient . . . . . . . . . . . . . . . . . . 261
5.2.3 independence and correlation . . . . . . . . . . . . . . . . . . . . . . . 263
5.2.4 computing correlation from data . . . . . . . . . . . . . . . . . . . . . 265
5.3 conditional pmf and pdf . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
5.3.1 conditional pmf . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
5.3.2 conditional pdf . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
5.4 conditional expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
5.4.1 definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
5.4.2 the law of total expectation . . . . . . . . . . . . . . . . . . . . . . . 276
5.5 sum of two random variables . . . . . . . . . . . . . . . . . . . . . . . . . . 280
5.5.1 intuition through convolution . . . . . . . . . . . . . . . . . . . . . . . 280
5.5.2 main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
5.5.3 sum of common distributions . . . . . . . . . . . . . . . . . . . . . . . 282
5.6 random vectors and covariance matrices . . . . . . . . . . . . . . . . . . . . 286
5.6.1 pdf of random vectors . . . . . . . . . . . . . . . . . . . . . . . . . . 286
5.6.2 expectation of random vectors . . . . . . . . . . . . . . . . . . . . . . 288
5.6.3 covariance matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
5.6.4 multidimensional gaussian . . . . . . . . . . . . . . . . . . . . . . . . 290
5.7 transformation of multidimensional gaussians . . . . . . . . . . . . . . . . . 293
5.7.1 linear transformation of mean and covariance . . . . . . . . . . . . . . 293
5.7.2 eigenvalues and eigenvectors . . . . . . . . . . . . . . . . . . . . . . . 295
5.7.3 covariance matrices are always positive semi-definite . . . . . . . . . . 297
5.7.4 gaussian whitening . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
5.8 principal-component analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 303
5.8.1 the main idea: eigendecomposition . . . . . . . . . . . . . . . . . . . 303
5.8.2 the eigenface problem . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
5.8.3 what cannot be analyzed by pca? . . . . . . . . . . . . . . . . . . . 311
5.9 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
5.10 references . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
5.11 problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
6 sample statistics 319
6.1 moment-generating and characteristic functions . . . . . . . . . . . . . . . . 324
6.1.1 moment-generating function . . . . . . . . . . . . . . . . . . . . . . . . 324
6.1.2 sum of independent variables via mgf . . . . . . . . . . . . . . . . . 327
6.1.3 characteristic functions . . . . . . . . . . . . . . . . . . . . . . . . . . 329
6.2 probability inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
6.2.1 union bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
6.2.2 the cauchy-schwarz inequality . . . . . . . . . . . . . . . . . . . . . . 335
6.2.3 jensen‚Äôs inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
6.2.4 markov‚Äôs inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
6.2.5 chebyshev‚Äôs inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
6.2.6 chernoff‚Äôs bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
6.2.7 comparing chernoff and chebyshev . . . . . . . . . . . . . . . . . . . 344
6.2.8 hoeffding‚Äôs inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
6.3 law of large numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
6.3.1 sample average . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
xcontents
6.3.2 weak law of large numbers (wlln) . . . . . . . . . . . . . . . . . . . 354
6.3.3 convergence in probability . . . . . . . . . . . . . . . . . . . . . . . . 356
6.3.4 can we prove wlln using chernoff‚Äôs bound? . . . . . . . . . . . . . 358
6.3.5 does the weak law of large numbers always hold? . . . . . . . . . . . . 359
6.3.6 strong law of large numbers . . . . . . . . . . . . . . . . . . . . . . . . 360
6.3.7 almost sure convergence . . . . . . . . . . . . . . . . . . . . . . . . . . 362
6.3.8 proof of the strong law of large numbers . . . . . . . . . . . . . . . . . 364
6.4 central limit theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
6.4.1 convergence in distribution . . . . . . . . . . . . . . . . . . . . . . . . 367
6.4.2 central limit theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 372
6.4.3 examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
6.4.4 limitation of the central limit theorem . . . . . . . . . . . . . . . . 378
6.5 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380
6.6 references . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
6.7 problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
7 regression 389
7.1 principles of regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
7.1.1 intuition: how to fit a straight line? . . . . . . . . . . . . . . . . . . . 395
7.1.2 solving the linear regression problem . . . . . . . . . . . . . . . . . . . 397
7.1.3 extension: beyond a straight line . . . . . . . . . . . . . . . . . . . . . 401
7.1.4 overdetermined and underdetermined systems . . . . . . . . . . . . . 409
7.1.5 robust linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . 412
7.2 overfitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418
7.2.1 overview of overfitting . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
7.2.2 analysis of the linear case . . . . . . . . . . . . . . . . . . . . . . . . . 420
7.2.3 interpreting the linear analysis results . . . . . . . . . . . . . . . . . . 425
7.3 bias and variance trade-off . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429
7.3.1 decomposing the testing error . . . . . . . . . . . . . . . . . . . . . . 430
7.3.2 analysis of the bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
7.3.3 variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436
7.3.4 bias and variance on the learning curve . . . . . . . . . . . . . . . . . 438
7.4 regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440
7.4.1 ridge regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440
7.4.2 lasso regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
7.5 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
7.6 references . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
7.7 problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
8 estimation 465
8.1 maximum-likelihood estimation . . . . . . . . . . . . . . . . . . . . . . . . . 468
8.1.1 likelihood function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468
8.1.2 maximum-likelihood estimate . . . . . . . . . . . . . . . . . . . . . . . 472
8.1.3 application 1: social network analysis . . . . . . . . . . . . . . . . . . 478
8.1.4 application 2: reconstructing images . . . . . . . . . . . . . . . . . . 481
8.1.5 more examples of ml estimation . . . . . . . . . . . . . . . . . . . . . 484
8.1.6 regression versus ml estimation . . . . . . . . . . . . . . . . . . . . . 487
8.2 properties of ml estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491
xicontents
8.2.1 estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491
8.2.2 unbiased estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492
8.2.3 consistent estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
8.2.4 invariance principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . 500
8.3 maximum a posteriori estimation . . . . . . . . . . . . . . . . . . . . . . . . 502
8.3.1 the trio of likelihood, prior, and posterior . . . . . . . . . . . . . . . . 503
8.3.2 understanding the priors . . . . . . . . . . . . . . . . . . . . . . . . . 504
8.3.3 map formulation and solution . . . . . . . . . . . . . . . . . . . . . . 506
8.3.4 analyzing the map solution . . . . . . . . . . . . . . . . . . . . . . . 508
8.3.5 analysis of the posterior distribution . . . . . . . . . . . . . . . . . . . 511
8.3.6 conjugate prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
8.3.7 linking map with regression . . . . . . . . . . . . . . . . . . . . . . . 517
8.4 minimum mean-square estimation . . . . . . . . . . . . . . . . . . . . . . . . 520
8.4.1 positioning the minimum mean-square estimation . . . . . . . . . . . 520
8.4.2 mean squared error . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522
8.4.3 mmse estimate = conditional expectation . . . . . . . . . . . . . . . 523
8.4.4 mmse estimator for multidimensional gaussian . . . . . . . . . . . . 529
8.4.5 linking mmse and neural networks . . . . . . . . . . . . . . . . . . . 533
8.5 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534
8.6 references . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535
8.7 problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536
9 confidence and hypothesis 541
9.1 confidence interval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543
9.1.1 the randomness of an estimator . . . . . . . . . . . . . . . . . . . . . 543
9.1.2 understanding confidence intervals . . . . . . . . . . . . . . . . . . . . 545
9.1.3 constructing a confidence interval . . . . . . . . . . . . . . . . . . . . 548
9.1.4 properties of the confidence interval . . . . . . . . . . . . . . . . . . . 551
9.1.5 student‚Äôs t-distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 554
9.1.6 comparing student‚Äôs t-distribution and gaussian . . . . . . . . . . . . 558
9.2 bootstrapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 559
9.2.1 a brute force approach . . . . . . . . . . . . . . . . . . . . . . . . . . 560
9.2.2 bootstrapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 562
9.3 hypothesis testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 566
9.3.1 what is a hypothesis? . . . . . . . . . . . . . . . . . . . . . . . . . . . 566
9.3.2 critical-value test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567
9.3.3 p-value test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 571
9.3.4 z-test and t-test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 574
9.4 neyman-pearson test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 577
9.4.1 null and alternative distributions . . . . . . . . . . . . . . . . . . . . . 577
9.4.2 type 1 and type 2 errors . . . . . . . . . . . . . . . . . . . . . . . . . 579
9.4.3 neyman-pearson decision . . . . . . . . . . . . . . . . . . . . . . . . . 582
9.5 roc and precision-recall curve . . . . . . . . . . . . . . . . . . . . . . . . . 589
9.5.1 receiver operating characteristic (roc) . . . . . . . . . . . . . . . . 589
9.5.2 comparing roc curves . . . . . . . . . . . . . . . . . . . . . . . . . . 592
9.5.3 the roc curve in practice . . . . . . . . . . . . . . . . . . . . . . . . 598
9.5.4 the precision-recall (pr) curve . . . . . . . . . . . . . . . . . . . . . 601
9.6 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 605
xiicontents
9.7 reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 606
9.8 problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607
10 random processes 611
10.1 basic concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612
10.1.1 everything you need to know about a random process . . . . . . . . . 612
10.1.2 statistical and temporal perspectives . . . . . . . . . . . . . . . . . . . 614
10.2 mean and correlation functions . . . . . . . . . . . . . . . . . . . . . . . . . 618
10.2.1 mean function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 618
10.2.2 autocorrelation function . . . . . . . . . . . . . . . . . . . . . . . . . . 622
10.2.3 independent processes . . . . . . . . . . . . . . . . . . . . . . . . . . . 629
10.3 wide-sense stationary processes . . . . . . . . . . . . . . . . . . . . . . . . . 630
10.3.1 definition of a wss process . . . . . . . . . . . . . . . . . . . . . . . . 631
10.3.2 properties of rx(œÑ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 632
10.3.3 physical interpretation of rx(œÑ) . . . . . . . . . . . . . . . . . . . . . 633
10.4 power spectral density . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 636
10.4.1 basic concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 636
10.4.2 origin of the power spectral density . . . . . . . . . . . . . . . . . . . 640
10.5 wss process through lti systems . . . . . . . . . . . . . . . . . . . . . . . . 643
10.5.1 review of linear time-invariant systems . . . . . . . . . . . . . . . . . 643
10.5.2 mean and autocorrelation through lti systems . . . . . . . . . . . . . 644
10.5.3 power spectral density through lti systems . . . . . . . . . . . . . . . 646
10.5.4 cross-correlation through lti systems . . . . . . . . . . . . . . . . . . 649
10.6 optimal linear filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 653
10.6.1 discrete-time random processes . . . . . . . . . . . . . . . . . . . . . . 653
10.6.2 problem formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654
10.6.3 yule-walker equation . . . . . . . . . . . . . . . . . . . . . . . . . . . 656
10.6.4 linear prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 658
10.6.5 wiener filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 662
10.7 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 669
10.8 appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 670
10.8.1 the mean-square ergodic theorem . . . . . . . . . . . . . . . . . . . 674
10.9 references . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 675
10.10problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 676
a appendix 681
xiiicontents
xivchapter 1
mathematical background
‚Äúdata science‚Äù has different meanings to different people. if you ask a biologist, data science
could mean analyzing dna sequences. if you ask a banker, data science could mean pre-
dicting the stock market. if you ask a software engineer, data science could mean programs
and data structures; if you ask a machine learning scientist, data science could mean models
and algorithms. however, one thing that is common in all these disciplines is the concept of
uncertainty . we choose to learn from data because we believe that the latent information
is embedded in the data ‚Äî unprocessed, contains noise, and could have missing entries. if
there is no randomness, all data scientists can close their business because there is simply
no problem to solve. however, the moment we see randomness, our business comes back.
therefore, data science is the subject of making decisions in uncertainty.
the mathematics of analyzing uncertainty is probability . it is thetool to help us model,
analyze, and predict random events. probability can be studied in as many ways as you can
think of. you can take a rigorous course in probability theory, or a ‚Äúprobability for dummies‚Äù
on the internet, or a typical undergraduate probability course offered by your school. this
book is different from all these. our goal is to tell you how things work in the context of data
science. for example, why do we need those three axioms of probabilities and not others?
where does the ‚Äúbell shape‚Äù gaussian random variable come from? how many samples do
we need to construct a reliable histogram? these questions are at the core of data science,
and they deserve close attention rather than sweeping them under the rug.
to help you get used to the pace and style of this book, in this chapter, we review some
of the very familiar topics in undergraduate algebra and calculus. these topics are meant
to warm up your mathematics background so that you can follow the subsequent chapters.
specifically, in this chapter, we cover several topics. first, in section 1.1 we discuss infinite
series, something that will be used frequently when we evaluate the expectation and variance
of random variables in chapter 3. in section 1.2 we review the taylor approximation,
which will be helpful when we discuss continuous random variables. section 1.3 discusses
integration and reviews several tricks we can use to make integration easy. section 1.4
deals with linear algebra, aka matrices and vectors, which are fundamental to modern data
analysis. finally, section 1.5 discusses permutation and combination, two basic techniques
to count events.
1chapter 1. mathematical background
1.1 infinite series
imagine that you have a fair coin . if you get a tail, you flip it again. you do this repeatedly
until you finally get a head. what is the probability that you need to flip the coin three
times to get one head?
this is a warm-up exercise. since the coin is fair, the probability of obtaining a head
is1
2. the probability of getting a tail followed by a head is1
2√ó1
2=1
4. similarly, the
probability of getting two tails and then a head is1
2√ó1
2√ó1
2=1
8. if you follow this logic, you
can write down the probabilities for all other cases. for your convenience, we have drawn the
first few in figure 1.1 . as you have probably noticed, the probabilities follow the pattern
{1
2,1
4,1
8, . . .}.
figure 1.1: suppose you flip a coin until you see a head. this requires you to have n‚àí1tails followed
by a head. the probability of this sequence of events are1
2,1
4,1
8, . . . , which forms an infinite sequence.
we can also summarize these probabilities using a familiar plot called the histogram
as shown in figure 1.2 . the histogram for this problem has a special pattern, that every
value is one order higher than the preceding one, and the sequence is infinitely long.
1 2 3 4 5 6 7 8 9 1000.10.20.30.40.5
figure 1.2: the histogram of flipping a coin until we see a head. the x-axis is the number of coin flips,
and the y-axis is the probability.
let us ask something harder: on average, if you want to be 90% sure that you will
get a head, what is the minimum number of attempts you need to try? five attempts?
ten attempts? indeed, if you try ten attempts, you will very likely accomplish your goal.
however, this would seem to be overkill. if you try five attempts, then it becomes unclear
whether you will be 90% sure.
21.1. infinite series
this problem can be answered by analyzing the sequence of probabilities. if we make
two attempts, then the probability of getting a head is the sum of the probabilities for one
attempt and that of two attempts:
p[success after 1 attempt] =1
2= 0.5
p[success after 2 attempts] =1
2+1
4= 0.75
therefore, if you make 3 attempts or 4 attempts, you get the following probabilities:
p[success after 3 attempts] =1
2+1
4+1
8= 0.875
p[success after 4 attempts] =1
2+1
4+1
8+1
16= 0.9375.
so if we try four attempts, we will have a 93.75% probability of getting a head. thus, four
attempts is the answer.
the matlab / python codes we used to generate figure 1.2 are shown below.
% matlab code to generate a geometric sequence
p = 1/2;
n = 1:10;
x = p.^n;
bar(n,x,‚Äôfacecolor‚Äô,[0.8, 0.2,0.2]);
# python code to generate a geometric sequence
import numpy as np
import matplotlib.pyplot as plt
p = 1/2
n = np.arange(0,10)
x = np.power(p,n)
plt.bar(n,x)
this warm-up exercise has perhaps raised some of your interest in the subject. however,
we will not tell you everything now. we will come back to the probability in chapter 3
when we discuss geometric random variables. in the present section, we want to make sure
you have the basic mathematical tools to calculate quantities, such as a sum of fractional
numbers. for example, what if we want to calculate p[success after 107 attempts]? is there
a systematic way of performing the calculation?
remark . you should be aware that the 93.75% only says that the probability of achieving
the goal is high. if you have a bad day, you may still need more than four attempts. therefore,
when we stated the question, we asked for 90% ‚Äúon average‚Äù. sometimes you may need
more attempts and sometimes fewer attempts, but on average, you have a 93.75% chance
of succeeding.
1.1.1 geometric series
a geometric series is the sum of a finite or an infinite sequence of numbers with a constant
ratio between successive terms. as we have seen in the previous example, a geometric series
3chapter 1. mathematical background
appears naturally in the context of discrete events. in chapter 3 of this book, we will use
geometric series when calculating the expectation andmoments of a random variable.
definition 1.1. let0< r < 1, afinite geometric sequence of power nis a sequence
of numbers
1, r, r2, . . . , rn
.
aninfinite geometric sequence is a sequence of numbers

1, r, r2, r3, . . .
.
theorem 1.1. the sum of a finite geometric series of power nis
nx
k=0rk= 1 + r+r2+¬∑¬∑¬∑+rn=1‚àírn+1
1‚àír. (1.1)
proof . we multiply both sides by 1 ‚àír. the left hand side becomes
 nx
k=0rk!
(1‚àír) = 
1 +r+r2+¬∑¬∑¬∑+rn
(1‚àír)
= 
1 +r+r2+¬∑¬∑¬∑+rn
‚àí 
r+r2+r3+¬∑¬∑¬∑+rn+1
(a)= 1‚àírn+1,
where ( a) holds because terms are canceled due to subtractions.
‚ñ°
a corollary of equation (1.1) is the sum of an infinite geometric sequence.
corollary 1.1. let0< r < 1. the sum of an infinite geometric series is
‚àûx
k=0rk= 1 + r+r2+¬∑¬∑¬∑=1
1‚àír. (1.2)
proof . we take the limit in equation (1.1). this yields
‚àûx
k=0rk= lim
n‚Üí‚àûnx
k=0rk= lim
n‚Üí‚àû1‚àírn+1
1‚àír=1
1‚àír.
‚ñ°
remark . note that the condition 0 < r < 1 is important. if r > 1, then the limit
limn‚Üí‚àûrn+1in equation (1.2) will diverge. the constant rcannot equal to 1, for oth-
erwise the fraction (1 ‚àírn+1)/(1‚àír) is undefined. we are not interested in the case when
r= 0, because the sum is trivially 1:p‚àû
k=00k= 1 + 01+ 02+¬∑¬∑¬∑= 1.
41.1. infinite series
practice exercise 1.1 . compute the infinite series‚àûp
k=21
2k.
solution .
‚àûx
k=21
2k=1
4+1
8+¬∑¬∑¬∑+
=1
4
1 +1
2+1
4+¬∑¬∑¬∑
=1
4¬∑1
1‚àí1
2=1
2.
remark . you should not be confused about a geometric series and a harmonic series . a
harmonic series concerns with the sum of {1,1
2,1
3,1
4, . . .}. it turns out that1
‚àûx
n=11
n= 1 +1
2+1
3+1
4+¬∑¬∑¬∑=‚àû.
on the other hand, a squared harmonic series {1,1
22,1
32,1
42, . . .}converges:
‚àûx
n=11
n2= 1 +1
22+1
32+1
42+¬∑¬∑¬∑=œÄ2
6.
the latter result is known as the basel problem .
we can extend the main theorem by considering more complicated series, for example
the following one.
corollary 1.2. let0< r < 1. it holds that
‚àûx
k=1krk‚àí1= 1 + 2 r+ 3r2+¬∑¬∑¬∑=1
(1‚àír)2. (1.3)
proof . take the derivative on both sides of equation (1.2). the left hand side becomes
d
dr‚àûx
k=0rk=d
dr 
1 +r+r2+¬∑¬∑¬∑
= 1 + 2 r+ 3r2+¬∑¬∑¬∑=‚àûx
k=1krk‚àí1
the right hand side becomesd
dr1
1‚àír
=1
(1‚àír)2.
‚ñ°
practice exercise 1.2 . compute the infinite sump‚àû
k=1k¬∑1
3k.
1this result can be found in tom apostol, mathematical analysis , 2nd edition, theorem 8.11.
5chapter 1. mathematical background
solution . we can use the derivative result:
‚àûx
k=1k¬∑1
3k= 1¬∑1
3+ 2¬∑1
9+ 3¬∑1
27+¬∑¬∑¬∑
=1
3¬∑
1 + 2¬∑1
3+ 3¬∑1
9+¬∑¬∑¬∑
=1
3¬∑1
(1‚àí1
3)2=1
3¬∑1
4
9=3
4.
1.1.2 binomial series
a geometric series is useful when handling situations such as n‚àí1 failures followed by
a success. however, we can easily twist the problem by asking: what is the probability
of getting one head out of 3 independent coin tosses? in this case, the probability can be
determined by enumerating all possible cases:
p[1 head in 3 coins] = p[h,t,t] + p[t,h,t] + p[t,t,h]
=1
2√ó1
2√ó1
2
+1
2√ó1
2√ó1
2
+1
2√ó1
2√ó1
2
=3
8.
figure 1.3 illustrates the situation.
figure 1.3: when flipping three coins independently, the probability of getting exactly one head can
come from three different possibilities.
what lessons have we learned in this example? notice that you need to enumerate
all possible combinations of one head and two tails to solve this problem. the number is
3 in our example. in general, the number of combinations can be systematically studied
using combinatorics , which we will discuss later in the chapter. however, the number of
combinations motivates us to discuss another background technique known as the binomial
series. the binomial series is instrumental in algebra when handling polynomials such as
(a+b)2or (1 + x)3. it provides a valuable formula when computing these powers.
theorem 1.2 (binomial theorem ).for any real numbers aandb, the binomial series
of power nis
(a+b)n=nx
k=0n
k
an‚àíkbk, (1.4)
where n
k
=n!
k!(n‚àík)!.
thebinomial theorem is valid for any real numbers aandb. the quantity n
k
reads
as ‚Äúnchoose k‚Äù. its definition is
n
k
def=n!
k!(n‚àík)!,
61.1. infinite series
where n! =n(n‚àí1)(n‚àí2)¬∑¬∑¬∑3¬∑2¬∑1. we shall discuss the physical meaning of n
k
in
section 1.5. but we can quickly plug in the ‚Äú nchoose k‚Äù into the coin flipping example by
letting n= 3 and k= 1:
number of combinations for 1 head and 2 tails =3
1
=3!
1!2!= 3.
so you can see why we want you to spend your precious time learning about the binomial
theorem. in matlab and python, n
k
can be computed using the commands as follows.
% matlab code to compute (n choose k) and k!
n = 10;
k = 2;
nchoosek(n,k)
factorial(k)
# python code to compute (n choose k) and k!
from scipy.special import comb, factorial
n = 10
k = 2
comb(n, k)
factorial(k)
the binomial theorem makes the most sense when we also learn about the pascal‚Äôs
identity .
theorem 1.3 (pascal‚Äôs identity ).letnandkbe positive integers such that k‚â§n.
then,n
k
+n
k‚àí1
=n+ 1
k
. (1.5)
proof . we start by recalling the definition of n
k
. this gives us
n
k
+n
k‚àí1
=n!
k!(n‚àík)!+n!
(k‚àí1)!(n‚àí(k‚àí1))!
=n!1
k!(n‚àík)!+1
(k‚àí1)!(n‚àík+ 1)!
,
where we factor out n! to obtain the second equation. next, we observe that
1
k!(n‚àík)!√ó(n‚àík+ 1)
(n‚àík+ 1)=n‚àík+ 1
k!(n‚àík+ 1)!,
1
(k‚àí1)!(n‚àík+ 1)!√ók
k=k
k!(n‚àík+ 1)!.
7chapter 1. mathematical background
substituting into the previous equation we obtain
n
k
+n
k‚àí1
=n!n‚àík+ 1
k!(n‚àík+ 1)!+k
k!(n‚àík+ 1)!
=n!n+ 1
k!(n‚àík+ 1)!
=(n+ 1)!
k!(n+ 1‚àík)!
=n+ 1
k
.
‚ñ°
the pascal triangle is a visualization of the coefficients of ( a+b)nas shown in fig-
ure 1.4 . for example, when n= 5, we know that 5
3
= 10. however, by pascal‚Äôs identity, we
know that 5
3
= 4
2
+ 4
3
. so the number 10 is actually obtained by summing the numbers
4 and 6 of the previous row.
figure 1.4: pascal triangle for n= 0, . . . , 5. note that a number in one row is obtained by summing
two numbers directly above it.
practice exercise 1.3 . find (1 + x)3.
solution . using the binomial theorem, we can show that
(1 +x)3=nx
k=03
k
13‚àíkxk
= 1 + 3 x+ 3x2+x3.
practice exercise 1.4 . let 0 < p < 1. find
nx
k=0n
k
pn‚àík(1‚àíp)k.
81.1. infinite series
solution . by using the binomial theorem, we have
nx
k=0n
k
pn‚àík(1‚àíp)k= (p+ (1‚àíp))n= 1.
this result will be helpful when evaluating binomial random variables in chapter 3.
we now prove the binomial theorem. please feel free to skip the proof if this is your first
time reading the book.
proof of the binomial theorem . we prove by induction. when n= 1,
(a+b)1=a+b
=1x
k=0a1‚àíkbk.
therefore, the base case is verified. assume up to case n. we need to verify case n+ 1.
(a+b)n+1= (a+b)(a+b)n
= (a+b)nx
k=0n
k
an‚àíkbk
=nx
k=0n
k
an‚àík+1bk+nx
k=0n
k
an‚àíkbk+1.
we want to apply the pascal‚Äôs identity to combine the two terms. in order to do so, we note
that the second term in this sum can be rewritten as
nx
k=0n
k
an‚àíkbk+1=nx
k=0n
k
an+1‚àík‚àí1bk+1
=n+1x
‚Ñì=1n
‚Ñì‚àí1
an+1‚àí‚Ñìb‚Ñì, where ‚Ñì=k+ 1
=nx
‚Ñì=1n
‚Ñì‚àí1
an+1‚àí‚Ñìb‚Ñì+bn+1.
the first term in the sum can be written as
nx
k=0n
k
an‚àík+1bk=nx
‚Ñì=1n
‚Ñì
an+1‚àí‚Ñìb‚Ñì+an+1, where ‚Ñì=k.
therefore, the two terms can be combined using pascal‚Äôs identity to yield
(a+b)n+1=nx
‚Ñì=1n
‚Ñì
+n
‚Ñì‚àí1
an+1‚àí‚Ñìb‚Ñì+an+1+bn+1
=nx
‚Ñì=1n+ 1
‚Ñì
an+1‚àí‚Ñìb‚Ñì+an+1+bn+1=n+1x
‚Ñì=0n+ 1
‚Ñì
an+1‚àí‚Ñìb‚Ñì.
9chapter 1. mathematical background
hence, the ( n+ 1)th case is also verified. by the principle of mathematical induction, we
have completed the proof.
‚ñ°
the end of the proof. please join us again.
1.2 approximation
consider a function f(x) = log(1 + x), for x >0 as shown in figure 1.5 . this is a nonlinear
function, and we all know that nonlinear functions are not fun to deal with. for example,
if you want to integrate the functionrb
axlog(1 + x)dx, then the logarithm will force you
to do integration by parts. however, in many practical problems, you may not need the full
range of x >0. suppose that you are only interested in values x‚â™1. then the logarithm
can be approximated, and thus the integral can also be approximated.
0 1 2 3 4 500.511.52
0 0.05 0.1 0.15 0.200.050.10.150.2
figure 1.5: the function f(x) = log(1 + x)and the approximation bf(x) =x.
to see how this is even possible, we show in figure 1.5 the nonlinear function f(x) =
log(1 + x) and an approximation bf(x) =x. the approximation is carefully chosen such that
forx‚â™1, the approximation bf(x) is close to the true function f(x). therefore, we can
argue that for x‚â™1,
log(1 + x)‚âàx, (1.6)
thereby simplifying the calculation. for example, if you want to integrate xlog(1 + x) for
0< x < 0.1, then the integral can be approximated byr0.1
0xlog(1 + x)dx‚âàr0.1
0x2dx=
x3
3= 3.33√ó10‚àí4. (the actual integral is 3 .21√ó10‚àí4.) in this section we will learn about
the basic approximation techniques. we will use them when we discuss limit theorems in
chapter 6, as well as various distributions, such as from binomial to poisson.
101.2. approximation
1.2.1 taylor approximation
given a function f:r‚Üír, it is often useful to analyze its behavior by approximating f
using its local information. taylor approximation (or taylor series) is one of the tools for
such a task. we will use the taylor approximation on many occasions.
definition 1.2 (taylor approximation ).letf:r‚Üírbe a continuous function with
infinite derivatives. let a‚ààrbe a fixed constant. the taylor approximation of fat
x=ais
f(x) =f(a) +f‚Ä≤(a)(x‚àía) +f‚Ä≤‚Ä≤(a)
2!(x‚àía)2+¬∑¬∑¬∑
=‚àûx
n=0f(n)(a)
n!(x‚àía)n, (1.7)
where f(n)denotes the nth-order derivative of f.
taylor approximation is a geometry-based approximation. it approximates the function
according to the offset, slope, curvature, and so on. according to definition 1.2, the taylor
series has an infinite number of terms. if we use a finite number of terms, we obtain the
nth-order taylor approximation:
first-order : f(x) =f(a)|{z}
offset+f‚Ä≤(a)(x‚àía)|{z}
slope+o((x‚àía)2)
second-order : f(x) =f(a)|{z}
offset+f‚Ä≤(a)(x‚àía)|{z}
slope+f‚Ä≤‚Ä≤(a)
2!(x‚àía)2
|{z }
curvature+o((x‚àía)3).
here, the big-o notation o(Œµk) means any term that has an order at least power k. for
small Œµ, i.e., Œµ‚â™1, a high-order term o(Œµk)‚âà0 for large k.
example 1.1 . let f(x) = sin x. then the taylor approximation at x= 0 is
f(x)‚âàf(0) + f‚Ä≤(0)(x‚àí0) +f‚Ä≤‚Ä≤(0)
2!(x‚àí0)2+f‚Ä≤‚Ä≤‚Ä≤(0)
3!(x‚àí0)3
= sin(0) + (cos 0)( x‚àí0)‚àísin(0)
2!(x‚àí0)2‚àícos(0)
3!(x‚àí0)3
= 0 + x‚àí0‚àíx3
6=x‚àíx3
6.
we can expand further to higher orders, which yields
f(x) =x‚àíx3
3!+x5
5!‚àíx7
7!+¬∑¬∑¬∑
we show the first few approximations in figure 1.6 .
one should be reminded that taylor approximation approximates a function f(x)
at a particular point x=a. therefore, the approximation of fnear x= 0 and the
11chapter 1. mathematical background
approximation of fnearx=œÄ/2 are different. for example, the taylor approximation
atx=œÄ/2 for f(x) = sin xis
f(x) = sinœÄ
2+ cosœÄ
2
x‚àíœÄ
2
‚àísinœÄ
2
2!
x‚àíœÄ
22
‚àícosœÄ
2
3!
x‚àíœÄ
23
= 1 + 0 ‚àí1
4
x‚àíœÄ
22
‚àí0 = 1‚àí1
4
x‚àíœÄ
22
.
-10 -5 0 5 10
x-4-2024
sin x
3rd order
5th order
7th order
-10 -5 0 5 10
x-4-2024
sin x
3rd order
5th order
7th order
(a) approximate at x= 0 (b) approximate at x=œÄ/2
figure 1.6: taylor approximation of the function f(x) = sin x.
1.2.2 exponential series
an immediate application of the taylor approximation is to derive the exponential series .
theorem 1.4. letxbe any real number. then,
ex= 1 + x+x2
2+x3
3!+¬∑¬∑¬∑=‚àûx
k=0xk
k!. (1.8)
proof . let f(x) =exfor any x. then, the taylor approximation around x= 0 is
f(x) =f(0) + f‚Ä≤(0)(x‚àí0) +f‚Ä≤‚Ä≤(0)
2!(x‚àí0)2+¬∑¬∑¬∑
=e0+e0(x‚àí0) +e0
2!(x‚àí0)2+¬∑¬∑¬∑
= 1 + x+x2
2+¬∑¬∑¬∑=‚àûx
k=0xk
k!.
‚ñ°
practice exercise 1.5 . evaluate‚àûx
k=0Œªke‚àíŒª
k!.
121.2. approximation
solution .‚àûx
k=0Œªke‚àíŒª
k!=e‚àíŒª‚àûx
k=0Œªk
k!=e‚àíŒªeŒª= 1.
this result will be useful for poisson random variables in chapter 3.
if we substitute x=jŒ∏where j=‚àö‚àí1, then we can show that
ejŒ∏
|{z}
=cos Œ∏+jsinŒ∏= 1 + jŒ∏+(jŒ∏)2
2!+¬∑¬∑¬∑
=
1‚àíŒ∏2
2!+Œ∏4
4!+¬∑¬∑¬∑
| {z }
real+j
Œ∏‚àíŒ∏3
3!+¬∑¬∑¬∑
| {z }
imaginary
matching the real and the imaginary terms, we can show that
cosŒ∏= 1‚àíŒ∏2
2!+Œ∏4
4!+¬∑¬∑¬∑
sinŒ∏=Œ∏‚àíŒ∏3
3!+Œ∏5
5!+¬∑¬∑¬∑
this gives the infinite series representations of the two trigonometric functions.
1.2.3 logarithmic approximation
taylor approximation also allows us to find approximations to logarithmic functions. we
start by presenting a lemma.
lemma 1.1. let0< x < 1be a constant. then,
log(1 + x) =x‚àíx2+o(x3). (1.9)
proof . let f(x) = log(1 + x). then, the derivatives of fare
f‚Ä≤(x) =1
(1 +x),and f‚Ä≤‚Ä≤(x) =‚àí1
(1 +x)2.
taylor approximation at x= 0 gives
f(x) =f(0) + f‚Ä≤(0)(x‚àí0) +f‚Ä≤‚Ä≤(0)
2(x‚àí0)2+o(x3)
= log 1 +1
(1 + 0)
x‚àí1
(1 + 0)2
x2+o(x3)
=x‚àíx2+o(x3).
‚ñ°
the difference between this result and the result we showed in the beginning of this
section is the order of polynomials we used to approximate the logarithm:
13chapter 1. mathematical background
¬àfirst-order: log(1 + x) =x
¬àsecond-order: log(1 + x) =x‚àíx2.
what order of approximation is good? it depends on where you want the approximation to
be good, and how faryou want the approximation to go. the difference between first-order
and second-order approximations is shown in figure 1.7 .
0 1 2 3 4 500.511.52
0 1 2 3 4 500.511.52
first-order approximation second-order approximation
figure 1.7: the function f(x) = log(1 + x), the first-order approximation bf(x) =x, and the second-
order approximation bf(x) =x‚àíx2.
example 1.2 . when we prove the central limit theorem in chapter 6, we need to
use the following result.
lim
n‚Üí‚àû
1 +s2
2nn
=es2/2.
the proof of this equation can be done using the taylor approximation. consider
nlog
1 +s2
n
. by the logarithmic lemma, we can obtain the second-order approxi-
mation:
log
1 +s2
2n
=s2
2n‚àís4
4n2.
therefore, multiplying both sides by nyields
nlog
1 +s2
2n
=s2
2‚àís4
4n.
putting the limit n‚Üí ‚àû we can show that
lim
n‚Üí‚àû
nlog
1 +s2
2n
=s2
2.
taking exponential on both sides yields
exp
lim
n‚Üí‚àûnlog
1 +s2
2n
= exps2
2
.
moving the limit outside the exponential yields the result. figure 1.8 provides a pic-
torial illustration.
141.3. integration
0 0.2 0.4 0.6 0.8 111.21.41.61.8
figure 1.8: we plot a sequence of function fn(x) =
1 +s2
2nn
and its limit f(x) =es2/2.
1.3 integration
when you learned calculus, your teacher probably told you that there are two ways to
compute an integral:
¬àsubstitution : z
f(ax)dx=1
az
f(u)du.
¬àby parts : z
u dv =u v‚àíz
v du.
besides these two, we want to teach you two more. the first technique is even and odd
functions when integrating a function symmetrically about the y-axis. if a function is even,
you just need to integrate half of the function. if a function is odd, you will get a zero. the
second technique is to leverage the fact that a probability density function integrates to 1.
we will discuss the first technique here and defer the second technique to chapter 4.
besides the two integration techniques, we will review the fundamental theorem of
calculus. we will need it when we study cumulative distribution functions in chapter 4.
1.3.1 odd and even functions
definition 1.3. a function f:r‚Üíriseven if for any x‚ààr,
f(x) =f(‚àíx), (1.10)
andfisoddif
f(x) =‚àíf(‚àíx). (1.11)
15chapter 1. mathematical background
essentially, an even function flips over about the y-axis, whereas an odd function flips over
both the x- and y-axes.
example 1.3 . the function f(x) =x2‚àí0.4x4is even, because
f(‚àíx) = (‚àíx)2‚àí0.4(‚àíx)4=x2‚àí0.4x4=f(x).
seefigure 1.9 (a) for illustration. when integrating the function, we have
z1
‚àí1f(x)dx= 2z1
0f(x)dx= 2z1
0x2‚àí0.44dx= 2x3
3‚àí0.4
5x5x=1
x=0=38
75.
example 1.4 . the function f(x) =xexp(‚àíx2/2) is odd, because
f(‚àíx) = (‚àíx) exp
‚àí(‚àíx)2
2
=‚àíxexp
‚àíx2
2
=‚àíf(x).
seefigure 1.9 (b) for illustration. when integrating the function, we can let u=‚àíx.
then, the integral becomes
z1
‚àí1f(x)dx=z0
‚àí1f(x)dx+z1
0f(x)dx
=z1
0f(‚àíu)du+z1
0f(x)dx
=‚àíz1
0f(u)du+z1
0f(x)dx= 0.
-1.5 -1 -0.5 0 0.5 1 1.5
x-1-0.500.51
-1.5 -1 -0.5 0 0.5 1 1.5
x-1-0.500.51
(a) even function (b) odd function
figure 1.9: an even function is symmetric about the y-axis, and so the integrationra
‚àíaf(x)dx=
2ra
0f(x)dx. an odd function is anti-symmetric about the y-axis. thus,ra
‚àíaf(x)dx= 0.
161.3. integration
1.3.2 fundamental theorem of calculus
our following result is the fundamental theorem of calculus . it is a handy tool that links
integration and differentiation.
theorem 1.5 (fundamental theorem of calculus ).letf: [a, b]‚Üírbe a continu-
ous function defined on a closed interval [a, b]. then, for any x‚àà(a, b),
f(x) =d
dxzx
af(t)dt, (1.12)
before we prove the result, let us understand the theorem if you have forgotten its meaning.
example 1.5 . consider a function f(t) =t2. if we integrate the function from 0 to
x, we will obtain another function
f(x)def=zx
0f(t)dt=zx
0t2dt=x3
3.
on the other hand, we can differentiate f(x) to obtain f(x):
f(x) =d
dxf(x) =d
dxx3
3=x2.
the fundamental theorem of calculus basically puts the two together:
f(x) =d
dxzx
0f(t)dt.
that‚Äôs it. nothing more and nothing less.
how can the fundamental theorem of calculus ever be useful when studying probabil-
ity? very soon you will learn two concepts: probability density function andcumulative
distribution function . these two functions are related to each other by the fundamental
theorem of calculus. to give you a concrete example, we write down the probability density
function of an exponential random variable. (please do not panic about the exponential
random variable. just think of it as a ‚Äúrapidly decaying‚Äù function.)
f(x) =e‚àíx, x‚â•0.
it turns out that the cumulative distribution function is
f(x) =zx
0f(t)dt=zx
0e‚àítdt= 1‚àíe‚àíx.
you can also check that f(x) =d
dxf(x). the fundamental theorem of calculus says that if
you tell me f(x) =rx
0e‚àítdt(for whatever reason), i will be able to tell you that f(x) =e‚àíx
merely by visually inspecting the integrand without doing the differentiation.
figure 1.10 illustrates the pair of functions f(x) =e‚àíxandf(x) = 1‚àíe‚àíx. one thing
you should notice is that the height off(x) is the area under the curve of f(t) from ‚àí‚àûtox.
for example, in figure 1.10 we show the area under the curve from 0 to 2. correspondingly
inf(x), the height is f(2).
17chapter 1. mathematical background
0 1 2 3 4 500.20.40.60.81
0 1 2 3 4 500.20.40.60.81
f(x) f(x)
figure 1.10: the pair of functions f(x) =e‚àíxandf(x) = 1‚àíe‚àíx
the following proof of the fundamental theorem of calculus can be skipped if it is your
first time reading the book.
proof . our proof is based on stewart (6th edition), section 5.3. define the integral as a
function f:
f(x) =zx
af(t)dt.
the derivative of fwith respect to xis
d
dxf(x) = lim
h‚Üí0f(x+h)‚àíf(x)
h
= lim
h‚Üí01
h zx+h
af(t)dt‚àízx
af(t)dt!
= lim
h‚Üí01
hzx+h
xf(t)dt
(a)
‚â§lim
h‚Üí01
hzx+h
x
max
x‚â§œÑ‚â§x+hf(œÑ)
dt
= lim
h‚Üí0
max
x‚â§œÑ‚â§x+hf(œÑ)
.
here, the inequality in ( a) holds because
f(t)‚â§max
x‚â§œÑ‚â§x+hf(œÑ)
for all x‚â§t‚â§x+h. the maximum exists because fis continuous in a closed interval.
181.3. integration
using the parallel argument, we can show that
d
dxf(x) = lim
h‚Üí0f(x+h)‚àíf(x)
h
= lim
h‚Üí01
h zx+h
af(t)dt‚àízx
af(t)dt!
= lim
h‚Üí01
hzx+h
xf(t)dt
‚â•lim
h‚Üí01
hzx+h
x
min
x‚â§œÑ‚â§x+hf(œÑ)
dt
= lim
h‚Üí0
min
x‚â§œÑ‚â§x+hf(œÑ)
.
combining the two results, we have that
lim
h‚Üí0
min
x‚â§œÑ‚â§x+hf(œÑ)
‚â§d
dxf(x)‚â§lim
h‚Üí0
max
x‚â§œÑ‚â§x+hf(œÑ)
.
however, since the two limits are both converging to f(x) as h‚Üí0, we conclude that
d
dxf(x) =f(x).
‚ñ°
remark . an alternative proof is to use mean value theorem in terms of riemann-stieltjes
integrals (see, e.g., tom apostol, mathematical analysis , 2nd edition, theorem 7.34). to
handle more general functions such as delta functions, one can use techniques in lebesgue‚Äôs
integration. however, this is beyond the scope of this book.
this is the end of the proof. please join us again.
in many practical problems, the fundamental theorem of calculus needs to be used in
conjunction with the chain rule .
corollary 1.3. letf: [a, b]‚Üírbe a continuous function defined on a closed interval
[a, b]. let g:r‚Üí[a, b]be a continuously differentiable function. then, for any x‚àà
(a, b),
d
dxzg(x)
af(t)dt=g‚Ä≤(x)¬∑f(g(x)). (1.13)
proof . we can prove this with the chain rule: let y=g(x). then we have
d
dxzg(x)
af(t)dt=dy
dx¬∑d
dyzy
af(t)dt=g‚Ä≤(x)f(y),
which completes the proof.
‚ñ°
19chapter 1. mathematical background
practice exercise 1.6 . evaluate the integral
d
dxzx‚àí¬µ
01‚àö
2œÄœÉ2exp
‚àít2
2œÉ2
dt.
solution . let y=x‚àí¬µ. then by using the fundamental theorem of calculus, we can
show that
d
dxzx‚àí¬µ
01‚àö
2œÄœÉ2exp
‚àít2
2œÉ2
dt=dy
dx¬∑d
dyzy
01‚àö
2œÄœÉ2exp
‚àít2
2œÉ2
dt
=d(x‚àí¬µ)
dx¬∑1‚àö
2œÄœÉ2exp
‚àíy2
2œÉ2
=1‚àö
2œÄœÉ2exp
‚àí(x‚àí¬µ)2
2œÉ2
.
this result will be useful when we do linear transformations of a gaussian random
variable in chapter 4.
1.4 linear algebra
the two most important subjects for data science are probability , which is the subject of the
book you are reading, and linear algebra , which concerns matrices and vectors. we cannot
cover linear algebra in detail because this would require another book. however, we need to
highlight some ideas that are important for doing data analysis.
1.4.1 why do we need linear algebra in data science?
consider a dataset of the crime rate of several cities as shown below, downloaded from
https://web.stanford.edu/ ~hastie/statlearnsparsity/data.html .
the table shows that the crime rate depends on several factors such as funding for the
police department, the percentage of high school graduates, etc.
city crime rate funding hs no-hs college college4
1 478 40 74 11 31 20
2 494 32 72 11 43 18
3 643 57 71 18 16 16
4 341 31 71 11 25 19
.....................
50 940 66 67 26 18 16
201.4. linear algebra
what questions can we ask about this table? we can ask: what is the most influential
cause of the crime rate? what are the leading contributions to the crime rate? to answer
these questions, we need to describe these numbers. one way to do it is to put the numbers
in matrices and vectors. for example,
ycrime =Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞478
494
...
940Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,xfund=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞40
32
...
66Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,xhs=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞74
72
...
67Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª, . . .
with this vector expression of the data, the analysis questions can roughly be translated
to finding Œ≤‚Äôs in the following equation:
ycrime =Œ≤fundxfund+Œ≤hsxhs+¬∑¬∑¬∑+Œ≤college4 xcollege4 .
this equation offers a lot of useful insights. first, it is a linear model ofycrime. we call
it a linear model because the observable ycrime is written as a linear combination of the
variables xfund,xhs, etc. the linear model assumes that the variables are scaled and added
to generate the observed phenomena. this assumption is not always realistic, but it is often
a fair assumption that greatly simplifies the problem. for example, if we can show that all
Œ≤‚Äôs are zero except Œ≤fund, then we can conclude that the crime rate is solely dependent on
the police funding. if two variables are correlated, e.g., high school graduate and college
graduate, we would expect the Œ≤‚Äôs to change simultaneously.
the linear model can further be simplified to a matrix-vector equation:
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞|
|
ycrime
|
|Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞| | |
| | |
xfund xhs¬∑¬∑¬∑xcollege4
| | |
| | |Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ªÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£∞Œ≤fund
Œ≤hs
...
Œ≤college4Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
here, the lines ‚Äú |‚Äù emphasize that the vectors are column vectors. if we denote the matrix
in the middle as aand the vector as Œ≤, then the equation is equivalent to y=aŒ≤. so we
can find Œ≤by appropriately inverting the matrix a. if two columns of aare dependent, we
will not be able to resolve the corresponding Œ≤‚Äôs uniquely.
as you can see from the above data analysis problem, matrices and vectors offer a way
to describe the data. we will discuss the calculations in chapter 7. however, to understand
how to interpret the results from the matrix-vector equations, we need to review some basic
ideas about matrices and vectors.
1.4.2 everything you need to know about linear algebra
throughout this book, you will see different sets of notations. for linear algebra, we also
have a set of notations. we denote x‚ààrdad-dimensional vector taking real numbers as its
entries. an m-by-nmatrix is denoted as x‚ààrm√ón. the transpose of a matrix is denoted
asxt. a matrix xcan be viewed according to its columns and its rows:
x=Ô£Æ
Ô£∞| | |
x1x2¬∑¬∑¬∑xn
| | |Ô£π
Ô£ª,and x=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞‚Äîx1‚Äî
‚Äîx2‚Äî
...
‚Äîxm‚ÄîÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª.
21chapter 1. mathematical background
here, xjdenotes the jth column of x, andxidenotes the ith row of x. the ( i, j)th element
ofxis denoted as xijor [x]ij. the identity matrix is denoted as i. the ith column of i
is denoted as ei= [0, . . . , 1, . . . , 0]t, and is called the ithstandard basis vector . an all-zero
vector is denoted as 0= [0, . . . , 0]t.
what is the most important thing to know about linear algebra? from a data analysis
point of view, figure 1.11 gives us the answer. the picture is straightforward, but it captures
all the essence. in almost all the data analysis problems, ultimately, there are three things we
care about: (i) the observable vector y, (ii) the variable vectors xn, and (iii) the coefficients
Œ≤n. the set of variable vectors {xn}n
n=1spans a vector space in which all vectors are living.
some of these variable vectors are correlated, and some are not. however, for the sake of
this discussion, let us assume they are independent of each other. then for any observable
vector y, we can always project yin the directions determined by {xn}n
n=1. the projection
ofyontoxnis the coefficient Œ≤n. a larger value of Œ≤nmeans that the variable xnhas more
contributions.
figure 1.11: representing an observable vector yby a linear combination of variable vectors x1,x2
andx3. the combination weights are Œ≤1, Œ≤2, Œ≤3.
why is this picture so important? because most of the data analysis problems can be
expressed, or approximately expressed, by the picture:
y=nx
n=1Œ≤nxn.
if you recall the crime rate example, this equation is precisely the linear model we used to
describe the crime rate. this equation can also describe many other problems.
example 1.6 .polynomial fitting . consider a dataset of pairs of numbers ( tm, ym) for
m= 1, . . . , m , as shown in figure 1.12 . after a visual inspection of the dataset, we
propose to use a line to fit the data. a line is specified by the equation
ym=atm+b, m = 1, . . . , m,
where a‚ààris the slope and b‚ààris the y-intercept. the goal of this problem is to
find one line (which is fully characterized by ( a, b)) such that it has the best fit to all
the data pairs ( tm, ym) for m= 1, . . . , m . this problem can be described in matrices
221.4. linear algebra
and vectors by noting that
Ô£Æ
Ô£ØÔ£∞y1
...
ymÔ£π
Ô£∫Ô£ª
|{z}
y=a|{z}
Œ≤1Ô£Æ
Ô£ØÔ£∞t1
...
tmÔ£π
Ô£∫Ô£ª
|{z}
x1+b|{z}
Œ≤2Ô£Æ
Ô£ØÔ£∞1
...
1Ô£π
Ô£∫Ô£ª
|{z}
x2,
or more compactly,
y=Œ≤1x1+Œ≤2x2.
here, x1= [t1, . . . , t m]tcontains all the variable values, and x2= [1, . . . , 1]tcontains
a constant offset.
tm ym
0.1622 2.1227
0.7943 3.3354
......
0.7379 3.4054
0.2691 2.5672
0.4228 2.3796
0.6020 3.2942
0 0.2 0.4 0.6 0.8 112345
data
best fit
candidate
figure 1.12: example of fitting a set of data points. the problem can be described by y=
Œ≤1x1+Œ≤2x2.
example 1.7 .image compression . the jpeg compression for images is based on
the concept of discrete cosine transform (dct). the dct consists of a set of basis
vectors , or{xn}n
n=1using our notation. in the most standard setting, each basis vector
xnconsists of 8 √ó8 pixels, and there are n= 64 of these xn‚Äôs. given an image, we can
partition the image into msmall blocks of 8 √ó8 pixels. let us call one of these blocks
y. then, dct represents the observation yas a linear combination of the dct basis
vectors:
y=nx
n=1Œ≤nxn.
the coefficients {Œ≤n}n
n=1are called the dct coefficients. they provide a representa-
tionofy, because once we know {Œ≤n}n
n=1, we can completely describe ybecause the
basis vectors {xn}n
n=1are known and fixed. the situation is depicted in figure 1.13 .
how can we compress images using dct? in the 1970s, scientists found that most
images have strong leading dct coefficients but weak tail dct coefficients. in other
words, among the n= 64 Œ≤n‚Äôs, only the first few are important. if we truncate the
number of dct coefficients, we can effectively compress the number of bits required
to represent the image.
23chapter 1. mathematical background
figure 1.13: jpeg image compression is based on the concept of discrete cosine transform, which
can be formulated as a matrix-vector problem.
we hope by now you are convinced of the importance of matrices and vectors in the
context of data science. they are not ‚Äúyet another‚Äù subject but an essential tool you must
know how to use. so, what are the technical materials you must master? here we go.
1.4.3 inner products and norms
we assume that you know the basic operations such as matrix-vector multiplication, taking
the transpose, etc. if you have forgotten these, please consult any undergraduate linear
algebra textbook such as gilbert strang‚Äôs linear algebra and its applications . we will
highlight a few of the most important operations for our purposes.
definition 1.4 (inner product ).letx= [x1, . . . , x n]t, and y= [y1, . . . , y n]t. the
inner product xtyis
xty=nx
i=1xiyi. (1.14)
practice exercise 1.7 . letx= [1,0,‚àí1]t, and y= [3,2,0]t. find xty.
solution . the inner product is xty= (1)(3) + (0)(2) + ( ‚àí1)(0) = 3.
inner products are important because they tell us how two vectors are correlated.
figure 1.14 depicts the geometric meaning of an inner product. if two vectors are correlated
(i.e., nearly parallel), then the inner product will give us a large value. conversely, if the
two vectors are close to perpendicular, then the inner product will be small. therefore, the
inner product provides a measure of the closeness/similarity between two vectors.
figure 1.14: geometric interpretation of inner product: we project one vector onto the other vector.
the projected distance is the inner product.
241.4. linear algebra
creating vectors and computing the inner products are straightforward in matlab.
we simply need to define the column vectors xandyby using the command []with ;to
denote the next row. the inner product is done using the transpose operation x‚Äôand vector
multiplication *.
% matlab code to perform an inner product
x = [1 0 -1];
y = [3 2 0];
z = x‚Äô*y;
in python, constructing a vector is done using the command np.array . inside this
command, one needs to enter the array. for a column vector, we write [[1],[2],[3]] , with
an outer [], and three inner []for each entry. if the vector is a row vector, the one can omit
the inner []‚Äôs by just calling np.array([1, 2, 3]) . given two column vectors xand y,
the inner product is computed via np.dot(x.t,y) , where np.dot is the command for inner
product, and x.treturns the transpose of x. one can also call np.transpose(x) , which is
the same as x.t.
# python code to perform an inner product
import numpy as np
x = np.array([[1],[0],[-1]])
y = np.array([[3],[2],[0]])
z = np.dot(np.transpose(x),y)
print(z)
in data analytics, the inner product of two vectors can be useful. consider the vectors
intable 1.1 . just from looking at the numbers, you probably will not see anything wrong.
however, let‚Äôs compute the inner products. it turns out that xt
1x2=‚àí0.0031, whereas
xt
1x3= 2.0020. there is almost no correlation between x1andx2, but there is a substan-
tial correlation between x1andx3. what happened? the vectors x1andx2are random
vectors constructed independently and uncorrelated to each other. the last vector x3was
constructed by x3= 2x1‚àíœÄ/1000. since x3is completely constructed from x1, they have
to be correlated.
x1 x2 x3
0.0006 ‚àí0.0011 ‚àí0.0020
‚àí0.0014 ‚àí0.0024 ‚àí0.0059
‚àí0.0034 0 .0073 ‚àí0.0099
.........
0.0001 ‚àí0.0066 ‚àí0.0030
0.0074 0 .0046 0 .0116
0.0007 ‚àí0.0061 ‚àí0.0017
table 1.1: three example vectors.
one caveat for this example is that the naive inner product xt
ixjis scale-dependent.
for example, the vectors x3=x1andx3= 1000 x1have the same amount of correlation,
25chapter 1. mathematical background
but the simple inner product will give a larger value for the latter case. to solve this problem
we first define the norm of the vectors:
definition 1.5 (norm ).letx= [x1, . . . , x n]tbe a vector. the ‚Ñìp-norm of xis
‚à•x‚à•p= nx
i=1xp
i!1/p
, (1.15)
for any p‚â•1.
the norm essentially tells us the length of the vector. this is most obvious if we consider
the‚Ñì2-norm:
‚à•x‚à•2= nx
i=1x2
i!1/2
.
by taking the square on both sides, one can show that ‚à•x‚à•2
2=xtx. this is called the
squared ‚Ñì2-norm , and is the sum of the squares.
on matlab, computing the norm is done using the command norm. here, we can
indicate the types of norms, e.g., norm(x,1) returns the ‚Ñì1-norm whereas norm(x,2) returns
the‚Ñì2-norm (which is also the default).
% matlab code to compute the norm
x = [1 0 -1];
x_norm = norm(x);
on python, the norm command is listed in the np.linalg . to call the ‚Ñì1-norm, we use
np.linalg.norm(x,1) , and by default the ‚Ñì2-norm is np.linalg.norm(x) .
# python code to compute the norm
import numpy as np
x = np.array([[1],[0],[-1]])
x_norm = np.linalg.norm(x)
using the norm, one can define an angle called the cosine angle between two vectors.
definition 1.6. thecosine angle between two vectors xandyis
cosŒ∏=xty
‚à•x‚à•2‚à•y‚à•2. (1.16)
the difference between the cosine angle and the basic inner product is the normaliza-
tion in the denominator, which is the product ‚à•x‚à•2‚à•y‚à•2. this normalization factor scales
the vector xtox/‚à•x‚à•2andytoy/‚à•y‚à•2. the scaling makes the length of the new vector
equal to unity, but it does not change the vector‚Äôs orientation. therefore, the cosine angle
is not affected by a very long vector or a very short vector. only the angle matters. see
figure 1.15 .
261.4. linear algebra
figure 1.15: the cosine angle is the inner product divided by the norms of the vectors.
going back to the previous example, after normalization we can show that the cosine
angle between x1andx2is cos Œ∏1,2=‚àí0.0031, whereas the cosine angle between x1and
x3is cos Œ∏1,3= 0.8958. there is still a strong correlation between x1andx3, but now using
the cosine angle the value is between ‚àí1 and +1.
remark 1 : there are other norms one can use. the ‚Ñì1-norm is useful for sparse models
where we want to have the fewest possible non-zeros. the ‚Ñì1-norm of xis
‚à•x‚à•1=nx
i=1|xi|,
which is the sum of absolute values. the ‚Ñì‚àû-norm picks the maximum of {x1, . . . , x n}:
‚à•x‚à•‚àû= lim
p‚Üí‚àû nx
i=1xp
i!1/p
= max {x1, . . . , x n},
because as p‚Üí ‚àû , only the largest element will be amplified.
remark 2 : the standard ‚Ñì2-norm is a circle: just consider x= [x1, x2]t. the norm
is‚à•x‚à•2=p
x2
1+x2
2. we can convert the circle to ellipses by considering a weighted norm.
definition 1.7 (weighted ‚Ñì2-norm square ).letx= [x1, . . . , x n]tand let w=
diag(w1, . . . , w n)be a non-negative diagonal matrix. the weighted ‚Ñì2-norm square of
xis
‚à•x‚à•2
w=xtwx
=x1. . . x nÔ£Æ
Ô£ØÔ£∞w1. . . 0
.........
0. . . w nÔ£π
Ô£∫Ô£ªÔ£Æ
Ô£ØÔ£∞x1
...
xnÔ£π
Ô£∫Ô£ª=nx
i=1wix2
i. (1.17)
the geometry of the weighted ‚Ñì2-norm is determined by the matrix w. for example,
ifw=i(the identity operator), then ‚à•x‚à•2
w=‚à•x‚à•2
2, which defines a circle. if wis any
‚Äúnon-negative‚Äù matrix2, then ‚à•x‚à•2
wdefines an ellipse.
2the technical term for these matrices is positive semi-definite matrices.
27chapter 1. mathematical background
in matlab, the weighted inner product is just a sequence of two matrix-vector mul-
tiplications. this can be done using the command x‚Äô*w*x as shown below.
% matlab code to compute the weighted norm
w = [1 2 3; 4 5 6; 7 8 9];
x = [2; -1; 1];
z = x‚Äô*w*x
in python, constructing the matrix wand the column vector xis done using np.array .
the matrix-vector multiplication is done using two np.dot commands: one for np.dot(w,x)
and the other one for np.dot(x.t, np.dot(w,x)) .
# python code to compute the weighted norm
import numpy as np
w = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
x = np.array([[2],[-1],[1]])
z = np.dot(x.t, np.dot(w,x))
print(z)
1.4.4 matrix calculus
the last linear algebra topic we need to review is matrix calculus. as its name indicates,
matrix calculus is about the differentiation of matrices and vectors. why do we need differ-
entiation for matrices and vectors? because we want to find the minimum or maximum of
a scalar function with a vector input.
let us go back to the crime rate problem we discussed earlier. given the data, we
want to find the model coefficients Œ≤1, . . . , Œ≤ nsuch that the variables can best explain the
observation. in other words, we want to minimize the deviation between yand the prediction
offered by our model:
minimize
Œ≤1,...,Œ≤ ny‚àínx
n=1Œ≤nxn2
.
this equation is self-explanatory. the norm ‚à•‚ô£ ‚àí ‚ô°‚à•2measures the deviation. if ycan
be perfectly explained by {xn}n
n=1, then the norm can eventually go to zero by finding a
good set of {Œ≤1, . . . , Œ≤ n}. the symbol minimize
Œ≤1,...,Œ≤ nmeans to minimize the function by finding
{Œ≤1, . . . , Œ≤ n}. note that the norm is taking a vector as the input and generating a scalar as
the output. it can be expressed as
Œµ(Œ≤)def=y‚àínx
n=1Œ≤nxn2
,
to emphasize this relationship. here we define Œ≤= [Œ≤1, . . . , Œ≤ n]tas the collection of all
coefficients.
given this setup, how would you determine Œ≤such that the deviation is minimized?
our calculus teachers told us that we could take the function‚Äôs derivative and set it to zero
281.4. linear algebra
for scalar problems. it is the same story for vectors. what we do is to take the derivative of
the error and set it equal to zero:
d
dŒ≤Œµ(Œ≤) = 0 .
now the question arises, how do we take the derivatives of Œµ(Œ≤) when it takes a vector as
input? if we can answer this question, we will find the best Œ≤. the answer is straightforward.
since the function has one output and many inputs, take the derivative for each element
independently. this is called the scalar differentiation of vectors .
definition 1.8 (scalar differentiation of vectors ).letf:rn‚Üírbe a differentiable
scalar function, and let y=f(x)for some input x‚ààrn. then,
dy
dx=Ô£Æ
Ô£ØÔ£∞dy/dx 1
...
dy/dx nÔ£π
Ô£∫Ô£ª.
as you can see from this definition, there is nothing conceptually challenging here. the only
difficulty is that things can get tedious because there will be many terms. however, the good
news is that mathematicians have already compiled a list of identities for common matrix
differentiation. so instead of deriving every equation from scratch, we can enjoy the fruit of
their hard work by referring to those formulae. the best place to find these equations is the
matrix cookbook by petersen and pedersen.3here, we will mention two of the most useful
results.
example 1.8 . let y=xtaxfor any matrix a‚ààrn√ón. finddy
dx.
solution .d
dx 
xtax
=ax+atx.
now, if ais symmetric, i.e., a=at, then
d
dx 
xtax
= 2ax.
example 1.9 . let Œµ=‚à•ax‚àíy‚à•2
2, where a‚ààrn√ónis symmetric. finddŒµ
dx.
solution . first, we note that
Œµ=‚à•ax‚àíy‚à•2
2=xtatax‚àí2ytax+yty.
3https://www.math.uwaterloo.ca/ ~hwolkowi/matrixcookbook.pdf
29chapter 1. mathematical background
taking the derivative with respect to xyields
dŒµ
dx= 2atax‚àí2aty
= 2at(ax‚àíy).
going back to the crime rate problem, we can now show that
0 =dŒµ
dŒ≤‚à•y‚àíxŒ≤‚à•2= 2xt(xŒ≤‚àíy).
therefore, the solution is
bŒ≤= (xtx)‚àí1xy.
as you can see, if we do not have access to the matrix calculus, we will not be able to solve the
minimization problem. (there are alternative paths that do not require matrix calculus, but
they require an understanding of linear subspaces and properties of the projection operators.
so in some sense, matrix calculus is the easiest way to solve the problem.) when we discuss
the linear regression methods in chapter 7, we will cover the interpretation of the inverses
and related topics.
in matlab and python, matrix inversion is done using the command invin mat-
lab and np.linalg.inv in python. below is an example in python.
# python code to compute a matrix inverse
import numpy as np
x = np.array([[1, 3], [-2, 7], [0, 1]])
xtx = np.dot(x.t, x)
xtxinv = np.linalg.inv(xtx)
print(xtxinv)
sometimes, instead of computing the matrix inverse we are more interested in solving a
linear equation xŒ≤=y(the solution of which is bŒ≤= (xtx)‚àí1xy). in both matlab and
python, there are built-in commands to do this. in matlab, the command is \(backslash).
% matlab code to solve x beta = y
x = [1 3; -2 7; 0 1];
y = [2; 1; 0];
beta = x\y;
in python, the built-in command is np.linalg.lstsq .
# python code to solve x beta = y
import numpy as np
x = np.array([[1, 3], [-2, 7], [0, 1]])
y = np.array([[2],[1],[0]])
beta = np.linalg.lstsq(x, y, rcond=none)[0]
print(beta)
301.5. basic combinatorics
closing remark : in this section, we have given a brief introduction to a few of the most
relevant concepts in linear algebra. we will introduce further concepts in linear algebra in
later chapters, such as eigenvalues, principal component analysis, linear transformations,
and regularization, as they become useful for our discussion.
1.5 basic combinatorics
the last topic we review in this chapter is combinatorics . combinatorics concerns the
number of configurations that can be obtained from certain discrete experiments. it is useful
because it provides a systematic way of enumerating cases. combinatorics often becomes
very challenging as the complexity of the event grows. however, you may rest assured that
in this book, we will not tackle the more difficult problems of combinatorics; we will confine
our discussion to two of the most basic principles: permutation andcombination .
1.5.1 birthday paradox
to motivate the discussion of combinatorics, let us start with the following problem. suppose
there are 50 people in a room. what is the probability that at least one pair of people have
the same birthday (month and day)? (we exclude feb. 29 in this problem.)
the first thing you might be thinking is that since there are 365 days, we need at least
366 people to ensure that one pair has the same birthday. therefore, the chance that 2 of
50 people have the same birthday is low. this seems reasonable, but let‚Äôs do a simulated
experiment. in figure 1.16 we plot the probability as a function of the number of people.
for a room containing 50 people, the probability is 97%. to get a 50% probability, we just
need 23 people! how is this possible?
0 10 20 30 40 50 60 70 80 90 100
number of people00.10.20.30.40.50.60.70.80.91probability
figure 1.16: the probability for two people in a group to have the same birthday as a function of the
number of people in the group.
if you think about this problem more deeply, you will probably realize that to solve the
problem, we must carefully enumerate all the possible configurations. how can we do this?
well, suppose you walk into the room and sequentially pick two people. the probability
31chapter 1. mathematical background
that they have different birthdays is
p[the first 2 people have different birthdays] =365
365√ó364
365.
when you ask the first person to tell you their birthday, he or she can occupy any of the
365 slots. this gives us365
365. the second person has one slot short because the first person
has taken it, and so the probability that he or she has a different birthday from the first
person is364
365. note that this calculation is independent of how many people you have in the
room because you are picking them sequentially.
if you now choose a third person, the probability that they have different birthdays is
p[the first 3 people have different birthdays] =365
365√ó364
365√ó363
365.
this process can be visualized in figure 1.17 .
figure 1.17: the probability for two people to have the same birthday as a function of the number of
people in the group. when there is only one person, this person can land on any of the 365 days. when
there are two people, the first person has already taken one day (out of 365 days), so the second person
can only choose 364 days. when there are three people, the first two people have occupied two days,
so there are only 363 days left. if we generalize this process, we see that the number of configurations
is365√ó364√ó ¬∑¬∑¬∑ √ó (365‚àík+ 1), where kis the number of people in the room.
so imagine that you keep going down the list to the 50th person. the probability that
none of these 50 people will have the same birthday is
p[the first 50 people have different birthdays]
=365
365√ó364
365√ó363
365√ó ¬∑¬∑¬∑ √ó316
365‚âà0.03.
that means that the probability for 50 people to have different birthdays, the probability is
as little as 3%. if you take the complement, you can show that with 97% probability, there
is at least one pair of people having the same birthday.
the general equation for this problem is now easy to see:
p[the first kpeople have different birthdays] =365√ó364√ó ¬∑¬∑¬∑ √ó (365‚àík+ 1)
365√ó365√ó ¬∑¬∑¬∑ √ó 365
=365!
(365‚àík)!√ó1
365k.
321.5. basic combinatorics
the first term in our equation,365!
(365‚àík)!, is called the permutation of picking kdays from
365 options. we shall discuss this operation shortly.
why is the probability so high with only 50 people while it seems that we need 366
people to ensure two identical birthdays? the difference is the notion of probabilistic and
deterministic . the 366-people argument is deterministic. if you have 366 people, you are
certain that two people will have the same birthday. this has no conflict with the proba-
bilistic argument because the probabilistic argument says that with 50 people, we have a
97% chance of getting two identical birthdays. with a 97% success rate, you still have a
3% chance of failing. it is unlikely to happen, but it can still happen. the more people you
put into the room, the stronger guarantee you will have. however, even if you have 364
people and the probability is almost 100%, there is still no guarantee. so there is no conflict
between the two arguments since they are answering two different questions.
now, let‚Äôs discuss the two combinatorics questions.
1.5.2 permutation
permutation concerns the following question:
consider a set of ndistinct balls. suppose we want to pick kballs from the set without
replacement. how many ordered configurations can we obtain?
note that in the above question, the word ‚Äúordered‚Äù is crucial. for example, the set
a={a, b, c}can lead to 6 different ordered configurations
(a, b, c ),(a, c, b ),(b, a, c ),(b, c, a ),(c, a, b ),(c, b, a ).
as a simple illustration of how to compute the permutation, we can consider a set of
5 colored balls as shown in figure 1.18 .
figure 1.18: permutation. the number of choices is reduced in every stage. therefore, the total number
isn√ó(n‚àí1)√ó ¬∑¬∑¬∑ √ó (n‚àík+ 1) if there are kstages.
if you start with the base, which contains five balls, you will have five choices. at one
level up, since one ball has already been taken, you have only four choices. you continue
the process until you reached the number of balls you want to collect. the number of
configurations you have generated is the permutation. here is the formula:
33chapter 1. mathematical background
theorem 1.6. the number of permutations of choosing kout of nis
n!
(n‚àík)!
where n! =n(n‚àí1)(n‚àí2)¬∑¬∑¬∑3¬∑2¬∑1.
proof . let‚Äôs list all possible ways:
which ball to pick number of choices why?
the 1st ball n no has been picked, so we
have nchoices
the 2nd ball n‚àí1 the first ball has been
picked
the 3rd ball n‚àí2 the first two balls have
been picked
.........
thekth ball n‚àík+ 1 the first k‚àí1 balls have
been picked
total: n(n‚àí1)¬∑¬∑¬∑(n‚àík+ 1)
the total number of ordered configurations is n(n‚àí1)¬∑¬∑¬∑(n‚àík+ 1). this simplifies
to
n(n‚àí1)(n‚àí2)¬∑¬∑¬∑(n‚àík+ 1)
=n(n‚àí1)(n‚àí2)¬∑¬∑¬∑(n‚àík+ 1)¬∑(n‚àík)(n‚àík‚àí1)¬∑¬∑¬∑3¬∑2¬∑1
(n‚àík)(n‚àík‚àí1)¬∑¬∑¬∑3¬∑2¬∑1
=n!
(n‚àík)!.
‚ñ°
practice exercise 1.8 . consider a set of 4 balls {1,2,3,4}. we want to pick two
balls at random without replacement. the ordering matters. how many permutations
can we obtain?
solution . the possible configurations are (1,2), (2,1), (1,3), (3,1), (1,4), (4,1), (2,3),
(3,2), (2,4), (4,2), (3,4), (4,3). so totally there are 12 configurations. we can also
verify this number by noting that there are 4 balls altogether and so the number
of choices for picking the first ball is 4 and the number of choices for picking the
second ball is (4 ‚àí1) = 3. thus, the total is 4 ¬∑3 = 12. referring to the formula, this
result coincides with the theorem, which states that the number of permutations is
4!
(4‚àí2)!=4¬∑3¬∑2¬∑1
2¬∑1= 12.
1.5.3 combination
another operation in combinatorics is combination. combination concerns the following
question:
341.5. basic combinatorics
consider a set of ndistinct balls. suppose we want to pick kballs from the set without
replacement. how many unordered configurations can we obtain?
unlike permutation, combination treats a subset of balls with whatever ordering as
one single configuration. for example, the subset ( a, b, c ) is considered the same as ( a, c, b )
or (b, c, a ), etc.
let‚Äôs go back to the 5-ball exercise. suppose you have picked orange, green, and light
blue. this is the same combination as if you have picked {green, orange, and light blue },
or{green, light blue, and orange }.figure 1.19 lists all the six possible configurations for
these three balls. so what is combination? combination needs to take these repeated cases
into account.
figure 1.19: combination. in this problem, we are interested in picking 3 colored balls out of 5. this
will give us 5√ó4√ó3 = 60 permutations. however, since we are not interested in the ordering, some of
the permutations are repeated. for example, there are 6 combos of (green, light blue, orange), which is
computed from 3√ó2√ó1. dividing 60 permutations by these 6 choices of the orderings will give us 10
distinct combinations of the colors.
theorem 1.7. the number of combinations of choosing kout of nis
n!
k!(n‚àík)!
where n! =n(n‚àí1)(n‚àí2)¬∑¬∑¬∑3¬∑2¬∑1.
proof . we start with the permutation result, which gives usn!
(n‚àík)!permutations. note that
every permutation has exactly kballs. however, while these kballs can be arranged in any
order, in combination, we treat them as one single configuration. therefore, the task is to
count the number of possible orderings for these kballs.
to this end, we note that for a set of kballs, there are in total k! possible ways of
ordering them. the number k! comes from the following table.
35chapter 1. mathematical background
which ball to pick number of choices
the 1st ball k
the 2nd ball k‚àí1
......
thekth ball 1
total: k(k‚àí1)¬∑¬∑¬∑3¬∑2¬∑1
therefore, the total number of orderings for a set of kballs is k!. since permutation
gives usn!
(n‚àík)!and every permutation has k! repetitions due to ordering, we divide the
number by k!. thus the number of combinations is
n!
k!(n‚àík)!.
‚ñ°
practice exercise 1.9 . consider a set of 4 balls {1,2,3,4}. we want to pick two
balls at random without replacement. the ordering does not matter. how many com-
binations can we obtain?
solution . the permutation result gives us 12 permutations. however, among all these
12 permutations, there are only 6 distinct pairs of numbers. we can confirm this by
noting that since we picked 2 balls, there are exactly 2 possible orderings for these 2
balls. therefore, we have12
2= 6 number of combinations. using the formula of the
theorem, we check that the number of combinations is
4!
2!(4‚àí2)!=4¬∑3¬∑2¬∑1
(2¬∑1)(2¬∑1)= 6.
example 1.10 . (ross, 8th edition, section 1.6) consider the equation
x1+x2+¬∑¬∑¬∑+xk=n,
where {xk}are positive integers. how many combinations of solutions of this equation
are there?
solution . we can determine the number of combinations by considering the figure
below. the integer ncan be modeled as nballs in an urn. the number of variables k
is equivalent to the number of colors of these balls. since all variables are positive, the
problem can be translated to partitioning the nballs into kbuckets. this, in turn,
is the same as inserting k‚àí1 dividers among n‚àí1 holes. therefore, the number of
combinations is n‚àí1
k‚àí1
=(n‚àí1)!
(k‚àí1)!(n‚àík)!.
for example, if n= 16 and k= 4, then the number of solutions is
16‚àí1
4‚àí1
=15!
3!12!= 455 .
361.6. summary
figure 1.20: one possible solution for n= 16 andk= 4. in general, the problem is equivalent
to inserting k‚àí1dividers among n‚àí1balls.
closing remark . permutations and combinations are two ways to enumerate all the pos-
sible cases. while the conclusions are probabilistic, as the birthday paradox shows, permu-
tation and combination are deterministic. we do not need to worry about the distribution
of the samples, and we are not taking averages of anything. thus, modern data analysis
seldom uses the concepts of permutation and combination. accordingly, combinatorics does
not play a large role in this book.
does it mean that combinatorics is not useful? not quite, because it still provides us
with powerful tools for theoretical analysis. for example, in binomial random variables, we
need the concept of combination to calculate the repeated cases. the poisson random vari-
able can be regarded as a limiting case of the binomial random variable, and so combination
is also used. therefore, while we do not use the concepts of permutation per se, we use them
to define random variables.
1.6 summary
in this chapter, we have reviewed several background mathematical concepts that will be-
come useful later in the book. you will find that these concepts are important for under-
standing the rest of this book. when studying these materials, we recommend not just
remembering the ‚Äúrecipes‚Äù of the steps but focusing on the motivations andintuitions
behind the techniques.
we would like to highlight the significance of the birthday paradox. many of us come
from an engineering background in which we were told to ensure reliability and guarantee
success. we want to ensure that the product we deliver to our customers can survive even
in the worst-case scenario. we tend to apply deterministic arguments such as requiring 366
people to ensure complete coverage of the 365 days. in modern data analysis, the worst-case
scenario may not always be relevant because of the complexity of the problem and the cost
of such a warranty. the probabilistic argument, or the average argument, is more reasonable
and cost-effective, as you can see from our analysis of the birthday problem. the heart of
the problem is the trade-off between how much confidence you need versus how much effort
you need to expend. suppose an event is unlikely to happen, but if it happens, it will be
a disaster. in that case, you might prefer to be very conservative to ensure that such a
disaster event has a low chance of happening. industries related to risk management such
as insurance and investment banking are all operating under this principle.
37chapter 1. mathematical background
1.7 reference
introductory materials
1-1 erwin kreyszig, advanced engineering mathematics , wiley, 10th edition, 2011.
1-2 henry stark and john w. woods, probability and random processes with applications
to signal processing , prentice hall, 3rd edition, 2002. appendix.
1-3 michael j. evans and jeffrey s. rosenthal, probability and statistics: the science of
uncertainty , w. h. freeman, 2nd edition, 2009. appendix.
1-4 james stewart, single variable calculus, early transcendentals , thomson brooks/-
cole, 6th edition, 2008. chapter 5.
combinatorics
1-5 dimitri p. bertsekas and john n. tsitsiklis, introduction to probability , athena sci-
entific, 2nd edition, 2008. section 1.6.
1-6 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. section 2.6.
1-7 athanasios papoulis and s. unnikrishna pillai, probability, random variables and
stochastic processes , mcgraw-hill, 4th edition, 2001. chapter 3.
analysis
in some sections of this chapter, we use results from calculus and infinite series. many formal
proofs can be found in the standard undergraduate real analysis textbooks.
1-8 tom m. apostol, mathematical analysis , pearson, 1974.
1-9 walter rudin, principles of mathematical analysis , mcgraw hill, 1976.
1.8 problems
exercise 1. (video solution)
(a) show that
nx
k=0rk=1‚àírn+1
1‚àír.
for any 0 < r < 1. evaluatep‚àû
k=0rk.
(b) using the result of (a), evaluate
1 + 2 r+ 3r2+¬∑¬∑¬∑.
381.8. problems
(c) evaluate the sums
‚àûx
k=0k1
3k+1
,and‚àûx
k=2k1
4k‚àí1
.
exercise 2. (video solution)
recall that‚àûx
k=0Œªk
k!=eŒª.
evaluate‚àûx
k=0kŒªke‚àíŒª
k!,and‚àûx
k=0k2Œªke‚àíŒª
k!.
exercise 3. (video solution)
evaluate the integrals
(a)zb
a1
b‚àía
x‚àía+b
22
dx.
(b)z‚àû
0Œªxe‚àíŒªxdx.
(c)z‚àû
‚àí‚àûŒªx
2e‚àíŒª|x|dx.
exercise 4.
(a) compute the result of the following matrix vector multiplication using numpy. submit
your result and codes.
Ô£Æ
Ô£∞1 2 3
4 5 6
7 8 9Ô£π
Ô£ª√óÔ£Æ
Ô£∞1
2
3Ô£π
Ô£ª.
(b) plot a sine function on the interval [ ‚àíœÄ, œÄ] with 1000 data points.
(c) generate 10,000 uniformly distributed random numbers on interval [0, 1).
usematplotlib.pyplot.hist to generate a histogram of all the random numbers.
39chapter 1. mathematical background
exercise 5.
calculate‚àûx
k=0k2
3k+1
.
exercise 6.
let
x=x
y
,¬µ=1
0
,œÉ=4 1
1 1
.
(a) find œÉ‚àí1, the inverse of œÉ.
(b) find |œÉ|, the determinant of œÉ.
(c) simplify the two-dimensional function
f(x) =1
2œÄ|œÉ|1/2exp
‚àí1
2(x‚àí¬µ)tœÇ‚àí1(x‚àí¬µ)
.
(d) use matplotlib.pyplot.contour , plot the function f(x) for the range [ ‚àí3,3]√ó
[‚àí3,3].
exercise 7.
out of seven electrical engineering (ee) students and five mechanical engineering (me)
students, a committee consisting of three ees and two mes is to be formed. in how many
ways can this be done if
(a) any of the ees and any of the mes can be included?
(b) one particular ee must be on the committee?
(c) two particular mes cannot be on the committee?
exercise 8.
five blue balls, three red balls, and three white balls are placed in an urn. three balls are
drawn at random without regard to the order in which they are drawn. using the counting
approach to probability, find the probability that
(a) one blue ball, one red ball, and one white ball are drawn.
(b) all three balls drawn are red.
(c) exactly two of the balls drawn are blue.
exercise 9.
a collection of 26 english letters, a-z, is mixed in a jar. two letters are drawn at random,
one after the other.
401.8. problems
(a) what is the probability of drawing a vowel (a,e,i,o,u) and a consonant in either order?
(b) write a matlab / python program to verify your answer in part (a). randomly
draw two letters without replacement and check whether one is a vowel and the other
is a consonant. compute the probability by repeating the experiment 10000 times.
exercise 10.
there are 50 students in a classroom.
(a) what is the probability that there is at least one pair of students having the same
birthday? show your steps.
(b) write a matlab / python program to simulate the event and verify your answer
in (a). hint: you probably need to repeat the simulation many times to obtain a
probability. submit your code and result.
you may assume that a year only has 365 days. you may also assume that all days have an
equal likelihood of being taken.
41chapter 1. mathematical background
42chapter 2
probability
data and probability are inseparable. data is the computational side of the story, whereas
probability is the theoretical side of the story. any data science practice must be built on
the foundation of probability, and probability needs to address practical problems. however,
what exactly is ‚Äúprobability‚Äù? mathematicians have been debating this for centuries. the
frequentists argue that probability is the relative frequency of an outcome. for example,
flipping a fair coin has a 1/2 probability of getting a head because if you flip the coin
infinitely many times, you will have half of the time getting a head. the bayesians argue
that probability is a subjective belief. for example, the probability of getting an a in a
class is subjective because no one would want to take a class infinitely many times to obtain
the relative frequency. both the frequentists and bayesians have valid points. however, the
differentiation is often non-essential because the context of your problem will force you
to align with one or the other. for example, when you have a shortage of data, then the
subjectivity of the bayesians allows you to use prior knowledge, whereas the frequentists
tell us how to compute the confidence interval of an estimate.
no matter whether you prefer the frequentist‚Äôs view or the bayesian‚Äôs view, there is
something more fundamental thanks to andrey kolmogorov (1903-1987). the development
of this fundamental definition will take some effort on our part, but if we distill the essence,
we can summarize it as follows:
probability is a measure of the size of a set.
this sentence is not a formal definition; instead, it summarizes what we believe to be the
essence of probability. we need to clarify some puzzles later in this chapter, but if you can
understand what this sentence means, you are halfway done with this book. to spell out the
details, we will describe an elementary problem that everyone knows how to solve. as we
discuss this problem, we will highlight a few key concepts that will give you some intuitive
insights into our definition of probability, after which we will explain the sequence of topics
to be covered in this chapter.
prelude: probability of throwing a die
suppose that you have a fair die. it has 6 faces: {
,
,
,
,
,
}. what is the probability
that you get a number that is ‚Äúless than 5‚Äù and is ‚Äúan even number‚Äù? this is a straightfor-
43chapter 2. probability
ward problem. you probably have already found the answer, which is2
6because ‚Äúless than
5‚Äù and ‚Äúan even number‚Äù means {
,
}. however, let‚Äôs go through the thinking process
slowly by explicitly writing down the steps.
first of all, how do we know that the denominator in2
6is 6? well, because there are six
faces. these six faces form a set called the sample space . a sample space is the set containing
all possible outcomes, which in our case is œâ = {
,
,
,
,
,
}. the denominator 6 is the
size of the sample space.
how do we know that the numerator is 2? again, implicitly in our minds, we have
constructed two events :e1= ‚Äúless than 5‚Äù = {
,
,
,
}, and e2= ‚Äúan even number‚Äù
={
,
,
}. then we take the intersection between these two events to conclude the event
e={
,
}. the numerical value ‚Äú2‚Äù is the size of this event e.
so, when we say that ‚Äúthe probability is2
6,‚Äù we are saying that the size of the event
erelative to the sample space œâ is the ratio2
6. this process involves measuring the size
ofeand œâ. in this particular example, the measure we use is a ‚Äúcounter‚Äù that counts the
number of elements.
this example shows us all the necessary components of probability: (i) there is a
sample space , which is the set that contains all the possible outcomes. (ii) there is an event ,
which is a subset inside the sample space. (iii) two events e1ande2can be combined to
construct another event ethat is still a subset inside the sample space. (iv) probability is
a number assigned by certain rules such that it describes the relative size of the event e
compared with the sample space œâ. so, when we say that probability is a measure of the
size of a set , we create a mapping that takes in a set and outputs the size of that set.
organization of this chapter
as you can see from this example, since probability is a measure of the size of a set, we need
to understand the operations of sets to understand probability. accordingly, in section 2.1
we first define sets and discuss their operations. after learning these basic concepts, we move
on to define the sample space and event space in section 2.2. there, we discuss sample spaces
that are not necessarily countable and how probabilities are assigned to events. of course,
assigning a probability value to an event cannot be arbitrary; otherwise, the probabilities
may be inconsistent. consequently, in section 2.3 we introduce the probability axioms and
formalize the notion of measure. section 2.4 consists of a trio of topics that concern the
relationship between events using conditioning. we discuss conditional probability in section
2.4.1, independence in section 2.4.2, and bayes‚Äô theorem in section 2.4.3.
2.1 set theory
2.1.1 why study set theory?
in mathematics, we are often interested in describing a collection of numbers, for example, a
positive interval [ a, b] on the real line or the ordered pairs of numbers that define a circle on
a graph with two axes. these collections of numbers can be abstractly defined as sets. in a
nutshell, a set is simply a collection of things. these things can be numbers, but they can also
be alphabets, objects, or anything. set theory is a mathematical tool that defines operations
on sets. it provides the basic arithmetic for us to combine, separate, and decompose sets.
442.1. set theory
why do we start the chapter by describing set theory? because probability is a measure
of the size of a set . yes, probability is not just a number telling us the relative frequency of
events; it is an operator that takes a set and tells us how large the set is. using the example
we showed in the prelude, the event ‚Äúeven number‚Äù of a die is a set containing numbers
{
,
,
}. when we apply probability to this set, we obtain the number3
6, as shown in
figure 2.1 . thus sets are the foundation of the study of probability.
figure 2.1: probability is a measure of the size of a set. whenever we talk about probability, it has to
be the probability of a set.
2.1.2 basic concepts of a set
definition 2.1 (set).asetis a collection of elements. we denote
a={Œæ1, Œæ2, . . . , Œæ n} (2.1)
as a set, where Œæiis the ith element in the set.
in this definition, ais called a set. it is nothing but a collection of elements Œæ1, . . . , Œæ n. what
are these Œæi‚Äôs? they can be anything. let‚Äôs see a few examples below.
example 2.1(a) .a={apple ,orange ,pear}is a finite set.
example 2.1(b) .a={1,2,3,4,5,6}is a finite set.
example 2.1(c) .a={2,4,6,8, . . .}is a countable but infinite set.
example 2.1(d) .a={x|0< x < 1}is a uncountable set.
to say that an element Œæis drawn from a, we write Œæ‚ààa. for example, the number 1
is an element in the set {1,2,3}. we write 1 ‚àà {1,2,3}. there are a few common sets that
we will encounter. for example,
example 2.2(a) .ris the set of all real numbers including ¬±‚àû.
example 2.2(b) .r2is the set of ordered pairs of real numbers.
example 2.2(c) . [a, b] ={x|a‚â§x‚â§b}is a closed interval on r.
example 2.2(d) . (a, b) ={x|a < x < b }is an open interval on r.
example 2.2(e) . (a, b] ={x|a < x ‚â§b}is a semi-closed interval on r.
45chapter 2. probability
figure 2.2: from left to right: a closed interval, a semi-closed (or semi-open) interval, and an open
interval.
sets are not limited to numbers. a set can be used to describe a collection of functions .
example 2.3 .a={f:r‚Üír|f(x) =ax+b, a, b ‚ààr}. this is the set of all straight
lines in 2d. the notation f:r‚Üírmeans that the function ftakes an argument
fromrand sends it to another real number in r. the definition f(x) =ax+bsays
that fis taking the specific form of ax+b. since the constants aandbcan be any
real number, the equation f(x) =ax+benumerates all possible straight lines in 2d.
seefigure 2.3 (a).
example 2.4 .a={f:r‚Üí[‚àí1,1]|f(t) = cos( œâ0t+Œ∏), Œ∏‚àà[0,2œÄ]}. this is
the set of all cosine functions of a fixed carrier frequency œâ0. the phase Œ∏, however,
is changing. therefore, the equation f(t) = cos( œâ0t+Œ∏) says that the set ais the
collection of all possible cosines with different phases. see figure 2.3 (b).
-2 -1 0 1 2
t-1-0.500.51f(t)
-1 -0.5 0 0.5 1
t-2-1012f(t)
figure 2.3: (a) the set of straight lines a={f:r‚Üír|f(x) =ax+b, a, b ‚ààr}. (b) the set of
phase-shifted cosines a={f:r‚Üí[‚àí1,1]|f(t) = cos( œâ0t+Œ∏), Œ∏‚àà[0,2œÄ]}.
a set can also be used to describe a collection of sets. let aandbbe two sets. then
c={a, b}is a set of sets.
example 2.5 . let a={1,2}andb={apple ,orange }. then
c={a, b}={{1,2},{apple ,orange }}
462.1. set theory
is a collection of sets. note that here we are not saying cis the union of two sets. we
are only saying that cis a collection of two sets. see the next example.
example 2.6 . let a={1,2}andb={3}, then c={a, b}means that
c={{1,2},{3}}.
therefore ccontains only two elements. one is the set {1,2}and the other is the set
{3}. note that {{1,2},{3}} Ã∏={1,2,3}. the former is a set of two sets. the latter is a
set of three elements.
2.1.3 subsets
given a set, we often want to specify a portion of the set, which is called a subset .
definition 2.2 (subset ).bis asubset ofaif for any Œæ‚ààb,Œæis also in a. we
write
b‚äÜa (2.2)
to denote that bis a subset of a.
bis called a proper subset ofaifbis a subset of aandbÃ∏=a. we denote a proper subset
asb‚äÇa. two sets aandbare equal if and only if a‚äÜbandb‚äÜa.
example 2.7 .
¬àifa={1,2,3,4,5,6}, then b={1,3,5}is a proper subset of a.
¬àifa={1,2}, then b={1,2}is an improper subset of a.
¬àifa={t|t‚â•0}, then b={t|t >0}is a proper subset of a.
practice exercise 2.1 . let a={1,2,3}. list all the subsets of a.
solution . the subsets of aare:
a={‚àÖ,{1},{2},{3},{1,2},{1,3},{2,3},{1,2,3}}.
practice exercise 2.2 . prove that two sets aandbare equal if and only if a‚äÜb
andb‚äÜa.
solution . suppose a‚äÜbandb‚äÜa. assume by contradiction that aÃ∏=b. then
necessarily there must exist an xsuch that x‚ààabutxÃ∏‚ààb(or vice versa). but
a‚äÜbmeans that x‚ààawill necessarily be in b. so it is impossible to have xÃ∏‚ààb.
conversely, suppose that a=b. then any x‚ààawill necessarily be in b. therefore,
we have a‚äÜb. similarly, if a=bthen any x‚ààbwill be in a, and so b‚äÜa.
47chapter 2. probability
2.1.4 empty set and universal set
definition 2.3 (empty set ).a set is empty if it contains no element. we denote
an empty set as
a=‚àÖ. (2.3)
a set containing an element 0 is not an empty set. it is a set of one element, {0}. the
number of elements of the empty set is 0. the empty set is a subset of any set, i.e., ‚àÖ ‚äÜa
for any a. we use ‚äÜbecause acould also be an empty set.
example 2.8(a) . the set a={x|sinx >1}is empty because no x‚ààrcan make
sinx >1.
example 2.8(b) . the set a={x|x > 5 and x < 1}is empty because the two
conditions x >5 and x <1 are contradictory.
definition 2.4 (universal set ).theuniversal set is the set containing all elements
under consideration. we denote a universal set as
a= œâ. (2.4)
the universal set œâ contains itself, i.e., œâ ‚äÜœâ. the universal set is a relative concept.
usually, we first define a universal set œâ before referring to subsets of œâ. for example, we
can define œâ = rand refer to intervals in r. we can also define œâ = [0 ,1] and refer to
subintervals inside [0 ,1].
2.1.5 union
we now discuss basic set operations. by operations, we mean functions of two or more sets
whose output value is a set. we use these operations to combine and separate sets. let us
first consdier the union of two sets. see figure 2.4 for a graphical depiction.
definition 2.5 (finite union ).theunion of two sets aandbcontains all elements
inaorinb. that is,
a‚à™b={Œæ|Œæ‚ààaorŒæ‚ààb}. (2.5)
as the definition suggests, the union of two sets connects the sets using the logical operator
‚Äùor‚Äù. therefore, the union of two sets is always larger than or equal to the individual sets.
example 2.9(a) . ifa={1,2},b={1,5}, then a‚à™b={1,2,5}. the overlapping
element 1 is absorbed. also, note that a‚à™bÃ∏={{1,2},{1,5}}. the latter is a set of
sets.
example 2.9(b) . ifa= (3,4],b= (3.5,‚àû), then a‚à™b= (3,‚àû).
example 2.9(c) . ifa={f:r‚Üír|f(x) =ax}andb={f:r‚Üír|f(x) =b},
then a‚à™b= a set of sloped lines with a slope aplus a set of constant lines with
482.1. set theory
height b. note that a‚à™bÃ∏={f:r‚Üír|f(x) =ax+b}because the latter is a set of
sloped lines with arbitrary y-intercept.
example 2.9(d) . ifa={1,2}andb=‚àÖ, then a‚à™b={1,2}.
example . ifa={1,2}andb= œâ, then a‚à™b= œâ.
figure 2.4: the union of two sets contains elements that are either in aorbor both.
the previous example can be generalized in the following exercise. what it says is that
ifais a subset of another set b, then the union of aandbis just b. intuitively, this should
be straightforward because whatever you have in ais already in b, so the union will just
beb. below is a formal proof that illustrates how to state the arguments clearly. you may
like to draw a picture to convince yourself that the proof is correct.
practice exercise 2.3 : prove that if a‚äÜb, then a‚à™b=b.
solution : we will show that a‚à™b‚äÜbandb‚äÜa‚à™b. let Œæ‚ààa‚à™b. then Œæmust
be inside either aorb(or both). in any case, since we know that a‚äÜb, it holds
that if Œæ‚ààathen Œæmust also be in b. therefore, for any Œæ‚ààa‚à™bwe have Œæ‚ààb.
this shows a‚à™b‚äÜb. conversely, if Œæ‚ààb, then Œæmust be inside a‚à™bbecause
a‚à™bis a larger set than b. so if Œæ‚ààbthen Œæ‚ààa‚à™band hence b‚äÜa‚à™b. since
a‚à™bis a subset of bor equal to b, and bis a subset of a‚à™bor equal to a‚à™b, it
follows that a‚à™b=b.
what should we do if we want to take the union of an infinite number of sets? first,
we need to define the concept of an infinite union .
definition 2.6 (infinite union ).for an infinite sequence of sets a1, a2, . . ., the in-
finite union is defined as
‚àû[
n=1an={Œæ|Œæ‚ààanforat least one nthat is finite. }. (2.6)
an infinite union is a natural extension of a finite union. it is not difficult to see that
Œæ‚ààaorŒæ‚ààb‚áê‚áí Œæis inat least one of aandb.
49chapter 2. probability
similarly, an infinite union means that
Œæ‚ààa1orŒæ‚ààa2orŒæ‚ààa3. . .‚áê‚áí Œæis inat least one of a1, a2, a3, . . . .
the finite nrequirement says that we only evaluate the sets for a finite number of n‚Äôs. this
ncan be arbitrarily large, but it is finite. why are we able to do this? because the concept
of an infinite union is to determine a‚àû, which is the limit of a sequence. like any sequence
of real numbers, the limit of a sequence of sets has to be defined by evaluating the instances
of all possible finite cases.
consider a sequence of sets an=
‚àí1,1‚àí1
n
, for n= 1,2, . . .. for example, a1=
[‚àí1,0],a2=
‚àí1,1
2
,a3=
‚àí1,2
3
,a4=
‚àí1,3
4
, etc.
figure 2.5: the infinite union ofs‚àû
n=1
‚àí1,1‚àí1
n
. no matter how large ngets, the point 1is never
included. so the infinite union is [‚àí1,1)
to take the infinite union, we know that the set [ ‚àí1,1) is always included, because the
right-hand limit 1 ‚àí1
napproaches 1 as napproaches ‚àû. so the only question concerns the
number 1. should 1 be included? according to the definition above, we ask: is 1 an element
ofat least one of the sets a1,a2, . . . , an? clearly it is not: 1 Ã∏‚ààa1, 1Ã∏‚ààa2,. . .. in fact,
1Ã∏‚ààanfor any finite n. therefore 1 is not an element of the infinite union, and we conclude
that‚àû[
n=1an=‚àû[
n=1
‚àí1,1‚àí1
n
= [‚àí1,1).
practice exercise 2.4 . find the infinite union of the sequences where (a) an=
‚àí1,1‚àí1
n
, (b) an= 
‚àí1,1‚àí1
n
.
solution . (a)s‚àû
n=1an= [‚àí1,1). (b)s‚àû
n=1an= (‚àí1,1).
2.1.6 intersection
the union of two sets is based on the logical operator or. if we use the logical operator and,
then the result is the intersection of two sets.
definition 2.7 (finite intersection ).theintersection of two sets aandbcontains
all elements in aandinb. that is,
a‚à©b={Œæ|Œæ‚ààaandŒæ‚ààb}. (2.7)
figure 2.6 portrays intersection graphically. intersection finds the common elements of the
two sets. it is not difficult to show that a‚à©b‚äÜaanda‚à©b‚äÜb.
502.1. set theory
figure 2.6: the intersection of two sets contains elements in both aandb.
example 2.10(a) . ifa={1,2,3,4},b={1,5,6}, then a‚à©b={1}.
example 2.10(b) . ifa={1,2},b={5,6}, then a‚à©b=‚àÖ.
example 2.10(c) . ifa= (3,4],b= [3.5,‚àû), then a‚à©b= [3.5,4].
example 2.10(d) . ifa= (3,4],b=‚àÖ, then a‚à©b=‚àÖ.
example 2.10(e) . ifa= (3,4],b= œâ, then a‚à©b= (3,4].
example 2.11 . ifa={f:r‚Üír|f(x) =ax}andb={f:r‚Üír|f(x) =b}, then
a‚à©b= the intersection of a set of sloped lines with a slope aand a set of constant lines
with height b. the only line that can satisfy both sets is the line f(x) = 0. therefore,
a‚à©b={f|f(x) = 0}.
example 2.12 . ifa={{1},{2}}andb={{2,3},{4}}, then a‚à©b=‚àÖ. this is
because ais a set containing two sets, and bis a set containing two sets. the two sets
{2}and{2,3}are not the same. thus, aandbhave no elements in common, and so
a‚à©b=‚àÖ.
similarly to the infinite union, we can define the concept of infinite intersection .
definition 2.8 (infinite intersection ).for an infinite sequence of sets a1, a2, . . .,
theinfinite intersection is defined as
‚àû\
n=1an={Œæ|Œæ‚ààanfor every finite n.} (2.8)
to understand this definition, we note that
Œæ‚ààaand Œæ‚ààb‚áê‚áí Œæis inevery one ofaandb.
as a result, it follows that
Œæ‚ààa1and Œæ‚ààa2and Œæ‚ààa3. . .‚áê‚áí Œæis inevery one of a1, a2, a3, . . . .
51chapter 2. probability
since the infinite intersection requires that Œæis in every one of a1,a2,. . .,an, if there is a
setaithat does not contain Œæ, the infinite intersection is an empty set.
consider the problem of finding the infinite intersection oft‚àû
n=1an, where
an=
0,1 +1
n
.
we note that the sequence of sets is [0 ,2], [0 ,1.5], [0 ,1.33], . . . . as n‚Üí ‚àû , we note that
the limit is either [0 ,1) or [0 ,1]. should the right-hand limit 1 be included in the infinite
intersection? according to the definition above, we know that 1 ‚ààa1, 1‚ààa2, . . . , 1 ‚ààan
for any finite n. therefore, 1 is included and so
‚àû\
n=1an=‚àû\
n=1
0,1 +1
n
= [0,1].
figure 2.7: the infinite intersection oft‚àû
n=1
0,1 +1
n
. no matter how large ngets, the point 1is
never included. so the infinite intersection is [0,1]
practice exercise 2.5 . find the infinite intersection of the sequences where (a)
an=
0,1 +1
n
, (b) an= 
0,1 +1
n
, (c)an=
0,1‚àí1
n
, (d) an=
0,1‚àí1
n
.
solution .
(a)t‚àû
n=1an= [0,1].
(b)t‚àû
n=1an= (‚àí1,1].
(c)t‚àû
n=1an= [0,0) =‚àÖ.
(d)t‚àû
n=1an= [0,0] ={0}.
2.1.7 complement and difference
besides union and intersection, there is a third basic operation on sets known as the com-
plement .
definition 2.9 (complement ).thecomplement of a set ais the set containing all
elements that are in œâbut not in a. that is,
ac={Œæ|Œæ‚ààœâandŒæÃ∏‚ààa}. (2.9)
figure 2.8 graphically portrays the idea of a complement. the complement is a set that
contains everything in the universal set that is not in a. thus the complement of a set is
always relative to a specified universal set.
522.1. set theory
figure 2.8: [left] the complement of a set acontains all elements that are not in a. [right] the
difference a\bcontains elements that are in abut not in b.
example 2.13(a) . let a={1,2,3}and œâ = {1,2,3,4,5,6 }. then ac={4,5,6}.
example 2.13(b) . let a={even integers }and œâ = {integers }. then ac={odd
integers }.
example 2.13(c) . let a={integers }and œâ = r. then ac={any real number that
is not an integer }.
example 2.13(d) . let a= [0,5) and œâ = r. then ac= (‚àí‚àû,0)‚à™[5,‚àû).
example 2.13(e) . let a=rand œâ = r. then ac=‚àÖ.
the concept of the complement will help us understand the concept of difference .
definition 2.10 (difference ).thedifference a\bis the set containing all elements
inabut not in b.
a\b={Œæ|Œæ‚ààaandŒæÃ∏‚ààb}. (2.10)
figure 2.8 portrays the concept of difference graphically. note that a\bÃ∏=b\a. the former
removes the elements in bwhereas the latter removes the elements in a.
example 2.14(a) . let a={1,3,5,6}andb={2,3,4}. then a\b={1,5,6}and
b\a={2,4}.
example 2.14(b) . let a= [0,1],b= [2,3], then a\b= [0,1], and b\a= [2,3].
this example shows that if the two sets do not overlap, there is nothing to subtract.
example 2.14(c) . let a= [0,1],b=r, then a\b=‚àÖ, and b\a= (‚àí‚àû,0)‚à™(1,‚àû).
this example shows that if one of the sets is the universal set, then the difference will
either return the empty set or the complement.
53chapter 2. probability
figure 2.9: [left] aandbare overlapping. [right] aandbare disjoint.
practice exercise 2.6 . show that for any two sets aandb, the differences a\b
andb\anever overlap, i.e., ( a\b)‚à©(b\a) =‚àÖ.
solution . suppose, by contradiction, that the intersection is not empty so that there
exists an Œæ‚àà(a\b)‚à©(b\a). then, by the definition of intersection, Œæis an element
of (a\b)and(b\a). but if Œæis an element of ( a\b), it cannot be an element of b.
this implies that Œæcannot be an element of ( b\a) since it is a subset of b. this is a
contradiction because we just assumed that the Œæcan live in both ( a\b) and ( b\a).
difference can be defined in terms of intersection and complement:
theorem 2.1. letaandbbe two sets. then
a\b=a‚à©bc(2.11)
proof . let x‚ààa\b. then x‚ààaandxÃ∏‚ààb. since xÃ∏‚ààb, we have x‚ààbc. therefore,
x‚ààaandx‚ààbc. by the definition of intersection, we have x‚ààa‚à©bc. this shows
that a\b‚äÜa‚à©bc. conversely, let x‚ààa‚à©bc. then, x‚ààaandx‚ààbc, which implies
that x‚ààaandxÃ∏‚ààb. by the definition of a\b, we have that x‚ààa\b. this shows that
a‚à©bc‚äÜa\b.
‚ñ°
2.1.8 disjoint and partition
it is important to be able to quantify situations in which two sets are not overlapping. in
this situation, we say that the sets are disjoint .
definition 2.11 (disjoint ).two sets aandbaredisjoint if
a‚à©b=‚àÖ. (2.12)
for a collection of sets {a1, a2, . . . , a n}, we say that the collection is disjoint if, for
any pair iÃ∏=j,
ai‚à©aj=‚àÖ. (2.13)
a pictorial interpretation can be found in figure 2.9 .
542.1. set theory
example 2.15(a) . let a={x >1}andb={x <0}. then aandbare disjoint.
example 2.15(b) . let a={1,2,3}andb=‚àÖ. then aandbare disjoint.
example 2.15(c) . let a= (0,1) and b= [1,2). then aandbare disjoint.
with the definition of disjoint, we can now define the powerful concept of partition .
definition 2.12 (partition ).a collection of sets {a1, . . . , a n}is apartition of the
universal set œâif it satisfies the following conditions:
¬à(non-overlap ){a1, . . . , a n}is disjoint:
ai‚à©aj=‚àÖ. (2.14)
¬à(decompose ) union of {a1, . . . , a n}gives the universal set:
n[
i=1ai= œâ. (2.15)
in plain language, a partition is a collection of non-overlapping subsets whose union is
the universal set. partition is important because it is a decomposition of œâ into a smaller
subset, and since these subsets do not overlap, they can be analyzed separately. partition
is a handy tool for studying probability because it allows us to decouple complex events by
treating them as isolated sub-events.
figure 2.10: a partition of œâcontains disjoint subsets of which the union gives us œâ.
example 2.16 . let œâ = {1,2,3,4,5,6}. the following sets form a partition:
a1={1,2,3}, a 2={4,5}, a 3={6}
example 2.17 . let œâ = {1,2,3,4,5,6}. the collection
a1={1,2,3}, a 2={4,5}, a 3={5,6}
does not form a partition, because a2‚à©a3={5}.
55chapter 2. probability
if{a1, a2, . . . , a n}forms a partition of the universal set œâ, then for any b‚äÜœâ, we
can decompose bintondisjoint subsets: b‚à©a1,b‚à©a2, . . .b‚à©an. two properties hold:
¬àb‚à©aiandb‚à©ajare disjoint if iÃ∏=j.
¬àthe union of b‚à©a1,b‚à©a2, . . .b‚à©anisb.
practice exercise 2.7 . prove the above two statements.
solution . to prove the first statement, we can pick Œæ‚àà(b‚à©ai). this means that
Œæ‚ààbandŒæ‚ààai. since Œæ‚ààai, it cannot be in ajbecause aiandajare disjoint.
therefore Œæcannot live in b‚à©aj. this completes the proof, because we just showed
that any Œæ‚ààb‚à©aicannot simultaneously live in b‚à©aj.
to prove the second statement, we pick Œæ‚ààsn
i=1(b‚à©ai). since Œælives in the
union, it has to live in at least one of the ( b‚à©ai) for some i. now suppose Œæ‚ààb‚à©ai.
this means that Œæis in both bandai, so it must live in b. therefore,sn
i=1(b‚à©ai)‚äÜ
b. now, suppose we pick Œæ‚ààb. then since it is an element in b, it must be an element
in all of the ( b‚à©ai)‚Äôs for any i. therefore, Œæ‚ààsn
i=1(b‚à©ai), and so we showed that
b‚äÜsn
i=1(b‚à©ai). combining the two directions, we conclude thatsn
i=1(b‚à©ai) =b.
example 2.18 . let œâ = {1,2,3,4,5,6}and let a partition of œâ be a1={1,2,3},
a2={4,5},a3={6}. let b={1,3,4}. then, by the result we just proved, bcan
be decomposed into three subsets:
b‚à©a1={1,3}, b‚à©a2={4}, b‚à©a3=‚àÖ.
thus we can see that b‚à©a1,b‚à©a2andb‚à©a3are disjoint. furthermore, the union
of these three sets gives b.
2.1.9 set operations
when handling multiple sets, it would be useful to have some basic set operations. there
are four basic theorems concerning set operations that you need to know for our purposes
in this book:
theorem 2.2 (commutative ).(order does not matter)
a‚à©b=b‚à©a, and a‚à™b=b‚à™a. (2.16)
theorem 2.3 (associative ).(how to do multiple union and intersection)
a‚à™(b‚à™c) = (a‚à™b)‚à™c,
a‚à©(b‚à©c) = (a‚à©b)‚à©c. (2.17)
562.1. set theory
theorem 2.4 (distributive ).(how to mix union and intersection)
a‚à©(b‚à™c) = (a‚à©b)‚à™(a‚à©c),
a‚à™(b‚à©c) = (a‚à™b)‚à©(a‚à™c). (2.18)
theorem 2.5 (de morgan‚Äôs law ).(how to complement over intersection and union)
(a‚à©b)c=ac‚à™bc,
(a‚à™b)c=ac‚à©bc. (2.19)
example 2.19 . consider [1 ,4]‚à©([0,2]‚à™[3,5]). by the distributive property we can
simplify the set as
[1,4]‚à©([0,2]‚à™[3,5]) = ([1 ,4]‚à©[0,2])‚à™([1,4]‚à©[3,5])
= [1,2]‚à™[3,4].
example 2.20 . consider ([0 ,1]‚à™[2,3])c. by de morgan‚Äôs law we can rewrite the set
as
([0,2]‚à™[1,3])c= [0,2]c‚à©[1,3]c.
2.1.10 closing remarks about set theory
it should be apparent why set theory is useful: it shows us how to combine, split, and
remove sets. in figure 2.11 we depict the intersection of two sets a={even number }and
b={less than or equal to 3 }. set theory tells us how to define the intersection so that the
probability can be applied to the resulting set.
figure 2.11: when there are two events aandb, the probability of a‚à©bis determined by first taking
the intersection of the two sets and then evaluating its probability.
universal sets and empty sets are useful too. universal sets cover all the possible
outcomes of an experiment, so we should expect p[œâ] = 1. empty sets contain nothing,
and so we should expect p[‚àÖ] = 0. these two properties are essential to define a probability
because no probability can be greater than 1, and no probability can be less than 0.
57chapter 2. probability
2.2 probability space
we now formally define probability. our discussion will be based on the slogan probability
is a measure of the size of a set . three elements constitute a probability space :
¬àsample space œâ: the set of all possible outcomes from an experiment.
¬àevent space f: the collection of all possible events. an event eis a subset in œâ that
defines an outcome or a combination of outcomes.
¬àprobability law p: a mapping from an event eto a number p[e] which, ideally,
measures the size of the event.
therefore, whenever you talk about ‚Äúprobability,‚Äù you need to specify the triplet (œâ ,f,p)
to define the probability space.
the necessity of the three elements is illustrated in figure 2.12 . the sample space
is the interface with the physical world . it is the collection of all possible states that can
result from an experiment. some outcomes are more likely to happen, and some are less
likely, but this does not matter because the sample space contains every possible outcome.
theprobability law is the interface with the data analysis . it is this law that defines the
likelihood of each of the outcomes. however, since the probability law measures the size of
a set, the probability law itself must be a function, a function whose argument is a set and
whose value is a number. an outcome in the sample space is not a set. instead, a subset in
the sample space is a set. therefore, the probability should input a subset and map it to a
number. the collection of all possible subsets is the event space .
figure 2.12: given an experiment, we define the collection of all outcomes as the sample space. a
subset in the sample space is called an event. the probability law is a mapping that maps an event to
a number that denotes the size of the event.
a perceptive reader like you may be wondering why we want to complicate things to
this degree when calculating probability is trivial, e.g., throwing a die gives us a probability
1
6per face. in a simple world where problems are that easy, you can surely ignore all these
complications and proceed to the answer1
6. however, modern data analysis is not so easy.
if we are given an image of size 64 √ó64 pixels, how do we tell whether this image is of a cat
or a dog? we need to construct a probability model that tells us the likelihood of having a
582.2. probability space
particular 64 √ó64 image. what should be included in this probability model? we need to
know all the possible cases ( the sample space ), all the possible events ( the event space ),
and the probability of each of the events ( the probability law ). if we know all these, then our
decision will be theoretically optimal. of course, for high-dimensional data like images, we
need approximations to such a probability model. however, we first need to understand the
theoretical foundation of the probability space to know what approximations would make
sense.
2.2.1 sample space œâ
we start by defining the sample space œâ. given an experiment, the sample space œâ is the
set containing all possible outcomes of the experiment.
definition 2.13. asample space œâis the set of all possible outcomes from an ex-
periment. we denote Œæas an element in œâ.
a sample space can contain discrete outcomes or continuous outcomes, as shown in
the examples below and figure 2.13 .
example 2.21 : (discrete outcomes)
¬àcoin flip: œâ = {h, t}.
¬àthrow a die: œâ = {
,
,
,
,
,
}.
¬àpaper / scissor / stone: œâ = {paper ,scissor ,stone}.
¬àdraw an even integer: œâ = {2,4,6,8, . . .}.
example 2.22 : (continuous outcomes)
¬àwaiting time for a bus in west lafayette: œâ = {t|0‚â§t‚â§30 minutes }.
¬àphase angle of a voltage: œâ = {Œ∏|0‚â§Œ∏‚â§2œÄ}.
¬àfrequency of a pitch: œâ = {f|0‚â§f‚â§fmax}.
figure 2.13 also shows a functional example of the sample space. in this case, the
sample space contains functions . for example,
¬àset of all straight lines in 2d:
œâ ={f|f(x) =ax+b, a, b ‚ààr}.
¬àset of all cosine functions with a phase offset:
œâ ={f|f(t) = cos(2 œÄœâ0t+ Œ∏),0‚â§Œ∏‚â§2œÄ}.
as we see from the above examples, the sample space is nothing but a universal set.
the elements inside the sample space are the outcomes of the experiment. if you change
59chapter 2. probability
figure 2.13: the sample space can take various forms: it can contain discrete numbers, or continuous
intervals, or even functions.
the experiment, the possible outcomes will be different so that the sample space will be
different. for example, flipping a coin has different possible outcomes from throwing a die.
what if we want to describe a composite experiment where we flip a coin and throw a
die? here is the sample space:
example 2.23 : if the experiment contains flipping a coin and throwing a die, then
the sample space is

(h,
),(h,
),(h,
),(h,
),(h,
),(h,
),
(t,
),(t,
),(t,
),(t,
),(t,
),(t,
)
.
in this sample space, each element is a pair of outcomes.
practice exercise 2.8 . there are 8 processors on a computer. a computer job sched-
uler chooses one processor randomly. what is the sample space? if the computer job
scheduler can choose two processors at once, what is the sample space then?
solution . the sample space of the first case is œâ = {1,2,3,4,5,6,7,8}. the sample
space of the second case is œâ = {(1,2),(1,3),(1,4), . . . , (7,8)}.
practice exercise 2.9 . a cell phone tower has a circular average coverage area of
radius of 10 km. we observe the source locations of calls received by the tower. what
is the sample space of all possible source locations?
solution . assume that the center of the tower is located at ( x0, y0). the sample space
is the set
œâ ={(x, y)|p
(x‚àíx0)2+ (y‚àíy0)2‚â§10}.
not every set can be a sample space. a sample space must be exhaustive andexclusive .
the term ‚Äúexhaustive‚Äù means that the sample space has to cover allpossible outcomes. if
602.2. probability space
there is one possible outcome that is left out, then the set is no longer a sample space. the
term ‚Äúexclusive‚Äù means that the sample space contains unique elements so that there is no
repetition of elements.
example 2.24 . (counterexamples)
the following two examples are not sample spaces.
¬àthrow a die: œâ = {1,2,3}is not a sample space because it is not exhaustive .
¬àthrow a die: œâ = {1,1,2,3,4,5,6}is not a sample space because it is not exclu-
sive.
therefore, a valid sample space must contain all possible outcomes, and each element
must be unique.
we summarize the concept of a sample space as follows.
what is a sample space œâ?
¬àa sample space œâ is the collection of all possible outcomes.
¬àthe outcomes can be numbers, alphabets, vectors, or functions. the outcomes
can also be images, videos, eeg signals, audio speeches, etc.
¬àœâ must be exhaustive and exclusive.
2.2.2 event space f
the sample space contains all the possible outcomes. however, in many practical situations,
we are not interested in each of the individual outcomes; we are interested in the com-
binations of the outcomes. for example, when throwing a die, we may ask ‚Äúwhat is the
probability of rolling an odd number?‚Äù or ‚Äúwhat is the probability of rolling a number that
is less than 3?‚Äù clearly, ‚Äúodd number‚Äù is not an outcome of the experiment because the
possible outcomes are {
,
,
,
,
,
}. we call ‚Äúodd number‚Äù an event . an event must
be a subset in the sample space.
definition 2.14. anevent eis a subset in the sample space œâ. the set of all possible
events is denoted as f.
while this definition is extremely simple, we need to keep in mind a few facts about events.
first, an outcome Œæis an element in œâ but an event eis a subset contained in œâ, i.e.,
e‚äÜœâ. thus, an event can contain one outcome but it can also contain many outcomes.
the following example shows a few cases of events:
example 2.25 . throw a die. let œâ = {
,
,
,
,
,
}. the following are two pos-
sible events, as illustrated in figure 2.14 .
¬àe1={even numbers }={
,
,
}.
61chapter 2. probability
¬àe2={less than 3 }={
,
}.
figure 2.14: two examples of events: the first event contains numbers {2,4,6}, and the second
event contains numbers {1,2}.
practice exercise 2.10 . the ‚Äúping‚Äù command is used to measure round-trip times
for internet packets. what is the sample space of all possible round-trip times? what
is the event that a round-trip time is between 10 ms and 20 ms?
solution . the sample space is œâ = [0 ,‚àû). the event is e= [10 ,20].
practice exercise 2.11 . a cell phone tower has a circular average coverage area of
radius 10 km. we observe the source locations of calls received by the tower. what is
the event when the source location of a call is between 2 km and 5 km from the tower?
solution . assume that the center of the tower is located at ( x0, y0). the event is
e={(x, y)|2‚â§p
(x‚àíx0)2+ (y‚àíy0)2‚â§5}.
the second point we should remember is the cardinality of œâ and that of f. a sample
space containing nelements has a cardinality n. however, the event space constructed from
œâ will contain 2nevents. to see why this is so, let‚Äôs consider the following example.
example 2.26 . consider an experiment with 3 outcomes œâ = {‚ô£,‚ô°,‚ú†}. we can list
out all the possible events: ‚àÖ,{‚ô£},{‚ô°},{‚ú†},{‚ô£,‚ô°},{‚ô£,‚ú†},{‚ô°,‚ô£},{‚ô£,‚ô°,‚ú†}. so
in total there are 23= 8 possible events. figure 2.15 depicts the situation. what is
the difference between ‚ô£and{‚ô£}? the former is an element, whereas the latter is a
set. thus, {‚ô£}is an event but ‚ô£is not an event. why is ‚àÖan event? because we can
ask ‚Äúwhat is the probability that we get an odd number and an even number?‚Äù the
probability is obviously zero, but the reason it is zero is that the event is an empty set.
622.2. probability space
figure 2.15: the event space contains all the possible subsets inside the sample space.
in general, if there are nelements in the sample space, then the number of events
is 2n. to see why this is true, we can assign to each element a binary value: either 0 or 1.
for example, in table 2.1 we consider throwing a die. for each of the six faces, we assign a
binary code. this will give us a binary string for each event. for example, the event {
,
}
is encoded as the binary string 100010 because only
 and
 are activated. we can count
the total number of unique strings, which is the number of strings that can be constructed
from nbits. it is easily seen that this number is 2n.
event
 binary code
‚àÖ √ó √ó √ó √ó √ó √ó 000000
{
,
} ‚Éù √ó √ó √ó ‚Éù √ó 100010
{
,
,
} √ó √ó ‚Éù ‚Éù ‚Éù √ó 001110
............
{
,
,
,
,
}√ó ‚Éù ‚Éù ‚Éù ‚Éù ‚Éù 011111
{
,
,
,
,
,
}‚Éù ‚Éù ‚Éù ‚Éù ‚Éù ‚Éù 111111
table 2.1: an event space contains 2nevents, where nis the number of elements in the sample space.
to see this, we encode each outcome with a binary code. the resulting binary string then forms a unique
index of the event. counting the total number of events gives us the cardinality of the event space.
the box below summarizes what you need to know about event spaces.
what is an event space f?
¬àan event space fis the set of all possible subsets. it is a set of sets.
¬àwe need fbecause the probability law pis mapping a set to a number. pdoes
not take an outcome from œâ but a subset inside œâ.
63chapter 2. probability
event spaces: some advanced topics
the following discussions can be skipped if it is your first time reading the book.
what else do we need to take care of in order to ensure that an event is well defined? a
few set operations seem to be necessary. for example, if e1={
}ande2={
}are events,
it is necessary that e=e1‚à™e2={
,
}is an event too. another example: if e1={
,
}
ande2={
,
}are events, then it is necessary that e=e1‚à©e2={
}is also an event.
the third example: if e1={
,
,
,
}is an event, then e=ec
1={
,
}should be
an event. as you can see, there is nothing sophisticated in these examples. they are just
some basic set operations. we want to ensure that the event space is closed under these
set operations. that is, we do not want to be surprised by finding that a set constructed
from two events is not an event. however, since all set operations can be constructed from
union, intersection and complement, ensuring that the event space is closed under these
three operations effectively ensures that it is closed to allset operations.
the formal way to guarantee these is the notion of a field. this term may seem to be
abstract, but it is indeed quite useful:
definition 2.15. for an event space fto be valid, fmust be a fieldf. it is a field
if it satisfies the following conditions
¬à‚àÖ ‚àà f andœâ‚àà f.
¬à(closed under complement) if f‚àà f, then also fc‚àà f.
¬à(closed under union and intersection) if f1‚àà f andf2‚àà f, then f1‚à©f2‚àà f
andf1‚à™f2‚àà f.
for a finite set, i.e., a set that contains nelements, the collection of all possible subsets
is indeed a field. this is not difficult to see if you consider rolling a die. for example,
ife={
,
,
,
}is inside f, then ec={
,
}is also inside f. this is because f
consists of 2nsubsets each being encoded by a unique binary string. so if e= 001111, then
ec= 110000, which is also in f. similar reasoning applies to intersection and union.
at this point, you may ask:
¬àwhy bother constructing a field? the answer is that probability is a measure of the
size of a set, so we must input a set to a probability measure pto get a number. the
set being input to pmust be a subset inside the sample space; otherwise, it will be
undefined. if we regard pas a mapping, we need to specify the collection of all its
inputs, which is the set of all subsets, i.e., the event space. so if we do not define the
field, there is no way to define the measure p.
¬àwhat if the event space is not a field? if the event space is not a field, then we can
easily construct pathological cases where we cannot assign a probability. for example,
if the event space is not a field, then it would be possible that the complement of
e={
,
,
,
}(which is ec={
,
}) is not an event. this just does not make
sense.
the concept of a field is sufficient for finite sample spaces. however, there are two
other types of sample spaces where the concept of a field is inadequate. the first type of
642.2. probability space
sets consists of the countably infinite sets, and the second type consists of the sets defined
on the real line . there are other types of sets, but these two have important practical
applications. therefore, we need to have a basic understanding of these two types.
sigma-field
the difficulty of a countably infinite set is that there are infinitely many subsets in the field
of a countably infinite set. having a finite union and a finite intersection is insufficient to
ensure the closedness of all intersections and unions. in particular, having f1‚à™f2‚àà fdoes
not automatically give uss‚àû
n=1fn‚àà f because the latter is an infinite union. therefore,
for countably infinite sets, their requirements to be a field are more restrictive as we need
to ensure infinite intersection and union. the resulting field is called the œÉ-field.
definition 2.16. a sigma-field ( œÉ-field )fis a field such that
¬àfis a field, and
¬àiff1, f2, . . .‚àà f, then the unions‚àû
i=1fiand the intersectiont‚àû
i=1fiare both
inf.
when do we need a œÉ-field? when the sample space is countable and has infinitely
many elements. for example, if the sample space contains all integers, then the collection
of all possible subsets is a œÉ-field. for another, if e1={2},e2={4},e3={6}, . . . , thens‚àû
n=1en={2,4,6,8, . . .}={positive even numbers }. clearly, we wants‚àû
n=1ento live in
the sample space.
borel sigma-field
while a sigma-field allows us to consider countable sets of events, it is still insufficient for
considering events defined on the real line, e.g., time, as these events are not countable.
so how do we define an event on the real line? it turns out that we need a different way
to define the smallest unit . for finite sets and countable sets, the smallest units are the
elements themselves because we can count them. for the real line, we cannot count the
elements because any non-empty interval is uncountably infinite.
the smallest unit we use to construct a field for the real line is a semi-closed interval
(‚àí‚àû, b]def={x| ‚àí ‚àû < x‚â§b}.
theborel œÉ-field is defined as the sigma-field generated by the semi-closed inter-
vals.
definition 2.17. theborel œÉ-fieldbis aœÉ-field generated from semi-closed intervals:
(‚àí‚àû, b]def={x| ‚àí ‚àû < x‚â§b}.
the difference between the borel œÉ-fieldband a regular œÉ-field is how we measure the
subsets. in a œÉ-field, we count the elements in the subsets, whereas, in a borel œÉ-field, we
use the semi-closed intervals to measure the subsets.
65chapter 2. probability
being a field, the borel œÉ-field is closed under complement, union, and intersection. in
particular, subsets of the following forms are also in the borel œÉ-fieldb:
(a, b),[a, b],(a, b],[a, b),[a,‚àû),(a,‚àû),(‚àí‚àû, b],{b}.
for example, ( a,‚àû) can be constructed from ( ‚àí‚àû, a]c, and ( a, b] can be constructed by
taking the intersection of ( ‚àí‚àû, b] and ( a,‚àû).
example 2.27 : waiting for a bus. let œâ = {0‚â§t‚â§30}. the borel œÉ-field contains
all semi-closed intervals ( a, b], where 0 ‚â§a‚â§b‚â§30. here are two possible events:
¬àf1={less than 10 minutes }={0‚â§t <10}={0} ‚à™({0< t‚â§10} ‚à© {10}c).
¬àf2={more than 20 minutes }={20< t‚â§30}.
further discussion of the borel œÉ-field can be found in leon-garcia (3rd edition,)
chapter 2.9.
this is the end of the discussion. please join us again.
2.2.3 probability law p
the third component of a probability space is the probability law p. its job is to assign a
number to an event.
definition 2.18. aprobability law is a function p:f ‚Üí [0,1]of an event eto a
real number in [0,1].
the probability law is thus a function , and therefore we must specify the input and
the output. the input to pis an event e, which is a subset in œâ and an element in f. the
output of pis a number between 0 and 1, which we call the probability .
the definition above does not specify how an event is being mapped to a number.
however, since probability is a measure of the size of a set, a meaningful pshould be
consistent for all events in f. this requires some rules, known as the axioms of probability ,
when we define the p. any probability law pmust satisfy these axioms; otherwise, we will
see contradictions. we will discuss the axioms in the next section. for now, let us look at
two examples to make sure we understand the functional nature of p.
example 2.28 . consider flipping a coin. the event space is f={‚àÖ,{h},{t},œâ}.
we can define the probability law as
p[‚àÖ] = 0,p[{h}] =1
2,p[{t}] =1
2,p[œâ] = 1 ,
as shown in figure 2.16 . this pis clearly consistent for all the events in f.
is it possible to construct an invalid p? certainly. consider the following proba-
662.2. probability space
bility law:
p[‚àÖ] = 0,p[{h}] =1
3,p[{t}] =1
3,p[œâ] = 1 .
this law is invalid because the individual events are p[{h}] =1
3andp[{t}] =1
3
but the union is p[œâ] = 1. to fix this problem, one possible solution is to define the
probability law as
p[‚àÖ] = 0,p[{h}] =1
3,p[{t}] =2
3,p[œâ] = 1 .
then, the probabilities for all the events are well defined and consistent.
figure 2.16: a probability law is a mapping from an event to a number. a probability law cannot be
arbitrarily assigned; it must satisfy the axioms of probability.
example 2.29 . consider a sample space containing three elements œâ = {‚ô£,‚ô°,‚ú†}.
the event space is then f=
‚àÖ,{‚ô£},{‚ô°},{‚ú†},{‚ô£,‚ô°},{‚ô°,‚ú†},{‚ô£,‚ú†},{‚ô£,‚ô°,‚ú†}
.
one possible pwe could define would be
p[‚àÖ] = 0,p[{‚ô£}] =p[{‚ô°}] =p[{‚ú†}] =1
3,
p[{‚ô£,‚ô°}] =p[{‚ô£,‚ú†}] =p[{‚ô°,‚ú†}] =2
3,p[{‚ô£,‚ô°,‚ú†}] = 1.
what is a probability law p?
¬àa probability law pis afunction .
¬àit takes a subset (an element in f) and maps it to a number between 0 and 1.
¬àpis ameasure of the size of a set.
¬àforpto be valid, it must satisfy the axioms of probability .
67chapter 2. probability
figure 2.17: probability is a measure of the size of a set. the probability can be a counter that counts
the number of elements, a ruler that measures the length of an interval, or an integration that measures
the area of a region.
a probability law pis a measure
consider the word ‚Äúmeasure‚Äù in our slogan: probability is a measure of the size of a set .
depending on the nature of the set, the measure can be a counter, ruler, scale, or even a
stopwatch. so far, all the examples we have seen are based on sets with a finite number of
elements. for these sets, the natural choice of the probability measure is a counter. however,
if the sets are intervals on the real line or regions in a plane, we need a different probability
law to measure their size. let‚Äôs look at the examples shown in figure 2.17 .
example 2.30 (finite set) . consider throwing a die, so that
œâ ={
,
,
,
,
,
}.
then the probability measure is a counter that reports the number of elements. if
the die is fair, i.e., all the 6 faces have equal probability of happening, then an event
e={
,
}will have a probability p[e] =2
6.
example 2.31 (intervals) . suppose that the sample space is a unit interval œâ = [0 ,1].
letebe an event such that e= [a, b] where a, bare numbers in [0 ,1]. then the
probability measure is a ruler that measures the length of the intervals. if all the
numbers on the real line have equal probability of appearing, then p[e] =b‚àía.
example 2.32 (regions) . suppose that the sample space is the square œâ = [ ‚àí1,1]√ó
[‚àí1,1]. let ebe a circle such that e={(x, y)|x2+y2< r2}, where r <1. then the
probability measure is an area measure that returns us the area of e. if we assume
that all coordinates in œâ are equally probable, then p[e] =œÄr2, forr <1.
because probability is a measure of the size of a set, two sets can be compared according
to their probability measures. for example, if œâ = {‚ô£,‚ô°,‚ú†}, and if e1={‚ô£} ande2=
{‚ô£,‚ô°}, then one possible pis to assign p[e1] =p[{‚ô£}] =1
3andp[e2] =p[{‚ô£,‚ô°}] = 2/3.
682.2. probability space
in this particular case, we see that e1‚äÜe2and thus
p[e1]‚â§p[e2].
let‚Äôs now consider the term ‚Äúsize.‚Äù notice that the concept of the size of a set is not
limited to the number of elements. a better way to think about size is to imagine that it is
the weight of the set. this might may seem fanciful at first, but it is quite natural. consider
the following example.
example 2.33 . (discrete events with different weights ) suppose we have a sample
space œâ = {‚ô£,‚ô°,‚ú†}. let us assign a different probability to each outcome:
p[{‚ô£}] =2
6,p[{‚ô°}] =1
6,p[{‚ú†}] =3
6.
as illustrated in figure 2.18 , since each outcome has a different weight, when de-
termining the probability of a set of outcomes we can add these weights (instead of
counting the number of outcomes). for example, when reporting p[{‚ô£}] we find its
weight p[{‚ô£}] =2
6, whereas when reporting p[{‚ô°,‚ú†}] we find the sum of their weights
p[{‚ô°,‚ú†}] =1
6+3
6=4
6. therefore, the notion of size does not refer to the number of
elements but to the total weight of these elements.
figure 2.18: this example shows the ‚Äúweights‚Äù of three elements in a set. the weights are numbers
between 0 and 1 such that the sum is 1. when applying a probability measure to this set, we sum the
weights for the elements in the events being considered. for example, p[‚ô°,‚ú†] =yellow + green, and
p[‚ô£] =purple.
example 2.34 . (continuous events with different weights ) suppose that the sample
space is an interval, say œâ = [ ‚àí1,1]. on this interval we define a weighting function
f(x) where f(x0) specifies the weight for x0. because œâ is an interval, events defined
on this œâ must also be intervals. for example, we can consider two events e1= [a, b]
ande2= [c, d]. the probabilities of these events are p[e1] =rb
af(x)dxandp[e2] =rd
cf(x)dx, as shown in figure 2.19 .
viewing probability as a measure is not just a game for mathematicians; rather, it
has fundamental significance for several reasons. first, it eliminates any dependency on
probability as relative frequency from the frequentist point of view. relative frequency is a
69chapter 2. probability
figure 2.19: if the sample space is an interval on the real line, then the probability of an event is the
area under the curve of the weighting function.
narrowly defined concept that is largely limited to discrete events, e.g., flipping a coin. while
we can assign weights to coin-toss events to deal with those biased coins, the extension to
continuous events becomes problematic. by thinking of probability as a measure, we can
generalize the notion to apply to intervals, areas, volumes, and so on.
second, viewing probability as a measure forces us to disentangle an event from mea-
sures . an event is a subset in the sample space. it has nothing to do with the measure
(e.g., a ruler) you use to measure the event. the measure, on the other hand, specifies the
weighting function you apply to measure the event when computing the probability. for
example, let œâ = [ ‚àí1,1] be an interval, and let e= [a, b] be an event. we can define two
weighting functions f(x) and g(x). correspondingly, we will have two different probability
measures fandgsuch that
f([a, b]) =z
edf=zb
af(x)dx,
g([a, b]) =z
edg=zb
ag(x)dx. (2.20)
to make sense of these notations, consider only p[[a, b]] and not f([a, b]) and g([a, b]). as you
can see, the event for both measures is e= [a, b] but the measures are different. therefore,
the values of the probability are different.
example 2.35 . (two probability laws are different if their weighting functions are
different .) consider two different weighting functions for throwing a die. the first one
assigns probability as the following:
p[{
}] =1
12,p[{
}] =2
12,p[{
}] =3
12,
p[{
}] =4
12,p[{
}] =1
12,p[{
}] =1
12,
whereas the second function assigns the probability like this:
p[{
}] =2
12,p[{
}] =2
12,p[{
}] =2
12,
p[{
}] =2
12,p[{
}] =2
12,p[{
}] =2
12.
702.2. probability space
let an event e={
,
}. letfbe the measure using the first set of probabilities, and
letgbe the measure of the second set of probabilities. then,
f(e) =f({
,
}) =1
12+2
12=3
12,
g(e) =g({
,
}) =2
12+2
12=4
12.
therefore, although the events are the same, the two different measures will give us
two different probability values.
remark . the notationr
edfin equation (2.20) is known as the lebesgue integral . you
should be aware of this notation, but the theory of lebesgue measure is beyond the scope
of this book.
2.2.4 measure zero sets
understanding the measure perspective on probability allows us to understand another
important concept of probability, namely measure zero sets . to introduce this concept, we
pose the question: what is the probability of obtaining a single point, say {0.5}, when the
sample space is œâ = [0 ,1]?
the answer to this question is rooted in the compatibility between the measure and
the sample space. in other words, the measure has to be meaningful for the events in the
sample space. using œâ = [0 ,1], since œâ is an interval, an appropriate measure would be the
length of this interval. you may add different weighting functions to define your measure,
but ultimately, the measure must be an integral. if you use a ‚Äúcounter‚Äù as a measure, then
the counter and the interval are not compatible because you cannot count on the real line.
now, suppose that we define a measure for œâ = [0 ,1] using a weighting function f(x).
this measure is determined by an integration. then, for e={0.5}, the measure is
p[e] =p[{0.5}] =z0.5
0.5f(x)dx= 0.
in fact, for any weighting function the integral will be zero because the length of the set
eis zero.1an event that gives us zero probability is known as an event with measure 0 .
figure 2.20 shows an example.
figure 2.20: the probability of obtaining a single point in a continuous interval is zero.
1we assume that fis continuous throughout [0 ,1]. if fis discontinuous at x= 0.5, some additional
considerations will apply.
71chapter 2. probability
what are measure zero sets?
¬àa set e(non-empty) is called a measure zero set when p[e] = 0.
¬àfor example, {0}is a measure zero set when we use a continuous measure f.
¬àbut{0}can have a positive measure when we use a discrete measure g.
example 2.36(a) . consider a fair die with œâ = {
,
,
,
,
,
}. then the set {
}
has a probability of1
6. the sample space does not have a measure zero event because
the measure we use is a counter.
example 2.36(b) . consider an interval with œâ = [1 ,6]. then the set {1}has measure
0 because it is an isolated point with respect to the sample space.
example 2.36(c) . for any intervals, p[[a, b]] =p[(a, b)] because the two end points
have measure zero: p[{a}] =p[{b}] = 0.
formal definitions of measure zero sets
the following discussion of the formal definitions of measure zero sets is optional for the
first reading of this book.
we can formally define measure zero sets as follows:
definition 2.19. letœâbe the sample space. a set a‚ààœâis said to have measure
zero if for any given œµ >0,
¬àthere exists a countable number of subsets ansuch that a‚äÜ ‚à™‚àû
n=1an, and
¬àp‚àû
n=1p[an]< œµ.
you may need to read this definition carefully. suppose we have an event a. we construct
a set of neighbors a1, . . . , a ‚àûsuch that ais included in the union ‚à™‚àû
n=1an. if the sum of
the all p[an] is still less than œµ, then the set awill have a measure zero.
to understand the difference between a measure for a continuous set and a countable
set, consider figure 2.21 . on the left side of figure 2.21 we show an interval œâ in which there
is an isolated point x0. the measure for this œâ is the length of the interval (relative to what-
ever weighting function you use). we define a small neighborhood a0= (x0‚àíœµ
2, x0+œµ
2)
surrounding x0. the length of this interval is not more than œµ. we then shrink œµ. how-
ever, regardless of how small œµis, since x0is an isolated point, it is always included in the
neighborhood. therefore, the definition is satisfied, and so {x0}has measure zero.
example 2.37 . let œâ = [0 ,1]. the set {0.5} ‚äÇœâ has measure zero, i.e., p[{0.5}] = 0.
to see this, we draw a small interval around 0.5, say [0 .5‚àíœµ/3,0.5 +œµ/3]. inside this
interval, there is really nothing to measure besides the point 0.5. thus we have found
an interval such that it contains 0.5, and the probability is p[[0.5‚àíœµ/3,0.5 +œµ/3]] =
722.2. probability space
2œµ/3< œµ. therefore, by definition, the set {0.5}has measure 0.
the situation is very different for the right-hand side of figure 2.21 . here, the measure
is not the length but a counter. so if we create a neighborhood surrounding the isolated
point x0, we can always make a count. as a result, if you shrink œµto become a very small
number (in this case less than1
4), then p[{x0}]< œµwill no longer be true. therefore, the
set{x0}has a non-zero measure when we use the counter as the measure.
figure 2.21: [left] for a continuous sample space, a single point event {x0}can always be surrounded
by a neighborhood a0whose size p[a0]< œµ. [right] if you change the sample space to discrete
elements, then a single point event {x0}can still be surrounded by a neighborhood a0. however, the
sizep[a0] = 1/4is a fixed number and will not work for anyœµ.
when we make probabilistic claims without considering the measure zero sets, we say
that an event happens almost surely .
definition 2.20. an event a‚ààris said to hold almost surely (a.s.) if
p[a] = 1 (2.21)
except for all measure zero sets in r.
therefore, if a set acontains measure zero subsets, we can simply ignore them because they
do not affect the probability of events. in this book, we will omit ‚Äúa.s.‚Äù if the context is
clear.
example 2.38(a) . let œâ = [0 ,1]. then p[(0,1)] = 1 almost surely because the points
0 and 1 have measure zero in œâ.
example 2.38(b) . let œâ = {x|x2‚â§1}and let a={x|x2<1}. then p[a] = 1
almost surely because the circumference has measure zero in œâ.
practice exercise 2.12 . let œâ = {f:r‚Üí[‚àí1,1]|f(t) = cos( œâ0t+Œ∏)}, where œâ0is
a fixed constant and Œ∏is random. construct a measure zero event and an almost sure
event.
solution . let
e={f:r‚Üí[‚àí1,1]|f(t) = cos( œâ0t+kœÄ/2)}
for any integer k. that is, econtains all the functions with a phase of œÄ/2, 2œÄ/2, 3œÄ/2,
etc. then ewill have measure zero because it is a countable set of isolated functions.
the event ecwill have probability p[ec] = 1 almost surely because ehas measure
73chapter 2. probability
zero.
this is the end of the discussion. please join us again.
2.2.5 summary of the probability space
after the preceding long journey through theory, let us summarize.
first, it is extremely important to understand our slogan: probability is a measure of
the size of a set . this slogan is precise, but it needs clarification. when we say probability
is ameasure , we are thinking of it as being the probability law p. of course, in practice, we
always think of probability as the number returned by the measure. however, the difference
is not crucial. also, ‚Äúsize‚Äù not only means the number of elements in the set, but it also
means the relative weight of the set in the sample space. for example, if we use a weight
function to weigh the set elements, then size would refer to the overall weight of the set.
when we put all these pieces together, we can understand why a probability space
must consist of the three components
(œâ,f,p), (2.22)
where œâ is the sample space that defines all possible outcomes, fis the event space generated
from œâ, and pis the probability law that maps an event to a number in [0 ,1]. can we drop
one or more of the three components? we cannot! if we do not specify the sample space œâ,
then there is no way to define the events. if we do not have a complete event space f,
then some events will become undefined, and further, if the probability law is applied only
to outcomes, we will not be able to define the probability for events. finally, if we do not
specify the probability law, then we do not have a way to assign probabilities.
2.3 axioms of probability
we now turn to a deeper examination of the properties. our motivation is simple. while
the definition of probability law has achieved its goal of assigning a probability to an event,
there must be restrictions on how the assignment can be made. for example, if we set
p[{h}] = 1 /3, then p[{t}] must be 2 /3; otherwise, the sum of having a head and a tail
will be greater than 1. the necessary restrictions on assigning a probability to an event are
collectively known as the axioms of probability .
definition 2.21. aprobability law is a function p:f ‚Üí [0,1]that maps an event
ato a real number in [0,1]. the function must satisfy the axioms of probability :
i.non-negativity :p[a]‚â•0, for any a‚äÜœâ.
ii.normalization :p[œâ] = 1 .
742.3. axioms of probability
iii.additivity : for any disjoint sets {a1, a2, . . .}, it must be true that
p"‚àû[
i=1ai#
=‚àûx
i=1p[ai]. (2.23)
an axiom is a proposition that serves as a premise or starting point in a logical system.
axioms are not definitions, nor are they theorems. they are believed to be true or true
within a certain context. in our case, the axioms are true within the context of bayesian
probability. the kolmogorov probability relies on another set of axioms. we will not dive
into the details of these historical issues; in this book, we will confine our discussion to the
three axioms given above.
2.3.1 why these three probability axioms?
why do we need three axioms? why not just two axioms? why these three particular
axioms? the reasons are summarized in the box below.
why these three axioms?
¬àaxiom i (non-negativity) ensures that probability is never negative.
¬àaxiom ii (normalization) ensures that probability is never greater than 1.
¬àaxiom iii (additivity) allows us to add probabilities when two events do not
overlap.
axiom i is called the non-negativity axiom. it ensures that a probability value cannot
be negative. non-negativity is a must for probability. it is meaningless to say that the
probability of getting an event is a negative number.
axiom ii is called the normalization axiom. it ensures that the probability of observing
all possible outcomes is 1. this gives the upper limit of the probability. the upper limit
does not have to be 1. it could be 10 or 100. as long as we are consistent about this upper
limit, we are good. however, for historical reasons and convenience, we choose 1 to be the
upper limit.
axiom iii is called the additivity axiom and is the most critical one among the three.
the additivity axiom defines how set operations can be translated into probability oper-
ations. in a nutshell, it says that if we have a set of disjoint events, the probabilities can
be added. from the measure perspective, axiom iii makes sense because if pmeasures the
size of an event, then two disjoint events should have their probabilities added. if two dis-
joint events do not allow their probabilities to be added, then there is no way to measure
a combined event. similarly, if the probabilities can somehow be added even for overlap-
ping events, there will be inconsistencies because there is no systematic way to handle the
overlapping regions.
thecountable additivity stated in axiom iii can be applied to both a finite number
or an infinite number of sets. the finite case states that for any two disjoint sets aandb,
we have
p[a‚à™b] =p[a] +p[b]. (2.24)
75chapter 2. probability
in other words, if aandbare disjoint, then the probability of observing either aorbis
the sum of the two individual probabilities. figure 2.22 illustrates this idea.
example 2.39 . let‚Äôs see why axiom iii is critical. consider throwing a fair die with
œâ ={
,
,
,
,
,
}. the probability of getting {
,
}is
p[{
,
}] =p[{
} ‚à™ {
 }] =p[{
}] +p[{
}] =1
6+1
6=2
6.
in this equation, the second equality holds because the events {
}and{
}are disjoint.
if we do not have axiom iii, then we cannot addprobabilities.
figure 2.22: axiom iii says p[a‚à™b] =p[a] +p[b]ifa‚à©b=‚àÖ.
2.3.2 axioms through the lens of measure
axioms are ‚Äúrules‚Äù we must abide by when we construct a measure. therefore, any valid
measure must be compatible with the axioms, regardless of whether we have a weighting
function or not. in the following two examples, we will see how the weighting functions are
used in the axioms.
example 2.40 . consider a sample space with œâ = {‚ô£,‚ô°,‚ú†}. the probability for
each outcome is
p[{‚ô£}] =2
6,p[{‚ô°}] =1
6,p[{‚ú†}] =3
6.
suppose we construct two disjoint events e1={‚ô£,‚ô°}ande2={‚ú†}. then axiom
iii says
p[e1‚à™e2] =p[e1] +p[e2] =2
6+1
6
+3
6= 1.
note that in this calculation, the measure pis still a measure p. if we endow it
with a nonuniform weight function, then papplies the corresponding weights to the
corresponding outcomes. this process is compatible with the axioms. see figure 2.23
for a pictorial illustration.
762.3. axioms of probability
example 2.41 . suppose the sample space is an interval œâ = [0 ,1]. the two events
aree1= [a, b] and e2= [c, d]. assume that the measure puses a weighting function
f(x). then, by axiom iii, we know that
p[e1‚à™e2] =p[e1] +p[e2]
=p[[a, b]] +p[[c, d]] (by axiom 3)
=zb
af(x)dx+zd
cf(x)dx, (apply the measure) .
as you can see, there is no conflict between the axioms and the measure. figure 2.24
illustrates this example.
figure 2.23: applying weighting functions to the measures: suppose we have three elements in the set.
to compute the probability p[{‚ô°,‚ú†} ‚à™ {‚ô£} ], we can write it as the sum of p[{‚ô°,‚ú†}]andp[{‚ô£}].
figure 2.24: the axioms are compatible with the measure, even if we use a weighting function.
2.3.3 corollaries derived from the axioms
the union of aandbis equivalent to the logical operator ‚Äúor‚Äù. once the logical operation
‚Äúor‚Äù is defined, all other logical operations can be defined. the following corollaries are
examples.
corollary 2.1. leta‚àà f be an event. then,
(a)p[ac] = 1‚àíp[a].
(b)p[a]‚â§1.
(c)p[‚àÖ] = 0.
77chapter 2. probability
proof . (a) since œâ = a‚à™ac, by finite additivity we have p[œâ] =p[a‚à™ac] =p[a] +p[ac].
by the normalization axiom, we have p[œâ] = 1. therefore, p[ac] = 1‚àíp[a].
(b) we prove by contradiction. assume p[a]>1. consider the complement acwhere
a‚à™ac= œâ. since p[ac] = 1‚àíp[a], we must have p[ac]<0 because by hypothesis p[a]>1.
butp[ac]<0 violates the non-negativity axiom. so we must have p[a]‚â§1.
(c) since œâ = œâ ‚à™ ‚àÖ, by the first corollary we have p[‚àÖ] = 1‚àíp[œâ] = 0.
‚ñ°
corollary 2.2 (unions of two non-disjoint sets ).for any aandbinf,
p[a‚à™b] =p[a] +p[b]‚àíp[a‚à©b]. (2.25)
this statement is different from axiom iii because aandbare not necessarily disjoint.
figure 2.25: for any aandb,p[a‚à™b] =p[a] +p[b]‚àíp[a‚à©b].
proof . first, observe that a‚à™bcan be partitioned into three disjoint subsets as a‚à™b=
(a\b)‚à™(a‚à©b)‚à™(b\a). since a\b=a‚à©bcandb\a=b‚à©ac, by finite additivity we
have that
p[a‚à™b] =p[a\b] +p[a‚à©b] +p[b\a] =p[a‚à©bc] +p[a‚à©b] +p[b‚à©ac]
(a)=p[a‚à©bc] +p[a‚à©b] +p[b‚à©ac] +p[a‚à©b]‚àíp[a‚à©b]
(b)=p[a‚à©(bc‚à™b)] +p[(ac‚à™a)‚à©b]‚àíp[a‚à©b]
=p[a‚à©œâ] +p[œâ‚à©b]‚àíp[a‚à©b] =p[a] +p[b]‚àíp[a‚à©b],
where in (a) we added and subtracted a term p[a‚à©b], and in (b) we used finite additivity
so that p[a‚à©bc] +p[a‚à©b] =p[(a‚à©bc)‚à™(a‚à©b)] =p[a‚à©(bc‚à™b)].
‚ñ°
example 2.42 . the corollary is easy to understand if we consider the following ex-
ample. let œâ = {
,
,
,
,
,
}be the sample space of a fair die. let a={
,
,
}
andb={
,
,
}. then
p[a‚à™b] =p[{
,
,
,
,
}] =5
6.
782.3. axioms of probability
we can also use the corollary to obtain the same result:
p[a‚à™b] =p[a] +p[b]‚àíp[a‚à©b]
=p[{
,
,
}] +p[{
,
,
}]‚àíp[{
}]
=3
6+3
6‚àí1
6=5
6.
corollary 2.3 (inequalities) .letaandbbe two events in f. then,
(a)p[a‚à™b]‚â§p[a] +p[b]. (union bound)
(b) if a‚äÜb, then p[a]‚â§p[b].
proof . (a) since p[a‚à™b] =p[a]+p[b]‚àíp[a‚à©b] and by non-negativity axiom p[a‚à©b]‚â•0,
we must have p[a‚à™b]‚â§p[a] +p[b]. (b) if a‚äÜb, then there exists a set b\asuch that
b=a‚à™(b\a). therefore, by finite additivity we have p[b] =p[a]+p[b\a]‚â•p[a]. since
p[b\a]‚â•0, it follows that p[a] +p[b\a]‚â•p[a]. thus we have p[b]‚â•p[a].
‚ñ°
union bound is a frequently used tool for analyzing probabilities when the intersection
a‚à©bis difficult to evaluate. part (b) is useful when considering two events of different
‚Äúsizes.‚Äù for example, in the bus-waiting example, if we let a={t‚â§5}, and b={t‚â§10},
thenp[a]‚â§p[b] because we have to wait for the first 5 minutes to go into the remaining
5 minutes.
practice exercise 2.13 . let the events aandbhavep[a] =x,p[b] =yand
p[a‚à™b] =z. find the following probabilities: p[a‚à©b],p[ac‚à™bc], and p[a‚à©bc].
solution .
(a) note that z=p[a‚à™b] =p[a] +p[b]‚àíp[a‚à©b]. thus, p[a‚à©b] =x+y‚àíz.
(b) we can take the complement to obtain the result:
p[ac‚à™bc] = 1‚àíp[(ac‚à™bc)c] = 1‚àíp[a‚à©b] = 1‚àíx‚àíy+z.
(c)p[a‚à©bc] =p[a]‚àíp[a‚à©b] =x‚àí(x+y‚àíz) =z‚àíy.
practice exercise 2.14 . consider a sample space
œâ ={f:r‚Üír|f(x) =ax,for all a‚ààr, x‚ààr}.
there are two events: a={f|f(x) =ax, a ‚â•0}, and b={f|f(x) =ax, a ‚â§0}.
so, basically, ais the set of all straight lines with positive slope, and bis the set of
straight lines with negative slope. show that the union bound is tight.
79chapter 2. probability
solution . first of all, we note that
p[a‚à™b] =p[a] +p[b]‚àíp[a‚à©b].
the intersection is
p[a‚à©b] =p[{f|f(x) = 0}].
since this is a point set in the real line, it has measure zero. thus, p[a‚à©b] = 0 and
hence p[a‚à™b] =p[a] +p[b]. so the union bound is tight.
closing remark . the development of today‚Äôs probability theory is generally credited to
andrey kolmogorov‚Äôs 1933 book foundations of the theory of probability . we close this
section by citing one of the tables of the book. the table summarizes the correspondence
between set theory and random events.
theory of sets random events
aandbare disjoint, i.e., a‚à©b=‚àÖevents aandbare incompatible
a1‚à©a2¬∑¬∑¬∑ ‚à© an=‚àÖ events a1, . . . , a nare incompatible
a1‚à©a2¬∑¬∑¬∑ ‚à© an=x event xis defined as the simultaneous occur-
rence of events a1, . . . , a n
a1‚à™a2¬∑¬∑¬∑ ‚à™ an=x event xis defined as the occurrence of at least
one of the events a1, . . . , a n
acthe opposite event acconsisting of the non-
occurrence of event a
a=‚àÖ event ais impossible
a= œâ event amust occur
a1, . . . , a nform a partition of œâ the experiment consists of determining which
of the events a1, . . . , a noccurs
b‚äÇa from the occurrence of event bfollows the
inevitable occurrence of a
table 2.2: kolmogorov‚Äôs summary of set theory results and random events.
2.4 conditional probability
in many practical data science problems, we are interested in the relationship between two
or more events. for example, an event amay cause bto happen, and bmay cause c
to happen. a legitimate question in probability is then: if ahas happened, what is the
probability that balso happens? of course, if aandbare correlated events, then knowing
one event can tell us something about the other event. if the two events have no relationship,
knowing one event will not tell us anything about the other.
in this section, we study the concept of conditional probability . there are three sub-
topics in this section. we summarize the key points below.
802.4. conditional probability
the three main messages of this section are:
¬àsection 2.4.1: conditional probability . conditional probability of agiven bis
p[a|b] =p[a‚à©b]
p[b].
¬àsection 2.4.2: independence . two events are independent if the occurrence of
one does not influence the occurrence of the other: p[a|b] =p[a].
¬àsection 2.4.3: bayes‚Äô theorem and the law of total probability . bayes‚Äô theorem
allows us to switch the order of the conditioning: p[a|b] vs.p[b|a], whereas the
law of total probability allows us to decompose an event into smaller events.
2.4.1 definition of conditional probability
we start by defining conditional probability .
definition 2.22. consider two events aandb. assume p[b]Ã∏= 0. the conditional
probability ofagiven bis
p[a|b]def=p[a‚à©b]
p[b]. (2.26)
according to this definition, the conditional probability of agiven bis the ratio of
p[a‚à©b] top[b]. it is the probability that ahappens when we know that bhas already
happened. since bhas already happened, the event that ahas also happened is represented
bya‚à©b. however, since we are only interested in the relative probability of awith respect
tob, we need to normalize using b. this can be seen by comparing p[a|b] andp[a‚à©b]:
p[a|b] =p[a‚à©b]
p[b]andp[a‚à©b] =p[a‚à©b]
p[œâ]. (2.27)
the difference is illustrated in figure 2.26 : the intersection p[a‚à©b] calculates the overlap-
ping area of the two events. we make no assumptions about the cause-effect relationship.
figure 2.26: illustration of conditional probability and its comparison with p[a‚à©b].
what justifies this ratio? suppose that bhas already happened. then, anything out-
sidebwill immediately become irrelevant as far as the relationship between aandbis
concerned. so when we ask: ‚Äúwhat is the probability that ahappens given that bhas
happened?‚Äù, we are effectively asking for the probability that a‚à©bhappens under the
81chapter 2. probability
condition that bhas happened. note that we need to consider a‚à©bbecause we know
that bhas already happened. if we take aonly, then there exists a region a\bwhich
does not contain anything about b. however, since we know that bhas happened, a\bis
impossible. in other words, among the elements of a, only those that appear in a‚à©bare
meaningful.
example 2.43 . let
a={purdue gets big ten championship },
b={purdue wins 15 games consecutively }.
in this example,
p[a] = prob. that purdue gets the championship ,
p[b] = prob. that purdue wins 15 games consecutively ,
p[a‚à©b] = prob. that purdue gets the championship and wins 15 games ,
p[a|b] = prob. that purdue gets the championship given that
purdue won 15 games .
if purdue has won 15 games consecutively, then it is unlikely that purdue will get
the championship because the sample space of all possible competition results is large.
however, if we have already won 15 games consecutively, then the denominator of the
probability becomes much smaller. in this case, the conditional probability is high.
example 2.44 . consider throwing a die. let
a={getting a 3 }and b={getting an odd number }.
findp[a|b] andp[b|a].
solution . the following probabilities are easy to calculate:
p[a] =p[{
}] =1
6, and p[b] =p[{
,
,
}] =3
6.
also, the intersection is
p[a‚à©b] =p[{
}] =1
6.
given these values, the conditional probability of agiven bcan be calculated as
p[a|b] =p[a‚à©b]
p[b]=1
6
3
6=1
3.
in other words, if we know that we have an odd number, then the probability of
obtaining a 3 has to be computed over {
,
,
}, which give us a probability1
3. if we
822.4. conditional probability
do not know that we have an odd number, then the probability of obtaining a 3 has
to be computed from the sample space {
,
,
,
,
,
}, which will give us1
6.
the other conditional probability is
p[b|a] =p[a‚à©b]
p[a]= 1.
therefore, if we know that we have rolled a 3, then the probability for this number
being an odd number is 1.
example 2.45 . consider the situation shown in figure 2.27 . there are 12 points
with equal probabilities of happening. find the probabilities p[a|b] andp[b|a].
solution . in this example, we can first calculate the individual probabilities:
p[a] =5
12,andp[b] =6
12,andp[a‚à©b] =2
12.
then the conditional probabilities are
p[a|b] =p[a‚à©b]
p[b]=2
12
6
12=1
3,
p[b|a] =p[a‚à©b]
p[a]=2
12
5
12=2
5.
figure 2.27: visualization of example 2.45: [left] all the sets. [middle] p(a|b)is the ratio between
dots inside the light yellow region over those in yellow, which is2
6. [right] p[a|b]is the ratio between
dots inside the light pink region over those in pink, which is2
5.
example 2.46 . consider a tetrahedral (4-sided) die. let xbe the first roll and y
be the second roll. let bbe the event that min( x, y) = 2 and mbe the event that
max( x, y) = 3. find p[m|b].
solution . as shown in figure 2.28 , the event bis highlighted in green. (why?)
similarly, the event mis highlighted in blue. (again, why?) therefore, the probability
83chapter 2. probability
is
p[m|b] =p[m‚à©b]
p[b]=2
16
5
16=2
5.
figure 2.28: visualization of example 2.46. [left] event b. [middle] event m. [right] p(m|b)is the
ratio of the number of blue squares inside the green region to the total number of green squares, which
is2
5.
remark . notice that if p[b]‚â§p[œâ], then p[a|b] is always larger than or equal to p[a‚à©b],
i.e.,
p[a|b]‚â•p[a‚à©b].
conditional probabilities are legitimate probabilities
conditional probabilities are legitimate probabilities. that is, given b, the probability
p[a|b] satisfies axioms i, ii, iii.
theorem 2.6. letp[b]>0. the conditional probability p[a|b]satisfies axioms i,
ii, and iii.
proof . let‚Äôs check the axioms:
¬àaxiom i: we want to show
p[a|b] =p[a‚à©b]
p[b]‚â•0.
sincep[b]>0 and axiom i requires p[a‚à©b]‚â•0, we therefore have p[a|b]‚â•0.
¬àaxiom ii:
p[œâ|b] =p[œâ‚à©b]
p[b]
=p[b]
p[b]= 1.
842.4. conditional probability
¬àaxiom iii: consider two disjoint sets aandc. then,
p[a‚à™c|b] =p[(a‚à™c)‚à©b]
p[b]
=p[(a‚à©b)‚à™(c‚à©b)]
p[b]
(a)=p[a‚à©b]
p[b]+p[c‚à©b]
p[b]
=p[a|b] +p[c|b],
where ( a) holds because if aandcare disjoint then ( a‚à©b)‚à©(c‚à©b) =‚àÖ.
‚ñ°
to summarize this subsection, we highlight the essence of conditional probability.
what are conditional probabilities?
¬àconditional probability of agiven bis the ratiop[a‚à©b]
p[b].
¬àit is again a measure . it measures the relative size of ainside b.
¬àbecause it is a measure, it must satisfy the three axioms.
2.4.2 independence
conditional probability deals with situations where two events aandbare related. what
if the two events are unrelated? in probability, we have a technical term for this situation:
statistical independence .
definition 2.23. two events aandbare statistically independent if
p[a‚à©b] =p[a]p[b]. (2.28)
why define independence in this way? recall that p[a|b] =p[a‚à©b]
p[b]. ifaandbare
independent, then p[a‚à©b] =p[a]p[b] and so
p[a|b] =p[a‚à©b]
p[b]=p[a]p[b]
p[b]=p[a]. (2.29)
this suggests an interpretation of independence: if the occurrence of bprovides no addi-
tional information about the occurrence of a, then aandbare independent.
therefore, we can define independence via conditional probability:
definition 2.24. letaandbbe two events such that p[a]>0andp[b]>0. then
85chapter 2. probability
aandbareindependent if
p[a|b] =p[a]orp[b|a] =p[b]. (2.30)
the two statements are equivalent as long as p[a]>0 and p[b]>0. this is because
p[a|b] =p[a‚à©b]/p[b]. ifp[a|b] =p[a] then p[a‚à©b] =p[a]p[b], which implies that
p[b|a] =p[a‚à©b]/p[a] =p[b].
a pictorial illustration of independence is given in figure 2.29 . the key message is that
if two events aandbare independent, then p[a|b] =p[a]. the conditional probability
p[a|b] is the ratio of p[a‚à©b] over p[b], which is the intersection over b(the blue set).
the probability p[a] is the yellow set over the sample space œâ.
figure 2.29: independence means that the conditional probability p[a|b]is the same as p[a]. this
implies that the ratio of p[a‚à©b]overp[b], and the ratio of p[a‚à©œâ]overp[œâ]are the same.
disjoint versus independent
disjoint ‚áéindependent . (2.31)
the statement says that disjoint and independent are two completely different concepts.
ifaandbare disjoint, then a‚à©b=‚àÖ. this only implies that p[a‚à©b] = 0.
however, it says nothing about whether p[a‚à©b] can be factorized into p[a]p[b]. ifa
andbare independent, then we have p[a‚à©b] =p[a]p[b]. but this does not imply that
p[a‚à©b] = 0. the only condition under which disjoint ‚áîindependence is when p[a] = 0 or
p[b] = 0. figure 2.30 depicts the situation. when two sets are independent, the conditional
probability (which is a ratio) remains unchanged compared to unconditioned probability.
when two sets are disjoint, they simply do not overlap.
practice exercise 2.15 . throw a die twice. are aandbindependent, where
a={1st die is 3 }and b={2nd die is 4 }.
solution . we can show that
p[a‚à©b] =p[(3,4)] =1
36,p[a] =1
6,andp[b] =1
6.
sop[a‚à©b] =p[a]p[b]. thus, aandbare independent.
862.4. conditional probability
figure 2.30: independent means that the conditional probability, which is a ratio, is the same as the
unconditioned probability. disjoint means that the two sets do not overlap.
figure 2.31: the two events aandbare independent because p[a] =1
6andp[a|b] =1
6.
a pictorial illustration of this example is shown in figure 2.31 . the two events are
independent because ais one row in the 2d space, which yields a probability of1
6. the
conditional probability p[a|b] is the coordinate (3 ,4) over the event b, which is a column.
it happens that p[a|b] =1
6. thus, the two events are independent.
practice exercise 2.16 . throw a die twice. are aandbindependent?
a={1st die is 3 }and b={sum is 7 }.
solution . note that
p[a‚à©b] =p[(3,4)] =1
36, p[a] =1
6,
p[b] =p[(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)] =1
6.
sop[a‚à©b] =p[a]p[b]. thus, aandbare independent.
a pictorial illustration of this example is shown in figure 2.32 . notice that whether the
two events intersect is not how we determine independence (that only determines disjoint or
87chapter 2. probability
not). the key is whether the conditional probability (which is the ratio) remains unchanged
compared to the unconditioned probability.
figure 2.32: the two events aandbare independent because p[a] =1
6andp[a‚à©b] =1
6.
if we let b={sum is 8 }, then the situation is different. the intersection a‚à©bhas a
probability1
5relative to b, and therefore p[a|b] =1
5. hence, the two events aandbare
dependent. if you like a more intuitive argument, you can imagine that bhas happened,
i.e., the sum is 8. then the probability for the first die to be 1 is 0 because there is no way
to construct 8 when the first die is 1. as a result, we have eliminated one choice for the first
die, leaving only five options. therefore, since bhas influenced the probability of a, they
are dependent.
practice exercise 2.17 . throw a die twice. let
a={max is 2 }and b={min is 2 }.
areaandbindependent?
solution . let us first list out aandb:
a={(1,2),(2,1),(2,2)},
b={(2,2),(2,3),(2,4),(2,5),(2,6),(3,2),(4,2),(5,2),(6,2)}.
therefore, the probabilities are
p[a] =3
36,p[b] =9
36,andp[a‚à©b] =p[(2,2)] =1
36.
clearly, p[a‚à©b]Ã∏=p[a]p[b] and so aandbare dependent.
what is independence?
¬àtwo events are independent when the ratiop[a‚à©b]/p[b]remains unchanged
compared to p[a].
¬àindependence Ã∏= disjoint.
882.4. conditional probability
2.4.3 bayes‚Äô theorem and the law of total probability
theorem 2.7 (bayes‚Äô theorem ).for any two events aandbsuch that p[a]>0
andp[b]>0,
p[a|b] =p[b|a]p[a]
p[b].
proof . by the definition of conditional probabilities, we have
p[a|b] =p[a‚à©b]
p[b]andp[b|a] =p[b‚à©a]
p[a].
rearranging the terms yields
p[a|b]p[b] =p[b|a]p[a],
which gives the desired result by dividing both sides by p[b].
‚ñ°
bayes‚Äô theorem provides two views of the intersection p[a‚à©b] using two different con-
ditional probabilities. we call p[b|a] the conditional probability andp[a|b] the posterior
probability . the order of aandbis arbitrary. we can also call p[a|b] the conditional
probability and p[b|a] the posterior probability. the context of the problem will make this
clear.
bayes‚Äô theorem provides a way to switch p[a|b] andp[b|a]. the next theorem helps
us decompose an event into smaller events.
theorem 2.8 (law of total probability ).let{a1, . . . , a n}be a partition of œâ, i.e.,
a1, . . . , a nare disjoint and œâ =a1‚à™ ¬∑¬∑¬∑ ‚à™ an. then, for any b‚äÜœâ,
p[b] =nx
i=1p[b|ai]p[ai]. (2.32)
proof . we start from the right-hand side.
nx
i=1p[b|ai]p[ai](a)=nx
i=1p[b‚à©ai](b)=p"n[
i=1(b‚à©ai)#
(c)=p"
b‚à© n[
i=1ai!#
(d)=p[b‚à©œâ] =p[b],
where ( a) follows from the definition of conditional probability, ( b) is due to axiom iii, ( c)
holds because of the distributive property of sets, and ( d) results from the partition property
of{a1, a2, . . . , a n}.
‚ñ°
interpretation . the law of total probability can be understood as follows. if the sample
space œâ consists of disjoint subsets a1, . . . , a n, we can compute the probability p[b] by
89chapter 2. probability
summing over its portion p[b‚à©a1], . . . ,p[b‚à©an]. however, each intersection can be written
as
p[b‚à©ai] =p[b|ai]p[ai]. (2.33)
in other words, we write p[b‚à©ai] as the conditional probability p[b|ai] times the prior
probability p[ai]. when we sum all these intersections, we obtain the overall probability.
seefigure 2.33 for a graphical portrayal.
figure 2.33: the law of total probability decomposes the probability p[b]into multiple conditional
probabilities p[b|ai]. the probability of obtaining each p[b|ai]isp[ai].
corollary 2.4. let{a1, a2, . . . , a n}be a partition of œâ, i.e., a1, . . . , a nare disjoint
andœâ =a1‚à™a2‚à™ ¬∑¬∑¬∑ ‚à™ an. then, for any b‚äÜœâ,
p[aj|b] =p[b|aj]p[aj]pn
i=1p[b|ai]p[ai]. (2.34)
proof . the result follows directly from bayes‚Äô theorem:
p[aj|b] =p[b|aj]p[aj]
p[b]=p[b|aj]p[aj]pn
i=1p[b|ai]p[ai].
‚ñ°
example 2.47 . suppose there are three types of players in a tennis tournament: a,
b, and c. fifty percent of the contestants in the tournament are aplayers, 25% are
bplayers, and 25% are cplayers. your chance of beating the contestants depends on
the class of the player, as follows:
0.3 against an aplayer
0.4 against a bplayer
0.5 against a cplayer
if you play a match in this tournament, what is the probability of your winning the
match? supposing that you have won a match, what is the probability that you played
against an aplayer?
solution . we first list all the known probabilities. we know from the percentage
902.4. conditional probability
of players that
p[a] = 0.5,p[b] = 0.25,p[c] = 0.25.
now, let wbe the event that you win the match. then the conditional probabilities
are defined as follows:
p[w|a] = 0.3,p[w|b] = 0.4,p[w|c] = 0.5.
therefore, by the law of total probability, we can show that the probability of
winning the match is
p[w] =p[w|a]p[a] +p[w|b]p[b] +p[w|c]p[c]
= (0.3)(0.5) + (0 .4)(0.25) + (0 .5)(0.25) = 0 .375.
given that you have won the match, the probability of agiven wis
p[a|w] =p[w|a]p[a]
p[w]=(0.3)(0.5)
0.375= 0.4.
example 2.48 . consider the communication channel shown below. the probability
of sending a 1 is pand the probability of sending a 0 is 1 ‚àíp. given that 1 is sent, the
probability of receiving 1 is 1 ‚àíŒ∑. given that 0 is sent, the probability of receiving 0
is 1‚àíŒµ. find the probability that a 1 has been correctly received.
solution . define the events
s0= ‚Äú0 is sent‚Äù ,and r0= ‚Äú0 is received‚Äù .
s1= ‚Äú1 is sent‚Äù ,and r1= ‚Äú1 is received‚Äù .
then, the probability that 1 is received is p[r1]. however, p[r1]Ã∏= 1‚àíŒ∑because 1 ‚àíŒ∑
91chapter 2. probability
is the conditional probability that 1 is received given that 1 is sent. it is possible that
we receive 1 as a result of an error when 0 is sent. therefore, we need to consider the
probability that both s0ands1occur. using the law of total probability we have
p[r1] =p[r1|s1]p[s1] +p[r1|s0]p[s0]
= (1‚àíŒ∑)p+Œµ(1‚àíp).
now, suppose that we have received 1. what is the probability that 1 was origi-
nally sent? this is asking for the posterior probability p[s1|r1], which can be found
using bayes‚Äô theorem
p[s1|r1] =p[r1|s1]p[s1]
p[r1]=(1‚àíŒ∑)p
(1‚àíŒ∑)p+Œµ(1‚àíp).
when do we need to use bayes‚Äô theorem and the law of total probability?
¬àbayes‚Äô theorem switches the role of the conditioning, from p[a|b] top[b|a].
example:
p[win the game |play with a] and p[play with a |win the game] .
¬àthe law of total probability decomposes an event into smaller events.
example:
p[win] = p[win|a]p[a] +p[win|b]p[b].
2.4.4 the three prisoners problem
now that you are familiar with the concepts of conditional probabilities, we would like to
challenge you with the following problem, known as the three prisoners problem . if you
understand how this problem can be resolved, you have mastered conditional probability.
once upon a time, there were three prisoners a,b, and c. one day, the king decided
to pardon two of them and sentence the last one, as in this figure:
figure 2.34: the three prisoners problem: the king says that he will pardon two prisoners and sentence
one.
one of the prisoners, prisoner a, heard the news and wanted to ask a friendly guard
about his situation. the guard was honest. he was allowed to tell prisoner athat prisoner b
would be pardoned or that prisoner cwould be pardoned, but he could not tell awhether
he would be pardoned. prisoner athought about the problem, and he began to hesitate to
ask the guard. based on his present state of knowledge, his probability of being pardoned
922.4. conditional probability
is2
3. however, if he asks the guard, this probability will be reduced to1
2because the guard
would tell him that one of the two other prisoners would be pardoned, and would tell him
which one it would be. prisoner a reasons that his chance of being pardoned would then
drop because there are now only two prisoners left who may be pardoned, as illustrated in
figure 2.35 :
figure 2.35: the three prisoners problem: if you do not ask the guard, your chance of being released
is 2/3. if you ask the guard, the guard will tell you which one of the other prisoners will be released.
your chance of being released apparently drops to 1/2.
should prisoner aask the guard? what has gone wrong with his reasoning? this
problem is tricky in the sense that the verbal argument of prisoner aseems flawless. if
he asked the guard, indeed, the game would be reduced to two people. however, this does
not seem correct, because regardless of what the guard says, the probability for ato be
pardoned should remain unchanged. let‚Äôs see how we can solve this puzzle.
letxa,xb,xcbe the events of sentencing prisoners a,b,c, respectively. let gb
be the event that the guard says that the prisoner bis released. without doing anything,
we know that
p[xa] =1
3,p[xb] =1
3,p[xc] =1
3.
conditioned on these events, we can compute the following conditional probabilities that
the guard says bis pardoned:
p[gb|xa] =1
2,p[gb|xb] = 0,p[gb|xc] = 1.
why are these conditional probabilities? p[gb|xb] = 0 quite straightforward. if the king
decides to sentence b, the guard has no way of saying that bwill be pardoned. therefore,
p[gb|xb] must be zero. p[gb|xc] = 1 is also not difficult. if the king decides to
sentence c, then the guard has no way to tell you that bwill be pardoned because the
guard cannot say anything about prisoner a. finally, p[gb|xa] =1
2can be understood
as follows: if the king decides to sentence a, the guard can either tell you borc. in other
words, the guard flips a coin.
with these conditional probabilities ready, we can determine the probability. this is the
conditional probability p[xa|gb]. that is, supposing that the guard says bis pardoned,
what is the probability that awill be sentenced? this is the actual scenario that ais facing.
solving for this conditional probability is not difficult. by bayes‚Äô theorem we know that
p[xa|gb] =p[gb|xa]p[xa]
p[gb],
93chapter 2. probability
andp[gb] =p[gb|xa]p[xa] +p[gb|xb]p[xb] +p[gb|xc]p[xc] according to the law of
total probability. substituting the numbers into these equations, we have that
p[gb] =p[gb|xa]p[xa] +p[gb|xb]p[xb] +p[gb|xc]p[xc]
=1
2√ó1
3+ 0√ó1
3+ 1√ó1
3=1
2,
p[xa|gb] =p[gb|xa]p[xa]
p[gb]=1
2√ó1
3
1
2=1
3.
therefore, given that the guard says bis pardoned, the probability that awill be sentenced
remains1
3. in fact, what you can show in this example is that p[xa|gb] =1
3=p[xa].
therefore, the presence or absence of the guard does not alter the probability. this is because
what the guard says is independent of whether the prisoners will be pardoned. the lesson
we learn from this problem is not to rely on verbal arguments. we need to write down the
conditional probabilities and spell out the steps.
figure 2.36: the three prisoners problem is resolved by noting that p[xa|gb] =p[xa]. therefore,
the events xaandgbare independent.
how to resolve the three prisoners problem?
¬àthe key is that ga,gb,gcdo not form a partition . see figure 2.36 .
¬àgbÃ∏=xb. when gbhappens, the remaining set is not xa‚à™xc.
¬àthe ratio p[xa‚à©gb]/p[gb] equals p[xa]. this is independence .
942.5. summary
2.5 summary
by now, we hope that you have become familiar with our slogan probability is a measure
of the size of a set . let us summarize:
¬àprobability = a probability law p. you can also view it as the value returned by p.
¬àmeasure = a ruler, a scale, a stopwatch, or another measuring device. it is a tool that
tells you how large or small a set is. the measure has to be compatible with the set.
if a set is finite, then the measure can be a counter. if a set is a continuous interval,
then the measure can be the length of the interval.
¬àsize = the relative weight of the set for the sample space. measuring the size is done
by using a weighting function. think of a fair coin versus a biased coin. the former
has a uniform weight, whereas the latter has a nonuniform weight.
¬àset= an event. an event is a subset in the sample space. a probability law palways
maps a setto a number. this is different from a typical function that maps a number
to another number.
if you understand what this slogan means, you will understand why probability can be
applied to discrete events, continuous events, events in n-d spaces, etc. you will also under-
stand the notion of measure zero and the notion of almost sure . these concepts lie at the
foundation of modern data science, in particular, theoretical machine learning.
the second half of this chapter discusses the concept of conditional probability . con-
ditional probability is a metaconcept that can be applied to any measure you use. the
motivation of conditional probability is to restrict the probability to a subevent happening
in the sample space. if bhas happened, the probability for atoalsohappen is p[a‚à©b]/p[b].
if two events are not influencing each other, then we say that aandbare independent.
according to bayes‚Äô theorem, we can also switch the order of agiven bandbgiven a, ac-
cording to bayes‚Äô theorem. finally, the law of total probability gives us a way to decompose
events into subevents.
we end this chapter by mentioning a few terms related to conditional probabilities
that will become useful later. let us use the tennis tournament as an example:
¬àp[w|a] =conditional probability = given that you played with player a, what is
the probability that you will win?
¬àp[a] =prior probability = without even entering the game, what is the chance that
you will face player a?
¬àp[a|w] =posterior probability = after you have won the game, what is the proba-
bility that you have actually played with a?
in many practical engineering problems, the question of interest is often the last one. that
is, supposing that you have observed something, what is the most likely cause of that event?
for example, supposing we have observed this particular dataset, what is the best gaussian
model that would fit the dataset? questions like these require some analysis of conditional
probability, prior probability, and posterior probability.
95chapter 2. probability
2.6 references
introduction to probability
2-1 dimitri p. bertsekas and john n. tsitsiklis, introduction to probability , athena sci-
entific, 2nd edition, 2008. chapter 1.
2-2 mark d. ward and ellen gundlach, introduction to probability , w.h. freeman and
company, 2016. chapter 1 ‚Äì chapter 6.
2-3 roy d. yates and david j. goodman, probability and stochastic processes , 3rd edi-
tion, wiley 2013, chapter 1.
2-4 john a. gubner, probability and random processes for electrical and computer en-
gineers , cambridge university press, 2006. chapter 2.
2-5 sheldon ross, a first course in probability , prentice hall, 8th edition, 2010. chapter
2 and chapter 3.
2-6 ani adhikari and jim pitman, probability for data science ,http://prob140.org/
textbook/content/readme.html . chapters 1 and 2.
2-7 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. chapter 2.1 ‚Äì 2.7.
2-8 athanasios papoulis and s. unnikrishna pillai, probability, random variables and
stochastic processes , mcgraw-hill, 4th edition, 2001. chapter 2.
2-9 henry stark and john woods, probability and random processes with applications
to signal processing , prentice hall, 3rd edition, 2001. chapter 1.
measure-theoretic probability
2-10 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. chapter 2.8 and 2.9.
2-11 henry stark and john woods, probability and random processes with applications
to signal processing , prentice hall, 3rd edition, 2001. appendix d.
2-12 william feller, an introduction to probability theory and its applications , wiley and
sons, 3rd edition, 1950.
2-13 andrey kolmogorov, foundations of the theory of probability , 2nd english edition,
dover 2018. (translated from russian to english. originally published in 1950 by
chelsea publishing company new york.)
2-14 patrick billingsley, probability and measure , wiley, 3rd edition, 1995.
real analysis
2-15 tom m. apostol, mathematical analysis , pearson, 1974.
2-16 walter rudin, principles of mathematical analysis , mcgraw hill, 1976.
962.7. problems
2.7 problems
exercise 1.
a space sand three of its subsets are given by s={1,3,5,7,9,11},a={1,3,5},b=
{7,9,11}, and c={1,3,9,11}. find a‚à©b‚à©c,ac‚à©b,a‚àíc, and ( a‚àíb)‚à™b.
exercise 2.
leta= (‚àí‚àû, r] and b= (‚àí‚àû, s] where r‚â§s. find an expression for c= (r, s] in terms
ofaandb. show that b=a‚à™c, and a‚à©c=‚àÖ.
exercise 3. (video solution)
simplify the following sets.
(a) [1 ,4]‚à©([0,2]‚à™[3,5])
(b) ([0 ,1]‚à™[2,3])c
(c)t‚àû
i=1(‚àí1/n,+1/n)
(d)s‚àû
i=1[5,8‚àí(2n)‚àí1]
exercise 4.
we will sometimes deal with the relationship between two sets. we say that aimplies b
when ais a subset of b(why?). show the following results.
(a) show that if aimplies b, and bimplies c, then aimplies c.
(b) show that if aimplies b, then bcimplies ac.
exercise 5.
show that if a‚à™b=aanda‚à©b=a, then a=b.
exercise 6.
a space sis defined as s={1,3,5,7,9,22}, and three subsets as a={1,3,5},b=
{7,9,11},c={1,3,9,11}. assume that each element has probability 1 /6. find the following
probabilities:
(a)p[a]
(b)p[b]
(c)p[c]
(d)p[a‚à™b]
(e)p[a‚à™c]
(f)p[(a\c)‚à™b]
97chapter 2. probability
exercise 7. (video solution)
a collection of 26 letters, a-z, is mixed in a jar. two letters are drawn at random, one after
the other. what is the probability of drawing a vowel (a,e,i,o,u) and a consonant in either
order? what is the sample space?
exercise 8.
consider an experiment consisting of rolling a die twice. the outcome of this experiment is
an ordered pair whose first element is the first value rolled and whose second element is the
second value rolled.
(a) find the sample space.
(b) find the set arepresenting the event that the value on the first roll is greater than
or equal to the value on the second roll.
(c) find the set bcorresponding to the event that the first roll is a six.
(d) let ccorrespond to the event that the first valued rolled and the second value rolled
differ by two. find a‚à©c.
note that a,b, and cshould be subsets of the sample space specified in part (a).
exercise 9.
a pair of dice are rolled.
(a) find the sample space œâ
(b) find the probabilities of the events: (i) the sum is even, (ii) the first roll is equal to
the second, (iii) the first roll is larger than the second.
exercise 10.
leta,bandcbe events in an event space. find expressions for the following:
(a) exactly one of the three events occurs.
(b) exactly two of the events occurs.
(c) two or more of the events occur.
(d) none of the events occur.
exercise 11.
a system is composed of five components, each of which is either working or failed. consider
an experiment that consists of observing the status of each component, and let the outcomes
of the experiment be given by all vectors ( x1, x2, x3, x4, x5), where xiis 1 if component iis
working and 0 if component iis not working.
(a) how many outcomes are in the sample space of this experiment?
(b) suppose that the system will work if components 1 and 2 are both working, or if
components 3 and 4 are both working, or if components 1, 3, and 5 are all working.
letwbe the event that the system will work. specify all of the outcomes in w.
982.7. problems
(c) let abe the event that components 4 and 5 have both failed. how many outcomes
are in the event a?
(d) write out all outcomes in the event a‚à©w.
exercise 12. (video solution)
a number xis selected at random in the interval [ ‚àí1,2]. let the events a={x|x <0},
b={x||x‚àí0.5|<0.5},c={x|x >0.75}. find (a) p[a|b], (b)p[b|c], (c)p[a|cc],
(d)p[b|cc].
exercise 13. (video solution)
let the events aandbhavep[a] =x,p[b] =yandp[a‚à™b] =z. find the following
probabilities: (a) p[a‚à©b], (b)p[ac‚à©bc], (c)p[ac‚à™bc], (d)p[a‚à©bc], (e)p[ac‚à™b].
exercise 14.
(a) by using the fact that p[a‚à™b]‚â§p[a]+p[b], show that p[a‚à™b‚à™c]‚â§p[a]+p[b]+
p[c].
(b) by using the fact that p[sn
k=1ak]‚â§pn
k=1p[ak], show that
p"n\
k=1ak#
‚â•1‚àínx
k=1p[ac
k].
exercise 15.
use the distributive property of set operations to prove the following generalized distributive
law:
a‚à™ n\
i=1bi!
=n\
i=1(a‚à™bi).
hint: use mathematical induction. that is, show that the above is true for n= 2 and that
it is also true for n=k+ 1 when it is true for n=k.
exercise 16.
the following result is known as the bonferroni‚Äôs inequality.
(a) prove that for any two events aandb, we have
p(a‚à©b)‚â•p(a) +p(b)‚àí1.
(b) generalize the above to the case of nevents a1, a2, . . . , a n, by showing that
p(a1‚à©a2‚à© ¬∑¬∑¬∑ ‚à© an)‚â•p(a1) +p(a2) +¬∑¬∑¬∑+p(an)‚àí(n‚àí1).
hint: you may use the generalized union bound p(sn
i=1ai)‚â§pn
i=1p(ai).
exercise 17. (video solution)
leta,b,cbe events with probabilities p[a] = 0.5,p[b] = 0.2,p[c] = 0.4. find
99chapter 2. probability
(a)p[a‚à™b] ifaandbare independent.
(b)p[a‚à™b] ifaandbare disjoint.
(c)p[a‚à™b‚à™c] ifa,bandcare independent.
(d)p[a‚à™b‚à™c] ifa,bandcare pairwise disjoint; can this happen?
exercise 18. (video solution)
a block of information is transmitted repeated over a noisy channel until an error-free block
is received. let m‚â•1 be the number of blocks required for a transmission. define the
following sets.
(i)a={mis even }
(ii)b={mis a multiple of 3 }
(iii)c={mis less than or equal to 6 }
assume that the probability of requiring one additional block is half of the probability
without the additional block. that is:
p[m=k] =1
2k
, k = 1,2, . . . .
determine the following probabilities.
(a)p[a],p[b],p[c],p[cc]
(b)p[a‚à©b],p[a\b],p[a‚à©b‚à©c]
(c)p[a|b],p[b|a]
(d)p[a|b‚à©c],p[a‚à©b|c]
exercise 19. (video solution)
a binary communication system transmits a signal xthat is either a +2-voltage signal or
a‚àí2-voltage signal. a malicious channel reduces the magnitude of the received signal by
the number of heads it counts in two tosses of a coin. let ybe the resulting signal. possible
values of yare listed below.
2 heads 1 head no head
x=‚àí2y= 0 y=‚àí1y=‚àí2
x= +2 y= 0 y= +1 y= +2
assume that the probability of having x= +2 and x=‚àí2 is equal.
(a) find the sample space of y, and hence the probability of each value of y.
(b) what are the probabilities p[x= +2|y= 1] and p[y= 1|x=‚àí2]?
exercise 20. (video solution)
a block of 100 bits is transmitted over a binary communication channel with a probability
of bit error p= 10‚àí2.
1002.7. problems
(a) if the block has 1 or fewer errors, then the receiver accepts the block. find the prob-
ability that the block is accepted.
(b) if the block has more than 1 error, then the block is retransmitted. what is the
probability that 4 blocks are transmitted?
exercise 21. (video solution)
a machine makes errors in a certain operation with probability p. there are two types of
errors. the fraction of errors that are type a is Œ±and the fraction that are type b is 1 ‚àíŒ±.
(a) what is the probability of kerrors in noperations?
(b) what is the probability of k1type a errors in noperations?
(c) what is the probability of k2type b errors in noperations?
(d) what is the joint probability of k1type a errors and k2type b errors in noperations?
hint: there are n
k1 n‚àík1
k2
possibilities of having k1type a errors and k2type b errors
innoperations. (why?)
exercise 22. (video solution)
a computer manufacturer uses chips from three sources. chips from sources a,bandc
are defective with probabilities 0.005, 0.001 and 0.01, respectively. the proportions of chips
from a,bandcare 0.5, 0.1 and 0.4 respectively. if a randomly selected chip is found to
be defective, find
(a) the probability that the chips are from a.
(b) the probability that the chips are from b.
(c) the probability that the chips are from c.
exercise 23. (video solution)
in a lot of 100 items, 50 items are defective. suppose that mitems are selected for testing.
we say that the manufacturing process is malfunctioning if the probability that one or more
items are tested to be defective. call this failure probability p. what should be the minimum
msuch that p‚â•0.99?
exercise 24. (video solution)
one of two coins is selected at random and tossed three times. the first coin comes up heads
with probability p1= 1/3 and the second coin with probability p2= 2/3.
(a) what is the probability that the number of heads is k= 3?
(b) repeat (a) for k= 0,1,2.
(c) find the probability that coin 1 was tossed given that kheads were observed, for
k= 0,1,2,3.
(d) in part (c), which coin is more probably when 2 heads have been observed?
101chapter 2. probability
exercise 25. (video solution)
consider the following communication channel. a source transmits a string of binary symbols
through a noisy communication channel. each symbol is 0 or 1 with probability pand
1‚àíp, respectively, and is received incorrectly with probability Œµ0andŒµ1. errors in different
symbols transmissions are independent.
denote sas the source and ras the receiver.
(a) what is the probability that a symbol is correctly received? hint: find
p[r= 1‚à©s= 1] and p[r= 0‚à©s= 0].
(b) find the probability of receiving 1011 conditioned on that 1011 was sent, i.e.,
p[r= 1011 |s= 1011] .
(c) to improve reliability, each symbol is transmitted three times, and the received
string is decoded by the majority rule. in other words, a 0 (or 1) is transmitted as
000 (or 111, respectively), and it is decoded at the receiver as a 0 (or 1) if and only if
the received three-symbol string contains at least two 0s (or 1s, respectively). what
is the probability that the symbol is correctly decoded, given that we send a 0?
(d) suppose that the scheme of part (c) is used. what is the probability that a 0 was
sent if the string 101 was received?
(e) suppose the scheme of part (c) is used and given that a 0 was sent. for what value of
Œµ0is there an improvement in the probability of correct decoding? assume that
Œµ0Ã∏= 0.
102chapter 3
discrete random variables
when working on a data analysis problem, one of the biggest challenges is the disparity
between the theoretical tools we learn in school and the actual data our boss hands to us.
by actual data, we mean a collection of numbers, perhaps organized or perhaps not. when
we are given the dataset, the first thing we do would certainly not be to define the borel
œÉ-field and then define the measure. instead, we would normally compute the mean, the
standard deviation, and perhaps some scores about the skewness.
the situation is best explained by the landscape shown in figure 3.1 . on the one hand,
we have well-defined probability tools, but on the other hand, we have a set of practical
‚Äúbattle skills‚Äù for processing data. often we view them as two separate entities. as long as
we can pull the statistics from the dataset, why bother about the theory? alternatively, we
have a set of theories, but we will never verify them using the actual datasets. how can we
bridge the two? what are the missing steps in the probability theory we have learned so
far? the goal of this chapter (and the next) is to fill this gap.
figure 3.1: the landscape of probability and data. often we view probability and data analysis as two
different entities. however, probability and data analysis are inseparable. the goal of this chapter is to
link the two.
three concepts to bridge the gap between theory and practice
the starting point of our discussion is a probability space (œâ ,f,p). it is an abstract concept,
but we hope we have convinced you in chapter 2 of its significance. however, the probability
space is certainly not ‚Äúuser friendly‚Äù because no one would write a python program to
103chapter 3. discrete random variables
implement those theories. how do we make the abstract probability space more convenient
so that we can model practical scenarios?
the first step is to recognize that the sample space and the event space are all based
onstatements , for example, ‚Äúgetting a head when flipping a coin‚Äù or ‚Äúwinning the game.‚Äù
these statements are not numbers, but we (engineers) love numbers. therefore, we should
ask a very basic question: how do we convert a statement to a number? the answer is the
concept of random variables .
key concept 1: what are random variables?
random variables are mappings from events to numbers.
now, suppose that we have constructed a random variable that translates statements to
numbers. the next task is to endow the random variable with probabilities. more precisely,
we need to assign probabilities to the random variable so that we can perform computations.
this is done using the concept called probability mass function (pmf).
key concept 2: what are probability mass functions (pmfs)?
probability mass functions are the ideal histograms of random variables.
the best way to think about a pmf is a histogram, something we are familiar with.
a histogram has two axes: the x-axis denotes the set of states and the y-axis denotes
theprobability . for each of the states that the random variable possesses, the histogram
tells us the probability of getting a particular state. the pmf is the ideal histogram of a
random variable. it provides a complete characterization of the random variable. if you have
a random variable, you must specify its pmf. vice versa, if you tell us the pmf, you have
specified a random variable.
we ask the third question about pulling information from the probability mass func-
tion, such as the mean and standard deviation. how do we obtain these numbers from the
pmf? we are also interested in operations on the mean and standard deviations. for ex-
ample, if a professor offers ten bonus points to the entire class, how will it affect the mean
and standard deviation? if a store provides 20% off on all its products, what will happen to
its mean retail price and standard deviation? however, the biggest question is perhaps the
difference between the mean we obtain from a pmf and the mean we obtain from a his-
togram. understanding this difference will immediately help us build a bridge from theory
to practice.
key concept 3: what is expectation?
expectation = mean = average computed from a pmf.
organization of this chapter
the plan for this chapter is as follows. we will start with the basic concepts of random
variables in section 3.1. we will formally define the random variables and discuss their
relationship with the abstract probability space. once this linkage is built, we can put
1043.1. random variables
the abstract probability space aside and focus on the random variables. in section 3.2
we will define the probability mass function (pmf) of a random variable, which tells us
the probability of obtaining a state of the random variable. pmf is closely related to the
histogram of a dataset. we will explain the connection. in section 3.3 we take a small detour
to consider the cumulative distribution functions (cdf). then, we discuss the mean and
standard deviation in section 3.4. section 3.5 details a few commonly used random variables,
including bernoulli, binomial, geometric, and poisson variables.
3.1 random variables
3.1.1 a motivating example
consider an experiment with 4 outcomes œâ = {‚ô£,‚ô¢,‚ô°,‚ô†}. we want to construct the
probability space (œâ ,f,p). the sample space œâ is already defined. the event space fis the
set of all possible subsets in œâ, which, in our case, is a set of 24subsets. for the probability
lawp, let us assume that the probability of obtaining each outcome is
p[{‚ô£}] =1
6,p[{‚ô¢}] =2
6,p[{‚ô°}] =2
6,p[{‚ô†}] =1
6.
therefore, we have constructed a probability space (œâ ,f,p) where everything is perfectly
defined. so, in principle, they can live together happily forever.
a lazy data scientist comes, and there is a (small) problem. the data scientist does not
want to write the symbols ‚ô£,‚ô¢,‚ô°,‚ô†. there is nothing wrong with his motivation because
all of us want efficiency. how can we help him? well, the easiest solution is to encode each
symbol with a number, for example, ‚ô£ ‚Üê 1,‚ô¢ ‚Üê 2,‚ô° ‚Üê 3,‚ô† ‚Üê 4, where the arrow means
that we assign a number to the symbol. but we can express this more formally by defining
a function x: œâ‚Üírwith
x(‚ô£) = 1 , x(‚ô¢) = 2 , x(‚ô°) = 3 , x(‚ô†) = 4 .
there is nothing new here: we have merely converted the symbols to numbers, with the help
of a function x. however, with xdefined, the probabilities can be written as
p[x= 1] =1
6,p[x= 2] =2
6,p[x= 3] =2
6,p[x= 4] =1
6.
this is much more convenient, and so the data scientist is happy.
3.1.2 definition of a random variable
the story above is exactly the motivation for random variables. let us define a random
variable formally.
definition 3.1. arandom variable xis a function x: œâ‚Üírthat maps an outcome
Œæ‚ààœâto a number x(Œæ)on the real line.
105chapter 3. discrete random variables
this definition may be puzzling at first glance. why should we overcomplicate things by
defining a function and calling it a variable ?
if you recall the story above, we can map the notations of the story to the notations
of the definition as follows.
symbol meaning
œâ sample space = the set containing ‚ô£,‚ô¢,‚ô°,‚ô†
Œæ an element in the sample space, which is one of ‚ô£,‚ô¢,‚ô°,‚ô†
x a function that maps ‚ô£to the number 1, ‚ô¢to the number 2, etc
x(Œæ) a number on the real line, e.g., x(‚ô£) = 1
this explains our informal definition of random variables:
key concept 1: what are random variables?
random variables are mappings from events to numbers.
the random variable xis afunction . the input to the function is an outcome of the sample
space, whereas the output is a number on the real line. this type of function is somewhat
different from an ordinary function that often translates a number to another number.
nevertheless, xis a function.
figure 3.2: a random variable is a mapping from the outcomes in the sample space to numbers on the
real line. we can think of a random variable xas a translator that translates a statement to a number.
why do we call this function xavariable ?xis a variable because xhas multiple
states . as we illustrate in figure 3.2 , the mapping xtranslates every outcome Œæto a
number. there are multiple numbers, which are the states of x. each state has a certain
probability for xto land on. because xis not deterministic, we call it a random variable.
example 3.1 . suppose we flip a fair coin so that œâ = {head,tail}. we can define the
random variable x: œâ‚Üíras
x(head) = 1 , and x(tail) = 0 .
1063.1. random variables
therefore, when we write p[x= 1] we actually mean p[{head}]. is there any difference
between p[{head}] andp[x= 1]? no, because they are describing two identical events.
note that the assignment of the value is totally up to you. you can say ‚Äúhead‚Äù is equal
to the value 102. this is allowed and legitimate, but it isn‚Äôt very convenient.
example 3.2 . flip a coin 2 times. the sample space œâ is
œâ ={(head ,head) ,(head ,tail),(tail,head) ,(tail,tail)}.
suppose that xis a random variable that maps an outcome to a number representing
the sum of ‚Äúhead,‚Äù i.e.,
x(¬∑) = number of heads .
then, for the 4 Œæ‚Äôs in the sample space there are only 3 distinct numbers. more precisely,
if we let Œæ1= (head ,head), Œæ2= (head ,tail), Œæ3= (tail ,head), Œæ4= (tail ,tail), then,
we have
x(Œæ1) = 2 , x(Œæ2) = 1 , x(Œæ3) = 1 , x(Œæ4) = 0 .
a pictorial illustration of this random variable is shown in figure 3.3 . this example
shows that the mapping defined by the random variable is not necessarily a one-to-one
mapping because multiple outcomes can be mapped to the same number.
figure 3.3: a random variable that maps a pair of coins to a number, where the number represents the
number of heads.
3.1.3 probability measure on random variables
by now, we hope that you understand key concept 1: a random variable is a mapping
from a statement to a number . however, we are now facing another difficulty. we knew
how to measure the size of an event using the probability law pbecause p(¬∑) takes an event
e‚àà fand sends it to a number between [0 ,1]. after the translation x, we cannot send the
output x(Œæ) top(¬∑) because p(¬∑) ‚Äúeats‚Äù a set e‚àà fand not a number x(Œæ)‚ààr. therefore,
when we write p[x= 1], how do we measure the size of the event x= 1?
107chapter 3. discrete random variables
this question appears difficult but is actually quite easy to answer. since the prob-
ability law p(¬∑) is always applied to an event , we need to define an event for the random
variable x. if we write the sets clearly, we note that ‚Äú x=a‚Äù is equivalent to the set
e=
Œæ‚ààœâx(Œæ) =a
.
this is the set that contains all possible Œæ‚Äôs such that x(Œæ) =a. therefore, when we say
‚Äúfind the probability of x=a,‚Äù we are effectively asking the size of the set e={Œæ‚àà
œâ|x(Œæ) =a}.
how then do we measure the size of e? since eis a subset in the sample space, eis
measurable by p. all we need to do is to determine what eis for a given a. this, in turn,
requires us to find the pre-image x‚àí1(a), which is defined as
x‚àí1(a)def=
Œæ‚ààœâx(Œæ) =a
.
wait a minute, is this set just equal to e? yes, the event ewe are seeking is exactly the
pre-image x‚àí1(a). as such, the probability measure of eis
p[x=a] =p[x‚àí1(a)].
figure 3.4 illustrates a situation where two outcomes Œæ1andŒæ2are mapped to the same
value aon the real line. the corresponding event is the set x‚àí1(a) ={Œæ1, Œæ2}.
figure 3.4: when computing the probability of p[{Œæ‚ààœâ|x(Œæ) =a}], we effectively take the inverse
mapping x‚àí1(a)and compute the probability of the event p[{Œæ‚ààx‚àí1(a)}] =p[{Œæ1, Œæ2}].
example 3.3 . suppose we throw a die. the sample space is
œâ ={
,
,
,
,
,
}.
there is a natural mapping xthat maps x(
) = 1, x(
) = 2 and so on. thus,
1083.1. random variables
p[x‚â§3](a)=p[x= 1] + p[x= 2] + p[x= 3]
(b)=p[x‚àí1(1)] + p[x‚àí1(2)] + p[x‚àí1(3)]
(c)=p[{
}] +p[{
}] +p[{
}] =3
6.
in this derivation, step (a) is based on axiom iii, where the three events are disjoint.
step (b) is the pre-image due to the random variable x. step (c) is the list of ac-
tual events in the event space. note that there is no hand-waving argument in this
derivation. every step is justified by the concepts and theorems we have learned so
far.
example 3.4 . throw a die twice. the sample space is then
œâ ={(
,
),(
,
), . . . , (
,
)}.
these elements can be translated to 36 outcomes:
Œæ1= (
 ,
), Œæ2= (
 ,
), . . . , Œæ 36= (
 ,
).
let
x= sum of two numbers .
then, if we want to find the probability of getting x= 7, we can trace back and ask:
among the 36 outcomes, which of those Œæi‚Äôs will give us x(Œæ) = 7? or, what is the set
x‚àí1(7)? to this end, we can write
p[x= 7] = p[{(
,
),(
,
),(
,
),(
,
),(
,
),(
,
)}]
=p[(
,
)] +p[(
,
)] +p[(
,
)]
+p[(
,
)] +p[(
,
)] +p[(
,
)]
=1
36+1
36+1
36+1
36+1
36+1
36=1
6.
again, in this example, you can see that all the steps are fully justified by the concepts
we have learned so far.
closing remark . in practice, when the problem is clearly defined, we can skip the inverse
mapping x‚àí1(a). however, this does not mean that the probability triplet (œâ ,f,p) is gone;
it is still present. the triplet is now just the background of the problem.
the set of all possible values returned by xis denoted as x(œâ). since xis not
necessarily a bijection, the size of x(œâ) is not necessarily the same as the size of œâ. the
elements in x(œâ) are often denoted as aorx. we call aorxone of the states ofx. be
careful not to confuse xandx. the variable xis the random variable; it is a function.
the variable xis a state assigned by x. a random variable xhas multiple states. when
we write p[x=x], we describe the probability of a random variable xtaking a particular
state x. it is exactly the same as p[{Œæ‚ààœâ|x(Œæ) =x}].
109chapter 3. discrete random variables
3.2 probability mass function
random variables are mappings that translate events to numbers. after the translation,
we have a set of numbers denoting the states of the random variables. each state has a
different probability of occurring. the probabilities are summarized by a function known as
the probability mass function (pmf).
3.2.1 definition of probability mass function
definition 3.2. theprobability mass function (pmf) of a random variable xis a
function which specifies the probability of obtaining a number x(Œæ) =x. we denote a
pmf as
px(x) =p[x=x]. (3.1)
the set of all possible states of xis denoted as x(œâ).
do not get confused by the sample space œâ and the set of states x(œâ). the sample space œâ
contains all the possible outcomes of the experiments, whereas x(œâ) is the translation by
the mapping x. the event x=ais the set x‚àí1(a)‚äÜœâ. therefore, when we say p[x=x]
we really mean p[x‚àí1(x)].
the probability mass function is a histogram summarizing the probability of each of
the states xtakes. since it is a histogram, a pmf can be easily drawn as a bar chart.
example 3.5 . flip a coin twice. the sample space is œâ = {hh, ht, th, tt }. we
can assign a random variable x= number of heads. therefore,
x(‚Äúhh‚Äù) = 2 , x(‚Äúth‚Äù) = 1 , x(‚Äúht‚Äù) = 1 , x(‚Äútt‚Äù) = 0 .
so the random variable xtakes three states: 0, 1, 2. the pmf is therefore
px(0) =p[x= 0] = p[{‚Äútt‚Äù}] =1
4,
px(1) =p[x= 1] = p[{‚Äúth‚Äù ,‚Äúht‚Äù}] =1
2,
px(2) =p[x= 2] = p[{‚Äúhh‚Äù}] =1
4.
3.2.2 pmf and probability measure
in chapter 2, we learned that probability is a measure of the size of a set. we introduced a
weighting function that weights each of the elements in the set. the pmf is the weighing
function for discrete random variables. two random variables are different when their pmfs
are different because they are constructing two different measures.
1103.2. probability mass function
to illustrate the idea, suppose there are two dice. they each have probability masses
as follows.
p[{
}] =1
12,p[{
}] =2
12,p[{
}] =3
12,p[{
}] =4
12,p[{
}] =1
12,p[{
}] =1
12,
p[{
}] =2
12,p[{
}] =2
12,p[{
}] =2
12,p[{
}] =2
12,p[{
}] =2
12,p[{
}] =2
12,
let us define two random variables, xandy, for the two dice. then, the pmfs pxandpy
can be defined as
px(1) =1
12, px(2) =2
12, px(3) =3
12, px(4) =4
12, px(5) =1
12, px(6) =1
12,
py(1) =2
12, py(2) =2
12, py(3) =2
12, py(4) =2
12, py(5) =2
12, py(6) =2
12.
these two probability mass functions correspond to two different probability measures, let‚Äôs
sayfandg. define the event e={between 2 and 3 }. then, f(e) andg(e) will lead to
two different results:
f(e) =p[2‚â§x‚â§3] =px(2) + px(3) =1
12+2
12=3
12,
g(e) =p[2‚â§y‚â§3] =py(2) + py(3) =2
12+2
12=4
12.
note that even though for some particular events two final results could be the same (e.g.,
2‚â§x‚â§4 and 2 ‚â§y‚â§4), the underlying measures are completely different.
figure 3.5 shows another example of two different measures fandgon the same
sample space œâ = {‚ô£,‚ô¢,‚ô°,‚ô†}. since the pmfs of the two measures are different, even
when given the same event e, the resulting probabilities will be different.
figure 3.5: if we want to measure the size of a set e, using two different pmfs is equivalent to using
two different measures. therefore, the probabilities will be different.
does px=pyimply x=y?if two random variables xandyhave the same pmf,
does it mean that the random variables are the same? the answer is no. consider a random
variable with a symmetric pmf, e.g.,
px(‚àí1) =1
4, p x(0) =1
2, p x(1) =1
4. (3.2)
suppose y=‚àíx. then, py(‚àí1) =1
4,py(0) =1
2, and py(1) =1
4, which is the same as px.
however, xandyare two different random variables. if the sample space is {‚ô£,‚ô¢,‚ô°}, we
can define the mappings x(¬∑) and y(¬∑) as
x(‚ô£) =‚àí1, x (‚ô¢) = 0 , x (‚ô°) = +1 ,
y(‚ô£) = +1 , y (‚ô¢) = 0 , y (‚ô°) =‚àí1.
111chapter 3. discrete random variables
therefore, when we say px(‚àí1) =1
4, the underlying event is ‚ô£. but when we say py(‚àí1) =1
4,
the underlying event is ‚ô°. the two random variables are different, although their pmfs have
exactly the same shape.
3.2.3 normalization property
here we must mention one important property of a probability mass function. this property
is known as the normalization property , which is a useful tool for a sanity check.
theorem 3.1. a pmf should satisfy the condition that
x
x‚ààx(œâ)px(x) = 1 . (3.3)
proof . the proof follows directly from axiom ii, which states that p[œâ] = 1. since xcovers
all numerical values xcan take, and since each xis distinct, by axiom iii we have
x
x‚ààx(œâ)p[x=x] =x
x‚ààx(œâ)p[{Œæ‚ààœâ|x(Œæ) =x}]
=pÔ£Æ
Ô£∞[
Œæ‚ààœâ{Œæ‚ààœâ|x(Œæ) =x}Ô£π
Ô£ª=p[œâ] = 1 .
‚ñ°
practice exercise 3.1 . let px(k) =c 1
2k, where k= 1,2, . . .. find c.
solution . sincep
k‚ààx(œâ)px(k) = 1, we must have
‚àûx
k=11
2k
= 1.
evaluating the geometric series on the right-hand side, we can show that
‚àûx
k=1c1
2k
=c
2‚àûx
k=01
2k
=c
2¬∑1
1‚àí1
2
=c=‚áí c= 1.
practice exercise 3.2 . let px(k) =c¬∑sin œÄ
2k
, where k= 1,2, . . .. find c.
solution . the reader may might be tempted to sum px(k) over all the possible k‚Äôs:
‚àûx
k=1sinœÄ
2k
= 1 + 0 ‚àí1 + 0 + ¬∑¬∑¬∑?= 0.
1123.2. probability mass function
however, a more careful inspection reveals that px(k) is actually negative when k=
3,7,11, . . .. this cannot happen because a probability mass function must be non-
negative. therefore, the problem is not defined, and so there is no solution.
0.06250.1250.250.5
1 2 3 4 5 6 7 8 9 10
-1-0.500.51
1 2 3 4 5 6 7 8 9 10
(a) (b)
figure 3.6: (a) the pmf of px(k) =c 1
2k, for k= 1,2, . . .. (b) the pmf of px(k) = sin œÄ
2k
,
where k= 1,2, . . .. note that this is not a valid pmf because probability cannot have negative values.
3.2.4 pmf versus histogram
pmfs are closely related to histograms. a histogram is a plot that shows the frequency of
a state. as we see in figure 3.6 , the x-axis is a collection of states, whereas the y-axis is
the frequency. so a pmf is indeed a histogram.
viewing a pmf as a histogram can help us understand a random variable. for better
or worse, treating a random variable as a histogram could help you differentiate a random
variable from a variable. an ordinary variable only has one state, but a random variable
has multiple states. at any particular instance, we do not know which state will show up
before our observation. however, we do know the probability. for example, in the coin-flip
example, while we do not know whether we will get ‚Äúhh,‚Äù we know that the chance of
getting ‚Äúhh‚Äù is 1/4. of course, having a probability of 1/4 does not mean that we will get
‚Äúhh‚Äù once every four trials. it only means that if we run an infinite number of experiments,
then 1/4 of the experiments will give us ‚Äúhh.‚Äù
the linkage between pmf and histogram can be quite practical. for example, while
we do not know the true underlying distribution of the 26 letters of the english alphabet, we
can collect a large number of words and plot the histogram. the example below illustrates
how we can empirically define a random variable from the data.
example . there are 26 english letters, but the frequencies of the letters in writing are
different. if we define a random variable xas a letter we randomly draw from an english
text, we can think of xas an object with 26 different states. the mapping associated with the
random variable is straightforward: x(‚Äúa‚Äù) = 1, x(‚Äúb‚Äù) = 2, etc. the probability of landing
on a particular state approximately follows a histogram shown in figure 3.7 . the histogram
provides meaningful values of the probabilities, e.g., px(1) = 0 .0847, px(2) = 0 .0149, etc.
the true probability of the states may not be exactly these values. however, when we have
enough samples, we generally expect the histogram to approach the theoretical pmf. the
matlab and python codes used to generate this histogram are shown below.
% matlab code to generate the histogram
load(‚Äòch3_data_english‚Äô);
bar(f/100,‚Äòfacecolor‚Äô,[0.9,0.6,0.0]);
113chapter 3. discrete random variables
a b c d e f g h i j k l m n o p q r s t u v w x y z00.020.040.060.080.10.12
figure 3.7: the frequency of the 26 english letters. data source: wikipedia.
xticklabels({‚Äòa‚Äô,‚Äòb‚Äô,‚Äòc‚Äô,‚Äòd‚Äô,‚Äòe‚Äô,‚Äòf‚Äô,‚Äòg‚Äô,‚Äòh‚Äô,‚Äòi‚Äô,‚Äòj‚Äô,‚Äòk‚Äô,‚Äòl‚Äô,...
‚Äòm‚Äô,‚Äòn‚Äô,‚Äòo‚Äô,‚Äòp‚Äô,‚Äòq‚Äô,‚Äòr‚Äô,‚Äòs‚Äô,‚Äòt‚Äô,‚Äòu‚Äô,‚Äòv‚Äô,‚Äòw‚Äô,‚Äòx‚Äô,‚Äòy‚Äô,‚Äòz‚Äô});
xticks(1:26);
yticks(0:0.02:0.2);
axis([1 26 0 0.13]);
# python code generate the histogram
import numpy as np
import matplotlib.pyplot as plt
f = np.loadtxt(‚Äò./ch3_data_english.txt‚Äô)
n = np.arange(26)
plt.bar(n, f/100)
ntag = [‚Äòa‚Äô,‚Äòb‚Äô,‚Äòc‚Äô,‚Äòd‚Äô,‚Äòe‚Äô,‚Äòf‚Äô,‚Äòg‚Äô,‚Äòh‚Äô,‚Äòi‚Äô,‚Äòj‚Äô,‚Äòk‚Äô,‚Äòl‚Äô,‚Äòm‚Äô,...
‚Äòn‚Äô,‚Äòo‚Äô,‚Äòp‚Äô,‚Äòq‚Äô,‚Äòr‚Äô,‚Äòs‚Äô,‚Äòt‚Äô,‚Äòu‚Äô,‚Äòv‚Äô,‚Äòw‚Äô,‚Äòx‚Äô,‚Äòy‚Äô,‚Äòz‚Äô]
plt.xticks(n, ntag)
pmf = ideal histograms
if a random variable is more or less a histogram, why is the pmf such an important concept?
the answer to this question has two parts. the first part is that the histogram generated
from a dataset is always an empirical histogram, so-called because the dataset comes from
observation or experience rather than theory. thus the histograms may vary slightly every
time we collect a dataset.
as we increase the number of data points in a dataset, the histogram will eventually
converge to an ideal histogram, or a distribution . for example, counting the number of
heads in 100 coin flips will fluctuate more in percentage terms than counting the heads in 10
million coin flips. the latter will almost certainly have a histogram that is closer to a 50‚Äì50
distribution. therefore, the ‚Äúhistogram‚Äù generated by a random variable can be considered
the ultimate histogram or the limiting histogram of the experiment.
to help you visualize the difference between a pmf and a histogram, we show in
figure 3.8 an experiment in which a die is thrown ntimes. assuming that the die is fair,
the pmf is simply px(k) = 1 /6 for k= 1, . . . , 6, which is a uniform distribution across
the 6 states. now, we can throw the die many times. as nincreases, we observe that the
1143.2. probability mass function
1 2 3 4 5 600.050.10.150.2n = 100
1 2 3 4 5 600.050.10.150.2n = 1000
(a)n= 100 (b) n= 1000
1 2 3 4 5 600.050.10.150.2n = 10000
1 2 3 4 5 600.050.10.150.2n = 
(c)n= 10000 (d) pmf
figure 3.8: histogram and pmf, when throwing a fair die ntimes. as nincreases, the histograms are
becoming more similar to the pmf.
histogram becomes more like the pmf. you can imagine that when ngoes to infinity, the
histogram will eventually become the pmf. therefore, when given a dataset, one way to
think of it is to treat the data as random realizations drawn from a certain pmf. the more
data points you have, the closer the histogram will become to the pmf.
the matlab and python codes used to generate figure 3.8 are shown below. the
two commands we use here are randi (in matlab), which generates random integer num-
bers, and hist, which computes the heights and bin centers of a histogram. in python,
the corresponding commands are np.random.randint andplt.hist . note that because of
the different indexing schemes in matlab and python, we offset the maximum index in
np.random.randint to 7 instead of 6. also, we shift the x-axes so that the bars are centered
at the integers.
% matlab code to generate the histogram
x = [1 2 3 4 5 6];
q = randi(6,100,1);
figure;
[num,val] = hist(q,x-0.5);
bar(num/100,‚Äòfacecolor‚Äô,[0.8, 0.8,0.8]);
axis([0 7 0 0.24]);
# python code generate the histogram
import numpy as np
import matplotlib.pyplot as plt
q = np.random.randint(7,size=100)
115chapter 3. discrete random variables
plt.hist(q+0.5,bins=6)
thisgenerative perspective is illustrated in figure 3.9 . we assume that the underlying
latent random variable has some pmf that can be described by a few parameters, e.g., the
mean and variance. given the data points, if we can infer these parameters, we might retrieve
the entire pmf (up to the uncertainty level intrinsic to the dataset). we refer to this inverse
process as statistical inference.
figure 3.9: when analyzing a dataset, one can treat the data points are samples drawn according to a
latent random variable with certain a pmf. the dataset we observe is often finite, and so the histogram
we obtain is empirical. a major task in data analysis is statistical inference, which tries to retrieve the
model information from the available measurements.
returning to the question of why we need to understand the pmfs, the second part
of the answer is the difference between synthesis andanalysis . in synthesis, we start with
a known random variable and generate samples according to the pmf underlying the ran-
dom variable. for example, on a computer, we often start with a gaussian random variable
and generate random numbers according to the histogram specified by the gaussian ran-
dom variable. synthesis is useful because we can predict what will happen. we can, for
example, create millions of training samples to train a deep neural network. we can also
evaluate algorithms used to estimate statistical quantities such as mean, variance, moments,
etc., because the synthesis approach provides us with ground truth. in supervised learning
scenarios, synthesis is vital to ensuring sufficient training data.
the other direction of synthesis is analysis. the goal is to start with a dataset and
deduce the statistical properties of the dataset. for example, suppose we want to know
whether the underlying model is indeed a gaussian model. if we know that it is a gaussian
(or if we choose to use a gaussian), we want to know the parameters that define this
gaussian. the analysis direction addresses this model selection and parameter estimation
problem. moving forward, once we know the model and the parameters, we can make a
prediction or do recovery, both of which are ubiquitous in machine learning.
we summarize our discussions below, which is key concept 2 of this chapter.
key concept 2: what are probability mass functions (pmfs)?
pmfs are the ideal histograms of random variables.
1163.2. probability mass function
3.2.5 estimating histograms from real data
the following discussions about histogram estimation can be skipped if it is your first
time reading the book.
if you have a dataset, how would you plot the histogram? certainly, if you have access
to matlab or python, you can call standard functions such as hist (in matlab) or
np.histogram (in python). however, when plotting a histogram, you need to specify the
number of bins (or equivalently the width of bins). if you use larger bins, then you will have
fewer bins with many elements in each bin. conversely, if the bin width is too small, you
may not have enough samples to fill the histogram. figure 3.10 illustrates two histograms
in which the bins are respectively too large and too small.
0 2 4 6 8 1002004006008001000
k = 5
0 2 4 6 8 1001020304050
k = 200
(a) 5 bins (b) 200 bins
figure 3.10: the width of the histogram has substantial influence on the information that can be
extracted from the histogram.
the matlab and python codes used to generate figure 3.10 are shown below. note
that here we are using an exponential random variable (to be discussed in chapter 4). in
matlab, calling an exponential random variable is done using exprnd , whereas in python
the command is np.random.exponential . for this experiment, we can specify the number
of bins k, which can be set to k= 200 or k= 5. to suppress the python output of the array,
we can add a semicolon ;. a final note is that lambda is a reserved variable in python. use
something else.
% matlab code used to generate the plots
lambda = 1;
k = 1000;
x = exprnd(1/lambda,[k,1]);
[num,val] = hist(x,200);
bar(val,num,‚Äòfacecolor‚Äô,[1, 0.5,0.5]);
# python code used to generate the plots
import numpy as np
import matplotlib.pyplot as plt
lambd = 1
117chapter 3. discrete random variables
k = 1000
x = np.random.exponential(1/lambd, size=k)
plt.hist(x,bins=200);
in statistics, there are various rules to determine the bin width of a histogram. we
mention a few of them here. let kbe the number of bins and nthe number of samples.
¬àsquare-root: k=‚àö
n
¬àsturges‚Äô formula: k= log2n+ 1.
¬àrice rule: k= 23‚àö
n
¬àscott‚Äôs normal reference rule: k=maxx‚àíminx
h, where h=3.5‚àö
var[x]
3‚àö
nis the bin
width.
for the example data shown in figure 3.10 , the histograms obtained using the above rules
are given in figure 3.11 . as you can see, different rules have different suggested bin widths.
some are more conservative, e.g., using fewer bins, whereas some are less conservative. in
any case, the suggested bin widths do seem to provide better histograms than the original
ones in figure 3.10 . however, no bin width is the best for all purposes.
0 1 2 3 4 50100200300400500
square-root, k =  32
0 1 2 3 4 50100200300400500
sturges rule, k =  11
0 1 2 3 4 50100200300400500
rice rule, k =  20
0 1 2 3 4 50100200300400500
scott rule, k =  22
figure 3.11: histograms of a dataset using different bin width rules.
beyond these predefined rules, there are also algorithmic tools to determine the bin
width. one such tool is known as cross-validation . cross-validation means defining some
kind of cross-validation score that measures the statistical risk associated with the his-
togram. a histogram having a lower score has a lower risk, and thus it is a better histogram.
1183.2. probability mass function
note that the word ‚Äúbetter‚Äù is relative to the optimality criteria associated with the cross-
validation score. if you do not agree with our cross-validation score, our optimal bin width is
not necessarily the one you want. in this case, you need to specify your optimality criteria.
theoretically, deriving a meaningful cross-validation score is beyond the scope of this
book. however, it is still possible to understand the principle. let hbe the bin width of the
histogram, kthe number of bins, and nthe number of samples. given a dataset, we follow
this procedure:
¬àstep 1: choose a bin width h.
¬àstep 2: construct a histogram from the data, using the bin width h. the histogram will
have the empirical pmf values bp1,bp2, . . . ,bpk, which are the heights of the histograms
normalized so that the sum is 1.
¬àstep 3: compute the cross-validation score (see wasserman, all of statistics , section
20.2):
j(h) =2
(n‚àí1)h‚àín+ 1
(n‚àí1)h 
bp2
1+bp2
2+¬∑¬∑¬∑+bp2
k
(3.4)
¬àrepeat steps 1, 2, 3, until we find an hthat minimizes j(h).
note that when we use a different h, the pmf values bp1,bp2, . . . ,bpkwill change, and the
number of bins kwill also change. therefore, when changing h, we are changing not only
the terms in j(h) that explicitly contain hbut also terms that are implicitly influenced.
20 40 60 80 100 120 140 160 180 200
number of bins-3.5-3.4-3.3-3.2-3.1-3-2.9cross-validation score10-3
20 40 60 80 100 120 140 160 180 200
number of bins012345cross-validation score10-4
(a) one dataset (b) average of many datasets
figure 3.12: cross-validation score for the histogram. (a) the score of one particular dataset. (b) the
scores for many different datasets generated by the same model.
for the dataset we showed in figure 3.10 , the cross-validation score j(h) is shown in
figure 3.12 . we can see that although the curve is noisy, there is indeed a reasonably clear
minimum happening around 20 ‚â§k‚â§30, which is consistent with some of the rules.
the matlab and python codes we used to generate figure 3.12 are shown below.
the key step is to implement equation (3.4) inside a for-loop, where the loop goes through
the range of bins we are interested in. to obtain the pmf values bp1, . . . ,bpk, we call hist
in matlab and np.histogram in python. the bin width his the number of samples n
divided by the number of bins m.
119chapter 3. discrete random variables
% matlab code to perform the cross validation
lambda = 1;
n = 1000;
x = exprnd(1/lambda,[n,1]);
m = 6:200;
j = zeros(1,195);
for i=1:195
[num,binc] = hist(x,m(i));
h = n/m(i);
j(i) = 2/((n-1)*h)-((n+1)/((n-1)*h))*sum( (num/n).^2 );
end
plot(m,j,‚Äòlinewidth‚Äô,4,‚Äòcolor‚Äô,[0.9,0.2,0.0]);
# python code to perform the cross validation
import numpy as np
import matplotlib.pyplot as plt
lambd = 1
n = 1000
x = np.random.exponential(1/lambd, size=n)
m = np.arange(5,200)
j = np.zeros((195))
for i in range(0,195):
hist,bins = np.histogram(x,bins=m[i])
h = n/m[i]
j[i] = 2/((n-1)*h)-((n+1)/((n-1)*h))*np.sum((hist/n)**2)
plt.plot(m,j);
infigure 3.12 (b), we show another set of curves from the same experiment. the
difference here is that we assume access to the true generative model so that we can generate
the many datasets of the same distribution. in this experiment we generated t= 1000
datasets. we compute the cross-validation score j(h) for each of the datasets, yielding t
score functions j(1)(h), . . . , j(t)(h). we subtract the minimum because different realizations
have different offsets. then we compute the average:
j(h) =1
ttx
t=1
j(t)(h)‚àímin
h
j(t)(h)	
. (3.5)
this gives us a smooth red curve as shown in figure 3.12 (b). the minimum appears to be
atn= 25. this is the optimal n, concerning the cross-validation score, on the average of
all datasets.
all rules, including cross-validation, are based on optimizing for a certain objective.
your objective could be different from our objective, and so our optimum is not necessarily
your optimum. therefore, cross-validation may not be the best. it depends on your problem.
end of the discussion.
1203.3. cumulative distribution functions (discrete)
3.3 cumulative distribution functions (discrete)
while the probability mass function (pmf) provides a complete characterization of a dis-
crete random variable, the pmfs themselves are technically not ‚Äúfunctions‚Äù because the
impulses in the histogram are essentially delta functions. more formally, a pmf px(k)
should actually be written as
px(x) =x
k‚ààx(œâ)px(k)|{z}
pmf values¬∑Œ¥(x‚àík)|{z}
delta function.
this is a train of delta functions, where the height is specified by the probability mass px(k).
for example, a random variable with pmf values
px(0) =1
4, px(1) =1
2, px(2) =1
4
will be expressed as
px(x) =1
4Œ¥(x) +1
2Œ¥(x‚àí1) +1
4Œ¥(x‚àí2).
since delta functions need to be integrated to generate values, the typical things we want to
do, e.g., integration and differentiation, are not as straightforward in the sense of riemann-
stieltjes.
the way to handle the unfriendliness of the delta functions is to consider mild modi-
fications of the pmf. this notation of ‚Äúcumulative‚Äù distribution functions will allow us to
resolve the delta function problems. we will defer the technical details to the next chap-
ter. for the time being, we will briefly introduce the idea to prepare you for the technical
discussion later.
3.3.1 definition of the cumulative distribution function
definition 3.3. letxbe a discrete random variable with œâ ={x1, x2, . . .}. the
cumulative distribution function (cdf) of xis
fx(xk)def=p[x‚â§xk] =kx
‚Ñì=1px(x‚Ñì). (3.6)
ifœâ ={. . . ,‚àí1,0,1,2, . . .}, then the cdf of xis
fx(k)def=p[x‚â§k] =kx
‚Ñì=‚àí‚àûpx(‚Ñì). (3.7)
a cdf is essentially the cumulative sum of a pmf from ‚àí‚àûtox, where the variable x‚Ä≤in
the sum is a dummy variable.
121chapter 3. discrete random variables
example 3.6 . consider a random variable xwith pmf px(0) =1
4,px(1) =1
2and
px(4) =1
4. the cdf of xcan be computed as
fx(0) =p[x‚â§0] =px(0) =1
4,
fx(1) =p[x‚â§1] =px(0) + px(1) =3
4,
fx(4) =p[x‚â§4] =px(0) + px(1) + px(4) = 1 .
as shown in figure 3.13 , the cdf of a discrete random variable is a staircase function.
0.250.50.751
0 1 4
0 1 40.250.50.751
(a) pmf px(k) (b) cdf fx(k)
figure 3.13: illustration of a pmf and a cdf.
the matlab code and the python code used to generate figure 3.13 are shown
below. the cdf is computed using the command cumsum in matlab and np.cumsum in
python.
% matlab code to generate a pmf and a cdf
p = [0.25 0.5 0.25];
x = [0 1 4];
f = cumsum(p);
figure(1);
stem(x,p,‚Äò.‚Äô,‚Äòlinewidth‚Äô,4,‚Äòmarkersize‚Äô,50);
figure(2);
stairs([-4 x 10],[0 f 1],‚Äò.-‚Äô,‚Äòlinewidth‚Äô,4,‚Äòmarkersize‚Äô,50);
% python code to generate a pmf and a cdf
import numpy as np
import matplotlib.pyplot as plt
p = np.array([0.25, 0.5, 0.25])
x = np.array([0, 1, 4])
f = np.cumsum(p)
plt.stem(x,p,use_line_collection=true); plt.show()
plt.step(x,f); plt.show()
1223.3. cumulative distribution functions (discrete)
why is cdf a better-defined function than pmf? there are technical reasons associ-
ated with whether a function is integrable. without going into the details of these discus-
sions, a short answer is that delta functions are defined through integrations; they are not
functions. a delta function is defined as a function such that Œ¥(x) = 0 everywhere except at
x= 0, andr
œâŒ¥(x)dx= 1. on the other hand, a staircase function is always well-defined.
the discontinuous points of a staircase can be well defined if we specify the gap between
two consecutive steps. for example, in figure 3.13 , as soon as we specify the gap 1 /4, 1/2,
and 1 /4, the staircase function is completely defined.
example .figure 3.14 shows the empirical histogram of the english letters and the corre-
sponding empirical cdf. we want to differentiate pmf versus histogram and cdf versus
empirical cdf. the empirical cdf is the cdf computed from a finite dataset.
00.020.040.060.080.10.12
a b c d e f g h i j k l m n o p q r s t u v w x y z
a b c d e f g h i j k l m n o p q r s t u v w x y z00.10.20.30.40.50.60.70.80.91
figure 3.14: pmf and a cdf of the frequency of english letters.
3.3.2 properties of the cdf
we observe from the example in figure 3.13 that a cdf has several properties. first, being
a staircase function, the cdf is non-decreasing. it can stay constant for a while, but it never
drops. second, the minimum value of a cdf is 0, whereas the maximum value is 1. it is 0
for any value that is smaller than the first state; it is 1 for any value that is larger than the
last state. third, the gap at each jump is exactly the probability mass at that state. let us
summarize these observations in the following theorem.
theorem 3.2. ifxis a discrete random variable, then the cdf of xhas the following
properties:
(i) the cdf is a sequence of increasing unit steps.
(ii) the maximum of the cdf is when x=‚àû:fx(+‚àû) = 1 .
(iii) the minimum of the cdf is when x=‚àí‚àû:fx(‚àí‚àû) = 0 .
(iv) the unit steps have jumps at positions where px(x)>0.
proof . statement (i) can be seen from the summation
fx(x) =x
x‚Ä≤‚â§xpx(x‚Ä≤).
123chapter 3. discrete random variables
since the probability mass function is non-negative, the value of fxis larger when the value
of the argument is larger. that is, x‚â§yimplies fx(x)‚â§fx(y). the second statement (ii)
is true because the summation includes all possible states. so we have
fx(+‚àû) =‚àûx
x‚Ä≤=‚àí‚àûpx(x‚Ä≤) = 1 .
similarly, for the third statement (iii),
fx(‚àí‚àû) =x
x‚Ä≤‚â§‚àí‚àûpx(x‚Ä≤).
the summation is taken over an empty set, and so fx(‚àí‚àû) = 0. statement (iv) is true
because the cumulative sum changes only when there is a non-zero mass in the pmf. ‚ñ°
as we can see in the proof, the basic argument of the cdf is the cumulative sum of
the pmf. by definition, a cumulative sum always adds mass. this is why the cdf is always
increasing, has 0 at ‚àí‚àû, and has 1 at + ‚àû. this last statement deserves more attention. it
implies that the unit step always has a solid dot on the left -hand side and an empty dot
on the right -hand side, because when the cdf jumps, the final value is specified by the
‚Äú‚â§‚Äù sign in equation (3.6). the technical term for this property is right continuous .
3.3.3 converting between pmf and cdf
theorem 3.3. ifxis a discrete random variable, then the pmf of xcan be obtained
from the cdf by
px(xk) =fx(xk)‚àífx(xk‚àí1), (3.8)
where we assumed that xhas a countable set of states {x1, x2, . . .}. if the sample space
of the random variable xcontains integers from ‚àí‚àû to+‚àû, then the pmf can be
defined as
px(k) =fx(k)‚àífx(k‚àí1). (3.9)
example 3.7 . continuing with the example in figure 3.13 , if we are given the cdf
fx(0) =1
4, f x(1) =3
4, f x(4) = 1 ,
how do we find the pmf? we know that the pmf will have non-negative values only
atx= 0,1,4. for each of these x, we can show that
px(0) = fx(0)‚àífx(‚àí‚àû) =1
4‚àí0 =1
4,
px(1) = fx(1)‚àífx(0) =3
4‚àí1
4=1
2,
px(4) = fx(4)‚àífx(1) = 1 ‚àí3
4=1
4.
1243.4. expectation
3.4 expectation
when analyzing data, it is often useful to extract certain key parameters such as the mean
and the standard deviation. the mean and the standard deviation can be seen from the lens
of random variables. in this section, we will formalize the idea using expectation .
3.4.1 definition of expectation
definition 3.4. theexpectation of a random variable xis
e[x] =x
x‚ààx(œâ)x px(x). (3.10)
expectation is the mean of the random variable x. intuitively, we can think of px(x) as the
percentage of times that the random variable xattains the value x. when this percentage
is multiplied by x, we obtain the contribution of each x. summing over all possible values
ofxthen yields the mean. to see this more clearly, we can write the definition as
e[x] =x
x‚ààx(œâ)| {z }
sum over all statesx|{z}
a state xtakespx(x)|{z }
the percentage.
figure 3.15 illustrates a pmf that contains five states x1, . . . , x 5. corresponding to each
state are px(x1), . . . , p x(x5). for this pmf to make sense, we must assume that px(x1) +
¬∑¬∑¬∑+px(x5) = 1. to simplify notation, let us define pidef=px(xi). then the expectation
ofxis just the sum of the products: value ( xi) times height ( pi). this gives e[x] =p5
i=1xipx(xi).
figure 3.15: the expectation of a random variable is the sum of xipi.
we emphasize that the definition of the expectation is exactly the same as the usual
way we calculate the average of a dataset. when we calculate the average of a dataset
d={x(1), x(2), . . . , x(n)}, we sum up these nsamples and divide by the number of samples.
this is what we called the empirical average or the sample average:
average =1
nnx
n=1x(n). (3.11)
125chapter 3. discrete random variables
of course, in a typical dataset, these nsamples often take distinct values. but suppose
that among these nsamples there are only kdifferent values. for example, if we throw a
die a million times, every sample we record will be one of the six numbers. this situation
is illustrated in figure 3.16 , where we put the samples into the correct bin storing these
values. in this case, to calculate the average we are effectively doing a binning:
average =1
nkx
k=1value xk√ónumber of samples with value xk. (3.12)
equation (3.12) is exactly the same as equation (3.11), as long as the samples can be grouped
intokdifferent values. with a little calculation, we can rewrite equation (3.12) as
average =kx
k=1|{z}
sum of all statesvalue xk|{z}
a state xtakes√ónumber of samples with value xk
n| {z }
the percentage,
which is the same as the definition of expectation.
figure 3.16: if we have a dataset dcontaining nsamples, and if there are only kdistinct values, we
can effectively put these nsamples into kbins. thus, the ‚Äúaverage‚Äù (which is the sum divided by the
number n) is exactly the same as our definition of expectation.
the difference between e[x] and the average is that e[x] is computed from the ideal
histogram, whereas average is computed from the empirical histogram. when the number of
samples napproaches infinity, we expect the average to approximate e[x]. however, when
nis small, the empirical average will have random fluctuations around e[x]. every time
we experiment, the empirical average may be slightly different. therefore, we can regard
e[x] as the true average of a certain random variable, and the empirical average as a finite-
sample average based on the particular experiment we are working with. this summarizes
key concept 3 of this chapter.
key concept 3: what is expectation?
expectation = mean = average computed from a pmf.
if we are given a dataset on a computer, computing the mean can be done by calling
the command mean in matlab and np.mean in python. the example below shows the
case of finding the mean of 10000 uniformly distributed random numbers.
1263.4. expectation
% matlab code to compute the mean of a dataset
x = rand(10000,1);
mx = mean(x);
# python code to compute the mean of a dataset
import numpy as np
x = np.random.rand(10000)
mx = np.mean(x)
example 3.8 . let xbe a random variable with pmf px(0) = 1 /4,px(1) = 1 /2 and
px(2) = 1 /4. we can show that the expectation is
e[x] = (0)1
4
|{z}
px(0)+ (1)1
2
|{z}
px(1)+ (2)1
4
|{z}
px(2)= 1.
on matlab and python, if we know the pmf then computing the expectation is
straight-forward. here is the code to compute the above example.
% matlab code to compute the expectation
p = [0.25 0.5 0.25];
x = [0 1 2];
ex = sum(p.*x);
# python code to compute the expectation
import numpy as np
p = np.array([0.25, 0.5, 0.25])
x = np.array([0, 1, 2])
ex = np.sum(p*x)
example 3.9 . flip an unfair coin, where the probability of getting a head is3
4. let
xbe a random variable such that x= 1 means getting a head. then we can show
thatpx(1) =3
4andpx(0) =1
4. the expectation of xis therefore
e[x] = (1) px(1) + (0) px(0) = (1)3
4
+ (0)1
4
=3
4.
center of mass . how would you interpret the result of this example? does it mean
that, on average, we will get 3 /4 heads (but there is not anything called 3 /4 heads!). recall
the definition of a random variable: it is a translator that translates a descriptive state
to a number on the real line. thus the expectation, which is an operation defined on the
real line, can only tell us what is happening on the real line, not in the original sample
127chapter 3. discrete random variables
figure 3.17: center of mass. if a state x2is more influential than another state x1, the center of mass
e[x]will lean towards x2.
space. on the real line, the expectation can be regarded as the center of mass , which is the
point where the ‚Äúforces‚Äù between the two states are ‚Äúbalanced‚Äù. in figure 3.17 we depict a
random variable with two states x1andx2. the state x1has less influence (because px(x1)
is smaller) than x2. therefore the center of mass is shifted towards x2. this result shows us
that the value e[x] is not necessarily in the sample space. e[x] is a deterministic number
with nothing to do with the sample space.
example 3.10 . let xbe a random variable with pmf px(k) =1
2k, fork= 1,2,3, . . ..
the expectation is
e[x] =‚àûx
k=1kpx(k) =‚àûx
k=1k¬∑1
2k
=1
2‚àûx
k=1k¬∑1
2k‚àí1=1
2¬∑1
(1‚àí1
2)2= 2.
on matlab and python, if you want to verify this answer you can use the following
code. here, we approximate the infinite sum by a finite sum of k= 1, . . . , 100.
% matlab code to compute the expectation
k = 1:100;
p = 0.5.^k;
ex = sum(p.*k);
# python code to compute the expectation
import numpy as np
k = np.arange(100)
p = np.power(0.5,k)
ex = np.sum(p*k)
example 3.11 . roll a die twice. let xbe the first roll and ybe the second roll.
letz= max( x, y). to compute the expectation e[z], we first construct the sample
space. since there are two rolls, we can construct a table listing all possible pairs of
outcomes. this will give us {(1,1),(1,2), . . . , (6,6)}. now, we calculate z, which is the
max of the two rolls. so if we have (1 ,3), then the max will be 3, whereas if we have
(5,2), then the max will be 5. we can complete a table as shown below.
1283.4. expectation
1 2 3 4 5 6
11 2 3 4 5 6
22 2 3 4 5 6
33 3 3 4 5 6
44 4 4 4 5 6
55 5 5 5 5 6
66 6 6 6 6 6
this table tell us that zhas 6 states. the pmf of zcan be determined by
counting the number of times a state shows up in the table. thus, we can show that
pz(1) =1
36, pz(2) =3
36, pz(3) =5
36,
pz(4) =7
36, pz(5) =9
36, pz(6) =11
36.
the expectation of zis therefore
e[z] = (1)1
36
+ (2)3
36
+ (3)5
36
+ (4)7
36
+ (5)9
36
+ (6)11
36
=161
36.
example 3.12 . consider a game in which we flip a coin 3 times. the reward of the
game is
‚Ä¢ $1 if there are 2 heads
‚Ä¢ $8 if there are 3 heads
‚Ä¢ $0 if there are 0 or 1 head
there is a cost associated with the game. to enter the game, the player has to pay
$1.50. we want to compute the net gain, on average.
to answer this question, we first note that the sample space contains 8 elements:
hhh, hht, hth, thh, tht, tth, htt, ttt. let xbe the number of heads.
then the pmf of xis
px(0) =1
8, px(1) =3
8, px(2) =3
8, px(3) =1
8.
we then let ybe the reward. the pmf of ycan be found by ‚Äúadding‚Äù the probabilities
ofx. this yields
py(0) = px(0) + px(1) =4
8, py(1) = px(2) =3
8, py(8) = px(3) =1
8.
129chapter 3. discrete random variables
the expectation of yis
e[x] = (0)4
8
+ (1)3
8
+ (8)1
8
=11
8.
since the cost of the game is12
8, the net gain (on average) is ‚àí1
8.
3.4.2 existence of expectation
does every pmf have an expectation? no, because we can construct a pmf such that the
expectation is undefined.
example 3.13 . consider a random variable xwith the following pmf:
px(k) =6
œÄ2k2, k = 1,2, . . . .
using a result from algebra, one can show thatp‚àû
k=11
k2=œÄ2
6. therefore, px(k) is a
legitimate pmf becausep‚àû
k=1px(k) = 1. however, the expectation diverges, because
e[x] =‚àûx
k=1kpx(k)
=6
œÄ2‚àûx
k=11
k‚Üí ‚àû ,
where the limit is due to the harmonic seriesa: 1 +1
2+1
3+¬∑¬∑¬∑=‚àû.
ahttps://en.wikipedia.org/wiki/harmonic_series_(mathematics)
a pmf has an expectation when it is absolutely summable .
definition 3.5. a discrete random variable xisabsolutely summable if
e[|x|]def=x
x‚ààx(œâ)|x|px(x)<‚àû. (3.13)
this definition tells us that not all random variables have a finite expectation. this
is a very important mathematical result, but its practical implication is arguably limited.
most of the random variables we use in practice are absolutely summable. also, note that
the property of absolute summability applies to discrete random variables. for continuous
random variables, we have a parallel concept called absolute integrability , which will be
discussed in the next chapter.
3.4.3 properties of expectation
the expectation of a random variable has several useful properties. we list them below.
note that these properties apply to both discrete and continuous random variables.
1303.4. expectation
theorem 3.4. the expectation of a random variable xhas the following properties:
(i)function . for any function g,
e[g(x)] =x
x‚ààx(œâ)g(x)px(x).
(ii)linearity . for any function gandh,
e[g(x) +h(x)] =e[g(x)] +e[h(x)].
(iii) scale . for any constant c,
e[cx] =ce[x].
(iv)dc shift . for any constant c,
e[x+c] =e[x] +c.
proof of (i) : a pictorial proof of (i) is shown in figure 3.18 . the key idea is a change of
variable.
figure 3.18: by letting g(x) =y, the pmfs are not changed. what changes are the states.
when we have a function y=g(x), the pmf of ywill have impulses moved from x
(the horizontal axis) to g(x) (the vertical axis). the pmf values (i.e., the probabilities or
the height of the stems), however, are not changed. if the mapping g(x) is many-to-one,
multiple pmf values will add to the same position. therefore, when we compute e[g(x)],
we compute the expectation along the vertical axis.
practice exercise 3.3 . prove statement (iii): for any constant c,e[cx] =ce[x].
solution . recall the definition of expectation:
e[cx] =x
x‚ààx(œâ)(cx)px(x) =cx
x‚ààx(œâ)xpx(x)
|{z }
=e[x]=ce[x].
statement (iii) is illustrated in figure 3.19 . here, we assume that the original pmf has 3
131chapter 3. discrete random variables
states x= 0,1,2. we multiply xby a constant c= 3. this changes xtocx= 0,3,6.
however, since the probabilities are not changed, the height of the pmf values remains.
therefore, when computing the expectation, we just multiply e[x] bycto get ce[x].
figure 3.19: pictorial representation of e[cx] =ce[x]. when we multiply xbyc, we fix the probabil-
ities but make the spacing between states wider/narrower.
practice exercise 3.4 . prove statement (ii): for any function gandh,e[g(x) +
h(x)] =e[g(x)] +e[h(x)].
solution . recall the definition of expectation:
e[g(x) +h(x)] =x
x‚ààx(œâ)[g(x) +h(x)]px(x)
=x
x‚ààx(œâ)g(x)px(x)
| {z }
=e[g(x)]+x
x‚ààx(œâ)h(x)px(x)
| {z }
=e[h(x)]
=e[g(x)] +e[h(x)].
practice exercise 3.5 . prove statement (iv): for any constant c,e[x+c] =e[x]+c.
solution . recall the definition of expectation:
e[x+c] =x
x‚ààx(œâ)(x+c)px(x)
=x
x‚ààx(œâ)xpx(x)
|{z }
=e[x]+c¬∑x
x‚ààx(œâ)px(x)
|{z}
=1
=e[x] +c.
this result is illustrated in figure 3.20 . as we add a constant to the random variable,
its pmf values remain the same but their positions are shifted. therefore, when computing
the mean, the mean will be shifted accordingly.
1323.4. expectation
figure 3.20: pictorial representation of e[x+c] =e[x]+c. when we add ctox, we fix the probabilities
and shift the entire pmf to the left or to the right.
example 3.14 . let xbe a random variable with four equally probable states 0 ,1,2,3.
we want to compute the expectation e[cos(œÄx/2)]. to do so, we note that
e[cos(œÄx/2)] =x
x‚ààx(œâ)cosœÄx
2
px(x)
= (cos 0)1
4
+ (cosœÄ
2)1
4
+ (cos2œÄ
2)1
4
+ (cos3œÄ
2)1
4
=1 + 0 + ( ‚àí1) + 0
4= 0.
example 3.15 . let xbe a random variable with e[x] = 1 and e[x2] = 3. we want
to find the expectation e[(ax+b)2]. to do so, we realize that
e[(ax+b)2](a)=e[a2x2+ 2abx+b2](b)=a2e[x2] + 2abe[x] +b2= 3a2+ 2ab+b2,
where ( a) is due to expansion of the square, and ( b) holds in two steps. the first step
is to apply statement (ii) for individual functions of expectations, and the second step
is to apply statement (iii) for scalar multiple of the expectations.
3.4.4 moments and variance
based on the concept of expectation, we can define a moment :
definition 3.6. thekth moment of a random variable xis
e[xk] =x
xxkpx(x). (3.14)
essentially, the kth moment is the expectation applied to xk. the definition follows from
statement (i) of the expectation‚Äôs properties. using this definition, we note that e[x] is the
first moment and e[x2] is the second moment. higher-order moments can be defined, but
in practice they are less commonly used.
133chapter 3. discrete random variables
example 3.16 . flip a coin 3 times. let xbe the number of heads. then
px(0) =1
8, px(1) =3
8, px(2) =3
8, px(3) =1
8.
the second moment e[x2] is
e[x2] = (0)21
8
+ (1)23
8
+ (2)23
8
+ (4)21
8
= 3.
example 3.17 . consider a random variable xwith pmf
px(k) =1
2k, k = 1,2, . . . .
the second moment e[x2] is
e[x2] =‚àûx
k=1k21
2k
=1
22‚àûx
k=1k(k‚àí1 + 1)1
2k‚àí2
=1
22‚àûx
k=1k(k‚àí1)1
2k‚àí2
+1
22‚àûx
k=1k1
2k‚àí2
=1
222
(1‚àí1
2)3
+1
21
(1‚àí1
2)2
= 6.
using the second moment, we can define the variance of a random variable.
definition 3.7. thevariance of a random variable xis
var[x] =e[(x‚àí¬µ)2], (3.15)
where ¬µ=e[x]is the expectation of x.
we denote œÉ2by var[ x]. the square root of the variance, œÉ, is called the standard deviation
ofx. like the expectation e[x], the variance var[ x] is computed using the ideal histogram
pmf. it is the limiting object of the usual standard deviation we calculate from a dataset.
on a computer, computing the variance of a dataset is done by calling built-in com-
mands such as varin matlab and np.var in python. the standard deviation is computed
using stdandnp.std , respectively.
% matlab code to compute the variance
x = rand(10000,1);
vx = var(x);
sx = std(x);
% python code to compute the variance
import numpy as np
1343.4. expectation
x = np.random.rand(10000)
vx = np.var(x)
sx = np.std(x)
what does the variance mean? it is a measure of the deviation of the random variable
xrelative to its mean. this deviation is quantified by the squared difference ( x‚àí¬µ)2. the
expectation operator takes the average of the deviation, giving us a deterministic number
e[(x‚àí¬µ)2].
theorem 3.5. the variance of a random variable xhas the following properties:
(i)moment .
var[x] =e[x2]‚àíe[x]2.
(ii)scale . for any constant c,
var[cx] =c2var[x].
(iii) dc shift . for any constant c,
var[x+c] = var[ x].
figure 3.21: pictorial representations of var[cx] =c2var[x]andvar[x+c] = var[ x].
practice exercise 3.6 . prove theorem 3.5 above.
solution . for statement (i), we show that
var[x] =e[(x‚àí¬µ)2] =e[x2‚àí2x¬µ+¬µ2] =e[x2]‚àí¬µ2.
135chapter 3. discrete random variables
statement (ii) holds because e[cx] =c¬µand
var[cx] =e[(cx‚àíe[cx])2]
=e[(cx‚àíc¬µ)2] =c2e[(x‚àí¬µ)2] =c2var[x].
statement (iii) holds because
var[x+c] =e[((x+c)‚àíe[x+c])2] =e[(x‚àíe[x])2] = var[ x].
the properties above are useful in various ways. the first statement provides a link connect-
ing variance and the second moment. statement (ii) implies that when xis scaled by c, the
variance should be scaled by c2because of the square in the second moment. statement (iii)
says that when xis shifted by a scalar c, the variance is unchanged. this is true because
no matter how we shift the mean, the fluctuation of the random variable remains the same.
practice exercise 3.7 . flip a coin with probability pto get a head. let xbe a
random variable denoting the outcome. the pmf of xis
px(0) = 1 ‚àíp, p x(1) = p.
finde[x],e[x2] and var[ x].
solution . the expectation of xis
e[x] = (0) px(0) + (1) px(1) = (0)(1 ‚àíp) + (1)( p) =p.
the second moment is
e[x2] = (0)2px(0) + (1)2px(1) = p.
the variance is
var[x] =e[x2]‚àíe[x]2=p‚àíp2=p(1‚àíp).
3.5 common discrete random variables
in the previous sections, we have conveyed three key concepts: one about the random vari-
able, one about the pmf, and one about the mean. the next step is to introduce a few
commonly used discrete random variables so that you have something concrete in your ‚Äútool-
box.‚Äù as we have mentioned before, these predefined random variables should be studied
from a synthesis perspective (sometimes called generative ). the plan for this section is to
introduce several models, derive their theoretical properties, and discuss examples.
note that some extra effort will be required to understand the origins of the random
variables. the origins of random variables are usually overlooked, but they are more impor-
tant than the equations. for example, we will shortly discuss the poisson random variable
1363.5. common discrete random variables
figure 3.22: a bernoulli random variable has two states with probability pand1‚àíp.
and its pmf px(k) =Œªke‚àíŒª
k!. why is the poisson random variable defined in this way? if
you know how the poisson pmf was originally derived, you will understand the assumptions
made during the derivation. consequently, you will know why poisson is a good model for
internet traffic, recommendation scores, and image sensors for computer vision applications.
you will also know under what situation the poisson model will fail. understanding the
physics behind the probability models is the focus of this section.
3.5.1 bernoulli random variable
we start discussing the simplest random variable, namely the bernoulli random variable .
a bernoulli random variable is a coin-flip random variable. the random variable has two
states: either 1 or 0. the probability of getting 1 is p, and the probability of getting 0 is
1‚àíp. see figure 3.22 for an illustration. bernoulli random variables are useful for all kinds
of binary state events: coin flip (h or t), binary bit (1 or 0), true or false, yes or no, present
or absent, democrat or republican, etc.
to make these notions more precise, we define a bernoulli random variable as follows.
definition 3.8. letxbe abernoulli random variable . then, the pmf of xis
px(0) = 1 ‚àíp, p x(1) = p,
where 0< p < 1is called the bernoulli parameter. we write
x‚àºbernoulli( p)
to say that xis drawn from a bernoulli distribution with a parameter p.
in this definition, the parameter pcontrols the probability of obtaining 1. in a coin-flip event,
pis usually1
2, meaning that the coin is fair. however, for biased coins pis not necessarily1
2.
for other situations such as binary bits (0 or 1), the probability of obtaining 1 could be very
different from the probability of obtaining 0.
in matlab and python, generating bernoulli random variables can be done by call-
ing the binomial random number generator np.random.binomial (python) and binornd
(matlab). when the parameter nis equal to 1, the binomial random variable is equiv-
alent to a bernoulli random variable. the matlab and python codes to synthesize a
bernoulli random variable are shown below.
137chapter 3. discrete random variables
% matlab code to generate 1000 bernoulli random variables
p = 0.5;
n = 1;
x = binornd(n,p,[1000,1]);
[num, ~] = hist(x, 10);
bar(linspace(0,1,10), num,‚Äòfacecolor‚Äô,[0.4, 0.4, 0.8]);
# python code to generate 1000 bernoulli random variables
import numpy as np
import matplotlib.pyplot as plt
p = 0.5
n = 1
x = np.random.binomial(n,p,size=1000)
plt.hist(x,bins=‚Äòauto‚Äô)
an alternative method in python is to call stats.bernoulli.rvs to generate random
bernoulli numbers.
# python code to call scipy.stats library
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
p = 0.5
x = stats.bernoulli.rvs(p,size=1000)
plt.hist(x,bins=‚Äòauto‚Äô);
properties of bernoulli random variables
let us now derive a few key statistical properties of a bernoulli random variable.
theorem 3.6. ifx‚àºbernoulli( p), then
e[x] =p, e[x2] =p, var[x] =p(1‚àíp).
proof . the expectation can be computed as
e[x] = (1) px(1) + (0) px(0) = (1)( p) + (0)(1 ‚àíp) =p.
the second moment is
e[x2] = (12)(p) + (02)(1‚àíp) =p.
therefore, the variance is
var[x] =e[x2]‚àí¬µ2=p‚àíp2=p(1‚àíp).
‚ñ°
a useful property of the python code is that we can construct an object rv. then we
can call rv‚Äôs attributes to determine its mean, variance, etc.
1383.5. common discrete random variables
# python code to generate a bernoulli rv object
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
p = 0.5
rv = stats.bernoulli(p)
mean, var = rv.stats(moments=‚Äòmv‚Äô)
print(mean, var)
in both matlab and python, we can plot the pmf of a bernoulli random variable,
such as the one shown in figure 3.23 . to do this in matlab, we call the function binopdf ,
with the evaluation points specified by x.
00.20.40.60.81
-0.2 0 0.2 0.4 0.6 0.8 1 1.2
figure 3.23: an example of a theoretical pmf (not the empirical histogram) plotted by matlab.
% matlab code to plot the pmf of a bernoulli
p = 0.3;
x = [0,1];
f = binopdf(x,1,p);
stem(x, f, ‚Äòbo‚Äô, ‚Äòlinewidth‚Äô, 8);
in python, we construct a random variable rv. with rv, we can call its pmf rv.pmf :
# python code to plot the pmf of a bernoulli
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
p = 0.3
rv = stats.bernoulli(p)
x = np.linspace(0, 1, 2)
f = rv.pmf(x)
plt.plot(x, f, ‚Äòbo‚Äô, ms=10);
plt.vlines(x, 0, f, colors=‚Äòb‚Äô, lw=5, alpha=0.5);
139chapter 3. discrete random variables
when will a bernoulli random variable have the maximum variance?
let us take a look at the variance of the bernoulli random variable. for any given p, the
variance is p(1‚àíp). this is a quadratic equation. if we let v(p) =p(1‚àíp), we can show that
the maximum is attained at p= 1/2. to see this, take the derivative of v(p) with respect
top. this will give usd
dpv(p) = 1 ‚àí2p. equating to zero yields 1 ‚àí2p= 0, so p= 1/2.
we know that p= 1/2 is a maximum and not a minimum point because the second order
derivative v‚Ä≤‚Ä≤(p) =‚àí2, which is negative. therefore v(p) is maximized at p= 1/2. now,
since 0 ‚â§p‚â§1, we also know that v(0) = 0 and v(1) = 0. therefore, the variance is
minimized at p= 0 and p= 1.figure 3.24 shows a graph of the variance.
figure 3.24: the variance of a bernoulli reaches maximum at p= 1/2.
does this result make sense? why is the variance maximized at p= 1/2? if we think
about this problem more carefully, we realize that a bernoulli random variable represents a
coin-flip experiment. if the coin is biased such that it always gives heads, on the one hand,
it is certainly a bad coin. however, on the other hand, the variance is zero because there
is nothing to vary; you will certainly get heads. the same situation happens if the coin is
biased towards tails. however, if the coin is fair, i.e., p= 1/2, then the variance is large
because we only have a 50% chance of getting a head or a tail whenever we flip a coin.
nothing is certain in this case. therefore, the maximum variance happening at p= 1/2
matches our intuition.
rademacher random variable
a slight variation of the bernoulli random variable is the rademacher random variable ,
which has two states: +1 and ‚àí1. the probability getting +1 and ‚àí1 is 1 /2. therefore, the
pmf of a rademacher random variable is
px(‚àí1) =1
2,and px(+1) =1
2.
practice exercise 3.8 . show that if xis a rademacher random variable then
(x+ 1)/2‚àºbernoulli(1 /2). also show the converse: if y‚àºbernoulli(1 /2) then 2 y‚àí1
is a rademacher random variable.
solution . since xcan either be +1 or ‚àí1, we show that if x= +1 then ( x+1)/2 = 1
and if x=‚àí1 then ( x+ 1)/2 = 0. the probabilities of getting +1 and ‚àí1 are equal.
thus, the probabilities of getting ( x+ 1)/2 = 1 and 0 are also equal. so the resulting
random variable is bernoulli(1/2). the other direction can be proved similarly.
1403.5. common discrete random variables
bernoulli in social networks: the erdÀù os-r¬¥ enyi graph
the study of networks is a big branch of modern data science. it includes social networks,
computer networks, traffic networks, etc. the history of network science is very long, but
one of the most basic models of a network is the erdÀù os-r¬¥ enyi graph, named after paul
erdÀù os and alfr¬¥ ed r¬¥ enyi. the underlying probabilistic model of the erdÀù os-r¬¥ enyi graph is
the bernoulli random variable.
to see how a graph can be constructed from a bernoulli random variable, we first
introduce the concept of a graph . a graph contains two elements: nodes and edges. for
node iand node j, we denote the edge connecting iandjasaij. therefore, if we have n
nodes, then we can construct a matrix aof size n√ón. we call this matrix the adjacency
matrix . for example, the adjacency matrix
a=Ô£Æ
Ô£ØÔ£ØÔ£∞0 1 1 0
1 0 0 0
1 0 0 1
0 0 1 0Ô£π
Ô£∫Ô£∫Ô£ª
will have edges for node pairs (1 ,2), (1 ,3), and (3 ,4). note that in this example we assume
that the adjacency matrix is symmetric, meaning that the graph is undirected. the ‚Äú1‚Äù in
the adjacency matrix indicates there is an edge, and ‚Äú0‚Äù indicates there is no edge. so a
represents a binary graph.
the erdÀù os-r¬¥ enyi graph model says that the probability of getting an edge is an inde-
pendent bernoulli random variable. that is
aij‚àºbernoulli( p),
fori < j . if we model the graph in this way, then the parameter pwill control the density
of the graph. high values of pmean that there is a higher chance for an edge to be present.
-2 0 2-3-2-1012p = 0.3
  1
  2  3
  4
  5  6  7
  8  9  10
  11  12
  13
  14  15  16  17  18  19  20
  21  22  23  24
  25  26  27
  28  29
  30  31
  32  33
  34  35
  36
  37
  38  39
  40
-2 0 2-4-3-2-10123p = 0.5
  1
  2  3  4
  5  6  7
  8
  9
  10  11
  12  13  14  15
  16  17
  18   19  20
  21
  22
  23  24  25
  26  27  28
  29  30
  31  32  33
  34
  35
  36
  37  38
  39  40
-4 -2 0 2 4-4-3-2-10123p = 0.7
  1
  2
  3  4
  5  6
  7
  8  9  10
  11  12
  13  14
  15  16  17
  18
  19  20  21
  22  23
  24
  25  26  27  28
  29  30  31
  32  33
  34  35  36
  37  38  39
  40
-4 -2 0 2 4-4-2024p = 0.9
  1
  2  3
  4  5
  6  7
  8
  9  10
  11  12  13
  14  15
  16
  17  18
  19  20
  21  22
  23  24
  25
  26  27  28
  29
  30  31  32  33  34  35
  36
  37  38
  39  40
figure 3.25: the erdÀù os-r¬¥ enyi graph. [top] the graphs. [bottom] the adjacency matrices.
141chapter 3. discrete random variables
to illustrate the idea of an erdÀù os-r¬¥ enyi graph, we show in figure 3.25 a graph of
40 nodes. the edges are randomly selected by flipping a bernoulli random variable with
parameter p= 0.3,0.5,0.7,0.9. as we can see in the figure, a small value of pgives a graph
with very sparse connectivity, whereas a large value of pgives a very densely connected
graph. the bottom row of figure 3.25 shows the corresponding adjacency matrices. here,
a white pixel denotes ‚Äú1‚Äù in the matrix and a black pixel denotes ‚Äú0‚Äù in the matrix.
while erdÀù os-r¬¥ enyi graphs are elementary, their variations can be realistic models of
social networks. the stochastic block model is one such model. in a stochastic block model,
nodes form small communities within a large network. for example, there are many majors
in a university. students within the same major tend to have more interactions than with
students of another major. the stochastic block model achieves this goal by partitioning
the nodes into communities. within each community, the nodes can have a high degree of
connectivity. across different communities, the connectivity will be much lower. figure 3.26
illustrates a network and the corresponding adjacency matrix. in this example, the network
has three communities.
-4 -3 -2 -1 0 1 2 3 4-6-4-2024
  1  2
  3  4  5
  6  7
  8
  9  10
  11  12
  13  14
  15
  16
  17  18  19
  20  21  22   23  24  25
  26  27  28
  29  30  31  32
  33
  34  35  36  37
  38  39
  40  41   42  43
  44  45
  46  47
  48
  49  50
  51
  52
  53  54  55
  56  57  58  59  60
  61   62  63
  64  65  66  67
  68  69
  70  71  72
  73  74  75
  76  77  78  79
  80
  81  82  83  84
  85  86  87
  88  89
  90
  91  92  93
  94  95
  96
  97  98  99  100
figure 3.26: a stochastic block model containing three communities. [left] the graph. [right] the
adjacency matrix.
in network analysis, one of the biggest problems is determining the community struc-
ture and recovering the underlying probabilities. the former task is about grouping the
nodes into blocks. this is a nontrivial problem because in practice the nodes are never
arranged nicely, as shown in figure 3.26 . for example, why should alice be node 1 and
bob be node 2? since we never know the correct ordering of the nodes, partitioning the
nodes into blocks requires various estimation techniques such as clustering or iterative esti-
mation. recovering the underlying probability is also not easy. given an adjacency matrix,
why can we assume that the underlying network is a stochastic block model? even if the
model is correct, there will be imperfect grouping in the previous step. as such, estimat-
ing the underlying probability in the presence of these uncertainties would pose additional
challenges.
today, network analysis remains one of the hottest areas in data science. its importance
derives from its broad scope and impact. it can be used to analyze social networks, opinion
polls, marketing, or even genome analysis. nevertheless, the starting point of these advanced
subjects is the bernoulli random variable, the random variable of a coin flip!
1423.5. common discrete random variables
3.5.2 binomial random variable
suppose we flip the coin ntimes count the number of heads. since each coin flip is a random
variable (bernoulli), the sum is also a random variable. it turns out that this new random
variable is the binomial random variable .
definition 3.9. letxbe abinomial random variable . then, the pmf of xis
px(k) =n
k
pk(1‚àíp)n‚àík, k = 0,1, . . . , n,
where 0< p < 1is the binomial parameter, and nis the total number of states. we
write
x‚àºbinomial( n, p)
to say that xis drawn from a binomial distribution with a parameter pof size n.
to understand the meaning of a binomial random variable, consider a simple experiment
consisting of flipping a coin three times. we know that all possible cases are hhh, hht,
hth, thh, tth, tht, htt and ttt. now, suppose we define x= number of heads.
we want to write down the probability mass function. effectively, we ask: what is the
probability of getting 0 head, one head, two heads, and three heads? we can, of course,
count and get the answer right away for a fair coin. however, suppose the coin is unfair,
i.e., the probability of getting a head is pwhereas that of a tail is 1 ‚àíp. the probability of
getting each of the 8 cases is shown in figure 3.27 below.
figure 3.27: the probability of getting kheads out of n= 3coins.
here are the detailed calculations. let us start with x= 3.
px(3) =p[{hhh}]
=p[{h} ‚à© {h} ‚à© {h}]
(a)=p[{h}]p[{h}]p[{h}]
(b)=p3,
where ( a) holds because the three events are independent. (recall that if aandbare
independent then p[a‚à©b] =p[a]p[b].) (b) holds because each p[{h}] =pby definition.
with exactly the same argument, we can show that px(0) =p[{ttt}] = (1 ‚àíp)3.
143chapter 3. discrete random variables
now, let us look at px(2), i.e., 2 heads. this probability can be calculated as follows:
px(2) =p[{hht} ‚à™ {hth} ‚à™ {thh}]
(c)=p[{hht}] +p[{hth}] +p[{thh}]
(d)=p2(1‚àíp) +p2(1‚àíp) +p2(1‚àíp) = 3 p2(1‚àíp),
where ( c) holds because the three events hht, hth and thh are disjoint in the sample
space. note that we are not using the independence argument in ( c) but the disjoint argu-
ment. we should not confuse the two. the step in ( d) uses independence, because each coin
flip is independent.
the above calculation shows an interesting phenomenon: although the three events
hht, hth, and thh are different (in fact, disjoint), the number of heads in all the cases
is the same. this happens because when counting the number of heads, the ordering of the
heads and tails does not matter. so the same problem can be formulated as finding the
number of combinations of {2 heads and 1 tail }, which in our case is 3
2
= 3.
to complete the story, let us also try px(1). this probability is
px(1) =p[{tth} ‚à™ {htt} ‚à™ {tht}] = 3p(1‚àíp)2.
again, we see that the combination 3
1
= 3 appears in front of the p(1‚àíp)2.
in general, the way to interpret the binomial random variable is to decouple the prob-
abilities p, (1‚àíp), and the number of combinations n
k
:
px(k) =n
k
|{z }
number of combinationspk
|{z}
prob getting kh‚Äôs(1‚àíp)n‚àík
| {z }
prob getting n‚àíkt‚Äôs.
the running index kshould go with 0 ,1, . . . , n . it starts with 0 because there could be zero
heads in the sample space. furthermore, we note that in this definition, two parameters are
driving a binomial random variable: the number of bernoulli trials nand the underlying
probability for each coin flip p. as such, the notation for a binomial random variable is
binomial( n, p), with two arguments.
the histogram of a binomial random variable is shown in figure 3.28 (a). here, we con-
sider the example where n= 10 and p= 0.5. to generate the histogram, we use 5000 samples.
in matlab and python, generating binomial random variables as in figure 3.28 (a) can
be done by calling binornd andnp.random.binomial .
% matlab code to generate 5000 binomial random variables
p = 0.5;
n = 10;
x = binornd(n,p,[5000,1]);
[num, ~] = hist(x, 10);
bar( num,‚Äòfacecolor‚Äô,[0.4, 0.4, 0.8]);
# python code to generate 5000 binomial random variables
import numpy as np
import matplotlib.pyplot as plt
1443.5. common discrete random variables
1 2 3 4 5 6 7 8 9 10020040060080010001200
00.050.10.150.20.25
0 2 4 6 8 10
(a) histogram based on 5000 samples (b) pmf
figure 3.28: an example of a binomial distribution with n= 10 ,p= 0.5.
p = 0.5
n = 10
x = np.random.binomial(n,p,size=5000)
plt.hist(x,bins=‚Äòauto‚Äô);
generating the ideal pmf of a binomial random variable as shown in figure 3.28 (b)
can be done by calling binopdf in matlab. in python, we can define a random variable
rvthrough stats.binom , and call the pmf using rv.pmf .
% matlab code to generate a binomial pmf
p = 0.5;
n = 10;
x = 0:10;
f = binopdf(x,n,p);
stem(x, f, ‚Äôo‚Äô, ‚Äôlinewidth‚Äô, 8, ‚Äôcolor‚Äô, [0.8, 0.4, 0.4]);
# python code to generate a binomial pmf
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
p = 0.5
n = 10
rv = stats.binom(n,p)
x = np.arange(11)
f = rv.pmf(x)
plt.plot(x, f, ‚Äôbo‚Äô, ms=10);
plt.vlines(x, 0, f, colors=‚Äôb‚Äô, lw=5, alpha=0.5);
the shape of the binomial pmf is shown in figure 3.29 . in this set of figures, we vary
one of the two parameters nandpwhile keeping the other fixed. in figure 3.29 (a), we fix
n= 60 and plot three sets of p= 0.1,0.5,0.9. for small pthe pmf is skewed towards the
left, and for large pthe pmf is skewed toward the right. figure 3.29 (b) shows the pmf
145chapter 3. discrete random variables
for a fixed p= 0.5. as we increase n, the centroid of the pmf moves towards the right.
thus we should expect the mean of a binomial random variable to increase with p. another
interesting observation is that as nincreases, the shape of the pmf approaches the gaussian
function (the bell-shaped curve). we will explain the reason for this when we discuss the
central limit theorem.
00.050.10.150.2
0 10 20 30 40 50 60p = 0.1
p = 0.5
p = 0.9
00.10.20.30.4
0 10 20 30 40 50 60n = 5
n = 50
n = 100
(a)n= 60 (b) p= 0.5
figure 3.29: pmfs of a binomial random variable x‚àºbinomial (n, p). (a) we assume that n= 60 . by
varying the probability p, we see that the pmf shifts from the left to the right, and the shape changes.
(b) we assume that p= 0.5. by varying the number of trials, the pmf shifts and the shape becomes
more ‚Äúbell-shaped.‚Äù
the expectation, second moment, and variance of a binomial random variable are
summarized in theorem 3.7.
theorem 3.7. ifx‚àºbinomial( n, p), then
e[x] =np,
e[x2] =np(np+ (1‚àíp)),
var[x] =np(1‚àíp).
we will prove that e[x] =npusing the first principle. for e[x2] and var[ x], we will skip
the proofs here and will introduce a ‚Äúshortcut‚Äù later.
proof . let us start with the definition.
e[x] =nx
k=0k¬∑n
k
pk(1‚àíp)n‚àík
=nx
k=0k¬∑n!
k!(n‚àík)!pk(1‚àíp)n‚àík
= 0¬∑n!
0!(n‚àí0)!p0(1‚àíp)n‚àí0
| {z }
0+nx
k=1k¬∑n!
k!(n‚àík)!pk(1‚àíp)n‚àík
=nx
k=1n!
(k‚àí1)!(n‚àík)!pk(1‚àíp)n‚àík.
1463.5. common discrete random variables
note that we have shifted the index from k= 0 to k= 1. now let us apply a trick:
e[x] =nx
k=1n!
(k‚àí1)!(n‚àík)!pk(1‚àíp)n‚àík
=nx
k=1n!
(k‚àí1)!(n‚àík‚àí1 + 1)!pk(1‚àíp)n‚àík.
using this trick, we can show that
nx
k=1n!
(k‚àí1)!(n‚àík‚àí1 + 1)!pk(1‚àíp)n‚àík
=nx
k=1n!
(k‚àí1)!((n‚àí1)‚àí(k‚àí1))!pk(1‚àíp)n‚àík
=nx
k=1n(n‚àí1)!
(k‚àí1)!((n‚àí1)‚àí(k‚àí1))!pk(1‚àíp)n‚àík
=npnx
k=1(n‚àí1)!
(k‚àí1)!((n‚àí1)‚àí(k‚àí1))!pk‚àí1(1‚àíp)n‚àík
with a simple substitution of ‚Ñì=k‚àí1, the above equation can be rewritten as
e[x] =np¬∑n‚àí1x
‚Ñì=0(n‚àí1)!
‚Ñì!((n‚àí1)‚àí‚Ñì)!p‚Ñì(1‚àíp)n‚àí1‚àí‚Ñì
=np¬∑n‚àí1x
‚Ñì=0n‚àí1
k
p‚Ñì(1‚àíp)n‚àí1‚àí‚Ñì
| {z }
summing pmf of binomial( n‚àí1,p)=np.
‚ñ°
in matlab, the mean and variance of a binomial random variable can be found by
calling the command binostat(n,p) (matlab).
in python, the command is rv = stats.binom(n,p) followed by calling rv.stats .
% matlab code to compute the mean and var of a binomial rv
p = 0.5;
n = 10;
[m,v] = binostat(n, p)
# python code to compute the mean and var of a binomial rv
import scipy.stats as stats
p = 0.5
n = 10
rv = stats.binom(n,p)
m, v = rv.stats(moments=‚Äòmv‚Äô)
print(m, v)
147chapter 3. discrete random variables
an alternative view of the binomial random variable . as we discussed, the origin of a
binomial random variable is the sum of a sequence of bernoulli random variables. because
of this intrinsic definition, we can derive some useful results by exploiting this fact. to do so,
let us define i1, . . . , i nas a sequence of bernoulli random variables with ij‚àºbernoulli( p)
for all i= 1, . . . , n . then the resulting variable
x=i1+i2+¬∑¬∑¬∑+in
is a binomial random variable of size nand parameter p. using this definition, we can
compute the expectation as follows:
e[x] =e[i1+i2+¬∑¬∑¬∑+in]
(a)=e[i1] +e[i2] +¬∑¬∑¬∑+e[in]
=p+p+¬∑¬∑¬∑+p
=np.
in this derivation, the step ( a) depends on a useful fact about expectation (which we have not
yet proved): for any two random variables xandy, it holds that e[x+y] =e[x]+e[y].
therefore, we can show that the expectation of xisnp. this line of argument not only
simplifies the proof but also provides a good intuition of the expectation. if each coin flip
has an expectation of e[ii] =p, then the expectation of the sum should be simply ntimes
ofp, given np.
how about the variance? again, we are going to use a very useful fact about variance:
if two random variables xandyare independent, then var[ x+y] = var[ x] + var[ y].
with this result, we can show that
var[x] = var[ i1+¬∑¬∑¬∑+in]
= var[ i1] +¬∑¬∑¬∑+ var[ in]
=p(1‚àíp) +¬∑¬∑¬∑+p(1‚àíp)
=np(1‚àíp).
finally, using the fact that var[ x] =e[x2]‚àí¬µ2, we can show that
e[x2] = var[ x] +¬µ2
=np(1‚àíp) + (np)2.
practice exercise 3.9 . show that the binomial pmf sums to 1.
solution . we use the binomial theorem to prove this result:
nx
k=0px(k) =nx
k=0n
k
pk(1‚àíp)n‚àík= (p+ (1‚àíp))n= 1.
the cdf of the binomial random variable is not very informative. it is basically the
cumulative sum of the pmf:
fx(k) =kx
‚Ñì=0n
‚Ñì
p‚Ñì(1‚àíp)n‚àí‚Ñì.
1483.5. common discrete random variables
00.050.10.150.2
0 5 10 15 20 25 30
0 5 10 15 20 25 3000.20.40.60.81
figure 3.30: pmf and cdf of a binomial random variable x‚àºbinomial (n, p).
the shapes of the pmf and the cdf is shown in figure 3.30 .
in matlab, plotting the cdf of a binomial can be done by calling the function
binocdf . you may also call f = binopdf(x,n,p) , and define f = cumsum(f) as the cumu-
lative sum of the pmf. in python, the corresponding command is rv = stats.binom(n,p)
followed by rv.cdf .
% matlab code to compute the mean and var of a binomial rv
x = 0:10;
p = 0.5;
n = 10;
f = binocdf(x,n,p);
figure; stairs(x,f,‚Äò.-‚Äô,‚Äòlinewidth‚Äô,4,‚Äòmarkersize‚Äô,30);
# python code to compute the mean and var of a binomial rv
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
p = 0.5
n = 10
rv = stats.binom(n,p)
x = np.arange(11)
f = rv.cdf(x)
plt.plot(x, f, ‚Äôbo‚Äô, ms=10);
plt.vlines(x, 0, f, colors=‚Äôb‚Äô, lw=5, alpha=0.5);
3.5.3 geometric random variable
in some applications, we are interested in trying a binary experiment until we succeed. for
example, we may want to keep calling someone until the person picks up the call. in this
case, the random variable can be defined as the outcome of many failures followed by a final
success. this is called the geometric random variable .
definition 3.10. letxbe ageometric random variable . then, the pmf of xis
px(k) = (1 ‚àíp)k‚àí1p, k = 1,2, . . . ,
149chapter 3. discrete random variables
where 0< p < 1is the geometric parameter. we write
x‚àºgeometric( p)
to say that xis drawn from a geometric distribution with a parameter p.
a geometric random variable is easy to understand. we define it as bernoulli trials with
k‚àí1 consecutive failures followed by one success. this can be seen from the definition:
px(k) = (1 ‚àíp)k‚àí1
| {z }
k‚àí1 failuresp|{z}
final success.
note that in geometric random variables, there is no n
k
because we must have k‚àí1
consecutive failures before one success. there is no alternative combination of the sequence.
the histogram and pmf of a geometric random variable are illustrated in figure 3.31 .
here, we assume that p= 0.5.
0 1 2 3 4 5 6 7 8 9 10050010001500200025003000
00.10.20.30.40.5
0 2 4 6 8 10
(a) histogram based on 5000 samples (b) pmf
figure 3.31: an example of a geometric distribution with p= 0.5.
in matlab, generating geometric random variables can be done by calling the com-
mands geornd . in python, it is np.random.geometric .
% matlab code to generate 1000 geometric random variables
p = 0.5;
x = geornd(p,[5000,1]);
[num, ~] = hist(x, 0:10);
bar(0:10, num, ‚Äòfacecolor‚Äô,[0.4, 0.4, 0.8]);
# python code to generate 1000 geometric random variables
import numpy as np
import matplotlib.pyplot as plt
p = 0.5
x = np.random.geometric(p,size=1000)
plt.hist(x,bins=‚Äòauto‚Äô);
to generate the pmf plots, in matlab we call geopdf and in python we call
rv = stats.geom followed by rv.pmf .
1503.5. common discrete random variables
% matlab code to generate geometric pmf
p = 0.5; x = 0:10;
f = geopdf(x,p);
stem(x, f, ‚Äòo‚Äô, ‚Äòlinewidth‚Äô, 8, ‚Äòcolor‚Äô, [0.8, 0.4, 0.4]);
# python code to generate 1000 geometric random variables
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
x = np.arange(1,11)
rv = stats.geom(p)
f = rv.pmf(x)
plt.plot(x, f, ‚Äòbo‚Äô, ms=8, label=‚Äògeom pmf‚Äô)
plt.vlines(x, 0, f, colors=‚Äòb‚Äô, lw=5, alpha=0.5)
practice exercise 3.10 . show that the geometric pmf sums to one.
solution . we can apply infinite series to show the result:
‚àûx
k=1px(k) =‚àûx
k=1(1‚àíp)k‚àí1p
=p¬∑‚àûx
k=1(1‚àíp)k‚àí1, ‚Ñì =k‚àí1
=p¬∑‚àûx
‚Ñì=0(1‚àíp)‚Ñì
=p¬∑1
1‚àí(1‚àíp)= 1.
it is interesting to compare the shape of the pmfs for various values of p. infigure 3.32
we show the pmfs. we vary the parameter p= 0.25,0.5,0.9. for small p, the pmf starts
with a low value and decays at a slow speed. the opposite happens for a large p, where the
pmf starts with a high value and decays rapidly.
furthermore, we can derive the following properties of the geometric random variable.
theorem 3.8. ifx‚àºgeometric( p), then
e[x] =1
p,e[x2] =2
p2‚àí1
p, (3.16)
var[x] =1‚àíp
p2.
proof . we will prove that the mean is 1 /pand leave the second moment and variance as
151chapter 3. discrete random variables
00.20.40.60.81
0 2 4 6 8p = 0.25
00.20.40.60.81
0 2 4 6 8p = 0.5
00.20.40.60.81
0 2 4 6 8p = 0.9
figure 3.32: pmfs of a geometric random variable x‚àºgeometric (p).
an exercise.
e[x] =‚àûx
k=1kp(1‚àíp)k‚àí1=p ‚àûx
k=1k(1‚àíp)k‚àí1!
(a)=p1
(1‚àí(1‚àíp))2
=1
p,
where ( a) follows from the infinite series identity in chapter 1.
‚ñ°
3.5.4 poisson random variable
in many physical systems, the arrivals of events are typically modeled as a poisson ran-
dom variable, e.g., photon arrivals, electron emissions, and telephone call arrivals. in social
networks, the number of conversations per user can also be modeled as a poisson. in e-
commerce, the number of transactions per paying user is again modeled using a poisson.
definition 3.11. letxbe apoisson random variable . then, the pmf of xis
px(k) =Œªk
k!e‚àíŒª, k = 0,1,2, . . . ,
where Œª >0is the poisson rate. we write
x‚àºpoisson( Œª)
to say that xis drawn from a poisson distribution with a parameter Œª.
in this definition, the parameter Œªdetermines the rate of the arrival. the histogram and
pmf of a poisson random variable are illustrated in figure 3.33 . here, we assume that
Œª= 1.
the matlab code and python code used to generate the histogram are shown below.
% matlab code to generate 5000 poisson numbers
lambda = 1;
x = poissrnd(lambda,[5000,1]);
1523.5. common discrete random variables
0 1 2 3 4 5 6 7 8 9 100500100015002000
00.10.20.30.4
0 2 4 6 8 10
(a) histogram based on 5000 samples (b) pmf
figure 3.33: an example of a poisson distribution with Œª= 1.
[num, ~] = hist(x, 0:10);
bar(0:10, num, ‚Äòfacecolor‚Äô,[0.4, 0.4, 0.8]);
# python code to generate 5000 poisson random variables
import numpy as np
import matplotlib.pyplot as plt
lambd = 1
x = np.random.poisson(lambd,size=5000)
plt.hist(x,bins=‚Äòauto‚Äô);
for the pmf, in matlab we can call poisspdf , and in python we can call rv.pmf
with rv = stats.poisson .
% matlab code to plot the poisson pmf
lambda = 1;
x = 0:10;
f = poisspdf(x,lambda);
stem(x, f, ‚Äòo‚Äô, ‚Äòlinewidth‚Äô, 8, ‚Äòcolor‚Äô, [0.8, 0.4, 0.4]);
# python code to plot the poisson pmf
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
x = np.arange(0,11)
rv = stats.poisson(lambd)
f = rv.pmf(x)
plt.plot(x, f, ‚Äòbo‚Äô, ms=8, label=‚Äògeom pmf‚Äô)
plt.vlines(x, 0, f, colors=‚Äòb‚Äô, lw=5, alpha=0.5)
the shape of the poisson pmf changes with Œª. as illustrated in figure 3.34 ,px(k) is
more concentrated at lower values for smaller Œªand becomes spread out for larger Œª. thus,
we should expect that the mean and variance of a poisson random variable will change
153chapter 3. discrete random variables
together as a function of Œª. in the same figure, we show the cdf of a poisson random
variable. the cdf of a poisson is
fx(k) =p[x‚â§k] =kx
‚Ñì=0Œª‚Ñì
‚Ñì!e‚àíŒª. (3.17)
00.10.20.30.4
0 5 10 15 20 = 1
 = 4
 = 10
0 5 10 15 2000.20.40.60.81
 = 1
 = 4
 = 10
figure 3.34: a poisson random variable using different Œª‚Äôs. [left] probability mass function px(k).
[right] cumulative distribution function fx(k).
example 3.18 . let xbe a poisson random variable with parameter Œª. find p[x > 4]
andp[x‚â§5].
solution .
p[x > 4] = 1 ‚àíp[x‚â§4] = 1 ‚àí4x
k=0Œªk
k!e‚àíŒª,
p[x‚â§5] =5x
k=0Œªk
k!e‚àíŒª.
practice exercise 3.11 . show that the poisson pmf sums to 1.
solution . we use the exponential series to prove this result:
‚àûx
k=0px(k) =‚àûx
k=0Œªk
k!e‚àíŒª=e‚àíŒª¬∑‚àûx
k=0Œªk
k!
|{z}
=eŒª= 1.
poisson random variables in practice
(1) computational photography . in computational photography, the poisson random vari-
able is one of the most widely used models for photon arrivals. the reason pertains to the
1543.5. common discrete random variables
origin of the poisson random variable, which we will discuss shortly. when photons are emit-
ted from the source, they travel through the medium as a sequence of independent events.
during the integration period of the camera, the photons are accumulated to generate a
voltage that is then translated to digital bits.
figure 3.35: the poisson random variable can be used to model photon arrivals.
if we assume that the photon arrival rate is Œ±(photons per second), and suppose that
the total amount of integration time is t, then the average number of photons that the sensor
can see is Œ±t. let xbe the number of photons seen during the integration time. then if we
follow the poisson model, we can write down the pmf of x:
p[x=k] =(Œ±t)k
k!e‚àíŒ±t.
therefore, if a pixel is bright, meaning that Œ±is large, then xwill have a higher likelihood
of landing on a large number.
(2) traffic model . the poisson random variable can be used in many other problems. for
example, we can use it to model the number of passengers on a bus or the number of spam
phone calls. the required modification to figure 3.35 is almost trivial: merely replace the
photons with your favorite cartoons, e.g., a person or a phone, as shown in figure 3.36 . in
the united states, shared-ride services such as uber and lyft need to model the vacant cars
and the passengers. as long as they have an arrival rate and certain degrees of independence
between events, the poisson random variable will be a good model.
as you can see from these examples, the poisson random variable has broad applica-
bility. before we continue our discussion of its applications, let us introduce a few concepts
related to the poisson random variable.
properties of a poisson random variable
we now derive the mean and variance of a poisson random variable.
theorem 3.9. ifx‚àºpoisson( Œª), then
e[x] =Œª, e[x2] =Œª+Œª2, (3.18)
var[x] =Œª.
155chapter 3. discrete random variables
figure 3.36: the poisson random variable can be used to model passenger arrivals and the number of
phone calls, and can be used by uber or lyft to provide shared rides.
proof . let us first prove the mean. it can be shown that
e[x] =‚àûx
k=0k¬∑Œªk
k!e‚àíŒª=‚àûx
k=1Œªk
(k‚àí1)!e‚àíŒª
=Œªe‚àíŒª‚àûx
k=1Œªk‚àí1
(k‚àí1)!=Œªe‚àíŒª‚àûx
‚Ñì=0Œª‚Ñì
‚Ñì!=Œªe‚àíŒªeŒª=Œª.
the second moment can be computed as
e[x2] =‚àûx
k=0k2¬∑Œªk
k!e‚àíŒª
=‚àûx
k=0k¬∑Œªk
(k‚àí1)!e‚àíŒª
=‚àûx
k=0(k‚àí1 + 1) ¬∑Œªk
(k‚àí1)!e‚àíŒª
=‚àûx
k=1(k‚àí1)¬∑Œªk
(k‚àí1)!e‚àíŒª+‚àûx
k=1Œªk
(k‚àí1)!e‚àíŒª
=Œª2¬∑‚àûx
k=2Œªk‚àí2e‚àíŒª
(k‚àí2)!
|{z}
=1+Œª¬∑‚àûx
k=1Œªk‚àí1e‚àíŒª
(k‚àí1)!
|{z}
=1.
the variance can be computed using var[ x] =e[x2]‚àí¬µ2.
‚ñ°
to compute the mean and variance of a poisson random variable, we can call poisstat
in matlab and rv.stats(moments=‚Äòmv‚Äô) in python.
% matlab code to compute poisson statistics
lambda = 1;
[m,v] = poisstat(lambda);
1563.5. common discrete random variables
# python code to compute poisson statistics
import scipy.stats as stats
lambd = 1
rv = stats.poisson(lambd)
m, v = rv.stats(moments=‚Äômv‚Äô)
the poisson random variable is special in the sense that the mean and the variance are
equal. that is, if the mean arrival number is higher, the variance is also higher. this is very
different from some other random variables, e.g., the normal random variable where the mean
and variance are independent. for certain engineering applications such as photography, this
plays an important role in defining the signal-to-noise ratio. we will come back to this point
later.
origin of the poisson random variable
we now address one of the most important questions about the poisson random variable:
where does it come from? answering this question is useful because the derivation process
will reveal the underlying assumptions that lead to the poisson pmf. when you change
the problem setting, you will know when the poisson pmf will hold and when the poisson
pmf will fail.
our approach to addressing this problem is to consider the photon arrival process.
(as we have shown, there is conceptually no difference if you replace the photons with
pedestrians, passengers, or phone calls.) our derivation follows the argument of j. goodman,
statistical optics , section 3.7.2.
to begin with, we consider a photon arrival process. the total number of photons
observed over an integration time tis defined as x(t). because x(t) is a poisson random
variable, its arguments must be integers. the probability of observing x(t) =kis therefore
p[x(t) =k].figure 3.37 illustrates the notations and concepts.
figure 3.37: notations for deriving the poisson pmf.
we propose three hypotheses with the photon arrival process:
¬àfor sufficiently small ‚àÜ t, the probability of a small impulse occurring in the time
interval [ t, t+ ‚àÜt] is equal to the product of ‚àÜ tand the rate Œª, i.e.,
p[x(t+ ‚àÜt)‚àíx(t) = 1] = Œª‚àÜt.
this is a linearity assumption, which typically holds for a short duration of time.
157chapter 3. discrete random variables
¬àfor sufficiently small ‚àÜ t, the probability that more than one impulse falls in ‚àÜ tis
negligible. thus, we have that p[x(t+ ‚àÜt)‚àíx(t) = 0] = 1 ‚àíŒª‚àÜt.
¬àthe number of impulses in non-overlapping time intervals is independent.
the significance of these three hypotheses is that if the underlying photon arrival process
violates any of these assumptions, then the poisson pmf will not hold. one example is the
presence of scattering effects, where a photon has a certain probability of going off due to
the scattering medium and a certain probability of coming back. in this case, the events will
no longer be independent.
assuming that these hypotheses hold, then at time t+‚àÜt, the probability of observing
x(t+ ‚àÜt) =kcan be computed as
p[x(t+ ‚àÜt) =k]
=p[x(t) =k]¬∑ (1‚àíŒª‚àÜt)|{z}
=p[x(t+‚àÜt)‚àíx(t)=0]+p[x(t) =k‚àí1]¬∑ (Œª‚àÜt)|{z}
=p[x(t+‚àÜt)‚àíx(t)=1]
=p[x(t) =k]‚àíp[x(t) =k]Œª‚àÜt+p[x(t) =k‚àí1]Œª‚àÜt.
by rearranging the terms we show that
p[x(t+ ‚àÜt) =k]‚àíp[x(t) =k]
‚àÜt=Œª
p[x(t) =k‚àí1]‚àíp[x(t) =k]
.
setting the limit of ‚àÜ t‚Üí0, we arrive at an ordinary differential equation
d
dtp[x(t) =k] =Œª
p[x(t) =k‚àí1]‚àíp[x(t) =k]
. (3.19)
we claim that the poisson pmf, i.e.,
p[x(t) =k] =(Œªt)k
k!e‚àíŒªt,
would solve this differential equation. to see this, we substitute the pmf into the equation.
the left-hand side gives us
d
dtp[x(t) =k] =d
dt(Œªt)k
k!e‚àíŒªt
=Œªk(Œªt)k‚àí1
k!e‚àíŒªt+ (‚àíŒª)(Œªt)k
k!e‚àíŒªt
=Œª(Œªt)k‚àí1
k!e‚àíŒªt‚àíŒª(Œªt)k
k!e‚àíŒªt
=Œª
p[x(t) =k‚àí1]‚àíp[x(t) =k]
,
which is the right-hand side of the equation. to retrieve the basic form of poisson, we can
just set t= 1 in the pmf so that
p[x(1) = k] =Œªk
k!e‚àíŒª.
1583.5. common discrete random variables
the origin of poisson random variables
¬àwe assume independent arrivals.
¬àprobability of seeing one event is linear with the arrival rate.
¬àtime interval is short enough so that you see either one event or no event.
¬àpoisson is derived by solving a differential equation based on these assumptions.
¬àpoisson becomes invalid when these assumptions are violated, e.g., in the case
of scattering of photons due to turbid medium.
there is an alternative approach to deriving the poisson pmf. the idea is to drive
the parameter nin the binomial random variable to infinity while pushing pto zero. in this
limit, the binomial pmf will converge to the poisson pmf. we will discuss this shortly.
however, we recommend the physics approach we have just described because it has a rich
meaning and allows us to validate our assumptions.
poisson approximation to binomial
we present one additional result about the poisson random variable. the result shows that
poisson can be regarded as a limiting distribution of a binomial random variable.
theorem 3.10. (poisson approximation to binomial) . for small pand large n,
n
k
pk(1‚àíp)n‚àík‚âàŒªk
k!e‚àíŒª,
where Œªdef=np.
before we prove the result, let us see how close the approximation can be. in figure 3.38 ,
we show a binomial distribution and a poisson approximation. the closeness of the approx-
imation can easily be seen.
in matlab, the code to approximate a binomial distribution with a poisson formula
is shown below. here, we draw 10,000 random binomial numbers and plot their histogram.
on top of the plot, we use poisspdf to compute the poisson pmf. this gives us figure 3.38 .
a similar set of commands can be called in python.
% matlab code to approximate binomial using poisson
n = 1000; p = 0.05;
x = binornd(n,p,[10000,1]);
t = 0:100;
[num,val] = hist(x,t);
lambda = n*p;
f_pois = poisspdf(t,lambda);
bar(num/10000,‚Äòfacecolor‚Äô,[0.9 0.9 0],‚Äòbarwidth‚Äô,1); hold on;
plot(f_pois, ‚Äòlinewidth‚Äô, 4);
159chapter 3. discrete random variables
00.020.040.06probability
0 20 40 60 80 100 120
kbinomial, n = 5000, p = 0.01
poisson,  = 50
figure 3.38: poisson approximation of binomial distribution.
# python code to approximate binomial using poisson
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
n = 1000; p = 0.05
rv1 = stats.binom(n,p)
x = rv1.rvs(size=10000)
plt.figure(1); plt.hist(x,bins=np.arange(0,100));
rv2 = stats.poisson(n*p)
f = rv2.pmf(bin)
plt.figure(2); plt.plot(f);
proof . let Œª=np. then,
n
k
pk(1‚àíp)n‚àík=n!
k!(n‚àík)!Œª
nk
1‚àíŒª
nn‚àík
=Œªk
k!n(n‚àí1)¬∑¬∑¬∑(n‚àík+ 1)
n¬∑n¬∑¬∑¬∑n
1‚àíŒª
nn‚àík
=Œªk
k!(1)
1‚àí1
n
¬∑¬∑¬∑
1‚àík‚àí1
n
| {z }
‚Üí1 asn‚Üí‚àû
1‚àíŒª
n‚àík
|{z}
‚Üí1 asn‚Üí‚àû
1‚àíŒª
nn
=Œªk
k!
1‚àíŒª
nn
.
we claim that 
1‚àíŒª
nn‚Üíe‚àíŒª. this can be proved by noting that
log(1 + x)‚âàx, x ‚â™1.
it then follows that log 
1‚àíŒª
n
‚âà ‚àíŒª
n. hence, 
1‚àíŒª
nn‚âàe‚àíŒª
‚ñ°
1603.5. common discrete random variables
example 3.19 . consider an optical communication system. the bit arrival rate is 109
bits/sec, and the probability of having one error bit is 10‚àí9. suppose we want to find
the probability of having five error bits in one second.
letxbe the number of error bits. in one second there are 109bits. since we
do not know the location of these 5 bits, we have to enumerate all possibilities. this
leads to a binomial distribution. using the binomial distribution, we know that the
probability of having kerror bits is
p[x=k] =n
k
pk(1‚àíp)n‚àík
=109
k
(10‚àí9)k(1‚àí10‚àí9)109‚àík.
this quantity is difficult to calculate in floating-point arithmetic.
using the poisson to binomial approximation, we can see that the probability can
be approximated by
p[x=k]‚âàŒªk
k!e‚àíŒª,
where Œª=np= 109(10‚àí9) = 1. setting k= 5 yields p[x= 5]‚âà0.003.
photon arrival statistics
poisson random variables are useful in computer vision, but you may skip this discussion
if it is your first reading of the book.
the strong connection between poisson statistics and physics makes the poisson ran-
dom variable a very good fit for many physical experiments. here we demonstrate an appli-
cation in modeling photon shot noise.
an image sensor is a photon sensitive device which is used to detect incoming photons.
in the simplest setting, we can model a pixel in the object plane as xm,n, for some 2d
coordinate [ m, n]‚ààr2. written as an array, an m√ónimage in the object plane can be
visualized as
x= object =Ô£Æ
Ô£ØÔ£∞x1,1x1,2¬∑¬∑¬∑ x1,n
............
xm,1xm,2¬∑¬∑¬∑xm,nÔ£π
Ô£∫Ô£ª.
without loss of generality, we assume that xm,nis normalized so that 0 ‚â§xm,n‚â§1 for
every coordinate [ m, n]. to model the brightness, we multiply xm,nby a scalar Œ± > 0. if
a pixel Œ±xm,nhas a large value, then it is a bright pixel; conversely, if Œ±xm,nhas a small
value, then it is a dark pixel. at a particular pixel location [ m, n]‚ààr2, the observed pixel
value ym,nis a random variable following the poisson statistics. this situation is illustrated
161chapter 3. discrete random variables
infigure 3.39 , where we see that an object-plane pixel will generate an observed pixel
through the poisson pmf.1
figure 3.39: the image formation process is governed by the poisson random variable. given a pixel
in the object plane xm,n, the observed pixel ym,nis a poisson random variable with mean Œ±xm,n.
therefore, a brighter pixel will have a higher poisson mean, whereas a darker pixel will have a lower
poisson mean.
written as an array, the image is
y= observed image = poisson
Œ±x
=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞poisson {Œ±x1,1}poisson {Œ±x1,2} ¬∑¬∑¬∑ poisson {Œ±x1,n}
poisson {Œ±x2,1}poisson {Œ±x2,2} ¬∑¬∑¬∑ poisson {Œ±x2,n}
............
poisson {Œ±xm,1}poisson {Œ±xm,2} ¬∑¬∑¬∑ poisson {Œ±xm,n}Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
here, by poisson {Œ±xm,n}we mean that ym,nis a random integer with probability mass
p[ym,n=k] =[Œ±xm,n]k
k!e‚àíŒ±xm,n.
note that this model implies that the images seen by our cameras are more or less
an array of poisson random variables. (we say ‚Äúmore or less‚Äù because of other sources of
uncertainties such as read noise, dark current, etc.) because the observed pixels ym,nare
random variables, they fluctuate about the mean values, and hence they are noisy. we refer
to this type of random fluctuation as the shot noise . the impact of the shot noise can be
seen in figure 3.40 . here, we vary the sensor gain level Œ±. we see that for small Œ±the image
is dark and has much random fluctuation. as Œ±increases, the image becomes brighter and
the fluctuation becomes smaller.
in matlab, simulating the poisson photon arrival process for an image requires the
image-processing toolbox. the command to read an image is imread . depending on the data
type, the input array could be unit8 integers. to convert them to floating-point numbers
between 0 and 1, we use the command im2double . drawing poisson measurements from the
clean image is done using poissrnd . finally, we can use imshow to display the image.
% matlab code to simulate a photon arrival process
x0 = im2double(imread(‚Äôcameraman.tif‚Äô));
1the color of an image is often handled by a color filter array , which can be thought of as a wavelength
selector that allows a specific wavelength to pass through.
1623.5. common discrete random variables
 = 10
  = 100
  = 1000
figure 3.40: illustration of the poisson random variable in photographing images. here, Œ±denotes the
gain level of the sensor: larger Œ±means that there are more photons coming to the sensor.
x = poissrnd(10*x0);
figure(1); imshow(x0, []);
figure(2); imshow(x, []);
similar commands can be found in python with the help of the cv2library. when
reading an image, we call cv2.imread . the option 0is used to read a gray-scale image;
otherwise, we will have a 3-channel color image. the division /255 ensures that the input
array ranges between 0 to 1. generating the poisson random numbers can be done using
np.random.poisson , or by calling the statistics library with stats.poisson.rvs(10*x0) .
to display the images, we call plt.imshow , with the color map option set to cmap = ‚Äôgray‚Äô .
# python code code to simulate a photon arrival process
import numpy as np
import matplotlib.pyplot as plt
import cv2
x0 = cv2.imread(‚Äô./cameraman.tif‚Äô, 0)/255
plt.figure(1); plt.imshow(x0,cmap=‚Äôgray‚Äô);
x = np.random.poisson(10*x0)
plt.figure(2); plt.imshow(x, cmap=‚Äôgray‚Äô);
why study poisson? what is shot noise?
¬àthe poisson random variable is used to model photon arrivals.
¬àshot noise is the random fluctuation of the photon counts at the pixels. shot
noise is present even if you have an ideal sensor.
signal-to-noise ratio of poisson
now let us answer a question we asked before. a poisson random variable has a variance
equal to the mean. thus, if the scene is brighter, the variance will be larger. how come our
simulation in figure 3.40 shows that the fluctuation becomes smaller as the scene becomes
brighter?
163chapter 3. discrete random variables
the answer to this question lies in the signal-to-noise ratio (snr) of the poisson
random variable. the snr of an image defines its quality. the higher the snr, the better
the image. the mathematical definition of snr is the ratio between the signal power and
the noise power. in our case, the snr is
snr =signal power
noise powerdef=e[y]p
var[y](a)=Œª‚àö
Œª=‚àö
Œª,
where y=ym,nis one of the observed pixels and Œª=Œ±xm,nis the the corresponding object
pixel. in this equation, the step ( a) uses the properties of the poisson random variable y
where e[y] = var[ y] =Œª. the result snr =‚àö
Œªis very informative. it says that if the
underlying mean photon flux (which is Œª) increases, the snr increases at a rate of‚àö
Œª.
so, yes, the variance becomes larger when the scene is brighter. however, the gain in signal
e[y] overrides the gain in noisep
var[y]. as a result, the big fluctuation in bright images
is compensated by the strong signal. thus, to minimize the shot noise one has to use a
longer exposure to increase the mean photon flux. when the scene is dark and the aperture
is small, shot noise is unavoidable.
poisson modeling is useful for describing the problem. however, the actual engineering
question is that, given a noise observation ym,n, how would you reconstruct the clean image
xm,n? this is a very difficult inverse problem . the typical strategy is to exploit the spatial
correlations between nearby pixels, e.g., usually smooth except along some sharp edges.
other information about the image, e.g., the likelihood of obtaining texture patterns, can
also be leveraged. modern image-processing methods are rich, ranging from classical filtering
techniques to deep neural networks. static images are easier to recover because we can often
leverage multiple measurements of the same scene to boost the snr. dynamic scenes are
substantially harder when we need to track the motion of any underlying objects. there are
also newer image sensors with better photon sensitivity. the problem of imaging in the dark
is an important research topic in computational imaging . new solutions are developed at
the intersection of optics, signal processing, and machine learning.
the end of our discussions on photon statistics.
3.6 summary
arandom variable is so called because it can take more than one state. the probability mass
function specifies the probability for it to land on a particular state. therefore, whenever
you think of a random variable you should immediately think of its pmf (or histogram
if you prefer). the pmf is a unique characterization of a random variable. two random
variables with the same pmf are effectively the same random variables. (they are not
identical because there could be measure-zero sets where the two differ.) once you have the
pmf, you can derive the cdf, expectation, moments, variance, and so on.
when your boss hands a dataset to you, which random variable (which model) should
you use? this is a very practical and deep question. we highlight three steps for you to
consider:
1643.7. references
¬à(i)model selection : which random variable is the best fit for our problem? some-
times we know by physics that, for example, photon arrivals or internet traffic follow a
poisson random variable. however, not all datasets can be easily described by simple
models. the models we have learned in this chapter are called the parametric mod-
els because they are characterized by one or two parameters. some datasets require
nonparametric models, e.g., natural images, because they are just too complex. some
data scientists refer to deep neural networks as parametric models because the net-
work weights are essentially the parameters. some do not because when the number
of parameters is on the order of millions, sometimes even more than the number of
training samples, it seems more reasonable to call these models nonparametric. how-
ever, putting this debate aside, shortlisting a few candidate models based on prior
knowledge is essential. even if you use deep neural networks, selecting between con-
volutional structures versus long short-term memory models is still a legitimate task
that requires an understanding of your problem.
¬à(ii)parameter estimation : suppose that you now have a candidate model; the next
task is to estimate the model parameter using the available training data. for example,
for poisson we need to determine Œª, and for binomial we need to determine ( n, p). the
estimation problem is an inverse problem. often we need to use the pmf to construct
certain optimization problems. by solving the optimization problem we will find the
best parameter (for that particular candidate model). modern machine learning is
doing significantly better now than in the old days because optimization methods
have advanced greatly.
¬à(iii)validation . when each candidate model has been optimized to best fit the data,
we still need to select the best model. this is done by running various testings. for
example, we can construct a validation set and check which model gives us the best
performance (such as classification rate or regression error). however, a model with
the best validation score is not necessarily the best model. your goal should be to seek
agood model and not the best model because determining the best requires access to
the testing data, which we do not have. everything being equal, the common wisdom
is to go with a simpler model because it is generally less susceptible to overfitting.
3.7 references
probability textbooks
3-1 dimitri p. bertsekas and john n. tsitsiklis, introduction to probability , athena sci-
entific, 2nd edition, 2008. chapter 2.
3-2 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. chapter 3.
3-3 athanasios papoulis and s. unnikrishna pillai, probability, random variables and
stochastic processes , mcgraw-hill, 4th edition, 2001. chapters 3 and 4.
3-4 john a. gubner, probability and random processes for electrical and computer en-
gineers , cambridge university press, 2006. chapters 2 and3.
165chapter 3. discrete random variables
3-5 sheldon ross, a first course in probability , prentice hall, 8th edition, 2010. chap-
ter 4.
3-6 henry stark and john woods, probability and random processes with applications
to signal processing , prentice hall, 3rd edition, 2001. chapters 2 and 4.
advanced probability textbooks
3-7 william feller, an introduction to probability theory and its applications , wiley and
sons, 3rd edition, 1950.
3-8 andrey kolmogorov, foundations of the theory of probability , 2nd english edition,
dover 2018. (translated from russian to english. originally published in 1950 by
chelsea publishing company new york.)
cross-validation
3-9 larry wasserman, all of statistics , springer 2004. chapter 20.
3-10 mats rudemo, ‚Äúempirical choice of histograms and kernel density estimators,‚Äù
scandinavian journal of statistics , vol. 9, no. 2 (1982), pp. 65-78.
3-11 david w. scott, multivariate density estimation: theory, practice, and visualization ,
wiley, 1992.
poisson statistics
3-12 joseph goodman, statistical optics , wiley, 2015. chapter 3.
3-13 henry stark and john woods, probability and random processes with applications
to signal processing , prentice hall, 3rd edition, 2001. section 1.10.
3.8 problems
exercise 1. (video solution)
consider an information source that produces numbers kin the set sx={1,2,3,4}. find
and plot the pmf in the following cases:
(a)pk=p1/k, fork= 1,2,3,4. hint: find p1.
(b)pk+1=pk/2 for k= 1,2,3.
(c)pk+1=pk/2kfork= 1,2,3.
(d) can the random variables in parts (a)-(c) be extended to take on values in the set
{1,2, . . .}? why or why not? hint: you may use the fact that the series 1+1
2+1
3+¬∑¬∑¬∑
diverges.
exercise 2. (video solution)
two dice are tossed. let xbe the absolute difference in the number of dots facing up.
1663.8. problems
(a) find and plot the pmf of x.
(b) find the probability that x‚â§2.
(c) find e[x] and var[ x].
exercise 3. (video solution)
letxbe a random variable with pmf pk=c/2kfork= 1,2, . . ..
(a) determine the value of c.
(b) find p(x > 4) and p(6‚â§x‚â§8).
(c) find e[x] and var[ x].
exercise 4.
letxbe a random variable with pmf pk=c/2kfork=‚àí1,0,1,2,3,4,5.
(a) determine the value of c.
(b) find p(1‚â§x < 3) and p(1< x‚â§5).
(c) find p[x3<5].
(d) find the pmf and the cdf of x.
exercise 5. (video solution)
a modem transmits a +2 voltage signal into a channel. the channel adds to this sig-
nal a noise term that is drawn from the set {0,‚àí1,‚àí2,‚àí3}with respective probabilities
{4/10,3/10,2/10,1/10}.
(a) find the pmf of the output yof the channel.
(b) what is the probability that the channel‚Äôs output is equal to the input of the channel?
(c) what is the probability that the channel‚Äôs output is positive?
(d) find the expected value and variance of y.
exercise 6.
on a given day, your golf score takes values from numbers 1 through 10, with equal proba-
bility of getting each one. assume that you play golf for three days, and assume that your
three performances are independent. let x1,x2, and x3be the scores that you get, and
letxbe the minimum of these three numbers.
(a) show that for any discrete random variable x,px(k) =p(x > k ‚àí1)‚àíp(x > k ).
(b) what is the probability p(x1> k) for k= 1, . . . , 10?
(c) use (a), determine the pmf px(k), for k= 1, . . . , 10.
167chapter 3. discrete random variables
(d) what is the average score improvement if you play just for one day compared with
playing for three days and taking the minimum?
exercise 7. (video solution)
let
g(x) =(
1,ifx > 10
0,otherwise .and h(x) =(
x‚àí10,ifx‚àí10>0
0, otherwise .
(a) find e[g(x)] for xas in problem 1(a) with sx={1, . . . , 15}.
(b) find e[h(x)] for xas in problem 1(b) with sx={1, . . . , 15}.
exercise 8. (video solution)
a voltage xis uniformly distributed in the set {‚àí3, . . . , 3,4}.
(a) find the mean and variance of x.
(b) find the mean and variance of y=‚àí2x2+ 3.
(c) find the mean and variance of w= cos( œÄx/8).
(d) find the mean and variance of z= cos2(œÄx/8).
exercise 9. (video solution)
(a) if xis poisson( Œª), compute e[1/(x+ 1)].
(b) if xis bernoulli( p) and yis bernoulli( q), compute e[(x+y)3] ifxandyare
independent.
(c) let xbe a random variable with mean ¬µand variance œÉ2. let ‚àÜ( Œ∏) =e[(x‚àíŒ∏)2].
find Œ∏that minimizes the error ‚àÜ( Œ∏).
(d) suppose that x1, . . . , x nare independent uniform random variables in {0,1, . . . , 100}.
evaluate p[min( x1, . . . , x n)> ‚Ñì] for any ‚Ñì‚àà {0,1, . . . , 100}.
exercise 10. (video solution)
(a) consider the binomial probability mass function px(k) = n
k
pk(1‚àíp)n‚àík. show that
the mean is e[x] =np.
(b) consider the geometric probability mass function px(k) =p(1‚àíp)kfork= 0,1, . . ..
show that the mean is e[x] = (1 ‚àíp)/p.
(c) consider the poisson probability mass function px(k) =Œªk
k!e‚àíŒª. show that the vari-
ance is var[ x] =Œª.
(d) consider the uniform probability mass function px(k) =1
lfork= 1, . . . , l . show that
the variance is var[ x] =l2‚àí1
12. hint: 1 + 2 + ¬∑¬∑¬∑+n=n(n+1)
2and 12+ 22+¬∑¬∑¬∑+n2=
n3
3+n2
2+n
6.
1683.8. problems
exercise 11. (video solution)
an audio player uses a low-quality hard drive. the probability that the hard drive fails after
being used for one month is 1 /12. if it fails, the manufacturer offers a free-of-charge repair
for the customer. for the cost of each repair, however, the manufacturer has to pay $20.
the initial cost of building the player is $50, and the manufacturer offers a 1-year warranty.
within one year, the customer can ask for a free repair up to 12 times.
(a) let xbe the number of months when the player fails. what is the pmf of x? hint:
p[x= 1] may not be very high because if the hard drive fails it will be fixed by the
manufacturer. once fixed, the drive can fail again in the remaining months. so saying
x= 1 is equivalent to saying that there is only one failure in the entire 12-month
period.
(b) what is the average cost per player?
exercise 12. (video solution)
a binary communication channel has a probability of bit error of p= 10‚àí6. suppose that
transmission occurs in blocks of 10,000 bits. let nbe the number of errors introduced by
the channel in a transmission block.
(a) what is the pmf of n?
(b) find p[n= 0] and p[n‚â§3].
(c) for what value of pwill the probability of 1 or more errors in a block be 99%?
hint: use the poisson approximation to binomial random variables.
exercise 13. (video solution)
the number of orders waiting to be processed is given by a poisson random variable with
parameter Œ±=Œª/n¬µ , where Œªis the average number of orders that arrive in a day, ¬µis the
number of orders that an employee can process per day, and nis the number of employees.
letŒª= 5 and ¬µ= 1. find the number of employees required so the probability that more
than four orders are waiting is less than 10%.
hint: you need to use trial and error for a few n‚Äôs.
exercise 14.
letxbe the number of photons counted by a receiver in an optical communication system.
it is known that xis a poisson random variable with a rate Œª1when a signal is present and a
poisson random variable with the rate Œª0< Œª1when a signal is absent. the probability that
the signal is present is p. suppose that we observe x=kphotons. we want to determine a
threshold tsuch that if k‚â•twe claim that the signal is present, and if k < t we claim
that the signal is absent. what is the value of t?
169chapter 3. discrete random variables
170chapter 4
continuous random variables
if you are coming to this chapter from chapter 3, we invite you to take a 30-second pause
and switch your mind from discrete events to continuous events. everything is continuous
now. the sample space is continuous, the event space is continuous, and the probability
measure is continuous. continuous random variables are similar in many ways to discrete
random variables. they are characterized by the probability density functions (the continu-
ous version of the probability mass functions); they have cumulative distribution functions;
they have means, moments, and variances. the most significant difference is perhaps the use
of integration instead of summation, but this change is conceptually straightforward, aside
from the difficulties associated with integrating functions. so why do we need a separate
chapter for continuous random variables? there are several reasons.
¬àfirst, how would you define the probability of a continuous event? note that we cannot
count because a continuous event is uncountable. there is also nothing called the
probability mass because there are infinitely many masses. to define the probability
of continuous events, we need to go back to our ‚Äúslogan‚Äù: probability is a measure
of the size of a set . because probability is a measure, we can speak meaningfully
about the probability of continuous events so long as we have a well-defined measure
for them. defining such a measure requires some effort. we will develop the intuitions
and the formal definitions in section 4.1. in section 4.2, we will discuss the expectation
and variance of continuous random variables.
¬àthe second challenge is the unification between continuous and discrete random vari-
ables. since the two types of random variables ultimately measure the size of a set, it
is natural to ask whether we can unify them. our approach to unifying them is based
on the cumulative distribution functions (cdfs), which are well-defined functions for
discrete and continuous random variables. based on the cdf and the fundamental
theorem of calculus, we can show that the probability density functions and proba-
bility mass functions can be derived from the derivative of the cdfs. these will be
discussed in section 4.3, and in section 4.4 we will discuss some additional results
about the mode and median.
¬àthe third challenge is to understand several widely used continuous random variables.
we will discuss the uniform random variable and the exponential random variable
in section 4.5. section 4.6 deals with the important topic of the gaussian random
variable . where does a gaussian random variable come from? why does it have a bell
171chapter 4. continuous random variables
shape? why are gaussian random variables so popular in data science? what are the
useful properties of gaussian random variables? what are the relationships between
a gaussian random variable and other random variables? these important questions
will be answered in section 4.6.
¬àthe final challenge is the transformation of random variables. imagine that you have a
random variable xand a function g. what will the probability mass/density function
ofg(x) be? addressing this problem is essential because almost all practical engineer-
ing problems involve the transformation of random variables. for example, suppose
we have voltage measurements and we would like to compute the power. this requires
taking the square of the voltage. we will discuss the transformation in section 4.7,
and we will also discuss an essential application in generating random numbers in
section 4.8.
4.1 probability density function
4.1.1 some intuitions about probability density functions
let‚Äôs begin by outlining some intuitive reasoning, which is needed to define the probability
of continuous events properly. these intuitions are based on the fact that probability is a
measure . in the following discussion you will see a sequence of logical arguments for con-
structing such a measure for continuous events. some arguments are discussed in chapter 2,
but now we place them in the context of continuous random variables.
suppose we are given an event athat is a subset in the sample space œâ, as illustrated
infigure 4.1 . in order to calculate the probability of a, the measure perspective suggests
that we consider the relative size of the set
p[{x‚ààa}] =‚Äúsize‚Äù of a
‚Äúsize‚Äù of œâ.
the right-hand side of this equation captures everything about the probability: it is a
measure of the size of a set. it is relative to the sample space. it is a number between 0 and
1. it can be applied to discrete sets, and it can be applied to continuous sets.
figure 4.1: [left] an event ain the sample space œâ. the probability that ahappens can be calculated
as the ‚Äúsize‚Äù of arelative to the ‚Äúsize‚Äù of œâ. [right] a specific example on the real line. note that the
same definition of probability applies: the probability is the size of the interval arelative to that of the
sample space œâ.
1724.1. probability density function
how do we measure the ‚Äúsize‚Äù of a continuous set? one possible way is by means of
integrating the length, area, or volume covered by the set. consider an example: suppose
that the sample space is the interval œâ = [0 ,5] and the event is a= [2,3]. to measure the
‚Äúsize‚Äù of a, we can integrate ato determine the length. that is,
p[{x‚àà[2,3]}] =‚Äúsize‚Äù of a
‚Äúsize‚Äù of œâ=r
adxr
œâdx=r3
2dx
r5
0dx=1
5.
therefore, we have translated the ‚Äúsize‚Äù of a set to an integration. however, this definition
is a very special case because when we calculate the ‚Äúsize‚Äù of a set, we treat all the elements
in the set with equal importance. this is a strong assumption that will be relaxed later. but
if you agree with this line of reasoning, we can rewrite the probability as
p[{x‚ààa}] =r
adxr
œâdx=r
adx
|œâ|
=z
a1
|œâ||{z}
equally important over œâdx.
this equation says that under our assumption (that all elements are equiprobable), the
probability of ais calculated as the integration of ausing an integrand 1 /|œâ|(note that
1/|œâ|is a constant with respect to x). if we evaluate the probability of another event b, all
we need to do is to replace awith band computer
b1
|œâ|dx.
what happens if we want to relax the ‚Äúequiprobable‚Äù assumption? perhaps we can
adopt something similar to the probability mass function (pmf). recall that a pmf px
evaluated at a point xis the probability that the state xhappens, i.e., px(x) =p[x=x].
so,px(x) is the relative frequency of x. following the same line of thinking, we can define a
function fxsuch that fx(x) tells us something related to the ‚Äúrelative frequency‚Äù. to this
end, we can treat fxas a continuous histogram with infinitesimal bin width as shown in
figure 4.2 . using this fx, we can replace the constant function 1 /|œâ|with the new function
fx(x). this will give us
p[{x‚ààa}] =z
afx(x)|{z}
replace 1 /|œâ|dx. (4.1)
if we compare it with a pmf, we note that when xis discrete,
p[{x‚ààa}] =x
x‚ààapx(x).
hence, fxcan be considered a continuous version of px, although we do not recommend
this way of thinking for the following reason: px(x) is a legitimate probability, but fx(x) is
not a probability. rather, fxis the probability per unit length , meaning that we need to
integrate fx(times dx) in order to generate a probability value. if we only look at fxat
a point x, then this point is a measure-zero set because the length of this set is zero.
equation (4.1) should be familiar to you from chapter 2. the function fx(x) is pre-
cisely the weighting function we described in that chapter.
173chapter 4. continuous random variables
figure 4.2: [left] a probability mass function (pmf) tells us the relative frequency of a state when
computing the probability. in this example, the ‚Äúsize‚Äù of aispx(x2) +px(x3). [right] a probability
density function (pdf) is the infinitesimal version of the pmf. thus, the ‚Äúsize‚Äù of ais the integration
over the pdf.
what is a pdf?
¬àa pdf is the continuous version of a pmf.
¬àwe integrate a pdf to compute the probability.
¬àwe integrate instead of sum because continuous events are not countable.
to summarize, we have learned that when measuring the size of a continuous event,
the discrete technique (counting the number of elements) does not work. generalizing to
continuous space requires us to integrate the event. however, since different elements in an
event have different relative emphases, we use the probability density function fx(x) to tell
us the relative frequency for a state xto happen. this pdf serves the role of the pmf.
4.1.2 more in-depth discussion about pdfs
a continuous random variable xis defined by its probability density function fx. this
function has to satisfy several criteria, summarized as follows.
definition 4.1. a probability density function fxof a random variable xis a map-
ping fx: œâ‚Üír, with the properties
¬ànon-negativity :fx(x)‚â•0for all x‚ààœâ
¬àunity :r
œâfx(x)dx= 1
¬àmeasure of a set :p[{x‚ààa}] =r
afx(x)dx
if all elements of the sample space are equiprobable, then the pdf is f(x) = 1 /|œâ|. you can
easily check that it satisfies all three criteria.
let us take a closer look at the three criteria:
¬ànon-negativity: the non-negativity criterion fx(x)‚â•0 is reminiscent of probability
axiom i. it says that no matter what xwe are looking at, the probability density
function fxevaluated at xshould never give a negative value. axiom i ensures that
we will not get a negative probability.
1744.1. probability density function
¬àunity: the unity criterionr
œâf(x)dx= 1 is reminiscent of probability axiom ii,
which says that measuring over the entire sample space will give 1.
¬àmeasure of a set: the third criterion gives us a way to measure the size of an event a.
it says that since each x‚ààœâ has a different emphasis when calculating the size of
a, we need to scale the elements properly. this scaling is done by the pdf fx(x),
which can be regarded as a histogram with a continuous x-axis. the third criterion
is a consequence of probability axiom iii, because if there are two events aandb
that are disjoint, then p[{x‚ààa} ‚à™ {x‚ààb}] =r
afx(x)dx+r
bfx(x)dxbecause
fx(x)‚â•0 for all x.
if the random variable xtakes real numbers in 1d, then a more ‚Äúuser-friendly‚Äù definition
of the pdf can be given.
definition 4.2. letxbe a continuous random variable. the probability density
function (pdf) ofxis a function fx: œâ‚Üírthat, when integrated over an interval
[a, b], yields the probability of obtaining a‚â§x‚â§b:
p[a‚â§x‚â§b] =zb
afx(x)dx. (4.2)
this definition is just a rewriting of the previous definition by explicitly writing out
the definition of aas an interval [ a, b]. here are a few examples.
example 4.1 . let fx(x) = 3 x2with œâ = [0 ,1]. let a= [0,0.5]. then the probability
p[{x‚ààa}] is
p[0‚â§x‚â§0.5] =z0.5
03x2dx=1
8.
example 4.2 . let fx(x) = 1 /|œâ|with œâ = [0 ,5]. let a= [3,5]. then the probability
p[{x‚ààa}] is
p[3‚â§x‚â§5] =z5
31
|œâ|dx=z5
31
5dx=2
5.
example 4.3 . let fx(x) = 2 xwith œâ = [0 ,1]. let a={0.5}. then the probability
p[{x‚ààa}] is
p[x= 0.5] =p[0.5‚â§x‚â§0.5] =z0.5
0.52x dx = 0.
this example shows that evaluating the probability at an isolated point for a contin-
uous random variable will yield 0.
175chapter 4. continuous random variables
practice exercise 4.1 . let xbe the phase angle of a voltage signal. without any
prior knowledge about xwe may assume that xhas an equal probability of any value
between 0 to 2 œÄ. find the pdf of xand compute p[0‚â§x‚â§œÄ/2].
solution . since xhas an equal probability for any value between 0 to 2 œÄ, the pdf
ofxis
fx(x) =1
2œÄ, for 0‚â§x‚â§2œÄ.
therefore, the probability p[0‚â§x‚â§œÄ/2] can be computed as
ph
0‚â§x‚â§œÄ
2i
=zœÄ/2
01
2œÄdx=1
4.
looking at equation (4.2), you may wonder: if the pdf fxis analogous to pmf
px, why didn‚Äôt we require 0 ‚â§fx(x)‚â§1 instead of requiring only fx(x)‚â•0? this is
an excellent question, and it points exactly to the difference between a pmf and a pdf.
notice that fxis a mapping from the sample space œâ to the real line r. it does not map
œâ to [0 ,1]. on the other hand, since px(x) is the actual probability, it maps œâ to [0 ,1].
thus, fx(x) can take very large values but will not explode, because we have the unity
constraintr
œâfx(x)dx= 1. even if fx(x) takes a large value, it will be compensated by the
small dx. if you recall, there is nothing like dxin the definition of a pmf. whenever there
is a probability mass, we need to sum or, putting it another way, the dxin the discrete case
is always 1. therefore, while the probability mass pmf must not exceed 1, a probability
density pdf can exceed 1.
iffx(x)‚â•1, then what is the meaning of fx(x)? isn‚Äôt it representing the probability
of having an element x=x? if it were a discrete random variable, then yes; px(x) is the
probability of having x=x(so the probability mass cannot go beyond 1). however, for a
continuous random variable, fx(x) isnotthe probability of having x=x. the probability
of having x=x(i.e., exactly at x) is 0 because an isolated point has zero measure in the
continuous space. thus, even though fx(x) takes a value larger than 1, the probability of
xbeing xis zero.
at this point you can see why we call pdf a density , or density function, because each
value fx(x) is the probability per unit length . if we want to calculate the probability of
x‚â§x‚â§x+Œ¥, for example, then according to our definition, we have
p[x‚â§x‚â§x+Œ¥] =zx+Œ¥
xfx(x)dx‚âàfx(x)¬∑Œ¥.
therefore, the probability of p[x‚â§x‚â§x+Œ¥] can be regarded as the ‚Äúper unit length‚Äù
density fx(x) multiplied with the ‚Äúlength‚Äù Œ¥. asŒ¥‚Üí0, we can see that p[x=x] = 0. see
figure 4.3 for an illustration.
why are pdfs called a density function?
¬àbecause fx(x) is the probability per unit length .
¬àyou need to integrate fx(x) to obtain a probability.
1764.1. probability density function
figure 4.3: the probability p[x‚â§x‚â§x+Œ¥]can be approximated by the density fx(x)multiplied by
the length Œ¥.
example 4.4 . consider a random variable xwith pdf fx(x) =1
2‚àöxfor any
0< x‚â§1, and is 0 otherwise. we can show that fx(x)‚Üí ‚àû asx‚Üí0. however,
fx(x) remains a valid pdf because
z‚àû
‚àí‚àûfx(x)dx=z1
01
2‚àöxdx=‚àöx1
0= 1.
remark . since isolated points have zero measure in the continuous space, the probability
of an open interval ( a, b) is the same as the probability of a closed interval:
p[[a, b]] =p[(a, b)] =p[(a, b]] =p[[a, b)].
the exception is that when the pdf of fx(x) has a delta function at aorb. in this case,
the probability measure at aorbwill be non-zero. we will discuss this when we talk about
the cdfs.
practice exercise 4.2 . let fx(x) =c(1‚àíx2) for‚àí1‚â§x‚â§1, and 0 otherwise. find
the constant c.
solution . sincer
œâfx(x)dx= 1, it follows that
z
œâfx(x)dx=z1
‚àí1c(1‚àíx2)dx=4c
3‚áíc= 3/4.
practice exercise 4.3 . let fx(x) =x2for|x| ‚â§a, and 0 otherwise. find a.
solution . note that
z
œâfx(x)dx=za
‚àíax2dx=x3
3a
‚àía=2a3
3.
setting2a3
3= 1 yields a=3q
3
2.
177chapter 4. continuous random variables
4.1.3 connecting with the pmf
the probability density function is more general than the probability mass function. to see
this, consider a discrete random variable xwith a pmf px(x). because pxis defined on
a countable set œâ, we can write it as a train of delta functions and define a corresponding
pdf:
fx(x) =x
xk‚ààœâpx(xk)Œ¥(x‚àíxk).
example 4.5 . ifxis a bernoulli random variable with pmf px(1) = pandpx(0) =
1‚àíp, then the corresponding pdf can be written as
fx(x) =p Œ¥(x‚àí1) + (1 ‚àíp)Œ¥(x‚àí0).
example 4.6 . ifxis a binomial random variable with pmf px(k) = n
k
pk(1‚àíp)n‚àík,
then the corresponding pdf can be written as
fx(x) =nx
k=0px(k)Œ¥(x‚àík)
=nx
k=0n
k
pk(1‚àíp)n‚àíkŒ¥(x‚àík).
strictly speaking, delta functions are not really functions. they are defined through
integrations. they satisfy the properties that Œ¥(x‚àíxk) =‚àûifx=xk,Œ¥(x‚àíxk) = 0 if
xÃ∏=xk, andzxk+œµ
xk‚àíœµŒ¥(x‚àíxk)dx= 1,
for any œµ >0. suppose we ignore the fact that delta functions are not functions and merely
treat them as ordinary functions with some interesting properties. in this case, we can
imagine that for every probability mass px(xk), there exists an interval [ a, b] such that
there is one and only one state xkthat lies in [ a, b], as shown in figure 4.4 .
figure 4.4: we can view a pmf as a train of impulses. when computing the probability x=xk, we
integrate the pmf over the interval [a, b].
1784.1. probability density function
if we want to calculate the probability of obtaining x=xk, we can show that
p[x=xk](a)=p[a‚â§x‚â§b]
=zb
afx(x)dx
(b)=zb
apx(xk)Œ¥(x‚àíxk)dx
(c)=px(xk)zb
aŒ¥(x‚àíxk)dx
| {z }
=1=px(xk).
here, step ( a) holds because within [ a, b], there is no other event besides x=xk. step ( b)
is just the definition of our fx(x) (inside the interval [ a, b]). step ( c) shows that the delta
function integrates to 1, thus leaving the probability mass px(xk) as the final result. let us
look at an example and then comment on this intuition.
example 4.7 . let xbe a discrete random variable with pmf
px(k) =1
2k, k = 1,2, . . .
the continuous representation of the pmf can be written as
fx(x) =‚àûx
k=1px(k)Œ¥(x‚àík) =‚àûx
k=11
2k
Œ¥(x‚àík).
suppose we want to compute the probability p[1‚â§x‚â§2]. this can be computed as
p[1‚â§x‚â§2] =z2
1fx(x)dx=z2
1‚àûx
k=11
2k
Œ¥(x‚àík)dx
=z2
11
2Œ¥(x‚àí1) +1
4Œ¥(x‚àí2) +¬∑¬∑¬∑
dx
=1
2z2
1Œ¥(x‚àí1)dx
|{z }
=1+1
4z2
1Œ¥(x‚àí2)dx
|{z }
=1
+1
8z2
1Œ¥(x‚àí3)dx
|{z }
=0+¬∑¬∑¬∑|{z}
=0
=1
2+1
4=3
4.
however, if we want to compute the probability p[1< x‚â§2], then the integration
179chapter 4. continuous random variables
limit will not include the number 1 and so the delta function will remain 0. thus,
p[1< x‚â§2] =z2
1+fx(x)dx
=1
2z2
1+Œ¥(x‚àí1)dx
|{z }
=0+1
4z2
1+Œ¥(x‚àí2)dx
|{z }
=1=1
4.
closing remark . to summarize, we see that a pmf can be ‚Äúregarded‚Äù as a pdf. we are
careful to put a quotation around ‚Äúregarded‚Äù because pmf and pdf are defined for different
events. a pmf uses a discrete measure (i.e., a counter) for countable events, whereas a pdf
uses a continuous measure (i.e., integration) for continuous events. the way we link the two is
by using the delta functions. using the delta functions is valid, but the argument we provide
here is intuitive rather than rigorous. it is not rigorous because the integration we use is still
the riemann-stieltjes integration, which does not handle delta functions. therefore, while
you can treat a discrete pdf as a train of delta functions, it is important to remember the
limitations of the integrations we use.
4.2 expectation, moment, and variance
4.2.1 definition and properties
as with discrete random variables, we can define expectation for continuous random vari-
ables. the definition is analogous: just replace the summation with integration.
definition 4.3. theexpectation of a continuous random variable xis
e[x] =z
œâx fx(x)dx. (4.3)
example 4.8 . (uniform random variable ) let xbe a continuous random variable
with pdf fx(x) =1
b‚àíafora‚â§x‚â§b, and 0 otherwise. the expectation is
e[x] =z
œâxfx(x)dx=zb
ax¬∑1
b‚àíadx=1
b‚àíazb
ax dx
|{z}
=x2
2b
a
=1
b‚àía¬∑b2‚àía2
2=a+b
2.
1804.2. expectation, moment, and variance
example 4.9 . (exponential random variable ) let xbe a continuous random variable
with pdf fx(x) =Œªe‚àíŒªx, forx‚â•0. the expectation is
e[x] =z‚àû
0x Œªe‚àíŒªxdx
=‚àíz‚àû
0x de‚àíŒªx
=‚àíxe‚àíŒªx‚àû
0|{z}
=0+z‚àû
0e‚àíŒªxdx
=‚àí1
Œªe‚àíŒªx‚àû
0|{z}
=‚àí1=1
Œª,
where the colored step is due to integration by parts.
if a function gis applied to the random variable x, the expectation can be found using
the following theorem.
theorem 4.1. letg: œâ‚Üírbe a function and xbe a continuous random variable.
then
e[g(x)] =z
œâg(x)fx(x)dx. (4.4)
example 4.10 . (uniform random variable ) let xbe a continuous random variable
with fx(x) =1
b‚àíafora‚â§x‚â§b, and 0 otherwise. if g(¬∑) = (¬∑)2, then
e[g(x)] =e[x2] =z
œâx2fx(x)dx
=1
b‚àía¬∑zb
ax2dx
|{z}
=b3‚àía3
3=a2+ab+b2
3.
practice exercise 4.4 . let Œ∏ be a continuous random variable with pdf fŒ∏(Œ∏) =1
2œÄ
for 0‚â§Œ∏‚â§2œÄand is 0 otherwise. let y= cos( œât+ Œ∏). find e[y].
solution . referring to equation (4.4), the function gis
g(Œ∏) = cos( œât+Œ∏).
181chapter 4. continuous random variables
therefore, the expectation e[y] is
e[y] =z2œÄ
0cos(œât+Œ∏)fŒ∏(Œ∏)dŒ∏
=1
2œÄz2œÄ
0cos(œât+Œ∏)dŒ∏= 0,
where the last equality holds because the integral of a sinusoid over one period is 0.
practice exercise 4.5 . let a‚äÜœâ. let ia(x) be an indicator function such that
ia(x) =(
1,ifx‚ààa,
0,ifxÃ∏‚ààa.
finde[ia(x)].
solution . the expectation is
e[ia(x)] =z
œâia(x)fx(x)dx=z
x‚ààafx(x)dx=p[x‚ààa].
so the probability of {x‚ààa}can be equivalently represented in terms of expectation.
practice exercise 4.6 . is it true that e[1/x] = 1/e[x]?
solution . no. this is because
e1
x
=z
œâ1
xfx(x)dxÃ∏=1r
œâxfx(x)dx=1
e[x].
all the properties of expectation we learned in the discrete case can be translated to
the continuous case. specifically, we have that
¬àe[ax] =ae[x]: a scalar multiple of a random variable will scale the expectation.
¬àe[x+a] =e[x]+a: constant addition of a random variable will offset the expectation.
¬àe[ax+b] =ae[x] +b: affine transformation of a random variable will translate to
the expectation.
practice exercise 4.7 . prove the above three statements.
solution . the third statement is just the sum of the first two statements, so we just
1824.2. expectation, moment, and variance
need to show the first two:
e[ax] =z
œâaxfx(x)dx=az
œâxfx(x)dx=ae[x],
e[x+a] =z
œâ(x+a)fx(x)dx=z
œâxfx(x)dx+a=e[x] +a.
4.2.2 existence of expectation
as we discussed in the discrete case, not all random variables have an expectation.
definition 4.4. a random variable xhas an expectation if it is absolutely integrable ,
i.e.,
e[|x|] =z
œâ|x|fx(x)dx <‚àû. (4.5)
being absolutely integrable implies that the expectation is that e[|x|] is the upper
bound of e[x].
theorem 4.2. for any random variable x,
|e[x]| ‚â§e[|x|]. (4.6)
proof . note that fx(x)‚â•0. therefore,
‚àí|x|fx(x)‚â§x fx(x)‚â§ |x|, fx(x),‚àÄx.
thus, integrating all three terms yields
‚àíz
œâ|x|fx(x)dx‚â§z
œâx fx(x)dx‚â§z
œâ|x|fx(x)dx,
which is equivalent to ‚àíe[|x|]‚â§e[x]‚â§e[|x|].
‚ñ°
example 4.11 . here is a random variable whose expectation is undefined. let xbe
a random variable with pdf
fx(x) =1
œÄ(1 +x2), x ‚ààr.
this random variable is called the cauchy random variable . we can show that
e[x] =z‚àû
‚àí‚àûx¬∑1
œÄ(1 +x2)dx=1
œÄz‚àû
0x
(1 +x2)dx+1
œÄz0
‚àí‚àûx
(1 +x2)dx.
183chapter 4. continuous random variables
the first integral gives
z‚àû
0x
(1 +x2)dx=1
2log(1 + x2)‚àû
0=‚àû,
and the second integral gives ‚àí‚àû. since neither integral is finite, the expectation is
undefined. we can also check the absolutely integrability criterion:
e[|x|] =z‚àû
‚àí‚àû|x| ¬∑1
œÄ(1 +x2)dx
(a)= 2z‚àû
0x
œÄ(1 +x2)dx‚â•2z‚àû
1x
œÄ(1 +x2)dx
(b)
‚â•2z‚àû
1x
œÄ(x2+x2)dx=1
œÄlog(x)‚àû
1=‚àû,
where in (a) we use the fact that the function being integrated is even, and in (b) we
lower-bound1
1+x2‚â•1
x2+x2ifx >1.
4.2.3 moment and variance
the moment and variance of a continuous random variable can be defined analogously to
the moment and variance of a discrete random variable, replacing the summations with
integrations.
definition 4.5. thekth moment of a continuous random variable xis
e[xk] =z
œâxkfx(x)dx. (4.7)
definition 4.6. thevariance of a continuous random variable xis
var[x] =e[(x‚àí¬µ)2] =z
œâ(x‚àí¬µ)2fx(x)dx, (4.8)
where ¬µdef=e[x].
it is not difficult to show that the variance can also be expressed as
var[x] =e[x2]‚àí¬µ2,
because
var[x] =e[(x‚àí¬µ)2] =e[x2]‚àí2e[x]¬µ+¬µ2=e[x2]‚àí¬µ2.
1844.3. cumulative distribution function
practice exercise 4.8 . (uniform random variable ) let xbe a continuous random
variable with pdf fx(x) =1
b‚àíafora‚â§x‚â§b, and 0 otherwise. find var[ x].
solution . we have shown that e[x] =a+b
2ande[x2] =a2+ab+b2
3. therefore, the
variance is
var[x] =e[x2]‚àíe[x]2
=a2+ab+b2
3‚àía+b
22
=(b‚àía)2
12.
practice exercise 4.9 . (exponential random variable ) let xbe a continuous ran-
dom variable with pdf fx(x) =Œªe‚àíŒªxforx‚â•0, and 0 otherwise. find var[ x].
solution . we have shown that e[x] =1
Œª. the second moment is
e[x2] =z‚àû
0x2Œªe‚àíŒªxdx
=
‚àíx2e‚àíŒªx‚àû
0+z‚àû
02xe‚àíŒªxdx
=2
Œªz‚àû
0xŒªe‚àíŒªxdx
=2
Œª¬∑1
Œª=2
Œª2.
therefore,
var[x] =e[x2]‚àíe[x]2
=2
Œª2‚àí1
Œª2=1
Œª2.
4.3 cumulative distribution function
when we discussed discrete random variables, we introduced the concept of cumulative
distribution functions (cdfs). one of the motivations was that if we view a pmf as a train
of delta functions, they are technically not well-defined functions. however, it turns out that
the cdf is always a well-defined function. in this section, we will complete the story by first
discussing the cdf for continuous random variables. then, we will come back and show
you how the cdf can be derived for discrete random variables.
185chapter 4. continuous random variables
4.3.1 cdf for continuous random variables
definition 4.7. letxbe a continuous random variable with a sample space œâ =r.
thecumulative distribution function (cdf) ofxis
fx(x)def=p[x‚â§x] =zx
‚àí‚àûfx(x‚Ä≤)dx‚Ä≤. (4.9)
the interpretation of the cdf can be seen from figure 4.5 . given a pdf fx, the cdf
fxevaluated at xis the integration of fxfrom‚àí‚àûup to a point x. the integration of fx
from‚àí‚àûtoxis nothing but the area under the curve of fx. since fxis non-negative, the
larger value xwe use to evaluate in fx(x), the more area under the curve we are looking
at. in the extreme when x=‚àí‚àû, we can see that fx(‚àí‚àû) = 0, and when x= +‚àûwe
have that fx(+‚àû) =r‚àû
‚àí‚àûfx(x)dx= 1.
figure 4.5: a cdf is the integral of the pdf. thus, the height of a stem in the cdf corresponds to
the area under the curve of the pdf.
practice exercise 4.10 . (uniform random variable ) let xbe a continuous random
variable with pdf fx(x) =1
b‚àíafora‚â§x‚â§b, and is 0 otherwise. find the cdf of x.
solution . the cdf of xis given by
fx(x) =Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥0, x ‚â§a,rx
‚àí‚àûfx(x‚Ä≤)dx‚Ä≤=rx
a1
b‚àíadx‚Ä≤=x‚àía
b‚àía, a < x ‚â§b,
1, x > b.
as you can see from this practice exercise, we explicitly break the cdf into three segments.
the first segment gives fx(x) = 0 because for any x‚â§a, there is nothing to integrate,
since fx(x) = 0 for any x‚â§a. similarly, for the last segment, fx(x) = 1 for all x > b
because once xgoes beyond b, the integration will cover all the non-zeros of fx.figure 4.6
illustrates the pdf and cdf for this example.
in matlab, we can generate the pdf and cdf using the commands pdfand cdf
respectively. for the particular example shown in figure 4.6 , the following code can be used.
a similar set of commands can be implemented in python.
1864.3. cumulative distribution function
figure 4.6: example: fx(x) = 1 /(b‚àía)fora‚â§x‚â§b. the cdf has three segments.
% matlab code to generate the pdf and cdf
unif = makedist(‚Äôuniform‚Äô,‚Äôlower‚Äô,-3,‚Äôupper‚Äô,4);
x = linspace(-5, 10, 1500)‚Äô;
f = pdf(unif, x);
f = cdf(unif, x);
figure(1); plot(x, f, ‚Äôlinewidth‚Äô, 6);
figure(2); plot(x, f, ‚Äôlinewidth‚Äô, 6);
# python code to generate the pdf and cdf
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
x = np.linspace(-5,10,1500)
f = stats.uniform.pdf(x,-3,4)
f = stats.uniform.cdf(x,-3,4)
plt.plot(x,f); plt.show()
plt.plot(x,f); plt.show()
practice exercise 4.11 . (exponential random variable ) let xbe a continuous
random variable with pdf fx(x) =Œªe‚àíŒªxforx‚â•0, and 0 otherwise. find the cdf
ofx.
solution . clearly, for x <0, we have fx(x) = 0. for x‚â•0, we can show that
fx(x) =zx
0fx(x‚Ä≤)dx‚Ä≤=zx
0Œªe‚àíŒªx‚Ä≤dx‚Ä≤= 1‚àíe‚àíŒªx.
therefore, the complete cdf is (see figure 4.7 for illustration):
fx(x) =(
0, x < 0,
1‚àíe‚àíŒªx, x ‚â•0.
the matlab code and python code to generate this figure are shown below.
187chapter 4. continuous random variables
figure 4.7: example: fx(x) =Œªe‚àíŒªxforx‚â•0. the cdf has two segments.
% matlab code to generate the pdf and cdf
pd = makedist(‚Äôexp‚Äô,2);
x = linspace(-5, 10, 1500)‚Äô;
f = pdf(pd, x);
f = cdf(pd, x);
figure(1); plot(x, f, ‚Äôlinewidth‚Äô, 6);
figure(2); plot(x, f, ‚Äôlinewidth‚Äô, 6);
# python code to generate the pdf and cdf
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
x = np.linspace(-5,10,1500)
f = stats.expon.pdf(x,2)
f = stats.expon.cdf(x,2)
plt.plot(x,f); plt.show()
plt.plot(x,f); plt.show()
4.3.2 properties of cdf
let us now describe the properties of a cdf. if we compare these with those for the discrete
cases, we see that the continuous cases simply replace the summations by integrations.
therefore, we should expect to inherit most of the properties from the discrete cases.
proposition 4.1. letxbe a random variable (either continuous or discrete), then
the cdf of xhas the following properties:
(i) the cdf is nondecreasing .
(ii) the maximum of the cdf is when x=‚àû:fx(+‚àû) = 1 .
(iii) the minimum of the cdf is when x=‚àí‚àû:fx(‚àí‚àû) = 0 .
1884.3. cumulative distribution function
proof . for (i), we notice that fx(x) =rx
‚àí‚àûfx(x‚Ä≤)dx‚Ä≤. therefore, if s‚â§tthen
fx(s) =zs
‚àí‚àûfx(x‚Ä≤)dx‚Ä≤‚â§zt
‚àí‚àûfx(x‚Ä≤)dx‚Ä≤=fx(t).
thus it shows that fxis nondecreasing. (it does not need to be increasing because a cdf
can have a steady state.) for (ii) and (iii), we can show that
fx(+‚àû) =z+‚àû
‚àí‚àûfx(x‚Ä≤)dx‚Ä≤= 1,and fx(‚àí‚àû) =z‚àí‚àû
‚àí‚àûfx(x‚Ä≤)dx‚Ä≤= 0.‚ñ°
example 4.12 . we can show that the cdf we derived for the uniform random variable
satisfies these three properties. to see this, we note that
fx(x) =x‚àía
b‚àía, a‚â§x‚â§b.
the derivative of this function f‚Ä≤
x(x) =1
b‚àía>0 for a‚â§x‚â§b. also, note that
fx(x) = 0 for x < a andx > b , sofxis nondecreasing. the other two properties
follow because if x=b, then fx(b) = 1, and if x=athen fx(a) = 0. together with
the nondecreasing property, we show (ii) and (iii).
proposition 4.2. letxbe a continuous random variable. if the cdf fxis contin-
uous at any a‚â§x‚â§b, then
p[a‚â§x‚â§b] =fx(b)‚àífx(a). (4.10)
proof . the proof follows from the definition of the cdf, which states that
fx(b)‚àífx(a) =zb
‚àí‚àûfx(x‚Ä≤)dx‚Ä≤‚àíza
‚àí‚àûfx(x‚Ä≤)dx‚Ä≤
=zb
afx(x‚Ä≤)dx‚Ä≤=p[a‚â§x‚â§b]. ‚ñ°
this result provides a very handy tool for calculating the probability of an event
a‚â§x‚â§busing the cdf. it says that p[a‚â§x‚â§b] is the difference between fx(b) and
fx(a). so, if we are given fx, calculating the probability of a‚â§x‚â§bjust involves
evaluating the cdf at aandb. the result also shows that for a continuous random vari-
ablex,p[x=x0] =fx(x0)‚àífx(x0) = 0. this is consistent with our arguments from the
measure‚Äôs point of view.
example 4.13 . (exponential random variable ) we showed that the exponential ran-
dom variable xwith a pdf fx(x) =Œªe‚àíŒªxforx‚â•0 (and fx(x) = 0 for x < 0)
has a cdf given by fx(x) = 1‚àíe‚àíŒªxforx‚â•0. suppose we want to calculate the
189chapter 4. continuous random variables
probability p[1‚â§x‚â§3]. then the pdf approach gives us
p[1‚â§x‚â§3] =z3
1fx(x)dx=z3
1Œªe‚àíŒªxdx=‚àíe‚àíŒªx3
1=e‚àí3Œª‚àíe‚àíŒª.
if we take the cdf approach, we can show that
p[1‚â§x‚â§3] =fx(3)‚àífx(1)
= (1‚àíe‚àíŒª)‚àí(1‚àíe‚àí3Œª) =e‚àí3Œª‚àíe‚àíŒª,
which yields the same as the pdf approach.
example 4.14 . let xbe a random variable with pdf fx(x) = 2 xfor 0‚â§x‚â§1,
and is 0 otherwise. we can show that the cdf is
fx(x) =zx
0fx(t)dt=zx
02t dt=t2x
0=x2, 0‚â§x‚â§1.
therefore, to compute the probability p[1/3‚â§x‚â§1/2], we have
p1
3‚â§x‚â§1
2
=fx1
2
‚àífx1
3
=1
22
‚àí1
32
=5
36.
‚ñ°
a cdf can be used for both continuous and discrete random variables. however, before
we can do that, we need a tool to handle the discontinuities. the following definition is a
summary of the three types of continuity.
definition 4.8. a function fx(x)is said to be
¬àleft-continuous atx=biffx(b) =fx(b‚àí)def= lim h‚Üí0fx(b‚àíh);
¬àright-continuous atx=biffx(b) =fx(b+)def= lim h‚Üí0fx(b+h);
¬àcontinuous atx=bif it is both right-continuous and left-continuous at x=b.
in this case, we have
lim
h‚Üí0fx(b‚àíh) = lim
h‚Üí0fx(b+h) =f(b).
in this definition, the step size h >0 is shrinking to zero. the point b‚àíhstays at the left of
b, and b+hstays at the right of b. thus, if we set the limit h‚Üí0,b‚àíhwill approach a point
b‚àíwhereas b+hwill approach a point b+. if it happens that fx(b‚àí) =fx(b) then we say
thatfxis left-continuous at b. iffx(b+) =fx(b) then we say that fxis right-continuous
atb. these are summarized in figure 4.8 .
whenever fxhas a discontinuous point, it can be left-continuous, right-continuous, or
neither. (‚Äúneither‚Äù happens if fx(b) take a value other than fx(b+) orfx(b‚àí). you can
1904.3. cumulative distribution function
figure 4.8: the definition of left- and right-continuous at a point b.
always create a nasty function that satisfies this condition.) for continuous functions, it is
necessary that fx(b‚àí) =fx(b+). if this happens, there is no gap between the two points.
theorem 4.3. for any random variable x(discrete or continuous), fx(x)is always
right-continuous . that is,
fx(b) =fx(b+)def= lim
h‚Üí0fx(b+h) (4.11)
right-continuous means that if fx(x) is piecewise, it must have a solid left end and an
empty right end . figure 4.9 shows an example of a valid cdf and an invalid cdf.
figure 4.9: a cdf must be right-continuous.
the reason why fxis always right-continuous is that the inequality x‚â§xhas a
closed right-hand limit. imagine the following situation: a discrete random variable xhas
four states: 1 ,2,3,4. then,
lim
h‚Üí0fx(3 +h) = lim
h‚Üí0‚Äú3 + h‚Äùx
k=1px(k) =px(1) + px(2) + px(3) = fx(3).
similarly, if you have a continuous random variable xwith a pdf fx, then
lim
h‚Üí0fx(b+h) = lim
h‚Üí0zb+h
‚àí‚àûfx(t)dt=zb
‚àí‚àûfx(t)dt=fx(b).
191chapter 4. continuous random variables
in other words, the ‚Äú ‚â§‚Äù ensures that the rightmost state is included. if we defined cdf
using <, we would have gotten left-hand continuous, but this would be inconvenient because
the<requires us to deal with limits whenever we evaluate x < x .
theorem 4.4. for any random variable x(discrete or continuous), p[x=b]is
p[x=b] =(
fx(b)‚àífx(b‚àí),iffxis discontinuous at x=b
0,otherwise.(4.12)
this proposition states that when fx(x) is discontinuous at x=b, then p[x=b] is
the difference between fx(b) and the limit from the left. in other words, the height of the
gap determines the probability at the discontinuity. if fx(x) is continuous at x=b, then
fx(b) = lim h‚Üí0fx(b‚àíh) and so p[x=b] = 0.
figure 4.10: illustration of equation (4.12). since the cdf is discontinuous at a point x=b, the gap
fx(b)‚àífx(b‚àí)will define the probability p[x=b].
example 4.15 . consider a random variable xwith a pdf
fx(x) =Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥x,0‚â§x‚â§1,
1
2, x= 3,
0,otherwise .
the cdf fx(x) will consist of a few segments. the first segment is 0 ‚â§x <1. we
can show that
fx(x) =zx
0fx(t)dt=zx
0t dt=t2
2x
0=x2
2,0‚â§x <1.
the second segment is when 1 ‚â§x <3. since there is no new fxto integrate, the
cdf stays at fx(x) =fx(1) =1
2for 1‚â§x <3. the third segment is x >3. because
this range has covered the entire sample space, we have fx(x) = 1 for x >3. how
about x= 3? we can show that
fx(3) = fx(3+) = 1 .
1924.3. cumulative distribution function
therefore, to summarize, the cdf is
fx(x) =Ô£±
Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥0, x < 0,
x2
2, 0‚â§x <1,
1
2, 1‚â§x <3,
1, x ‚â•3.
a graphical illustration is shown in figure 4.11 .
figure 4.11: an example of converting a pdf to a cdf.
4.3.3 retrieving pdf from cdf
thus far, we have only seen how to obtain fx(x) from fx(x). in order to go in the reverse
direction, we recall the fundamental theorem of calculus. this states that if a function fis
continuous, then
f(x) =d
dxzx
af(t)dt
for some constant a. using this result for cdf and pdf, we have the following:
theorem 4.5. theprobability density function (pdf) is the derivative of the cu-
mulative distribution function (cdf):
fx(x) =dfx(x)
dx=d
dxzx
‚àí‚àûfx(x‚Ä≤)dx‚Ä≤, (4.13)
provided fxis differentiable at x. iffxis not differentiable at x=x0, then,
fx(x0) =p[x=x0]Œ¥(x‚àíx0).
example 4.16 . consider a cdf
fx(x) =(
0, x < 0,
1‚àí1
4e‚àí2x, x ‚â•0.
we want to find the pdf fx(x). to do so, we first show that fx(0) =3
4. this
193chapter 4. continuous random variables
corresponds to a discontinuity at x= 0, as shown in figure 4.12 .
figure 4.12: an example of converting a pdf to a cdf.
because of the discontinuity, we need to consider three cases:
fx(x) =Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥dfx(x)
dx, x < 0,
p[x= 0]Œ¥(x‚àí0), x = 0,
dfx(x)
dx, x > 0.
when x <0,fx(x) = 0, sodfx(x)
dx= 0. when x >0,fx(x) = 1‚àí1
4e‚àí2x, so
dfx(x)
dx=1
2e‚àí2x.
when x= 0, the probability p[x= 0] is determined by the gap between the solid dot
and the empty dot. this yields
p[x= 0] = fx(0)‚àílim
h‚Üí0fx(0‚àíh) =3
4‚àí0 =3
4.
therefore, the overall pdf is
fx(x) =Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥0, x < 0,
3
4Œ¥(x‚àí0), x = 0,
1
2e‚àí2x, x > 0.
figure 4.12 illustrates this example.
4.3.4 cdf: unifying discrete and continuous random variables
the cdf is always a well-defined function. it is integrable everywhere. if the underlying
random variable is continuous, the cdf is also continuous. if the underlying random variable
is discrete, the cdf is a staircase function. we have seen enough cdfs for continuous
random variables. let us (re)visit a few discrete random variables.
example 4.17 . (geometric random variable) consider a geometric random variable
with pmf px(k) = (1 ‚àíp)k‚àí1p, fork= 1,2, . . ..
1944.3. cumulative distribution function
figure 4.13: pmf and cdf of a geometric random variable.
we can show that the cdf is
fx(k) =kx
‚Ñì=1px(‚Ñì) =kx
‚Ñì=1(1‚àíp)‚Ñì‚àí1p=p¬∑1‚àí(1‚àíp)k
1‚àí(1‚àíp)= 1‚àí(1‚àíp)k.
for a sanity check, we can try to retrieve the pmf from the cdf:
px(k) =fx(k)‚àífx(k‚àí1)
= (1‚àí(1‚àíp)k)‚àí(1‚àí(1‚àíp)k‚àí1)
= (1‚àíp)k‚àí1p.
a graphical portrayal of this example is shown in figure 4.13 .
if we treat the pmfs as delta functions in the above example, then the continuous
definition also applies. since the cdf is a piecewise constant function, the derivative is
exactly a delta function. for some problems, it is easier to start with cdf and then compute
the pmf or pdf. here is an example.
example 4.18 . let x1,x2andx3be three independent discrete random variables
with sample space œâ = {1,2, . . . , 10}. define x= max {x1, x2, x3}. we want to
find the pmf of x. to tackle this problem, we first observe that the pmf for x1is
px1(k) =1
10. thus, the cdf of x1is
fx1(k) =kx
‚Ñì=1px1(‚Ñì) =k
10.
then, we can show that the cdf of xis
fx(k) =p[x‚â§k] =p[max{x1, x2, x3} ‚â§k]
(a)=p[x1‚â§k‚à©x2‚â§k‚à©x3‚â§k]
(b)=p[x1‚â§k]p[x2‚â§k]p[x3‚â§k]
=k
103
,
195chapter 4. continuous random variables
where in ( a) we use the fact that max {x1, x2, x3} ‚â§kif and only if all three elements
are less than k, and in ( b) we use independence. consequently, the pmf of xis
px(k) =fx(k)‚àífx(k‚àí1) =k
103
‚àík‚àí1
103
.
what is a cdf?
¬àcdf is fx(x) =p[x‚â§x]. it is the cumulative sum of the pmf/pdf.
¬àcdf is either a staircase function, a smooth function, or a hybrid. unlike a
pdf, which is not defined for discrete random variables, the cdf is always well
defined.
¬àcdfd
dx‚àí‚Üípdf.
¬àcdfr
‚Üê‚àípdf.
¬àgap of jump in cdf = height of delta in pdf.
4.4 median, mode, and mean
there are three statistical quantities that we are frequently interested in: mean, mode, and
median. we all know how to compute these from a dataset. for example, to compute the
median of a dataset, we sort the data and pick the number that sits in the 50th percentile.
however, the median computed in this way is the empirical median , i.e., it is a value
computed from a particular dataset. if the data is generated from a random variable (with
a given pdf), how do we compute the mean, median, and mode?
4.4.1 median
imagine you have a sequence of numbers as shown below.
n 1 2 3 4 5 6 7 8 9 ¬∑¬∑¬∑ 100
xn1.5 2.5 3.1 1.1 ‚àí0.4‚àí4.1 0.5 2.2 ‚àí3.4¬∑¬∑¬∑ ‚àí 1.4
how do we compute the median? we first sort the sequence (either in ascending order
or descending order), and then pick the middle one. on computer, we permute the samples
{x1‚Ä≤, x2‚Ä≤, . . . , x n‚Ä≤}= sort {x1, x2, . . . , x n},
such that x1‚Ä≤< x2‚Ä≤< . . . < x n‚Ä≤is ordered. the median is the one positioned at the middle.
there are, of course, built-in commands such as median in matlab and np.median in
python to perform the median operation.
now, how do we compute the median if we are given a random variable xwith a pdf
fx(x)? the answer is by integrating the pdf.
1964.4. median, mode, and mean
definition 4.9. letxbe a continuous random variable with pdf fx. the median
ofxis a point c‚ààrsuch that
zc
‚àí‚àûfx(x)dx=z‚àû
cfx(x)dx. (4.14)
why is the median defined in this way? this is becauserc
‚àí‚àûfx(x)dxis the area under
the curve on the left of c, andr‚àû
cfx(x)dxis the area under the curve on the right of c.
the area under the curve tells us the percentage of numbers that are less than the cutoff.
therefore, if the left area equals the right area, then cmust be the median.
how to find the median from the pdf
¬àfind a point cthat separates the pdf into two equal areas
figure 4.14: [left] the median is computed as the point such that the two areas under the curve are
equal. [right] the median is computed as the point such that fxhits 0.5.
the median can also be evaluated from the cdf as follows.
theorem 4.6. themedian of a random variable xis the point csuch that
fx(c) =1
2. (4.15)
proof . since fx(x) =rx
‚àí‚àûfx(x‚Ä≤)dx‚Ä≤, we have
fx(c) =zc
‚àí‚àûfx(x)dx
=z‚àû
cfx(x)dx= 1‚àífx(c).
rearranging the terms shows that fx(c) =1
2. ‚ñ°
197chapter 4. continuous random variables
how to find median from cdf
¬àfind a point csuch that fx(c) = 0 .5.
example 4.19 . (uniform random variable ) let xbe a continuous random variable
with pdf fx(x) =1
b‚àíafora‚â§x‚â§b, and is 0 otherwise. we know that the cdf of
xisfx(x) =x‚àía
b‚àíafora‚â§x‚â§b. therefore, the median of xis the number c‚ààr
such that fx(c) =1
2. substituting into the cdf yieldsc‚àía
b‚àía=1
2, which gives c=a+b
2.
example 4.20 . (exponential random variable ) let xbe a continuous random vari-
able with pdf fx(x) =Œªe‚àíŒªxforx‚â•0. we know that the cdf of xisfx(x) =
1‚àíe‚àíŒªxforx‚â•0. the median of xis the point csuch that fx(c) =1
2. this gives
1‚àíe‚àíŒªc=1
2, which is c=log 2
Œª.
4.4.2 mode
the mode is the peak of the pdf. we can see this from the definition below.
definition 4.10. letxbe a continuous random variable. the mode is the point c
such that fx(x)attains the maximum:
c=argmax
x‚ààœâfx(x) =argmax
x‚ààœâd
dxfx(x). (4.16)
the second equality holds because fx(x) =f‚Ä≤
x(x) =d
dxrx
‚àí‚àûfx(t)dt. a pictorial illustra-
tion of mode is given in figure 4.15 . note that the mode of a random variable is not unique,
e.g., a mixture of two identical gaussians with different means has two modes.
figure 4.15: [left] the mode appears at the peak of the pdf. [right] the mode appears at the steepest
slope of the cdf.
1984.4. median, mode, and mean
how to find mode from pdf
¬àfind a point csuch that fx(c) is maximized.
how to find mode from cdf
¬àcontinuous: find a point csuch that fx(c) has the steepest slope.
¬àdiscrete: find a point csuch that fx(c) has the biggest gap in a jump.
example 4.21 . let xbe a continuous random variable with pdf fx(x) = 6 x(1‚àíx)
for 0‚â§x‚â§1. the mode of xhappens at argmax
xfx(x). to find this maximum, we
take the derivative of fx. this gives
0 =d
dxfx(x) =d
dx6x(1‚àíx) = 6(1 ‚àí2x).
setting this equal to zero yields x=1
2.
to ensure that this point is a maximum, we take the second-order derivative:
d2
dx2fx(x) =d
dx6(1‚àí2x) =‚àí12<0.
therefore, we conclude that x=1
2is a maximum point. hence, the mode of xis
x=1
2.
4.4.3 mean
we have defined the mean as the expectation of x. here, we show how to compute the
expectation from the cdf. to simplify the demonstration, let us first assume that x > 0.
lemma 4.1. letx > 0. then e[x]can be computed from fxas
e[x] =z‚àû
0(1‚àífx(t))dt. (4.17)
proof . the trick is to change the integration order:
z‚àû
0(1‚àífx(t))dt=z‚àû
0[1‚àíp[x‚â§t]]dt=z‚àû
0p[x > t ]dt
=z‚àû
0z‚àû
tfx(x)dx dt(a)=z‚àû
0zx
0fx(x)dt dx
=z‚àû
0zx
0dtfx(x)dx=z‚àû
0xfx(x)dx=e[x].
here, step ( a) is due to the change of integration order. see figure 4.16 for an illustration.
‚ñ°
we draw a picture to illustrate the above lemma. as shown in figure 4.17 , the mean
of a positive random variable x > 0 is equivalent to the area above the cdf.
199chapter 4. continuous random variables
figure 4.16: the double integration can be evaluated by xthent, ortthenx.
figure 4.17: the mean of a positive random variable x > 0can be calculated by integrating the cdf‚Äôs
complement.
lemma 4.2. letx < 0. then e[x]can be computed from fxas
e[x] =z0
‚àí‚àûfx(t)dt. (4.18)
proof . the idea here is also to change the integration order.
z0
‚àí‚àûfx(t)dt=z0
‚àí‚àûp[x‚â§t]dt=z0
‚àí‚àûzt
‚àí‚àûfx(x)dx dt
=z0
‚àí‚àûz0
xfx(x)dt dx =z0
‚àí‚àûxfx(x)dx=e[x].
‚ñ°
theorem 4.7. the mean of a random variable xcan be computed from the cdf as
e[x] =z‚àû
0(1‚àífx(t))dt‚àíz0
‚àí‚àûfx(t)dt. (4.19)
2004.5. uniform and exponential random variables
proof . for any random variable x, we can partition x=x+‚àíx‚àíwhere x+andx‚àíare
the positive and negative parts, respectively. then, the above two lemmas will give us
e[x] =e[x+‚àíx‚àí] =e[x+]‚àíe[x‚àí]
=z‚àû
0(1‚àífx(t))dt‚àíz0
‚àí‚àûfx(t)dt.
‚ñ°
as illustrated in figure 4.18 , this equation is equivalent to computing the areas above
and below the cdf and taking the difference.
figure 4.18: the mean of a random variable xcan be calculated by computing the area in the cdf.
how to find the mean from the cdf
¬àa formula is given by equation (4.20):
e[x] =z‚àû
0(1‚àífx(t))dt‚àíz0
‚àí‚àûfx(t)dt. (4.20)
¬àthis result is not commonly used, but the proof technique of switching the inte-
gration order is important.
4.5 uniform and exponential random variables
there are many useful continuous random variables. in this section, we discuss two of them:
uniform random variables and exponential random variables. in the next section, we will
discuss the gaussian random variables. similarly to the way we discussed discrete random
variables, we take a generative / synthesis perspective when studying continuous random
variables. we assume we have access to the pdf of the random variables so we can derive
the theoretical mean and variance. the opposite direction, namely inferring the underlying
model parameters from a dataset, will be discussed later.
201chapter 4. continuous random variables
4.5.1 uniform random variables
definition 4.11. letxbe a continuous uniform random variable . the pdf of xis
fx(x) =(
1
b‚àía, a ‚â§x‚â§b,
0, otherwise ,(4.21)
where [a, b]is the interval on which xis defined. we write
x‚àºuniform( a, b)
to mean that xis drawn from a uniform distribution on an interval [a, b].
0 0.2 0.4 0.6 0.8 100.511.522.53
0 0.2 0.4 0.6 0.8 100.20.40.60.811.2
(a) pdf (b) cdf
figure 4.19: the pdf and cdf of x‚àºuniform (0.2,0.6).
the shape of the pdf of a uniform random variable is shown in figure 4.19 . in this
figure, we assume that the random variables x‚àºuniform(0 .2,0.6) are taken from the
sample space œâ = [0 ,1]. note that the height of the uniform distribution is greater than 1,
since
fx(x) =(
1
0.6‚àí0.2= 2.5, 0.2‚â§x‚â§0.6,
0, otherwise .
there is nothing wrong with this pdf, because fx(x) is the probability per unit length . if we
integrate fx(x) over any sub-interval between 0.2 and 0.6, we can show that the probability
is between 0 and 1.
the cdf of a uniform random variable can be determined by integrating fx(x):
fx(x) =zx
‚àí‚àûfx(t)dt
=zx
a1
b‚àíadt
=x‚àía
b‚àía, a ‚â§x‚â§b.
2024.5. uniform and exponential random variables
therefore, the complete cdf is
fx(x) =Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥0, x < a,
x‚àía
b‚àía, a ‚â§x‚â§b,
1, x > b.
the corresponding cdf for the pdf we showed in figure 4.19 (a) is shown in figure 4.19 (b).
it can be seen that although the height of the pdf exceeds 1, the cdf grows linearly and
saturates at 1.
remark . the uniform distribution can also be defined for discrete random variables. in
this case, the probability mass function is given by
px(k) =1
b‚àía+ 1, k =a, a+ 1, . . . , b.
the presence of ‚Äú1‚Äù in the denominator of the pmf is because kruns from atob, including
the two endpoints.
in matlab and python, generating uniform random numbers can be done by calling
commands unifrnd (matlab), and stats.uniform.rvs (python). for discrete uniform
random variables, in matlab the command is unidrnd , and in python the command is
stats.randint .
% matlab code to generate 1000 uniform random numbers
a = 0; b = 1;
x = unifrnd(a,b,[1000,1]);
hist(x);
# python code to generate 1000 uniform random numbers
import scipy.stats as stats
a = 0; b = 1;
x = stats.uniform.rvs(a,b,size=1000)
plt.hist(x);
to compute the empirical average and variance of the random numbers in matlab
we can call the command mean andvar. the corresponding command in python is np.mean
andnp.var . we can also compute the median and mode, as shown below.
% matlab code to compute empirical mean, var, median, mode
x = unifrnd(a,b,[1000,1]);
m = mean(x);
v = var(x);
med = median(x);
mod = mode(x);
# python code to compute empirical mean, var, median, mode
x = stats.uniform.rvs(a,b,size=1000)
m = np.mean(x)
v = np.var(x)
203chapter 4. continuous random variables
med = np.median(x)
mod = stats.mode(x)
the mean and variance of a uniform random variable are given by the theorem below.
theorem 4.8. ifx‚àºuniform( a, b), then
e[x] =a+b
2and var[x] =(b‚àía)2
12. (4.22)
proof . we have derived these results before. here is a recap for completeness:
e[x] =z‚àû
‚àí‚àûxfx(x)dx=zb
ax
b‚àíadx=a+b
2,
e[x2] =z‚àû
‚àí‚àûx2fx(x)dx=zb
ax2
b‚àíadx=a2+ab+b2
3,
var[x] =e[x2]‚àíe[x]2=(b‚àía)2
12.
‚ñ°
the result should be intuitive because it says that the mean is the midpoint of the
pdf.
when will we encounter a uniform random variable? uniform random variables are one
of the most elementary continuous random variables. given a uniform random variable, we
can construct any random variable by using an appropriate transformation. we will discuss
this technique as part of our discussion about generating random numbers.
in matlab, computing the mean and variance of a uniform random variable can be
done using the command unifstat . the python coommand is stats.uniform.stats .
% matlab code to compute mean and variance
a = 0; b = 1;
[m,v] = unifstat(a,b)
# python code to compute mean and variance
import scipy.stats as stats
a = 0; b = 1;
m, v = stats.uniform.stats(a,b,moments=‚Äômv‚Äô)
to evaluate the probability p[‚Ñì‚â§x‚â§u] for a uniform random variable, we can call
unifcdf in matlab and
% matlab code to compute the probability p(0.2 < x < 0.3)
a = 0; b = 1;
f = unifcdf(0.3,a,b) - unifcdf(0.2,a,b)
2044.5. uniform and exponential random variables
# python code to compute the probability p(0.2 < x < 0.3)
a = 0; b = 1;
f = stats.uniform.cdf(0.3,a,b)-stats.uniform.cdf(0.2,a,b)
an alternative is to define an object rv = stats.uniform , and call the cdf attribute:
# python code to compute the probability p(0.2 < x < 0.3)
a = 0; b = 1;
rv = stats.uniform(a,b)
f = rv.cdf(0.3)-rv.cdf(0.2)
4.5.2 exponential random variables
definition 4.12. letxbe an exponential random variable . the pdf of xis
fx(x) =(
Œªe‚àíŒªx, x ‚â•0,
0, otherwise ,(4.23)
where Œª >0is a parameter. we write
x‚àºexponential( Œª)
to mean that xis drawn from an exponential distribution of parameter Œª.
in this definition, the parameter Œªof the exponential random variable determines the rate
of decay. a large Œªimplies a faster decay. the pdf of an exponential random variable is
illustrated in figure 4.20 . we show two values of Œª. note that the initial value fx(0) is
fx(0) = Œªe‚àíŒª0=Œª.
therefore, as long as Œª >1,fx(0) will exceed 1.
the cdf of an exponential random variable can be determined by
fx(x) =zx
‚àí‚àûfx(t)dt
=zx
0Œªe‚àíŒªtdt= 1‚àíe‚àíŒªx, x ‚â•0.
therefore, if we consider the entire real line, the cdf is
fx(x) =(
0, x < 0,
1‚àíe‚àíŒªx, x ‚â•0.
the corresponding cdfs for the pdfs shown in figure 4.20 (a) are shown in fig-
ure 4.20 (b). for larger Œª, the pdf fx(x) decays faster but the cdf fx(x) increases faster.
205chapter 4. continuous random variables
0 0.2 0.4 0.6 0.8 10123456
 = 2
 = 5
0 0.2 0.4 0.6 0.8 100.20.40.60.811.2
 = 2
 = 5
(a) pdf (b) cdf
figure 4.20: (a) the pdf and (c) the cdf of x‚àºexponential (Œª).
in matlab, the code used to generate figure 4.20 (a) is shown below. there are
multiple ways of doing this. an alternative way is to call exppdf , which will return the same
result. in python, the corresponding command is stats.expon.pdf . note that in python
the parameter Œªis specified in scale option.
% matlab code to plot the exponential pdf
lambda1 = 1/2; lambda2 = 1/5;
x = linspace(0,1,1000);
f1 = pdf(‚Äôexp‚Äô,x, lambda1);
f2 = pdf(‚Äôexp‚Äô,x, lambda2);
plot(x, f1, ‚Äôlinewidth‚Äô, 4, ‚Äôcolor‚Äô, [0 0.2 0.8]); hold on;
plot(x, f2, ‚Äôlinewidth‚Äô, 4, ‚Äôcolor‚Äô, [0.8 0.2 0]);
# python code to plot the exponential pdf
lambd1 = 1/2
lambd2 = 1/5
x = np.linspace(0,1,1000)
f1 = stats.expon.pdf(x,scale=lambd1)
f2 = stats.expon.pdf(x,scale=lambd2)
plt.plot(x, f1)
plt.plot(x, f2)
to plot the cdf, we replace pdfbycdf. similarly, in python we replace expon.pdf
byexpon.cdf .
% matlab code to plot the exponential cdf
f = cdf(‚Äôexp‚Äô,x, lambda1);
plot(x, f, ‚Äôlinewidth‚Äô, 4, ‚Äôcolor‚Äô, [0 0.2 0.8]);
# python code to plot the exponential cdf
f = stats.expon.cdf(x,scale=lambd1)
plt.plot(x, f)
2064.5. uniform and exponential random variables
theorem 4.9. ifx‚àºexponential( Œª), then
e[x] =1
Œªand var[x] =1
Œª2. (4.24)
proof . we have discussed this proof before. here is a recap for completeness:
e[x] =z‚àû
‚àí‚àûxfx(x)dx=z‚àû
0Œªxe‚àíŒªxdx
=‚àíz‚àû
0xde‚àíŒªx
=‚àíxe‚àíŒªx‚àû
0+z‚àû
0e‚àíŒªxdx=1
Œª,
e[x2] =z‚àû
‚àí‚àûx2fx(x)dx=z‚àû
0Œªx2e‚àíŒªxdx
=‚àíz‚àû
0x2de‚àíŒªx
=‚àíx2e‚àíŒªx‚àû
0+z‚àû
02xe‚àíŒªxdx
= 0 +2
Œªe[x] =2
Œª2.
thus, var[ x] =e[x2]‚àíe[x]2=1
Œª2.
‚ñ°
computing the mean and variance of an exponential random variable in matlab and
python follows the similar procedures that we described above.
4.5.3 origin of exponential random variables
exponential random variables are closely related to poisson random variables. recall that
the definition of a poisson random variable is a random variable that describes the number
of events that happen in a certain period, e.g., photon arrivals, number of pedestrians, phone
calls, etc. we summarize the origin of an exponential random variable as follows.
what is the origin of exponential random variables?
¬àan exponential random variable is the interarrival time between two consecutive
poisson events.
¬àthat is, an exponential random variable is how much time it takes to go from n
poisson counts to n+ 1 poisson counts.
an example will clarify this concept. imagine that you are waiting for a bus, as illus-
trated in figure 4.21 . passengers arrive at the bus stop with an arrival rate Œªper unit time.
thus, for some time t, the average number of people that arrive is Œªt. let nbe a random
207chapter 4. continuous random variables
variable denoting the number of people. we assume that nis poisson with a parameter Œªt.
that is, for any duration t, the probability of observing npeople follows the pmf
p[n=n] =(Œªt)n
n!e‚àíŒªt.
figure 4.21: for any fixed period of time t, the number of people nis modeled as a poisson random
variable with a parameter Œªt.
figure 4.22: the interarrival time tbetween two consecutive poisson events is an exponential random
variable.
lettbe the interarrival time between two people, by which we mean the time between
two consecutive arrivals, as shown in figure 4.22 . note that tis a random variable because
tdepends on n, which is itself a random variable. to find the pdf of t, we first find the
cdf of t. we note that
p[t > t ](a)=p[interarrival time > t]
(b)=p[no arrival in t](c)=p[n= 0] =(Œªt)0
0!e‚àíŒªt=e‚àíŒªt.
in this set of arguments, (a) holds because tis the interarrival time, and (b) holds be-
cause interarrival time is between two consecutive arrivals. if the interarrival time is larger
than t, there is no arrival during the period. equality (c) holds because nis the number of
passengers.
sincep[t > t ] = 1‚àíft(t), where ft(t) is the cdf of t, we can show that
ft(t) = 1‚àíe‚àíŒªt,
ft(t) =d
dtft(t) =Œªe‚àíŒªt.
therefore, the interarrival time tfollows an exponential distribution.
since exponential random variables are tightly connected to poisson random variables,
we should expect them to be useful for modeling temporal events. we discuss two examples.
2084.5. uniform and exponential random variables
4.5.4 applications of exponential random variables
example 4.22 . (photon arrivals ) single-photon image sensors are designed to op-
erate in the photon-limited regime. the number-one goal of using these sensors is to
count the number of arriving photons precisely. however, for some applications not
all single-photon image sensors are used to count photons. some are used to measure
the time between two photon arrivals, such as time-of-flight systems. in this case, we
are interested in measuring the time it takes for a pulse to bounce back to the sensor.
the more time it takes for a pulse to come back, the greater the distance between the
object and the sensor. other applications utilize the time information. for example,
high-dynamic-range imaging can be achieved by recording the time between two pho-
ton arrivals because brighter regions have a higher poisson rate Œªand darker regions
have a lower Œª.
the figure above illustrates an example of high-dynamic-range imaging. when the
scene is bright, the large Œªwill generate more photons. therefore, the interarrival time
between the consecutive photons will be relatively short. if we plot the histogram of
the interarrival time, we observe that most of the interarrival time will be concentrated
at small values. dark regions behave in the opposite manner. the interarrival time will
typically be much longer. in addition, because there is more variation in the photon
arrival times, the histogram will look shorter and wider. nevertheless, both cases are
modeled by the exponential random variable.
example 4.23 . (energy-efficient escalator ) many airports today have installed variable-
speed escalators. these escalators change their speeds according to the traffic. if there
are no passengers for more than a certain period (say, 60 seconds), the escalator will
switch from the full-speed mode to the low-speed mode. for moderately busy esca-
lators, the variable-speed configuration can save energy. the interesting data-science
problem is to determine, given a traffic pattern, e.g., the one shown in figure 4.23 ,
whether we can predict the amount of energy savings?
we will not dive into the details of this problem, but we can briefly discuss the
principle. consider a fixed arrival rate Œª(say, the average from 07:00 to 08:00). the in-
terarrival time, according to our discussion above, follows an exponential distribution.
209chapter 4. continuous random variables
so we know that
ft(t) =Œªe‚àíŒªt.
suppose that the escalator switches to low-speed mode when the interarrival time
exceeds œÑ. then we can define a new variable yto denote the amount of time that
the escalator will operate in the low-speed mode. this new variable is
y=(
t‚àíœÑ, t > œÑ,
0, t ‚â§œÑ.
in other words, if the interarrival time tis more than œÑ, then the amount of time
saved y takes the value t‚àíœÑ, but if the interarrival time is less than œÑ, then there is
no saving.
figure 4.23: the variable-speed escalator problem. [left] we model the passengers as independent
poisson arrivals. thus, the interarrival time is exponential. [right] a hypothetical passenger arrival
rate (number of people per minute), from 06:00 to 23:00.
figure 4.24: the escalator problem requires modeling the cutoff threshold œÑsuch that if t > œÑ ,
the savings are y=t‚àíœÑ. ift < œÑ , then y= 0. the left-hand side of the figure shows how the
pdf of yis constructed.
the pdf of ycan be computed according to figure 4.24 . there are two parts
to the calculation. when y= 0, there is a probability mass such that
fy(0) =p[y= 0] =zœÑ
0ft(t)dt=zœÑ
0Œªe‚àíŒªtdt= 1‚àíe‚àíŒªœÑ.
for other values of y, we can show that
fy(y) =ft(y+œÑ) =Œªe‚àíŒª(y+œÑ).
therefore, to summarize, we can show that the pdf of yis
fy(y) =(
(1‚àíe‚àíŒªœÑ)Œ¥(y), y = 0,
Œªe‚àíŒª(y+œÑ), y > 0.
2104.6. gaussian random variables
consequently, we can compute e[y] and var[ y] and analyze how these values change
forŒª(which itself changes with the time of day). furthermore, we can analyze the
amount of savings in terms of dollars. we leave these problems as an exercise.
closing remark . the photon arrival problem and the escalator problem are two of many
examples we can find in which exponential random variables are useful for modeling a
problem. we did not go into the details of the problems because each of them requires some
additional modeling to address the real practical problem. we encourage you to explore these
problems further. our message is simple: many problems can be modeled by exponential
random variables, most of which are associated with time.
4.6 gaussian random variables
we now discuss themost important continuous random variable ‚Äî the gaussian random
variable (also known as the normal random variable ). we call it the most important random
variable because it is widely used in almost all scientific disciplines. many of us have used
gaussian random variables before, and perhaps its bell shape is the first lesson we learn in
statistics. however, there are many mysteries about gaussian random variables which you
may have missed, such as: where does the gaussian random variable come from? why does
it take a bell shape? what are the properties of a gaussian random variable? the objective
of this section is to explain everything you need to know about a gaussian random variable.
4.6.1 definition of a gaussian random variable
definition 4.13. agaussian random variable is a random variable xsuch that its
pdf is
fx(x) =1‚àö
2œÄœÉ2exp
‚àí(x‚àí¬µ)2
2œÉ2
, (4.25)
where (¬µ, œÉ2)are parameters of the distribution. we write
x‚àºgaussian (¬µ, œÉ2) or x‚àº n(¬µ, œÉ2)
to say that xis drawn from a gaussian distribution of parameter (¬µ, œÉ2).
gaussian random variables have two parameters ( ¬µ, œÉ2). it is noteworthy that the mean
is¬µand the variance is œÉ2‚Äî these two parameters are exactly the first moment and the
second central moment of the random variable. most other random variables do not have
this property.
note that a gaussian random variable is positive from ‚àí‚àû to‚àû. thus, fx(x) has
a non-zero value for any x, even though the value may be extremely small. a gaussian
random variable is also symmetric about ¬µ. if¬µ= 0, then fx(x) is an even function.
the shape of the gaussian is illustrated in figure 4.25 . when we fix the variance and
change the mean, the pdf of the gaussian moves left or right depending on the sign of the
mean. when we fix the mean and change the variance, the pdf of the gaussian changes
211chapter 4. continuous random variables
its width. since any pdf should integrate to unity, a wider gaussian means that the pdf
is shorter. note also that if œÉis very small, it is possible that fx(x)>1 although the
integration over œâ will still be 1.
-10 -5 0 5 1000.10.20.30.40.5
 = -3
 = -0.3
 = 0
 = 1.2
 = 4
-10 -5 0 5 1000.10.20.30.40.5
 = 0.8
 = 1
 = 2
 = 3
 = 4
¬µchanges, œÉ= 1 ¬µ= 0,œÉchanges
figure 4.25: a gaussian random variable with different ¬µandœÉ.
on a computer, plotting the gaussian pdf can be done by calling the function
pdf(‚Äônorm‚Äô,x) in matlab, and stats.norm.pdf in python.
% matlab to generate a gaussian pdf
x = linspace(-10,10,1000);
mu = 0; sigma = 1;
f = pdf(‚Äônorm‚Äô,x,mu,sigma);
plot(x, f);
# python to generate a gaussian pdf
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
x = np.linspace(-10,10,1000)
mu = 0; sigma = 1;
f = stats.norm.pdf(x,mu,sigma)
plt.plot(x,f)
our next result concerns the mean and variance of a gaussian random variable. you
may wonder why we need this theorem when we already know that ¬µis the mean and œÉ2is
the variance. the answer is that we have not proven these two facts.
theorem 4.10. ifx‚àºgaussian (¬µ, œÉ2), then
e[x] =¬µ,and var[x] =œÉ2. (4.26)
2124.6. gaussian random variables
proof . the expectation can be derived via substitution:
e[x] =1‚àö
2œÄœÉ2z‚àû
‚àí‚àûxe‚àí(x‚àí¬µ)2
2œÉ2dx
(a)=1‚àö
2œÄœÉ2z‚àû
‚àí‚àû(y+¬µ)e‚àíy2
2œÉ2dy
=1‚àö
2œÄœÉ2z‚àû
‚àí‚àûye‚àíy2
2œÉ2dy+1‚àö
2œÄœÉ2z‚àû
‚àí‚àû¬µe‚àíy2
2œÉ2dy
(b)= 0 + ¬µ1‚àö
2œÄœÉ2z‚àû
‚àí‚àûe‚àíy2
2œÉ2dy
(c)=¬µ,
where in (a) we substitute y=x‚àí¬µ, in (b) we use the fact that the first integrand is odd
so that the integration is 0, and in (c) we observe that integration over the entire sample
space of the pdf yields 1.
the variance is also derived by substitution.
var[x] =1‚àö
2œÄœÉ2z‚àû
‚àí‚àû(x‚àí¬µ)2e‚àí(x‚àí¬µ)2
2œÉ2dx
(a)=œÉ2
‚àö
2œÄz‚àû
‚àí‚àûy2e‚àíy2
2dy
=œÉ2
‚àö
2œÄ
‚àíye‚àíy2
2‚àû
‚àí‚àû
+œÉ2
‚àö
2œÄz‚àû
‚àí‚àûe‚àíy2
2dy
= 0 + œÉ21‚àö
2œÄz‚àû
‚àí‚àûe‚àíy2
2dy
=œÉ2,
where in (a) we substitute y= (x‚àí¬µ)/œÉ.
4.6.2 standard gaussian
we need to evaluate the probability p[a‚â§x‚â§b] of a gaussian random variable xin many
practical situations. this involves the integration of the gaussian pdf, i.e., determining the
cdf. unfortunately, there is no closed-form expression of p[a‚â§x‚â§b] in terms of ( ¬µ, œÉ2).
this leads to what we call the standard gaussian.
definition 4.14. thestandard gaussian (or standard normal) random variable x
has a pdf
fx(x) =1‚àö
2œÄe‚àíx2
2. (4.27)
that is, x‚àº n(0,1)is a gaussian with ¬µ= 0andœÉ2= 1.
the cdf of the standard gaussian can be determined by integrating the pdf. we have a
special notation for this cdf. figure 4.26 illustrates the idea.
213chapter 4. continuous random variables
definition 4.15. thecdf of the standard gaussian is defined as the œÜ(¬∑)function
œÜ(x)def=fx(x) =1‚àö
2œÄzx
‚àí‚àûe‚àít2
2dt. (4.28)
figure 4.26: definition of the cdf of the standard gaussian œÜ(x).
% matlab code to generate standard gaussian pdf and cdf
x = linspace(-5,5,1000);
f = normpdf(x,0,1);
f = normcdf(x,0,1);
figure; plot(x, f);
figure; plot(x, f);
# python code to generate standard gaussian pdf and cdf
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
x = np.linspace(-10,10,1000)
f = stats.norm.pdf(x)
f = stats.norm.cdf(x)
plt.plot(x,f); plt.show()
plt.plot(x,f); plt.show()
the standard gaussian‚Äôs cdf is related to a so-called error function defined as
erf(x) =2‚àöœÄzx
0e‚àít2dt. (4.29)
it is easy to link œÜ( x) with erf( x):
œÜ(x) =1
2
1 + erfx‚àö
2
, and erf( x) = 2œÜ( x‚àö
2)‚àí1.
with the standard gaussian cdf, we can define the cdf of an arbitrary gaussian.
2144.6. gaussian random variables
theorem 4.11 (cdf of an arbitrary gaussian ).letx‚àº n(¬µ, œÉ2). then
fx(x) = œÜx‚àí¬µ
œÉ
. (4.30)
proof . we start by expressing fx(x):
fx(x) =p[x‚â§x]
=zx
‚àí‚àû1‚àö
2œÄœÉ2e‚àí(t‚àí¬µ)2
2œÉ2dt.
substituting y=t‚àí¬µ
œÉ, and using the definition of standard gaussian, we have
zx
‚àí‚àû1‚àö
2œÄœÉ2e‚àí(t‚àí¬µ)2
2œÉ2dt=zx‚àí¬µ
œÉ
‚àí‚àû1‚àö
2œÄe‚àíy2
2dy
= œÜx‚àí¬µ
œÉ
.‚ñ°
if you would like to verify this on a computer, you can try the following code.
% matlab code to verify standardized gaussian
x = linspace(-5,5,1000);
mu = 3; sigma = 2;
f1 = normpdf((x-mu)/sigma,0,1); % standardized
f2 = normpdf(x, mu, sigma); % raw
# python code to verify standardized gaussian
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
x = np.linspace(-5,5,1000)
mu = 3; sigma = 2;
f1 = stats.norm.pdf((x-mu)/sigma,0,1) # standardized
f2 = stats.norm.cdf(x,mu,sigma) # raw
an immediate consequence of this result is that
p[a < x ‚â§b] = œÜb‚àí¬µ
œÉ
‚àíœÜa‚àí¬µ
œÉ
. (4.31)
to see this, note that
p[a < x ‚â§b] =p[x‚â§b]‚àíp[x‚â§a]
= œÜb‚àí¬µ
œÉ
‚àíœÜa‚àí¬µ
œÉ
.
the inequality signs of the two end points are not important. that is, the statement also
holds for p[a‚â§x‚â§b] orp[a < x < b ], because xis a continuous random variable at
every x. thus, p[x=a] =p[x=b] = 0 for any aandb. besides this, œÜ has several
properties of interest. see if you can prove these:
215chapter 4. continuous random variables
corollary 4.1. letx‚àº n(¬µ, œÉ2). then the following results hold:
¬àœÜ(y) = 1‚àíœÜ(‚àíy).
¬àp[x‚â•b] = 1‚àíœÜ
b‚àí¬µ
œÉ
.
¬àp[|x| ‚â•b] = 1‚àíœÜ
b‚àí¬µ
œÉ
+ œÜ
‚àíb‚àí¬µ
œÉ
.
4.6.3 skewness and kurtosis
in modern data analysis we are sometimes interested in high-order moments. here we con-
sider two useful quantities: skewness andkurtosis .
definition 4.16. for a random variable xwith pdf fx(x), define the following
central moments as
mean =e[x]def=¬µ,
variance =eh
(x‚àí¬µ)2idef=œÉ2,
skewness =e"x‚àí¬µ
œÉ3#
def=Œ≥,
kurtosis =e"x‚àí¬µ
œÉ4#
def=Œ∫, excess kurtosisdef=Œ∫‚àí3.
as you can see from the definitions above, skewness is the third central moment,
whereas kurtosis is the fourth central moment. both skewness and kurtosis can be regarded
as ‚Äúdeviations‚Äù from a standard gaussian ‚Äînot in terms of mean and variance but in terms
of shape.
skewness measures the asymmetry of the distribution. figure 4.27 shows three differ-
ent distributions: one with left skewness, one with right skewness, and one symmetric. the
skewness of a curve is
¬àskewed towards left: positive
¬àskewed towards right: negative
¬àsymmetric: zero
what is skewness?
¬àe
x‚àí¬µ
œÉ3
.
¬àmeasures the asymmetry of the distribution.
¬àgaussian has skewness 0.
2164.6. gaussian random variables
0 5 10 15 2000.10.20.30.4
positive skewness
symmetric
negative skewness
figure 4.27: skewness of a distribution measures the asymmetry of the distribution. in this example
the skewnesses are: orange = 0.8943, black = 0, blue = -1.414.
kurtosis measures how heavy-tailed the distribution is. there are two forms of kurtosis:
one is the standard kurtosis, which is the fourth central moment, and the other is the excess
kurtosis, which is Œ∫excess =Œ∫‚àí3. the constant 3 comes from the kurtosis of a standard
gaussian. excess kurtosis is more widely used in data analysis. the interpretation of kurtosis
is the comparison to a gaussian. if the kurtosis is positive, the distribution has a tail that
decays faster than a gaussian. if the kurtosis is negative, the distribution has a tail that
decays more slowly than a gaussian. figure 4.28 illustrates the (excess) kurtosis of three
different distributions.
-5 -4 -3 -2 -1 0 1 2 3 4 500.20.40.60.81
kurtosis > 0
kurtosis = 0
kurtosis < 0
figure 4.28: kurtosis of a distribution measures how heavy-tailed the distribution is. in this example,
the (excess) kurtoses are: orange = 2.8567, black = 0, blue = ‚àí0.1242.
what is kurtosis?
¬àŒ∫=e
x‚àí¬µ
œÉ4
.
¬àmeasures how heavy-tailed the distribution is. gaussian has kurtosis 3.
¬àsome statisticians prefer excess kurtosis Œ∫‚àí3, so that gaussian has excess
kurtosis 0.
217chapter 4. continuous random variables
random variable mean variance skewness excess kurtosis
¬µ œÉ2Œ≥ Œ∫ ‚àí3
bernoulli p p(1‚àíp)1‚àí2p‚àö
p(1‚àíp)1
1‚àíp+1
p‚àí6
binomial np np (1‚àíp)1‚àí2p‚àö
np(1‚àíp)6p2‚àí6p+1
np(1‚àíp)
geometric1
p1‚àíp
p22‚àíp‚àö1‚àípp2‚àí6p+6
1‚àíp
poisson Œª Œª1‚àö
Œª1
Œª
uniforma+b
2(b‚àía)2
120 ‚àí6
5
exponential1
Œª1
Œª2 2 6
gaussian ¬µ œÉ20 0
table 4.1: the first few moments of commonly used random variables.
on a computer, computing the empirical skewness and kurtosis is done by built-in
commands. their implementations are based on the finite-sample calculations
Œ≥‚âà1
nnx
n=1xn‚àí¬µ
œÉ3
,
Œ∫‚âà1
nnx
n=1xn‚àí¬µ
œÉ4
.
the matlab and python built-in commands are shown below, using a gamma distribution
as an example.
% matlab code to compute skewness and kurtosis
x = random(‚Äôgamma‚Äô,3,5,[10000,1]);
s = skewness(x);
k = kurtosis(x);
# python code to compute skewness and kurtosis
import scipy.stats as stats
x = stats.gamma.rvs(3,5,size=10000)
s = stats.skew(x)
k = stats.kurtosis(x)
example 4.24 . to further illustrate the behavior of skewness and kurtosis, we consider
an example using the gamma random variable x. the pdf of xis given by the
equation
fx(x) =1
Œ≥(k)Œ∏kxk‚àí1e‚àíx
Œ∏, (4.32)
where Œ≥( ¬∑) is known as the gamma function. if kis an integer, the gamma function is
2184.6. gaussian random variables
just the factorial: Œ≥( k) = (k‚àí1)!. a gamma random variable is parametrized by two
parameters ( k, Œ∏). as kincreases or decreases, the shape of the pdf will change. for
example, when k= 1, the distribution is simplified to an exponential distribution.
without going through the (tedious) integration, we can show that the skewness
and the (excess) kurtosis of gamma( k, Œ∏) are
skewness =2‚àö
k,
(excess) kurtosis =6
k.
as we can see from these results, the skewness and kurtosis diminish as kgrows. this
can be confirmed from the pdf of gamma( k, Œ∏) as shown in figure 4.29 .
0 5 10 15 20 25 3000.10.20.30.4
k = 2
k = 5
k = 10
k = 15
k = 20
figure 4.29: the pdf of a gamma distribution gamma (k, Œ∏), where Œ∏= 1. the skewness and
the kurtosis are decaying to zero.
example 4.25 . let us look at a real example. on april 15, 1912, rms titanic sank
after hitting an iceberg. the disaster killed 1502 out of 2224 passengers and crew. a
hundred years later, we want to analyze the data. at https://www.kaggle.com/c/
titanic/ there is a dataset collecting the identities, age, gender, etc., of the passengers.
we partition the dataset into two: one for those who died and the other one for those
who survived. we plot the histograms of the ages of the two groups and compute
several statistics of the dataset. figure 4.30 shows the two datasets.
0 20 40 60 80
age010203040
0 20 40 60 80
age010203040
group 1 (died) group 2 (survived)
219chapter 4. continuous random variables
figure 4.30: the titanic dataset https://www.kaggle.com/c/titanic/ .
statistics group 1 (died) group 2 (survived)
mean 30.6262 28.3437
standard deviation 14.1721 14.9510
skewness 0.5835 0.1795
excess kurtosis 0.2652 ‚àí0.0772
note that the two groups of people have very similar means and standard devia-
tions. in other words, if we only compare the mean and standard deviation, it is nearly
impossible to differentiate the two groups. however, the skewness and kurtosis provide
more information related to the shape of the histograms. for example, group 1 has
more positive skewness, whereas group 2 is almost symmetrical. one interpretation is
that more young people offered lifeboats to children and older people. the kurtosis of
group 1 is slightly positive, whereas that of group 2 is slightly negative. therefore,
high-order moments can sometimes be useful for data analysis.
4.6.4 origin of gaussian random variables
the gaussian random variable has a long history. here, we provide one perspective on why
gaussian random variables are so useful. we give some intuitive arguments but leave the
formal mathematical treatment for later when we introduce the central limit theorem.
let‚Äôs begin with a numerical experiment. consider throwing a fair die. we know that
this will give us a (discrete) uniform random variable x. if we repeat the experiment many
times we can plot the histogram, and it will return us a plot of 6 impulses with equal height,
as shown in figure 4.31 (a).
now, suppose we throw two dice. call them x1andx2, and let z=x1+x2, i.e.,
the sum of two dice. we want to find the distribution of z. to do so, we first list out all
the possible outcomes in the sample space; this gives us {(1,1),(1,2), . . . , (6,6)}. we then
sum the numbers, which gives us a list of states of z:{2,3,4, . . . , 12}. the probability of
getting these states is shown in figure 4.31 (b), which has a triangular shape. the triangular
shape makes sense because to get the state ‚Äú2‚Äù, we must have the pair (1 ,1), which is quite
unlikely. however, if we want to get the state 7, it would be much easier to get a pair, e.g.,
(6,1),(5,2),(4,3),(3,4),(2,5),(1,6) would all do the job.
now, what will happen if we throw 5 dice and consider z=x1+x2+¬∑¬∑¬∑+x5? it turns
out that the distribution will continue to evolve and give something like figure 4.31 (c).
this is starting to approximate a bell shape. finally, if we throw 100 dice and consider
z=x1+x2+¬∑¬∑¬∑+x100, the distribution will look like figure 4.31 (d). the shape is
becoming a gaussian! this numerical example demonstrates a fascinating phenomenon: as
we sum more random variables, the distribution of the sum will eventually converge to a
gaussian.
if you are curious about how we plot the above figures, the following matlab and
python code can be useful.
% matlab code to show the histogram of z = x1+x2+x3
n = 10000;
x1 = randi(6,1,n);
x2 = randi(6,1,n);
2204.6. gaussian random variables
(a)x1 (b)x1+x2 (c)x1+¬∑¬∑¬∑+x5 (d)x1+¬∑¬∑¬∑+x100
figure 4.31: when adding uniform random variables, the overall distribution approaches a gaussian as
the number of summed variables increase.
x3 = randi(6,1,n);
z = x1 + x2 + x3;
histogram(z, 2.5:18.5);
# python code to show the histogram of z = x1+x2+x3
import numpy as np
import matplotlib.pyplot as plt
n = 10000
x1 = np.random.randint(1,6,size=n)
x2 = np.random.randint(1,6,size=n)
x3 = np.random.randint(1,6,size=n)
z = x1 + x2 + x3
plt.hist(z,bins=np.arange(2.5,18.5))
can we provide a more formal description of this? yes, but we need some new mathe-
matical tools that we have not yet developed. so, for the time being, we will outline the flow
of the arguments and leave the technical details to a later chapter. suppose we have two
independent random variables with identical distributions, e.g., x1andx2, where both are
uniform. this gives us pdfs fx1(x) and fx2(x) that are two identical rectangular functions.
by what operation can we combine these two rectangular functions and create a triangle
function? the key lies in the concept of convolution . if you convolve two rectangle functions,
you will get a triangle function. here we define the convolution of fxas
(fx‚àófx)(x) =z‚àû
‚àí‚àûfx(œÑ)fx(x‚àíœÑ)dœÑ.
in fact, for any pair of random variables x1andx2(not necessarily uniform random vari-
ables), the sum z=x1+x2will have a pdf given by the convolution of the two pdfs. we
have not yet proven this, but if you trust what we are saying, we can effectively generalize
this argument to many random variables. if we have nrandom variables, then the sum
z=x1+x2+¬∑¬∑¬∑+xnwill have a pdf that is the result of nconvolutions of all the
individual pdfs.
what is the pdf of x+y?
¬àsumming x+yis equivalent to convolving the pdfs fx‚àófy.
221chapter 4. continuous random variables
¬àif you sum many random variables, you convolve all their pdfs.
how do we analyze these convolutions? we need a second set of tools related to fourier
transforms. the fourier transform of a pdf is known as the characteristic function , which
we will discuss later, but the name is not important now. what matters is the important
property of the fourier transform, that a convolution in the original space is multiplication
in the fourier space. that is,
f {(fx‚àófx‚àó ¬∑¬∑¬∑ ‚àó fx)}=f{fx} ¬∑ f{ fx} ¬∑ ¬∑¬∑¬∑ ¬∑ f{ fx}.
multiplication in the fourier space is much easier to analyze. in particular, for independent
and identically distributed random variables, the multiplication will easily translate to ad-
dition in the exponent. then, by truncating the exponent to the second order, we can show
that the limiting object in the fourier space is approaching a gaussian. finally, since the
inverse fourier transform of a gaussian remains a gaussian, we have shown that the infinite
convolution will give us a gaussian.
here is some numerical evidence for what we have just described. recall that the
fourier transform of a rectangle function is the sinc function. therefore, if we have an
infinite convolution of rectangular functions, equivalently, we have an infinite product of sinc
functions in the fourier space. multiplying sinc functions is reasonably easy. see figure 4.32
for the first three sincs. it is evident that with just three sinc functions, the shape closely
approximates a gaussian.
-10 -8 -6 -4 -2 0 2 4 6 8 10-0.5-0.2500.250.50.7511.25
(sin x)/x
(sin x)2/x2
(sin x)3/x3
figure 4.32: convolving the pdf of a uniform distribution is equivalent to multiplying their fourier
transforms in the fourier space. as the number of convolutions grows, the product is gradually becoming
gaussian.
how about distributions that are not rectangular? we invite you to numerically visu-
alize the effect when you convolve the function many times. you will see that as the number
of convolutions grows, the resulting function will become more and more like a gaussian.
regardless of what the input random variables are, as long as you add them, the sum will
have a distribution that looks like a gaussian:
x1+x2+¬∑¬∑¬∑+xn‚áùgaussian .
we use the notation ‚áùto emphasize that the convergence is not the usual form of conver-
gence. we will make this precise later.
2224.7. functions of random variables
the implication of this line of discussion is important. regardless of the underlying
true physical process, if we are only interested in the sum (or average), the distribution
will be more or less gaussian. in most engineering problems, we are looking at the sum
or average. for example, when generating an image using an image sensor, the sensor will
add a certain amount of read noise. read noise is caused by the random fluctuation of the
electrons in the transistors due to thermal distortions. for high-photon-flux situations, we
are typically interested in the average read noise rather than the electron-level read noise.
thus gaussian random variables become a reasonable model for that. in other applications,
such as imaging through a turbulent medium, the random phase distortions (which alter
the phase of the wavefront) can also be modeled as a gaussian random variable. here is the
summary of the origin of a gaussian random variable:
what is the origin of gaussian?
¬àwhen we sum many independent random variables, the resulting random vari-
able is a gaussian.
¬àthis is known as the central limit theorem . the theorem applies to anyran-
dom variable.
¬àsumming random variables is equivalent to convolving the pdfs. convolving
pdfs infinitely many times yields the bell shape.
4.7 functions of random variables
one common question we encounter in practice is the transformation of random variables.
the question can be summarized as follows: given a random variable xwith pdf fx(x)
and cdf fx(x), and supposing that y=g(x) for some function g, what are fy(y) and
fy(y)? this is a prevalent question. for example, we measure the voltage v, and we want
to analyze the power p=v2/r. this involves taking the square of a random variable.
another example: we know the distribution of the phase Œ∏, but we want to analyze the
signal cos( œât+ Œ∏). this involves a cosine transformation. how do we convert one variable
to another? answering this question is the goal of this section.
4.7.1 general principle
we will first outline the general principle for tackling this type of problem. in the following
subsection, we will give a few concrete examples.
suppose we are given a random variable xwith pdf fx(x) and cdf fx(x). let y=
g(x) for some known and fixed function g. for simplicity, we assume that gis monotonically
223chapter 4. continuous random variables
increasing. in this case, the cdf of ycan be determined as follows.
fy(y)(a)=p[y‚â§y](b)=p[g(x)‚â§y]
(c)=p[x‚â§g‚àí1(y)]
(d)=fx(g‚àí1(y)).
this sequence of steps is not difficult to understand. step (a) is the definition of cdf. step
(b) substitutes g(x) for y. step (c) uses the fact that since gis invertible, we can apply
the inverse of gto both sides of g(x)‚â§yto yield x‚â§g‚àí1(y). step (d) is the definition of
the cdf, but this time applied to p[x‚â§ ‚ô£] =fx(‚ô£), for some ‚ô£.
it will be useful to visualize the situation in figure 4.33 . here, we consider a uniformly
distributed xso that the cdf fx(x) is a straight line. according to fx, any samples
drawn according to fxare equally likely, as illustrated by the yellow dots on the x-axis.
as we transform the x‚Äôs through y=g(x), we increase/decrease the spacing between
two samples. therefore, some samples become more concentrated while some become less
concentrated. the distribution of these transformed samples (the yellow dots on the y-axis)
forms a new cdf fy(y). the result fy(y) =fx(g‚àí1(y)) holds when we look at y. the
samples are traveling with g‚àí1in order to go back to fx. therefore, we need g‚àí1in the
formula.
figure 4.33: when transforming a random variable xtoy=g(x), the distributions are defined
according to the spacing between samples. in this figure, a uniformly distributed xwill become squeezed
by some parts of gand widened in other parts of g.
why should we use the cdf and not the pdf in figure 4.33 ? the advantage of the
cdf is that it is an increasing function. therefore, no matter what the function gis, the
input and the output functions will still be increasing. if we use the pdf, then the non-
monotonic behavior of the pdf will interact with another nonlinear function g. it becomes
much harder to decouple the two.
we can carry out the integrations to determine fx(g‚àí1(y)). it can be shown that
fx(g‚àí1(y)) =zg‚àí1(y)
‚àí‚àûfx(x‚Ä≤)dx‚Ä≤, (4.33)
2244.7. functions of random variables
and hence, by the fundamental theorem of calculus, we have
fy(y) =d
dyfy(y) =d
dyfx(g‚àí1(y)) =d
dyzg‚àí1(y)
‚àí‚àûfx(x‚Ä≤)dx‚Ä≤
=d g‚àí1(y)
dy
¬∑fx(g‚àí1(y)), (4.34)
where the last step is due to the chain rule. based on this line of reasoning we can summarize
a ‚Äúrecipe‚Äù for this problem.
how to find the pdf of y=g(x)
¬àstep 1: find the cdf fy(y), which is fy(y) =fx(g‚àí1(y)).
¬àstep 2: find the pdf fy(y), which is fy(y) =
d g‚àí1(y)
dy
¬∑fx(g‚àí1(y)).
this recipe works when gis a one-to-one mapping. if gis not one-to-one, e.g., g(x) =x2
implies g‚àí1(y) =¬±‚àöy, then we will have some issues with the above two steps. when this
happens, then instead of writing x‚â§g‚àí1(y) we need to determine the set {x|g(x)‚â§y}.
4.7.2 examples
example 4.26 . (linear transform) let xbe a random variable with pdf fx(x) and
cdf fx(x). let y= 2x+ 3. find fy(y) and fy(y). express the answers in terms of
fx(x) and fx(x).
solution . we first note that
fy(y) =p[y‚â§y]
=p[2x+ 3‚â§y]
=p
x‚â§y‚àí3
2
=fxy‚àí3
2
.
therefore, the pdf is
fy(y) =d
dyfy(y)
=d
dyfxy‚àí3
2
=f‚Ä≤
xy‚àí3
2d
dyy‚àí3
2
=1
2fxy‚àí3
2
.
follow-up . (linear transformation of a gaussian random variable).suppose xis a gaus-
sian random variable with zero mean and unit variance, and let y=ax+b. then the cdf
225chapter 4. continuous random variables
and pdf of yare respectively
fy(y) =fxy‚àíb
a
= œÜy‚àíb
a
,
fy(y) =1
afxy‚àíb
a
=1‚àö
2œÄae‚àí(y‚àíb)2
2a2.
follow-up . (linear transformation of an exponential random variable). suppose xis an
exponential random variable with parameter Œª, and let y=ax+b. then the cdf and
pdf of yare respectively
fy(y) =fxy‚àíb
a
= 1‚àíe‚àíŒª
a(y‚àíb), y ‚â•b,
fy(y) =1
afxy‚àíb
a
=Œª
ae‚àíŒª
a(y‚àíb), y ‚â•b.
example 4.27 . let xbe a random variable with pdf fx(x) and cdf fx(x). sup-
posing that y=x2, find fy(y) and fy(y). express the answers in terms of fx(x)
andfx(x).
solution . we note that
fy(y) =p[y‚â§y] =p[x2‚â§y] =p[‚àí‚àöy‚â§x‚â§‚àöy]
=fx(‚àöy)‚àífx(‚àí‚àöy).
therefore, the pdf is
fy(y) =d
dyfy(y)
=d
dy(fx(‚àöy)‚àífx(‚àí‚àöy))
=f‚Ä≤
x(‚àöy)d
dy‚àöy‚àíf‚Ä≤
x(‚àí‚àöy)d
dy(‚àí‚àöy)
=1
2‚àöy(fx(‚àöy) +fx(‚àí‚àöy)).
2264.7. functions of random variables
figure 4.34: when transforming a random variable xtoy=x2, the cdf becomes fy(y) =‚àöy‚àía
b‚àíaand the pdf becomes fy(y) =1‚àöy(b‚àía).
follow up . (square of a uniform random variable) suppose xis a uniform random variable
in [a, b] (assume a >0), and let y=x2. then the cdf and pdf of yare respectively
fy(y) =‚àöy‚àía
b‚àía, a2‚â§y‚â§b2,
fy(y) =1‚àöy(b‚àía), a2‚â§y‚â§b2.
example 4.28 . let x‚àºuniform(0 ,2œÄ). suppose y= cos x. find fy(y) and fy(y).
solution . first, we need to find the cdf of x. this can be done by noting that
fx(x) =zx
‚àí‚àûfx(x‚Ä≤)dx‚Ä≤=zx
01
2œÄdx‚Ä≤=x
2œÄ.
thus, the cdf of yis
fy(y) =p[y‚â§y] =p[cosx‚â§y]
=p[cos‚àí1y‚â§x‚â§2œÄ‚àícos‚àí1y]
=fx(2œÄ‚àícos‚àí1y)‚àífx(cos‚àí1y)
= 1‚àícos‚àí1y
œÄ.
the pdf of yis
fy(y) =d
dyfy(y) =d
dy
1‚àícos‚àí1y
œÄ
=1
œÄp
1‚àíy2,
227chapter 4. continuous random variables
where we used the fact thatd
dycos‚àí1y=‚àí1‚àö
1‚àíy2.
example 4.29 . let xbe a random variable with pdf
fx(x) =aexe‚àíaex.
lety=ex, and find fy(y).
solution . we first note that
fy(y) =p[y‚â§y] =p[ex‚â§y]
=p[x‚â§logy] =zlogy
‚àí‚àûaexe‚àíaexdx.
to find the pdf, we recall the fundamental theorem of calculus. this gives us
fy(y) =d
dyzlogy
‚àí‚àûaexe‚àíaexdx
=d
dylogy 
d
dlogyzlogy
‚àí‚àûaexe‚àíaexdx!
=1
yaelogye‚àíaelogy=ae‚àíay.
closing remark . the transformation of random variables is a fundamental technique in
data science. the approach we have presented is the most rudimentary yet the most intuitive.
the key is to visualize the transformation and how the random samples are allocated after
the transformation. note that the density of the random samples is related to the slope of
the cdf. therefore, if the transformation maps many samples to similar values, the slope
of the cdf will be steep. once you understand this picture, the transformation will be a
lot easier to understand.
is it possible to replace the paper-and-pencil derivation of a transformation with a
computer? if the objective is to transform random realizations, then the answer is yes
because your goal is to transform numbers to numbers, which can be done on a computer.
for example, transforming a sample x1to‚àöx1is straightforward on a computer. however,
if the objective is to derive the theoretical expression of the pdf, then the answer is no.
why might we want to derive the theoretical pdf? we might want to analyze the mean,
variance, or other statistical properties. we may also want to reverse-engineer and determine
a transformation that can yield a specific pdf. this would require a paper-and-pencil
derivation. in what follows, we will discuss a handy application of the transformations.
what are the rules of thumb for transformation of random variables?
¬àalways find the cdf fy(y) =p[g(x)‚â§y]. ask yourself: what are the values
ofxsuch that g(x)‚â§y? think of the cosine example.
2284.8. generating random numbers
¬àsometimes you do not need to solve for fy(y) explicitly. the fundamental the-
orem of calculus can help you find fy(y).
¬àdraw pictures. ask yourself whether you need to squeeze or stretch the samples.
4.8 generating random numbers
most scientific computing software nowadays has built-in random number generators. for
common types of random variables, e.g., gaussian or exponential, these random number
generators can easily generate numbers according to the chosen distribution. however, if we
are given an arbitrary pdf (or pmf) that is not among the list of predefined distributions,
how can we generate random numbers according to the pdf or pmf we want?
4.8.1 general principle
generating random numbers according to the desired distribution can be formulated as
an inverse problem. suppose that we can generate uniformly random numbers according
to uniform(0,1). this is a fragile assumption, and this process can be done on almost all
computers today. let us call this random variable uand its realization u. suppose that we
also have a desired distribution fx(x) (and its cdf fx(x)). we can put the two random
variables uandxon the two axes of figure 4.35 , yielding an input-output relationship.
the inverse problem is: by using what transformation g, such that x=g(u), can we make
sure that xis distributed according to fx(x) (or fx(x))?
figure 4.35: generating random numbers according to a known cdf. the idea is to first generate a
uniform(0,1) random variable, then do an inverse mapping f‚àí1
x.
theorem 4.12. the transformation gthat can turn a uniform random variable into
229chapter 4. continuous random variables
a random variable following a distribution fx(x)is given by
g(u) =f‚àí1
x(u). (4.35)
that is, if g=f‚àí1
x, then g(u)will be distributed according to fx(orfx).
proof . first, we know that if u‚àºuniform(0 ,1), then fu(u) = 1 for 0 ‚â§u‚â§1, so
fu(u) =zu
‚àí‚àûfu(u)du=u,
for 0‚â§u‚â§1. let g=f‚àí1
xand define y=g(u). then the cdf of yis
fy(y) =p[y‚â§y] =p[g(u)‚â§y]
=p[f‚àí1
x(u)‚â§y]
=p[u‚â§fx(y)] =fx(y).
therefore, we have shown that the cdf of yis the cdf of x. ‚ñ°
the theorem above states that if we want a distribution fx, then the transformation
should be g=f‚àí1
x. this suggests a two-step process for generating random numbers.
how do we generate random numbers from an arbitrary distribution fx?
¬àstep 1: generate a random number u‚àºuniform(0 ,1).
¬àstep 2: let
y=f‚àí1
x(u). (4.36)
then the distribution of yisfx.
4.8.2 examples
example 4.30 . how can we generate gaussian random numbers with mean ¬µand
variance œÉ2from uniform random numbers?
first, we generate u‚àºuniform(0 ,1). the cdf of the ideal distribution is
fx(x) = œÜx‚àí¬µ
œÉ
.
therefore, the transformation gis
g(u) =f‚àí1
x(u) =œÉœÜ‚àí1(u) +¬µ.
infigure 4.36 , we plot the cdf of fxand the transformation g.
2304.8. generating random numbers
-10 -8 -6 -4 -2 0 2 4 6 8 1000.10.20.30.40.50.60.70.80.91
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1-3-2-1012345678910-4
(a)fx(¬∑) (b) g(¬∑)
figure 4.36: to generate random numbers according to gaussian (0,1), we plot its cdf in (a)
and the transformation gin (b).
to visualize the random variables before and after the transformation, we plot
the histograms in figure 4.37 .
0 0.2 0.4 0.6 0.8 10100200300400
-5 0 5 10020040060080010001200
(a) pdf of u (b) pdf of g(u)
figure 4.37: (a) pdf of the uniform random variable. (b) the pdf of the transformed random
variable.
the matlab and python codes used to generate the histograms above are shown
below.
% matlab code to generate gaussian from uniform
mu = 3;
sigma = 2;
u = rand(10000,1);
gu = sigma*icdf(‚Äônorm‚Äô,u,0,1)+mu;
figure; hist(u);
figure; hist(gu);
# python code to generate gaussian from uniform
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
231chapter 4. continuous random variables
mu = 3
sigma = 2
u = stats.uniform.rvs(0,1,size=10000)
gu = sigma*stats.norm.ppf(u)+mu
plt.hist(u); plt.show()
plt.hist(gu); plt.show()
example 4.31 . how can we generate exponential random numbers with parameter Œª
from uniform random numbers?
first, we generate u‚àºuniform(0 ,1). the cdf of the ideal distribution is
fx(x) = 1‚àíe‚àíŒªx.
therefore, the transformation gis
g(u) =f‚àí1
x(u) =‚àí1
Œªlog(1‚àíu).
the cdf of the exponential random variable and the transformation gare shown
infigure 4.38 .
0 1 2 3 4 500.10.20.30.40.50.60.70.80.91
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 156701234
(a)fx(¬∑) (b) g(¬∑)
figure 4.38: to generate random numbers according to exponential (1), we plot its cdf in (a)
and the transformation gin (b).
the pdf of the uniform random variable uand the pdf of the transformed
variable g(u) are shown in figure 4.39 .
2324.8. generating random numbers
0 0.2 0.4 0.6 0.8 10100200300400
0 2 4 6 8 10050010001500200025003000
(a) pdf of u (b) pdf of g(u)
figure 4.39: (a) pdf of the uniform random variable. (b) the pdf of the transformed random
variable.
the matlab and python codes for this transformation are shown below.
% matlab code to generate exponential random variables
lambda = 1;
u = rand(10000,1);
gu = -(1/lambda)*log(1-u);
# python code to generate exponential random variables
import numpy as np
import scipy.stats as stats
lambd = 1;
u = stats.uniform.rvs(0,1,size=10000)
gu = -(1/lambd)*np.log(1-u)
example 4.32 . how can we generate the 4 integers 1 ,2,3,4, according to the his-
togram [0 .1 0.5 0.3 0.1], from uniform random numbers?
first, we generate u‚àºuniform(0 ,1). the cdf of the ideal distribution is
fx(x) =Ô£±
Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥0.1, x = 1,
0.1 + 0 .5 = 0 .6, x = 2,
0.1 + 0 .5 + 0 .3 = 0 .9, x = 3,
0.1 + 0 .5 + 0 .3 + 0 .1 = 1 .0, x = 4.
this cdf is not invertible. however, we can still define the ‚Äúinverse‚Äù mapping
233chapter 4. continuous random variables
as
g(u) =f‚àí1
x(u)
=Ô£±
Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥1, 0.0‚â§u‚â§0.1,
2, 0.1< u‚â§0.6,
3, 0.6< u‚â§0.9,
4, 0.9< u‚â§1.0.
for example, if 0 .1< u‚â§0.6, then on the black curve shown in figure 4.40 (a), we
are looking at the second vertical line from the left. this will go to ‚Äú2‚Äù on the x-axis.
therefore, the inversely mapped value is 2 for 0 .1< u‚â§0.6.
0 1 2 3 4 50.10.60.91
0 0.1 0.6 0.9 101234
(a)fx(¬∑) (b) g(¬∑)
figure 4.40: to generate random numbers according to a predefined histogram, we first define
the cdf in (a) and the corresponding transformation in (b).
the pdfs of the transformed variables, before and after, are shown in fig-
ure 4.41 .
0 0.2 0.4 0.6 0.8 10100200300400
0 1 2 3 4 50100020003000400050006000
(a) pdf of u (b) pdf of g(u)
figure 4.41: (a) pdf of the uniform random variable. (b) the pdf of the transformed random
variable.
in matlab, the above pdfs can be plotted using the commands below. in python,
we need to use the logical comparison np.logical_and to identify the indices. an alternative
is to use gu[((u<=0.5)*(u>=0.0)).astype(np.bool)]=1 .
2344.9. summary
% matlab code to generate the desired random variables
u = rand(10000,1);
gu = zeros(10000,1);
gu((u>=0) & (u<=0.1)) = 1;
gu((u>0.1) & (u<=0.6)) = 2;
gu((u>0.6) & (u<=0.9)) = 3;
gu((u>0.9) & (u<=1)) = 4;
# python code to generate the desired random variables
import numpy as np
import scipy.stats as stats
u = stats.uniform.rvs(0,1,size=10000)
gu = np.zeros(10000)
gu[np.logical_and(u >= 0.0, u <= 0.1)] = 1
gu[np.logical_and(u > 0.1, u <= 0.6)] = 2
gu[np.logical_and(u > 0.6, u <= 0.9)] = 3
gu[np.logical_and(u > 0.9, u <= 1)] = 4
4.9 summary
let us summarize this chapter by revisiting the four bullet points from the beginning of the
chapter.
¬àdefinition of a continuous random variable . continuous random variables are mea-
sured by lengths, areas, and volumes, which are all defined by integrations. this makes
them different from discrete random variables, which are measured by counts (and
summations). because of the different measures being used to define random variables,
we consequently have different ways of defining expectation, variance, moments, etc.,
all in terms of integrations.
¬àunification of discrete and continuous random variables . the unification is done by
the cdf. the cdf of a discrete random variable can be written as a train of step
functions. after taking the derivative, we will obtain the pdf, which is a train of
impulses.
¬àorigin of gaussian random variables . the origin of the gaussian random variable lies
in the fact that many observable events in engineering are sums of independent events.
the summation of independent random variables is equivalent to taking convolutions
of the pdfs. at the limit, they will converge to a bell-shaped function, which is the
gaussian. gaussians are everywhere because we observe sums more often than we
observe individual states.
¬àtransformation of random variables . transformation of random variables is done
in the cdf space. the transformation can be used to generate random numbers
235chapter 4. continuous random variables
according to a predefined distribution. specifically, if we want to generate random
numbers according to fx, then the transformation is g=f‚àí1
x.
4.10 reference
pdf, cdf, expectation
4-1 dimitri p. bertsekas and john n. tsitsiklis, introduction to probability , athena sci-
entific, 2nd edition, 2008. chapter 3.1, 3.2.
4-2 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. chapter 4.1 - 4.3.
4-3 athanasios papoulis and s. unnikrishna pillai, probability, random variables and
stochastic processes , mcgraw-hill, 4th edition, 2001. chapter 4.
4-4 john a. gubner, probability and random processes for electrical and computer en-
gineers , cambridge university press, 2006. chapter 4.1, 4.2, 5.1, 5.3, 5.5.
4-5 sheldon ross, a first course in probability , prentice hall, 8th edition, 2010. chapter
4.10, 5.1, 5.2, 5.3.
4-6 henry stark and john woods, probability and random processes with applications
to signal processing , prentice hall, 3rd edition, 2001. chapter 2.4, 2.5, 4.1, 4.4.
gaussian random variables
4-7 dimitri p. bertsekas and john n. tsitsiklis, introduction to probability , athena sci-
entific, 2nd edition, 2008. chapter 3.3.
4-8 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. chapter 4.4.
4-9 sheldon ross, a first course in probability , prentice hall, 8th edition, 2010. chapter
5.4.
4-10 mark d. ward and ellen gundlach, introduction to probability , w.h. freeman and
company, 2016. chapter 35.
transformation of random variables
4-11 dimitri p. bertsekas and john n. tsitsiklis, introduction to probability , athena sci-
entific, 2nd edition, 2008. chapter 4.1.
4-12 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. chapter 4.5.
4-13 athanasios papoulis and s. unnikrishna pillai, probability, random variables and
stochastic processes , mcgraw-hill, 4th edition, 2001. chapter 5.
4-14 john a. gubner, probability and random processes for electrical and computer en-
gineers , cambridge university press, 2006. chapter 5.4.
2364.11. problems
4-15 sheldon ross, a first course in probability , prentice hall, 8th edition, 2010. chapter
5.7.
4-16 henry stark and john woods, probability and random processes with applications
to signal processing , prentice hall, 3rd edition, 2001. chapter 3.1, 3.2.
advanced probability textbooks
4-17 william feller, an introduction to probability theory and its applications , wiley and
sons, 3rd edition, 1950.
4-18 andrey kolmogorov, foundations of the theory of probability , 2nd english edition,
dover 2018. (translated from russian to english. originally published in 1950 by
chelsea publishing company new york.)
4.11 problems
exercise 1. (video solution)
letxbe a gaussian random variable with ¬µ= 5 and œÉ2= 16.
(a) find p[x > 4] and p[2‚â§x‚â§7].
(b) if p[x < a ] = 0.8869, find a.
(c) ifp[x > b ] = 0.1131, find b.
(d) if p[13< x‚â§c] = 0.0011, find c.
exercise 2. (video solution)
compute e[y] ande[y2] for the following random variables:
(a)y=acos(œât+Œ∏), where a‚àº n(¬µ, œÉ2).
(b)y=acos(œât+ Œ∏), where Œ∏ ‚àºuniform(0 ,2œÄ).
(c)y=acos(œât+Œ∏), where t‚àºuniform 
‚àíœÄ
œâ,œÄ
œâ
.
exercise 3. (video solution)
consider a cdf
fx(x) =Ô£±
Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥0, ifx <‚àí1,
0.5, if‚àí1‚â§x <0,
(1 +x)/2,if 0‚â§x <1,
1, otherwise .
(a) find p[x <‚àí1],p[‚àí0.5< x < 0.5] and p[x > 0.5].
(b) find fx(x).
237chapter 4. continuous random variables
exercise 4. (video solution)
a random variable xhas cdf:
fx(x) =(
0, ifx <0,
1‚àí1
4e‚àí2x, ifx‚â•0.
(a) find p[x‚â§2],p[x= 0],p[x < 0],p[2< x < 6] and p[x > 10].
(b) find fx(x).
exercise 5. (video solution)
a random variable xhas pdf
fx(x) =(
cx(1‚àíx2), 0‚â§x‚â§1,
0, otherwise .
find c,fx(x), and e[x].
exercise 6. (video solution)
a continuous random variable xhas a cumulative distribution
fx(x) =Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥0, x < 0,
0.5 +csin2(œÄx/2), 0‚â§x‚â§1,
1, x > 1.
(a) what values can cassume?
(b) find fx(x).
exercise 7. (video solution)
a continuous random variable xis uniformly distributed in [ ‚àí2,2].
(a) let y= sin( œÄx/8). find fy(y).
(b) let z=‚àí2x2+ 3. find fz(z).
hint: compute fy(y) from fx(x), and used
dysin‚àí1y=1‚àö
1‚àíy2.
exercise 8.
lety=ex.
(a) find the cdf and pdf of yin terms of the cdf and pdf of x.
(b) find the pdf of ywhen xis a gaussian random variable. in this case, yis said to
be a lognormal random variable.
exercise 9.
the random variable xhas the pdf
fx(x) =(
1
2‚àöx, 0‚â§x‚â§1,
0, otherwise .
2384.11. problems
letybe a new random variable
y=Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥0, x < 0,‚àö
x, 0‚â§x‚â§1,
1, x > 1.
find fy(y) and fy(y), for‚àí‚àû< y < ‚àû.
exercise 10.
a random variable xhas the pdf
fx(x) =(
2xe‚àíx2, x ‚â•0,
0, x < 0.
let
y=g(x) =(
1‚àíe‚àíx2, x ‚â•0,
0, x < 0.
find the pdf of y.
exercise 11.
a random variable xhas the pdf
fx(x) =1
2e‚àí|x|,‚àí‚àû< x < ‚àû.
lety=g(x) =e‚àíx. find the pdf of y.
exercise 12.
a random variable xhas the pdf
fx(x) =1‚àö
2œÄœÉ2e‚àíx2
2œÉ2,‚àí‚àû< x < ‚àû.
find the pdf of ywhere
y=g(x) =(
x, |x|> k,
‚àíx, |x|< k.
exercise 13.
a random variable xhas the pdf
fx(x) =1
x2‚àö
2œÄe‚àíx2
2,‚àí‚àû< x < ‚àû.
lety=g(x) =1
x. find the pdf of y.
exercise 14.
a random variable xhas the cdf
fx(x) =Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥0, x < 0,
xŒ±, 0‚â§x‚â§1,
1, x > 1,
239chapter 4. continuous random variables
with Œ± >0. find the cdf of yif
y=g(x) =‚àílogx.
exercise 15.
energy efficiency is an important aspect of designing electrical systems. in some modern
buildings (e.g., airports), traditional escalators are being replaced by a new type of ‚Äúsmart‚Äù
escalator which can automatically switch between a normal operating mode and a standby
mode depending on the flow of pedestrians.
(a) the arrival of pedestrians can be modeled as a poisson random variable. let nbe the
number of arrivals, and let Œªbe the arrival rate (people per minute). for a period of
tminutes, show that the probability that there are narrivals is
p(n=n) =(Œªt)n
n!e‚àíŒªt.
(b) let tbe a random variable denoting the interarrival time (i.e., the time between two
consecutive arrivals). show that
p(t > t ) =e‚àíŒªt.
also, determine ft(t) and ft(t). sketch ft(t).
(hint: note that p(t > t ) =p(no arrival in tminutes).)
(c) suppose that the escalator will go into standby mode if there are no pedestrians for
t0= 30 seconds. let ybe a random variable denoting the amount of time that the
escalator is in standby mode. that is, let
y=(
0, ift‚â§t0,
t‚àít0, ift > t 0.
finde[y].
240chapter 5
joint distributions
when you go to a concert hall, sometimes you may want to see a solo violin concert, but other
times you may want to see a symphony. symphonies are appealing because many instruments
are playing together. random variables are similar. while single random variables are useful
for modeling simple events, we use multiple random variables to describe complex events.
the multiple random variables can be either independent or correlated. when many random
variables are present in the problem, we enter the subject of joint distribution .
what are joint distributions?
in the simplest sense, joint distributions are extensions of the pdfs and pmfs we studied
in the previous chapters. we summarize them as follows.
joint distributions are high-dimensional pdfs (or pmfs or cdfs).
what do we mean by a high-dimensional pdf? we know that a single random variable is
characterized by a 1-dimensional pdf fx(x). if we have a pair of random variables, then
we use a 2-dimensional function fx,y(x, y), and if we have a triplet of random variables,
we use a 3-dimensional function fx,y,z (x, y, z ). in general, the dimensionality of the pdf
grows as the number of variables:
fx(x)|{z}
one variable=‚áífx1,x2(x1, x2)|{z }
two variables=‚áí ¬∑¬∑¬∑ =‚áífx1,...,x n(x1, . . . , x n)| {z }
nvariables.
for busy engineers like us, fx1,...,x n(x1, . . . , x n) is not a friendly notation. a more con-
cise way to write fx1,...,x n(x1, . . . , x n) is to define a vector of random variables x=
[x1, x2, . . . , x n]twith a vector of states x= [x1, x2, . . . , x n]t, and to define the pdf as
fx(x) =fx1,...,x n(x1, . . . , x n).
under what circumstance will we encounter creatures like fx(x)? believe it or not,
these high-dimensional pdfs are everywhere . in 2010, computer-vision scientists created
the imagenet dataset, containing 14 million images with ground-truth class labels. this
enormous dataset has enabled a great blossoming of machine learning over the past several
241chapter 5. joint distributions
figure 5.1: joint distributions are ubiquitous in modern data analysis. for example, an image from a
dataset can be represented by a high-dimensional vector x. each vector has a certain probability of
being present. this probability is described by the high-dimensional joint pdf fx(x). the goal of this
chapter is to understand the properties of this fx.
0
543 50.05
2 4 1 3
y2 0 1
x-10.1
0 -2 -1-2 -3-3 -4 -4 -5 -5
-5 -4 -3 -2 -1 0 1 2 3 4 5
x00.10.20.30.40.5
-5 -4 -3 -2 -1 0 1 2 3 4 5
y00.10.20.30.40.5
figure 5.2: a 2-dimensional pdf fx,y(x, y)of a pair of random variables (x, y)and their respective
1d pdfs fx(x)andfy(y).
decades, in which many advances in deep learning have been made. fundamentally, the
imagenet dataset provides a large collection of samples drawn from a latent distribution
that is high-dimensional. each sample in the imagenet dataset is a 224 √ó224√ó3 image (the
three numbers stand for the image‚Äôs height, width, and color). if we convert this image into
a vector, then the sample will have a dimension of 224 √ó224√ó3 = 150,528. in other words,
the sample is a vector x‚ààr150528 √ó1. the probability of obtaining a particular sample x
is determined by probability density function fx(x). for example, it is more likely to get
an image containing trees than one containing a ferrari. the manifold generated by fx(x)
can be extremely complex, as illustrated in figure 5.1 .
the story of imagenet is just one of the many instances for which we use a joint
distribution fx(x). joint distributions are ubiquitous. if you do data science, you must
understand joint distributions. however, extending a 1-dimensional function fx(x) to a
2-dimensional function fx,y(x, y) and then to a n-dimensional function fx(x) is not trivial.
the goal of this chapter is to guide you through these important steps.
plan of part 1 of this chapter: two variables
this chapter is broadly divided into two halves. in the first half, we will look at a pair of
random variables .
¬àdefinition of fx,y(x, y). the first thing we need to learn is the definition of a joint
distribution with two variables. since we have two variables, the joint probability
density function (or probability mass function) is a 2-dimensional function. a point
242on this 2d function is the probability density evaluated by a pair of variables x=x
andy=y, as illustrated in figure 5.2 . however, how do we formally define this 2d
function? how is it related to the probability measure? is there a way we can retrieve
fx(x) and fy(y) from fx,y(x, y), as illustrated on the right-hand sides of figure 5.2 ?
these questions will be answered in section 5.1.
¬àjoint expectation e[xy]. when we have a pair of random variables, how should we
define the expectation? in section 5.2, we will show that the most natural way to define
the joint expectation is in terms of e[xy], i.e., the expectation of the product. there
is a surprising and beautiful connection between this ‚Äúexpectation of the product‚Äù and
the cosine angle between two vectors, thereby showing that e[xy] is the correlation
between xandy.
¬àthe reason for studying a pair of random variables is to spell out the cause-effect
relationship between the variables. this cannot be done without conditional distri-
butions; this will be explained in section 5.3. conditional distributions provide an
extremely important computational tool for decoupling complex events into simpler
events. such decomposition allows us to solve difficult joint expectation problems via
simple conditional expectations ; this subject will be covered in section 5.4.
¬àif you recall our discussions about the origin of a gaussian random variable, we claimed
that the pdf of x+yis the convolution between fxandfy. why is this so? we
will answer this question in terms of joint distributions in section 5.5.
plan of part 2 of this chapter: nvariables
the second half of the chapter focuses on the general case of nrandom variables. this
requires the definitions of a random vector x= [x1, . . . , x n]t, a joint distribution fx(x),
and the corresponding expectations e[x]. to make our discussions concrete, we will focus
on the case of high-dimensional gaussian random variables and discuss the following topics.
¬àcovariance matrices/correlation matrices . if a pair of random variables can define
the correlation through the expectation of the product e[x1x2], then for a vector of
random variables we can consider a matrix of correlations in the form
r=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞e[x1x1]e[x1x2]¬∑¬∑¬∑e[x1xn]
e[x2x1]e[x2x2]¬∑¬∑¬∑e[x2xn]
............
e[xnx1]e[xnx2]¬∑¬∑¬∑e[xnxn]Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
what are the properties of the matrix? how does it affect the shape of the high-
dimensional gaussian? if we have a dataset of vectors, how do we estimate this matrix
from the data? we will answer these questions in section 5.6 and section 5.7.
¬àprincipal-component analysis . given the covariance matrix, we can perform some
very useful data analyses, such as the principal-component analysis in section 5.8.
the question we will ask is: among the many components, which one is the principal
component? if we can find the principal component(s), we can effectively perform
dimensionality reduction by projecting a high-dimensional vector into low-dimensional
representations. we will introduce an application for face detection.
243chapter 5. joint distributions
figure 5.3: when there is a pair of random variables, we can regard the sample space as a set of
coordinates. the random variables are 2d mappings from a coordinate œâinœâx√óœâyto another
coordinate x(œâ)inr2.
5.1 joint pmf and joint pdf
probability is a measure of the size of a set. this principle applies to discrete random vari-
ables, continuous random variables, single random variables, and multiple random variables.
in situations with a pair of random variables, the measure should be applied to the coordi-
nate ( x, y) represented by the random variables xandy. consequently, when measuring
the probability, we either count these coordinates or integrate the area covered by these
coordinates. in this section, we formalize this notion of measuring 2d events.
5.1.1 probability measure in 2d
consider two random variables xandy. let the sample space of xandybe œâ xand
œây, respectively. define the cartesian product of œâ xand œâ yas œâ x√óœây={(x, y)|x‚àà
œâxandy‚ààœây}. that is, œâ x√óœâycontains all possible pairs ( x, y).
example 5.1 . if œâ x={1,2}and œâ y={4,5}, then œâ x√óœây={(1,4),(1,5),
(2,4),(2,5)}.
2445.1. joint pmf and joint pdf
example 5.2 . if œâ x= [3,4] and œâ y= [1,2], then œâ x√óœây= a rectangle with two
diagonal vertices as (3 ,1) and (4 ,2).
random variables are mappings from the sample space to the real line. if œâ‚ààœâxis
mapped to x(œâ)‚ààr, and Œæ‚ààœâyis mapped to y(Œæ)‚ààr, then a coordinate œâ= (œâ, Œæ) in
the sample space œâ x√óœâyshould be mapped to a coordinate ( x(œâ), y(Œæ)) in the 2d plane.
œâdef=œâ
Œæ
7‚àí‚Üíx(œâ)
y(Œæ)
def=x(œâ).
we denote such a vector-to-vector mapping as x(¬∑) : œâ x√óœây‚Üír√ór, as illustrated in
figure 5.3 .
therefore, if we have an event a ‚ààr2, the probability that ahappens is
p[a] =p[{œâ|x(œâ)‚àà a} ]
=pœâ
Œæx(œâ)
y(Œæ)
‚àà a
=pœâ
Œæ
‚ààx‚àí1(a)
=p[œâ‚ààx‚àí1(a)].
in other words, we take the coordinate x(œâ) and find its inverse image x‚àí1(a). the size
of this inverse image x‚àí1(a) in the sample space œâ x√óœâyis then the probability. we
summarize this general principle as follows.
how to measure probability in 2d
for a pair of random variables x= (x, y), the probability of an event ais measured
in the product space œâ x√óœâywith the size
p[{œâ|x‚àí1(a)}].
this definition is quite abstract. to make it more concrete, we will look at discrete and
continuous random variables.
5.1.2 discrete random variables
suppose that the random variables xandyare discrete. let a={x(œâ) =x, y(Œæ) =y}
be a discrete event. then the above definition tells us that the probability of ais
p[a] =p
(œâ, Œæ)x(œâ) =x,andy(Œæ) =y
=p[x=xandy=y]| {z }
def=px,y(x,y).
we define this probability as the joint probability mass function (joint pmf) px,y(x, y).
245chapter 5. joint distributions
definition 5.1. letxandybe two discrete random variables. the joint pmf ofx
andyis defined as
px,y(x, y) =p[x=xandy=y] =p
(œâ, Œæ)x(œâ) =x,andy(Œæ) =y
.(5.1)
we sometimes write the joint pmf as px,y(x, y) =p[x=x, y =y].
figure 5.4: a joint pmf for a pair of discrete random variables consists of an array of impulses. to
measure the size of the event a, we sum all the impulses inside a.
figure 5.4 shows a graphical portrayal of the joint pmf. in a nutshell, px,y(x, y)
can be considered as a 2d extension of a single variable pmf. the probabilities are still
represented by the impulses, but the domain of these impulses is now a 2d plane. if we have
an event a, then the size of the event is
p[a] =x
(x,y)‚ààapx,y(x, y).
example 5.3 . let xbe a coin flip, ybe a die. the sample space of xis{0,1},
whereas the sample space of yis{1,2,3,4,5,6}. the joint pmf, according to our
definition, is the probability p[x=xandy=y], where xtakes a binary state and y
takes one of the 6 states. the following table summarizes all the 12 states of the joint
distribution.
y
1 2 3 4 5 6
x = 01
121
121
121
121
121
12
x = 11
121
121
121
121
121
12
in this table, since there are 12 coordinates, and each coordinate has an equal
chance of appearing, the probability for each coordinate becomes 1 /12. therefore, the
joint pmf of xandyis
px,y(x, y) =1
12, x = 0,1, y = 1,2,3,4,5,6.
2465.1. joint pmf and joint pdf
in this example, we observe that if xandyare not interacting with each other (for-
mally, independent ), the joint pmf is the product of the two individual probabilities.
example 5.4 . in the previous example, if we define a={x+y= 3}, the probability
p[a] is
p[a] =x
(x,y)‚ààapx,y(x, y) =px,y(0,3) +px,y(1,2)
=2
12.
ifb={min(x, y) = 1}, the probability p[b] is
p[b] =x
(x,y)‚ààbpx,y(x, y)
=px,y(1,1) +px,y(1,2) +px,y(1,3)
+px,y(1,4) +px,y(1,5) +px,y(1,6)
=6
12.
5.1.3 continuous random variables
the continuous version of the joint pmf is called the joint probability density function
(joint pdf ), denoted by fx,y(x, y). a joint pdf is analogous to a joint pmf. for example,
integrating it will give us the probability.
definition 5.2. letxandybe two continuous random variables. the joint pdf of
xandyis a function fx,y(x, y)that can be integrated to yield a probability
p[a] =z
afx,y(x, y)dx dy, (5.2)
for any event a ‚äÜœâx√óœây.
pictorially, we can view fx,yas a 2d function where the height at a coordinate ( x, y) is
fx,y(x, y), as can be seen from figure 5.5 . to compute the probability that ( x, y)‚àà a,
we integrate the function fx,ywith respect to the area covered by the set a. for example,
if the set ais a rectangular box a= [a, b]√ó[c, d], then the integration becomes
p[a] =p[a‚â§x‚â§b, c‚â§y‚â§d]
=zd
czb
afx,y(x, y)dx dy.
247chapter 5. joint distributions
figure 5.5: a joint pdf for a pair of continuous random variables is a surface in the 2d plane. to
measure the size of the event a, we integrate fx,y(x, y)inside a.
example 5.5 . consider a uniform joint pdf fx,y(x, y) defined on [0 ,2]2withfx,y(x, y) =
1
4. leta= [a, b]√ó[c, d]. find p[a].
solution .
p[a] =p[a‚â§x‚â§b, c‚â§x‚â§d]
=zd
czb
afx,y(x, y)dx dy =zd
czb
a1
4dx dy =(d‚àíc)(b‚àía)
4.
practice exercise 5.1 . in the previous example, let b={x+y‚â§2}. find p[b].
solution .
p[b] =z
bfx,y(x, y)dx dy =z2
0z2‚àíy
0fx,y(x, y)dx dy
=z2
0z2‚àíy
01
4dx dy =z2
02‚àíy
4dy=1
2.
here, the limits of the integration can be determined from figure 5.6 . the inner
integration (with respect to x) should start from 0 and end at 2 ‚àíy, which is the line
defining the set x+y‚â§2. since the inner integration is performed for every y, we
need to enumerate all the possible y‚Äôs to complete the outer integration. this leads to
the outer limit from 0 to 2.
5.1.4 normalization
the normalization property of a two-dimensional pmf and pdf is the property that, when
we enumerate all outcomes of the sample space, we obtain 1.
2485.1. joint pmf and joint pdf
figure 5.6: to calculate p[x+y‚â§2], we perform a 2d integration over a triangle.
theorem 5.1. letœâ = œâ x√óœây. all joint pmfs and joint pdfs satisfy
x
(x,y)‚ààœâpx,y(x, y) = 1 orz
œâfx,y(x, y)dx dy = 1. (5.3)
example 5.6 . consider a joint uniform pdf defined in the shaded area [0 ,3]√ó[0,3]
with pdf defined below. find the constant c.
fx,y(x, y) =(
c if (x, y)‚àà[0,3]√ó[0,3],
0 otherwise .
solution . to find the constant c, we note that
1 =z3
0z3
0fx,y(x, y)dx dy =z3
0z3
0c dx dy = 9c.
equating the two sides gives us c=1
9.
practice exercise 5.2 . consider a joint pdf
fx,y(x, y) =(
ce‚àíxe‚àíy0‚â§y‚â§x <‚àû,
0 otherwise .
find the constant c. tip: consider the area of integration as shown in figure 5.7 .
solution . there are two ways to take the integration shown in figure 5.7 . we choose
the inner integration w.r.t. yfirst.
z
œâfx,y(x, y)dx dy =z‚àû
0zx
0ce‚àíxe‚àíydy dx =z‚àû
0ce‚àíx(1‚àíe‚àíx) =c
2.
therefore, c= 2.
249chapter 5. joint distributions
figure 5.7: to integrate the probability p[0‚â§y‚â§x], we perform a 2d integration over a triangle.
the two subfigures show the two ways of integrating the triangle. [left]r
dxfirst, and thenr
dy.
[right]r
dyfirst, and thenr
dx.
5.1.5 marginal pmf and marginal pdf
if we only sum / integrate for one random variable, we obtain the pmf / pdf of the other
random variable. the resulting pmf / pdf is called the marginal pmf / pdf.
definition 5.3. themarginal pmf is defined as
px(x) =x
y‚ààœâypx,y(x, y)and py(y) =x
x‚ààœâxpx,y(x, y), (5.4)
and the marginal pdf is defined as
fx(x) =z
œâyfx,y(x, y)dyand fy(y) =z
œâxfx,y(x, y)dx. (5.5)
since fx,y(x, y) is a two-dimensional function, when integrating over yfrom‚àí‚àûto‚àû, we
project fx,y(x, y) onto the x-axis. therefore, the resulting function depends on xonly.
example 5.7 . consider the joint pdf fx,y(x, y) =1
4shown below. find the marginal
pdfs.
2505.1. joint pmf and joint pdf
solution . if we integrate over xandy, we have
fx(x) =Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥3, if 1< x‚â§2,
1, if 2< x‚â§3,
0, otherwise .and fy(y) =Ô£±
Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥1, if 1< x‚â§2,
2, if 2< x‚â§3,
1, if 3< x‚â§4,
0, otherwise .
so the marginal pdfs are the projection of the joint pdfs onto the x- and y-axes.
practice exercise 5.3 . a joint gaussian random variable ( x, y) has a joint pdf
given by
fx,y(x, y) =1
2œÄœÉ2exp
‚àí((x‚àí¬µx)2+ (y‚àí¬µy)2)
2œÉ2
.
find the marginal pdfs fx(x) and fy(y).
solution .
fx(x) =z‚àû
‚àí‚àûfx,y(x, y)dy
=z‚àû
‚àí‚àû1
2œÄœÉ2exp
‚àí((x‚àí¬µx)2+ (y‚àí¬µy)2)
2œÉ2
dy
=1‚àö
2œÄœÉ2exp
‚àí(x‚àí¬µx)2
2œÉ2
¬∑z‚àû
‚àí‚àû1‚àö
2œÄœÉ2exp
‚àí(y‚àí¬µy)2
2œÉ2
dy.
recognizing that the last integral is equal to unity because it integrates a gaussian
pdf over the real line, it follows that
fx(x) =1‚àö
2œÄœÉ2exp
‚àí(x‚àí¬µx)2
2œÉ2
.
similarly, we have
fy(y) =1‚àö
2œÄœÉ2exp
‚àí(y‚àí¬µy)2
2œÉ2
.
5.1.6 independent random variables
two random variables are said to be independent if and only if the joint pmf or pdf can
be factorized as a product of the marginal pmf / pdfs.
definition 5.4. random variables xandyareindependent if and only if
px,y(x, y) =px(x)py(y),orfx,y(x, y) =fx(x)fy(y).
this definition is consistent with the definition of independence of two events. recall that
two events aandbare independent if and only if p[a‚à©b] =p[a]p[b]. letting a={x=x}
251chapter 5. joint distributions
andb={y=y}, we see that if aandbare independent then p[x=x‚à©y=y] is the
product p[x=x]p[y=y]. this is precisely the relationship px,y(x, y) =px(x)py(y).
example 5.8 . consider two random variables with a joint pdf given by
fx,y(x, y) =1
2œÄœÉ2exp
‚àí(x‚àí¬µx)2+ (y‚àí¬µy)2
2œÉ2
.
arexandyindependent?
solution . we know that
fx,y(x, y) =1‚àö
2œÄœÉexp
‚àí(x‚àí¬µx)2
2œÉ2
| {z }
fx(x)√ó1‚àö
2œÄœÉexp
‚àí(y‚àí¬µy)2
2œÉ2
| {z }
fy(y).
therefore, the random variables xandyare independent.
practice exercise 5.4 . let xbe a coin and ybe a die. then the joint pmf is given
by the table below.
y
1 2 3 4 5 6
x = 01
121
121
121
121
121
12
x = 11
121
121
121
121
121
12
arexandyindependent?
solution . for any xandy, we have that
px,y(x, y) =1
12=1
2|{z}
px(x)√ó1
6|{z}
py(y).
therefore, the random variables xandyare independent.
example 5.9 . consider two random variables xandywith a joint pdf given bya
fx,y(x, y)‚àùexp
‚àí(x‚àíy)2	
= exp
‚àíx2+ 2xy‚àíy2	
= exp
‚àíx2	
|{z}
fx(x)exp{2xy}|{z}
extra termexp
‚àíy2	
|{z}
fy(y).
this pdf cannot be factorized into a product of two marginal pdfs. therefore, the
random variables are dependent.
awe use the notation ‚Äú ‚àù‚Äù to denote ‚Äúproportional to‚Äù. it implies that the normalization constant
is omitted.
2525.1. joint pmf and joint pdf
we can extrapolate the definition of independence to multiple random variables. if
there are many random variables x1, x2, . . . , x n, they will have a joint pdf
fx1,...,x n(x1, . . . , x n).
if these random variables x1, x2, . . . , x nare independent, then the joint pdf can be
factorized as
fx1,...,x n(x1, . . . , x n) =fx1(x1)¬∑fx2(x2)¬∑¬∑¬∑fxn(xn)
=ny
n=1fxn(xn).
this gives us the definition of independence for nrandom variables.
definition 5.5. a sequence of random variables x1, . . . , x nisindependent if and
only if their joint pdf (or joint pmf) can be factorized.
fx1,...,x n(x1, . . . , x n) =ny
n=1fxn(xn). (5.6)
example 5.10 . throw a die 4 times. let x1,x2,x3andx4be the outcomes. then,
since these four throws are independent, the probability mass function of any quadrable
(x1, x2, x3, x4) is
px1,x2,x3,x4(x1, x2, x3, x4) =px1(x1)px2(x2)px3(x3)px4(x4).
for example, the probability of getting (1 ,5,2,6) is
px1,x2,x3,x4(1,5,2,6) = px1(1)px2(5)px3(2)px4(6) =1
64
.
the example above demonstrates an interesting phenomenon. if the nrandom vari-
ables are independent, and if they all have the same distribution, then the joint pdf/pmf
is just one of the individual pdfs taken to the power n. random variables satisfying this
property are known as independent and identically distributed random variables.
definition 5.6 (independent and identically distributed (i.i.d.) ).a collection of
random variables x1, . . . , x nis called independent and identically distributed (i.i.d.)
if
¬àallx1, . . . , x nare independent; and
¬àallx1, . . . , x nhave the same distribution, i.e., fx1(x) =¬∑¬∑¬∑=fxn(x).
ifx1, . . . , x nare i.i.d., we have that
fx1,...,x n(x1, . . . , x 1) =ny
n=1fx1(xn),
253chapter 5. joint distributions
where the particular choice of x1is unimportant because fx1(x) =¬∑¬∑¬∑=fxn(x).
why is i.i.d. so important?
¬àif a set of random variables are i.i.d., then the joint pdf can be written as a
product of pdfs.
¬àintegrating a joint pdf is difficult. integrating a product of pdfs is much easier.
example 5.11 . let x1, x2, . . . , x nbe a sequence of i.i.d. gaussian random variables
where each xihas a pdf
fxi(x) =1‚àö
2œÄexp
‚àíx2
2
.
the joint pdf of x1, x2, . . . , x nis
fx1,...,x n(x1, . . . , x n) =ny
i=11‚àö
2œÄexp
‚àíx2
i
2
=1‚àö
2œÄn
exp(
‚àínx
i=1x2
i
2)
,
which is a function depending not on the individual values of x1, x2, . . . , x nbut on the
sumpn
i=1x2
i. so we have ‚Äúcompressed‚Äù an n-dimensional function into a 1d function.
example 5.12 . let Œ∏be a deterministic number that was sent through a noisy channel.
we model the noise as an additive gaussian random variable with mean 0 and variance
œÉ2. supposing we have observed measurements xi=Œ∏+wi, for i= 1, . . . , n , where
wi‚àºgaussian(0 , œÉ2), then the pdf of each xiis
fxi(x) =1‚àö
2œÄœÉ2exp
‚àí(x‚àíŒ∏)2
2œÉ2
.
thus the joint pdf of ( x1, x2, . . . , x n) is
fx1,...,x n(x1, . . . , x n) =ny
i=11‚àö
2œÄœÉ2exp
‚àí(xi‚àíŒ∏)2
2œÉ2
=1‚àö
2œÄœÉ2n
exp(
‚àínx
i=1(xi‚àíŒ∏)2
2œÉ2)
.
essentially, this joint pdf tells us the probability density of seeing sample data
x1, . . . , x n.
2545.1. joint pmf and joint pdf
5.1.7 joint cdf
we now introduce the cumulative distribution function (cdf) for multiple variables.
definition 5.7. letxandybe two random variables. the joint cdf ofxandy
is the function fx,y(x, y)such that
fx,y(x, y) =p[x‚â§x‚à©y‚â§y]. (5.7)
this definition can be more explicitly written as follows.
definition 5.8. ifxandyare discrete, then
fx,y(x, y) =x
y‚Ä≤‚â§yx
x‚Ä≤‚â§xpx,y(x‚Ä≤, y‚Ä≤). (5.8)
ifxandyare continuous, then
fx,y(x, y) =zy
‚àí‚àûzx
‚àí‚àûfx,y(x‚Ä≤, y‚Ä≤)dx‚Ä≤dy‚Ä≤. (5.9)
if the two random variables are independent , then we have
fx,y(x, y) =zx
‚àí‚àûfx(x‚Ä≤)dx‚Ä≤zy
‚àí‚àûfy(y‚Ä≤)dy‚Ä≤=fx(x)fy(y).
example 5.13 . let xandybe two independent uniform random variables
uniform(0 ,1). find the joint cdf.
solution .
fx,y(x, y) =fx(x)fy(y) =zx
0fx(x‚Ä≤)dx‚Ä≤zy
0fy(y‚Ä≤)dy‚Ä≤
=zx
01dx‚Ä≤zy
01dy‚Ä≤=xy.
practice exercise 5.5 . let xandybe two independent uniform random variables
gaussian( ¬µ, œÉ2). find the joint cdf.
solution . let œÜ( ¬∑) be the cdf of the standard gaussian.
fx,y(x, y) =fx(x)fy(y)
=zx
‚àí‚àûfx(x‚Ä≤)dx‚Ä≤zy
‚àí‚àûfy(y‚Ä≤)dy‚Ä≤= œÜx‚àí¬µ
œÉ
œÜy‚àí¬µ
œÉ
.
255chapter 5. joint distributions
here are a few properties of the cdf:
fx,y(x,‚àí‚àû) =z‚àí‚àû
‚àí‚àûzx
‚àí‚àûfx,y(x‚Ä≤, y‚Ä≤)dx‚Ä≤dy‚Ä≤=zx
‚àí‚àû0dx‚Ä≤= 0,
fx,y(‚àí‚àû, y) =zy
‚àí‚àûz‚àí‚àû
‚àí‚àûfx,y(x‚Ä≤, y‚Ä≤)dx‚Ä≤dy‚Ä≤=zy
‚àí‚àû0dy‚Ä≤= 0,
fx,y(‚àí‚àû,‚àí‚àû) =z‚àí‚àû
‚àí‚àûz‚àí‚àû
‚àí‚àûfx,y(x‚Ä≤, y‚Ä≤)dx‚Ä≤dy‚Ä≤= 0,
fx,y(‚àû,‚àû) =z‚àû
‚àí‚àûz‚àû
‚àí‚àûfx,y(x‚Ä≤, y‚Ä≤)dx‚Ä≤dy‚Ä≤= 1.
in addition, we can obtain the marginal cdf as follows.
proposition 5.1. letxandybe two random variables. the marginal cdf is
fx(x) =fx,y(x,‚àû), (5.10)
fy(y) =fx,y(‚àû, y). (5.11)
proof . we prove only the first case. the second case is similar.
fx,y(x,‚àû) =zx
‚àí‚àûz‚àû
‚àí‚àûfx,y(x‚Ä≤, y‚Ä≤)dy‚Ä≤dx‚Ä≤=zy
‚àí‚àûfx(x‚Ä≤)dx‚Ä≤=fx(x).‚ñ°
by the fundamental theorem of calculus, we can derive the pdf from the cdf.
definition 5.9. letfx,y(x, y)be the joint cdf of xandy. then, the joint pdf
is
fx,y(x, y) =‚àÇ2
‚àÇy ‚àÇxfx,y(x, y). (5.12)
the order of the partial derivatives can be switched, yielding a symmetric result:
fx,y(x, y) =‚àÇ2
‚àÇx ‚àÇyfx,y(x, y).
example 5.14 . let xandybe two uniform random variables with joint cdf
fx,y(x, y) =xyfor 0‚â§x‚â§1 and 0 ‚â§y‚â§1. find the joint pdf.
solution .
fx,y(x, y) =‚àÇ2
‚àÇx‚àÇyfx,y(x, y) =‚àÇ2
‚àÇx‚àÇyxy= 1,
which is consistent with the definition of a joint uniform random variable.
2565.2. joint expectation
practice exercise 5.6 . let xandybe two exponential random variables with joint
cdf
fx,y(x, y) = (1 ‚àíe‚àíŒªx)(1‚àíe‚àíŒªy), x ‚â•0, y‚â•0.
find the joint pdf.
solution .
fx,y(x, y) =‚àÇ2
‚àÇx‚àÇyfx,y(x, y) =‚àÇ2
‚àÇx‚àÇy(1‚àíe‚àíŒªx)(1‚àíe‚àíŒªy)
=‚àÇ
‚àÇx 
(1‚àíe‚àíŒªx)(Œªe‚àíŒªy)
=Œªe‚àíŒªxŒªe‚àíŒªy.
which is consistent with the definition of a joint exponential random variable.
5.2 joint expectation
5.2.1 definition and interpretation
when we have a single random variable, the expectation is defined as
e[x] =z
œâxfx(x)dx.
for a pair of random variables, what would be a good way of defining the expectation?
certainly, we cannot just replace fx(x) by fx,y(x, y) because the integration has to be-
come a double integration. however, if it is a double integration, where should we put the
variable y? it turns out that a useful way of defining the expectation for xandyis as
follows.
definition 5.10. letxandybe two random variables. the joint expectation is
e[xy] =x
y‚ààœâyx
x‚ààœâxxy¬∑px,y(x, y) (5.13)
ifxandyare discrete, or
e[xy] =z
y‚ààœâyz
x‚ààœâxxy¬∑fx,y(x, y)dx dy (5.14)
ifxandyare continuous. joint expectation is also called correlation .
the double summation and integration on the right-hand side of the equation is nothing
but the state times the probability. here, the state is the product xy, and the probability is
the joint pmf px,y(x, y) (or pdf). therefore, as long as you agree that joint expectation
should be defined as e[xy], the double summation and the double integration make sense.
257chapter 5. joint distributions
the biggest mystery here is e[xy]. you may wonder why the joint expectation should
be defined as the expectation of the product e[xy]. why not the sum e[x+y], or the
difference e[x‚àíy], or the quotient e[x/y ]? why are we so deeply interested in xtimes y?
these are excellent questions. that the joint expectation is defined as the product has to do
with the correlation between two random variables. we will take a small detour into linear
algebra.
let us consider two discrete random variables xandy, both with nstates. so x
will take the states {x1, x2, . . . , x n}andywill take the states {y1, y2, . . . , y n}. let‚Äôs define
them as two vectors: xdef= [x1, . . . , x n]tandydef= [y1, . . . , y n]t. since xandyare random
variables, they have a joint pmf px,y(x, y). the array of the pmf values can be written
as a matrix:
pmf as a matrix = pdef=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞px,y(x1, y1)px,y(x1, y2)¬∑¬∑¬∑ px,y(x1, yn)
px,y(x2, y1)px,y(x2, y2)¬∑¬∑¬∑ px,y(x2, yn)
............
px,y(xn, y1)px,y(xn, y2)¬∑¬∑¬∑px,y(xn, yn)Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
let‚Äôs try to write the joint expectation in terms of matrices and vectors. the definition
of a joint expectation tells us that
e[xy] =nx
i=1nx
j=1xiyj¬∑px,y(xi, yj),
which can be written as
e[xy] =x1¬∑¬∑¬∑xn
|{z }
xtÔ£Æ
Ô£ØÔ£∞px,y(x1, y1)¬∑¬∑¬∑ px,y(x1, yn)
.........
px,y(xn, y1)¬∑¬∑¬∑px,y(xn, yn)Ô£π
Ô£∫Ô£ª
| {z }
pÔ£Æ
Ô£ØÔ£∞y1
...
ynÔ£π
Ô£∫Ô£ª
|{z}
y=xtpy.
this is a weighted inner product between xandyusing the weight matrix p.
why correlation is defined as e[xy]
¬àe[xy] is a weighted inner product between the states:
e[xy] =xtpy.
¬àxandyare the states of the random variables xandy.
¬àthe inner product measures the similarity between two vectors.
example 5.15 . let xbe a discrete random variable with nstates, where each state
has an equal probability. thus, px(x) = 1 /nfor all x. let y=xbe another variable.
2585.2. joint expectation
then the joint pmf of ( x, y) is
px,y(x, y) =(
1
n, x =y,
0, x Ã∏=y.
it follows that the joint expectation is
e[xy] =nx
i=1nx
j=1xiyj¬∑px,y(xi, yj) =1
nnx
i=1xiyi.
equivalently, we can obtain the result via the inner product by defining
p=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞1
n0¬∑¬∑¬∑ 0
01
n¬∑¬∑¬∑ 0
............
0¬∑¬∑¬∑ ¬∑¬∑¬∑1
nÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª=1
ni.
in this case, the weighted inner product is
xtpy=xty
n=1
nnx
i=1xiyi=e[xy].
how do we understand the inner product? ignoring the matrix pfor a moment, we
recall an elementary result in linear algebra.
definition 5.11. letx‚ààrnandy‚ààrnbe two vectors. define the cosine angle
cosŒ∏as
cosŒ∏=xty
‚à•x‚à•‚à•y‚à•, (5.15)
where ‚à•x‚à•=qpn
i=1x2
iis the norm of the vector x, and ‚à•y‚à•=qpn
i=1y2
iis the
norm of the vector y.
this definition can be understood as the geometry between two vectors, as illustrated in
figure 5.8 . if the two vectors xandyare parallel so that x=Œ±yfor some Œ±, then the
angle Œ∏= 0. if xandyare orthogonal so that xty= 0, then Œ∏=œÄ/2. therefore, the inner
product xtytells us the degree of correlation between the vectors xandy.
now let‚Äôs come back to our discussion about the joint expectation. the cosine angle
definition tells us that if e[xy] =xtpy, the following form would make sense:
cosŒ∏=xtpy
‚à•x‚à•‚à•y‚à•=e[xy]
‚à•x‚à•‚à•y‚à•.
that is, as long as we can find out the norms ‚à•x‚à•and‚à•y‚à•, we will be able to interpret
e[xy] from the cosine angle perspective. but what would be a reasonable definition of ‚à•x‚à•
259chapter 5. joint distributions
figure 5.8: the geometry of joint expectation. e[xy]gives us the cosine angle between the two random
variables. this, in turn, tells us the correlation between the two random variables.
and‚à•y‚à•? we define the norm by first considering the variance of the random variable x
andy:
e[x2] =nx
i=1xixi¬∑px(xi)
=
x1¬∑¬∑¬∑xn
|{z }
xtÔ£Æ
Ô£ØÔ£∞px(x1)¬∑¬∑¬∑ 0
.........
0 ¬∑¬∑¬∑px(xn)Ô£π
Ô£∫Ô£ª
| {z }
pxÔ£Æ
Ô£ØÔ£∞x1
...
xnÔ£π
Ô£∫Ô£ª
|{z}
x
=xtpxx=‚à•x‚à•2
px,
where pxis the diagonal matrix storing the probability masses of the random variable x.
it is not difficult to show that px= diag( p1) by following the definition of the marginal
distributions (which are the column and row sums of the joint pmf). similarly we can define
e[y2] =nx
j=1yjyj¬∑py(yj)
=
y1¬∑¬∑¬∑yn
|{z }
ytÔ£Æ
Ô£ØÔ£∞py(y1)¬∑¬∑¬∑ 0
.........
0 ¬∑¬∑¬∑py(yn)Ô£π
Ô£∫Ô£ª
| {z }
pyÔ£Æ
Ô£ØÔ£∞y1
...
ynÔ£π
Ô£∫Ô£ª
|{z}
y
=ytpyy=‚à•y‚à•2
py.
therefore, one way to define the cosine angle is to start with
cosŒ∏=xtpxyy
‚à•x‚à•px‚à•y‚à•py,
where pxy=p,‚à•x‚à•px=p
xtpxxand‚à•y‚à•py=p
ytpyy. but writing it in terms of
the expectation, we observe that this cosine angle is exactly
cosŒ∏=xtpxyy
‚à•x‚à•px‚à•y‚à•py=e[xy]p
e[x2]p
e[y2].
2605.2. joint expectation
therefore, e[xy] defines the cosine angle between the two random variables, which, in turn,
defines the correlation between the two. a large |e[xy]|means that xandyare highly
correlated, and a small |e[xy]|means that xandyare not very correlated. if e[xy] = 0,
then the two random variables are uncorrelated. therefore, e[xy] tells us how the two
random variables are related to each other.
to further convince you thate[xy]‚àö
e[x2]‚àö
e[y2]can be interpreted as a cosine angle, we
show that
‚àí1‚â§e[xy]p
e[x2]p
e[y2]‚â§1,
because if this ratio can go beyond +1 and ‚àí1, it makes no sense to call it a cosine angle.
the argument follows from a very well-known inequality in probability, called the cauchy-
schwarz inequality (for expectation), which states that ‚àí1‚â§e[xy]‚àö
e[x2]‚àö
e[y2]‚â§1:
theorem 5.2 (cauchy-schwarz inequality ).for any random variables xandy,
(e[xy])2‚â§e[x2]e[y2]. (5.16)
the following proof can be skipped if you are reading the book the first time.
proof . let t‚ààrbe a constant. consider e[(x+ty)2] =e[x2+ 2txy +t2y2]. since
e[(x+ty)2]‚â•0 for any t, it follows that
e[x2+ 2txy +t2y2]‚â•0.
expanding the left-hand side yields t2e[y2] + 2te[xy] +e[x2]‚â•0. this is a quadratic
equation in t, and we know that for any quadratic equation at2+bt+c‚â•0 we must have
b2‚àí4ac‚â§0. therefore, in our case, we have that
(2e[xy])2‚àí4e[y2]e[x2]‚â§0,
which means ( e[xy])2‚â§e[x2]e[y2]. the equality holds when e[(x+ty)2] = 0. in this
case, x=‚àítyfor some t, i.e., the random variable xis a scaled version of yso that the
vector formed by the states of xis parallel to that of y.
‚ñ°
end of the proof.
5.2.2 covariance and correlation coefficient
in many practical problems, we prefer to work with central moments, i.e., e[(x‚àí¬µx)2] in-
stead of e[x2]. this essentially means that we subtract the mean from the random variable.
if we adopt such a centralized random variable, we can define the covariance as follows.
261chapter 5. joint distributions
definition 5.12. letxandybe two random variables. then the covariance ofx
andyis
cov(x, y) =e[(x‚àí¬µx)(y‚àí¬µy)], (5.17)
where ¬µx=e[x]and¬µy=e[y].
it is easy to show that if x=y, then the covariance simplifies to the variance:
cov(x, x ) =e[(x‚àí¬µx)(x‚àí¬µx)]
= var[ x].
thus, covariance is a generalization of variance. the former can handle a pair of variables,
whereas the latter is only for a single variable. we can also demonstrate the following result.
theorem 5.3. letxandybe two random variables. then
cov(x, y) =e[xy]‚àíe[x]e[y] (5.18)
proof . just apply the definition of covariance:
cov(x, y) =e[(x‚àí¬µx)(y‚àí¬µy)]
=e[xy‚àíx¬µy‚àíy ¬µx+¬µx¬µy]
=e[xy]‚àí¬µx¬µy.
‚ñ°
the next theorem concerns the sum of two random variables.
theorem 5.4. for any xandy,
a.e[x+y] =e[x] +e[y].
b.var[x+y] = var[ x] + 2cov( x, y) + var[ y].
proof . recall the definition of joint expectation:
e[x+y] =x
yx
x(x+y)px,y(x, y)
=x
yx
xxpx,y(x, y) +x
yx
xypx,y(x, y)
=x
xx x
ypx,y(x, y)!
+x
yy x
xpx,y(x, y)!
=x
xxpx(x) +x
yypy(y)
=e[x] +e[y].
2625.2. joint expectation
similarly,
var[x+y] =e[(x+y)2]‚àíe[x+y]2
=e[(x+y)2]‚àí(¬µx+¬µy)2
=e[x2+ 2xy+y2]‚àí(¬µ2
x+ 2¬µx¬µy+¬µ2
y)
=e[x2]‚àí¬µ2
x+e[y2]‚àí¬µ2
y+ 2(e[xy]‚àí¬µx¬µy)
= var[ x] + 2cov( x, y) + var[ y].
‚ñ°
with covariance defined, we can now define the correlation coefficient œÅ, which is the
cosine angle of the centralized variables. that is,
œÅ= cos Œ∏
=e[(x‚àí¬µx)(y‚àí¬µy)]p
e[(x‚àí¬µx)2]e[(y‚àí¬µy)2].
recognizing that the denominator of this expression is just the variance of xandy, we
define the correlation coefficient as follows.
definition 5.13. letxandybe two random variables. the correlation coefficient
is
œÅ=cov(x, y)p
var[x]var[y]. (5.19)
since ‚àí1‚â§cosŒ∏‚â§1,œÅis also between ‚àí1 and 1. the difference between œÅande[xy]
is that œÅisnormalized with respect to the variance of xandy, whereas e[xy] is not
normalized. the correlation coefficient has the following properties:
¬àœÅis always between ‚àí1 and 1, i.e., ‚àí1‚â§œÅ‚â§1. this is due to the cosine angle
definition.
¬àwhen x=y(fully correlated), œÅ= +1.
¬àwhen x=‚àíy(negatively correlated), œÅ=‚àí1.
¬àwhen xandyare uncorrelated, œÅ= 0.
5.2.3 independence and correlation
if two random variables xandyare independent, the joint expectation can be written as
a product of two individual expectations.
theorem 5.5. ifxandyare independent, then
e[xy] =e[x]e[y]. (5.20)
263chapter 5. joint distributions
proof . we only prove the discrete case because the continuous can be proved similarly. if
xandyare independent, we have px,y(x, y) =px(x)py(y). therefore,
e[xy] =x
yx
xxypx,y(x, y) =x
yx
xxypx(x)py(y)
= x
xxpx(x)! x
yypy(y)!
=e[x]e[y].
‚ñ°
in general, for any two independent random variables and two functions fandg,
e[f(x)g(y)] =e[f(x)]e[g(y)].
the following theorem illustrates a few important relationships between independence
and correlation.
theorem 5.6. consider the following two statements:
a.xandyare independent;
b.cov(x, y) = 0 .
statement (a) implies statement (b), but (b) does not imply (a). thus, independence
is a stronger condition than correlation.
proof . we first prove that (a) implies (b). if xandyare independent, then e[xy] =
e[x]e[y]. in this case,
cov(x, y) =e[xy]‚àíe[x]e[y] =e[x]e[y]‚àíe[x]e[y] = 0.
to prove that (b) does not imply (a), we show a counterexample. consider a discrete
random variable zwith pmf
pz(z) =1
41
41
41
4
.
letxandybe
x= cosœÄ
2zand y= sinœÄ
2z.
then we can show that e[x] = 0 and e[y] = 0. the covariance is
cov(x, y) =e[(x‚àí0)(y‚àí0)]
=eh
cosœÄ
2zsinœÄ
2zi
=e1
2sinœÄz
=1
2
(sinœÄ0)1
4+ (sin œÄ1)1
4+ (sin œÄ2)1
4+ (sin œÄ3)1
4
= 0.
2645.2. joint expectation
the next step is to show that xandyare dependent. to this end, we only need to show
thatpx,y(x, y)Ã∏=px(x)py(y). the joint pmf px,y(x, y) can be found by noting that
z= 0‚áíx= 1, y= 0,
z= 1‚áíx= 0, y= 1,
z= 2‚áíx=‚àí1, y= 0,
z= 3‚áíx= 0, y=‚àí1.
thus, the pmf is
px,y(x, y) =Ô£Æ
Ô£∞01
40
1
401
4
01
40Ô£π
Ô£ª.
the marginal pmfs are
px(x) =1
41
21
4
, p y(y) =1
41
21
4
.
the product px(x)py(y) is
px(x)py(y) =Ô£Æ
Ô£ØÔ£∞1
161
81
16
1
81
41
8
1
161
81
16Ô£π
Ô£∫Ô£ª.
therefore, px,y(x, y)Ã∏=px(x)py(y), although e[xy] =e[x]e[y].
‚ñ°
what is the relationship between independent and uncorrelated?
¬àindependent ‚áíuncorrelated.
¬àindependent ‚áçuncorrelated.
5.2.4 computing correlation from data
we close this section by discussing a very practical problem: given a dataset containing two
columns of data points, how do we determine whether the two columns are correlated?
recall that the correlation coefficient is defined as
œÅ=e[xy]‚àí¬µx¬µy
œÉxœÉy.
if we have a dataset containing ( xn, yi)n
n=1, then the correlation coefficient can be approxi-
mated by
bœÅ=1
npn
n=1xnyn‚àíxyq
1
npn
n=1(xn‚àíx)2q
1
npn
n=1(yn‚àíy)2,
where x=1
npn
n=1xnandy=1
npn
n=1ynare the means. this equation should not be a
surprise because essentially all terms are the empirical estimates. thus, bœÅis the empirical
correlation coefficient determined from the dataset. as n‚Üí ‚àû , we expect bœÅ‚ÜíœÅ.
265chapter 5. joint distributions
-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345
-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345
-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345
(a)bœÅ=‚àí0.0038 (b) bœÅ= 0.5321 (c) bœÅ= 0.9656
figure 5.9: visualization of correlated variables. each of these figures represent a scattered plot of a
dataset containing (xn, yn)n
n=1. (a) is uncorrelated. (b) is somewhat correlated. (c) is strongly correlated.
figure 5.9 shows three example datasets. we plot the ( xn, yn) pairs as coordinates in
the 2d plane. the first dataset contains samples that are almost uncorrelated. we can see
that xndoes not tell us anything about yn. the second dataset is moderately correlated.
the third dataset is highly correlated: if we know xn, we are almost certain to know the
corresponding yn, with a small number of perturbations.
on a computer, computing the correlation coefficient can be done using built-in com-
mands such as corrcoef in matlab and stats.pearsonr in python. the codes to gen-
erate the results in figure 5.9 (b) are shown below.
% matlab code to compute the correlation coefficient
x = mvnrnd([0,0],[3 1; 1 1],1000);
figure(1); scatter(x(:,1),x(:,2));
rho = corrcoef(x)
# python code to compute the correlation coefficient
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
x = stats.multivariate_normal.rvs([0,0], [[3,1],[1,1]], 10000)
plt.figure(); plt.scatter(x[:,0],x[:,1])
rho,_ = stats.pearsonr(x[:,0],x[:,1])
print(rho)
5.3 conditional pmf and pdf
whenever we have a pair of random variables xandythat are correlated, we can define
their conditional distributions, which quantify the probability of x=xgiven y=y. in
this section, we discuss the concepts of conditional pmf and pdf.
2665.3. conditional pmf and pdf
5.3.1 conditional pmf
we start by defining the conditional pmf for a pair of discrete random variables.
definition 5.14. letxandybe two discrete random variables. the conditional
pmf ofxgiven yis
px|y(x|y) =px,y(x, y)
py(y). (5.21)
the simplest way to understand this is to view px|y(x|y) asp[x=x|y=y]. that is,
given that y=y, what is the probability for x=x? to see why this perspective makes
sense, let us recall the definition of a conditional probability:
px|y(x|y) =px,y(x, y)
py(y)
=p[x=x‚à©y=y]
p[y=y]=p[x=x|y=y].
as we can see, the last two equalities are essentially the definitions of conditional probability
and the joint pmf.
how should we understand the notation px|y(x|y)? is it a one-variable function in xor
a two-variable function in ( x, y)? what does px|y(x|y) tell us? to answer these questions,
let us first try to understand the randomness exhibited in a conditional pmf. in px|y(x|y),
the random variable yisfixed to a specific value y=y. therefore there is nothing random
about y. all the possibilities of yhave already been taken care of by the denominator
py(y). only the variable xinpx|y(x|y) has randomness. what do we mean by ‚Äúfixed at a
value y=y‚Äù? consider the following example.
example 5.16 . suppose there are two coins. let
x= the sum of the values of two coins ,
y= the value of the first coin .
clearly, xhas 3 states: 0, 1, 2, and yhas two states: either 0 or 1. when we say
px|y(x|1), we refer to the probability mass function of xwhen fixing y= 1. if we do
not impose this condition, the probability mass of xis simple:
px(x) =1
4,1
2,1
4
.
however, if we include the conditioning, then
px|y(x|1) =px,y(x,1)
py(1)
=
0,2
4,1
4
1
6=
0,2
3,1
3
.
267chapter 5. joint distributions
to put this in plain words, when y= 1, there is no way for xto take the state 0. the
chance for xto take the state 1 is 2 /3 because either (0 ,1) or (1 ,0) can give x= 1.
the chance for xto take the state 2 is 1 /3 because it has to be (1 ,1) in order to give
x= 2. therefore, when we say ‚Äúconditioned on y= 1‚Äù, we mean that we limit our
observations to cases where y= 1. since yis already fixed at y= 1, there is nothing
random about y. the only variable is x. this example is illustrated in figure 5.10 .
figure 5.10: suppose xis the sum of two coins with pmf 0.25,0.5,0.25. let ybe the first coin.
when xis unconditioned, the pmf is just [0.25,0.5,0.25]. when xis conditioned on y= 1,
then ‚Äú x= 0‚Äù cannot happen. therefore, the resulting pmf px|y(x|1)only has two states. after
normalization we obtain the conditional pmf [0,0.66,0.33].
since yis already fixed at a particular value y=y,px|y(x|y) is a probability mass
function of x(we want to emphasize again that it is xand not y). so px|y(x|y) is a one-
variable function in x. it is not the same as the usual pmf px(x).px|y(x|y) is conditioned
ony=y. for example, px|y(x|1) is the pmf of xrestricted to the condition that y= 1.
in fact, it follows that
x
x‚ààœâxpx|y(x|y) =x
x‚ààœâxpx,y(x, y)
py(y)=p
x‚ààœâxpx,y(x, y)
py(y)=py(y)
py(y)= 1,
but this tells us that px|y(x|y) is a legitimate probability mass of x. if we sum over the y‚Äôs
instead, then we will hit a bump:
x
y‚ààœâypx|y(x|y) =x
y‚ààœâypx,y(x, y)
py(y)Ã∏= 1.
therefore, while px|y(x|y) is a legitimate probability mass function of x, it is not a prob-
ability mass function of y.
example 5.17 . consider a joint pmf given in the following table. find the conditional
pmf px|y(x|1) and the marginal pmf px(x).
y=
1 2 3 4
x = 11
201
201
200
20
21
202
203
201
20
31
202
203
201
20
40
201
201
201
20
2685.3. conditional pmf and pdf
solution . to find the marginal pmf, we sum over all the y‚Äôs for every x:
x= 1 : px(1) =4x
y=1px,y(1, y) =1
20+1
20+1
20+0
20=3
20,
x= 2 : px(2) =4x
y=1px,y(2, y) =1
20+2
20+2
20+1
20=6
20,
x= 3 : px(3) =4x
y=1px,y(3, y) =1
20+3
20+3
20+1
20=8
20,
x= 4 : px(4) =4x
y=1px,y(4, y) =0
20+1
20+1
20+1
20=3
20.
hence, the marginal pmf is
px(x) =3
206
208
203
20
.
the conditional pmf px|y(x|1) is
px|y(x|1) =px,y(x,1)
py(1)=1
201
201
200
20
3
20=1
31
31
30
.
practice exercise 5.7 . consider two random variables xandydefined as follows.
y=(
102, with prob 5 /6,
104, with prob 1 /6.x=Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥10‚àí4y, with prob 1 /2,
10‚àí3y, with prob 1 /3,
10‚àí2y, with prob 1 /6.
find px|y(x|y),px(x) and px,y(x, y).
solution . since ytakes two different states, we can enumerate y= 102andy= 104.
this gives us
px|y(x|102) =Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥1/2, ifx= 0.01,
1/3, ifx= 0.1,
1/6, ifx= 1.
px|y(x|104) =Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥1/2, ifx= 1,
1/3, ifx= 10,
1/6, ifx= 100 .
269chapter 5. joint distributions
the joint pmf px,y(x, y) is
px,y(x,102) =px|y(x|102)py(102) =Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥ 1
2 5
6
, x = 0.01, 1
3 5
6
, x = 0.1, 1
6 5
6
, x = 1.
px,y(x,104) =px|y(x|104)py(104) =Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥ 1
2 1
6
, x = 1, 1
3 1
6
, x = 10, 1
6 1
6
, x = 100 .
therefore, the joint pmf is given by the following table.
1040 01
121
181
36
102 5
125
185
360 0
0.01 0.1 1 10 100
the marginal pmf px(x) is thus
px(x) =x
ypx,y(x, y) =5
125
182
91
181
36
.
in the previous two examples, what is the probability p[x‚ààa|y=y] or the proba-
bilityp[x‚ààa] for some events a? the answers are giving by the following theorem.
theorem 5.7. letxandybe two discrete random variables, and let abe an event.
then
(i)p[x‚ààa|y=y] =x
x‚ààapx|y(x|y)
(ii)p[x‚ààa] =x
x‚ààax
y‚ààœâypx|y(x|y)py(y) =x
y‚ààœâyp[x‚ààa|y=y]py(y).
proof . the first statement is based on the fact that if acontains a finite number of elements,
thenp[x‚ààa] is equivalent to the sump
x‚ààap[x=x]. thus,
p[x‚ààa|y=y] =p[x‚ààa‚à©y=y]
p[y=y]
=p
x‚ààap[x=x‚à©y=y]
p[y=y]
=x
x‚ààapx|y(x|y).
the second statement holds because the inner summationp
y‚ààœâypx|y(x|y)py(y) is just
the marginal pmf px(x). thus the outer summation yields the probability.
‚ñ°
2705.3. conditional pmf and pdf
example 5.18 . let us follow up on example 5.17. what is the probability that
p[x > 2|y= 1]? what is the probability that p[x > 2]?
solution . since the problem asks about the conditional probability, we know that it
can be computed by using the conditional pmf. this gives us
p[x > 2|y= 1] =x
x>2px|y(x|1)
=px|y(1|1) +px|y(2|1) +px|y(3|1)|{z}
1
3+px|y(4|1)|{z}
0=1
3.
the other probability is
p[x > 2] =x
x>2px(x)
=px(1) +px(2) + px(3)|{z}
8
20+px(4)|{z}
3
20=11
20.
what is the rule of thumb for conditional distribution?
¬àthe pmf/pdf should match with the probability you are finding.
¬àif you want to find the conditional probability p[x‚ààa|y=y], use the condi-
tional pmf px|y(x|y).
¬àif you want to find the probability p[x‚ààa], use the marginal pmf px(x).
finally, we define the conditional cdf for discrete random variables.
definition 5.15. letxandybe discrete random variables. then the conditional
cdf ofxgiven y=yis
fx|y(x|y) =p[x‚â§x|y=y] =x
x‚Ä≤‚â§xpx|y(x‚Ä≤|y). (5.22)
5.3.2 conditional pdf
we now discuss the conditioning of a continuous random variable.
definition 5.16. letxandybe two continuous random variables. the conditional
pdf ofxgiven yis
fx|y(x|y) =fx,y(x, y)
fy(y). (5.23)
271chapter 5. joint distributions
example 5.19 . let xandybe two continuous random variables with a joint pdf
fx,y(x, y) =(
2e‚àíxe‚àíy, 0‚â§y‚â§x <‚àû,
0, otherwise .
find the conditional pdfs fx|y(x|y) and fy|x(y|x).
solution . we first find the marginal pdfs.
fx(x) =z‚àû
‚àí‚àûfx,y(x, y)dy=zx
02e‚àíxe‚àíydy= 2e‚àíx(1‚àíe‚àíx),
fy(y) =z‚àû
‚àí‚àûfx,y(x, y)dx=z‚àû
y2e‚àíxe‚àíydx= 2e‚àí2y.
thus, the conditional pdfs are
fx|y(x|y) =fx,y(x, y)
fy(y)
=2e‚àíxe‚àíy
2e‚àí2y=e‚àí(x+y), x‚â•y,
fy|x(y|x) =fx,y(x, y)
fx(x)
=2e‚àíxe‚àíy
2e‚àíx(1‚àíe‚àíx)=e‚àíy
1‚àíe‚àíx,0‚â§y < x.
where does the conditional pdf come from? we cannot duplicate the argument
we used for the discrete case because the denominator of a conditional pmf becomes
p[y=y] = 0 when yis continuous. to answer this question, we first define the conditional
cdf for continuous random variables.
definition 5.17. letxandybe continuous random variables. then the conditional
cdf ofxgiven y=yis
fx|y(x|y) =rx
‚àí‚àûfx,y(x‚Ä≤, y)dx‚Ä≤
fy(y). (5.24)
why should the conditional cdf of continuous random variable be defined in this way? one
way to interpret fx|y(x|y) is as the limiting perspective. we can define the conditional cdf
as
fx|y(x|y) = lim
h‚Üí0p(x‚â§x|y‚â§y‚â§y+h)
= lim
h‚Üí0p(x‚â§x‚à©y‚â§y‚â§y+h)
p[y‚â§y‚â§y+h].
2725.3. conditional pmf and pdf
with some calculations, we have that
lim
h‚Üí0p(x‚â§x‚à©y‚â§y‚â§y+h)
p[y‚â§y‚â§y+h]= lim
h‚Üí0rx
‚àí‚àûry+h
yfx,y(x‚Ä≤, y‚Ä≤)dy‚Ä≤dx‚Ä≤
ry+h
yfy(y‚Ä≤)dy‚Ä≤
= lim
h‚Üí0rx
‚àí‚àûfx,y(x‚Ä≤, y‚Ä≤)dx‚Ä≤¬∑h
fy(y)¬∑h
=rx
‚àí‚àûfx,y(x‚Ä≤, y‚Ä≤)dx‚Ä≤
fy(y).
the key here is that the small step size hin the numerator and the denominator will
cancel each other out. now, given the conditional cdf, we can verify the definition of the
conditional pdf. it holds that
fx|y(x|y) =d
dxfx|y(x|y)
=d
dx(rx
‚àí‚àûfx,y(x‚Ä≤, y)dx‚Ä≤
fy(y))
(a)=fx,y(x, y)
fy(y),
where (a) follows from the fundamental theorem of calculus.
just like the conditional pmf, we can calculate the probabilities using the conditional
pdfs. in particular, if we evaluate the probability where x‚ààagiven that ytakes a
particular value y=y, then we can integrate the conditional pdf fx|y(x|y), with respect
tox.
theorem 5.8. letxandybe continuous random variables, and let abe an event.
(i)p[x‚ààa|y=y] =r
afx|y(x|y)dx,
(ii)p[x‚ààa] =r
œâyp[x‚ààa|y=y]fy(y)dy.
example 5.20 . let xbe a random bit such that
x=(
+1,with prob 1 /2,
‚àí1,with prob 1 /2.
suppose that xis transmitted over a noisy channel so that the observed signal is
y=x+n,
where n‚àºgaussian(0 ,1) is the noise, which is independent of the signal x. find the
probabilities p[x= +1|y >0] and p[x=‚àí1|y >0].
solution . first, we know that
fy|x(y|+ 1) =1‚àö
2œÄe‚àí(y‚àí1)2
2 and fy|x(y| ‚àí1) =1‚àö
2œÄe‚àí(y+1)2
2.
273chapter 5. joint distributions
therefore, integrating yfrom 0 to ‚àûgives us
p[y >0|x= +1] =z‚àû
01‚àö
2œÄe‚àí(y‚àí1)2
2dy
= 1‚àíz0
‚àí‚àû1‚àö
2œÄe‚àí(y‚àí1)2
2dy
= 1‚àíœÜ0‚àí1
1
= 1‚àíœÜ(‚àí1).
similarly, we have p[y >0|x=‚àí1] = 1 ‚àíœÜ(+1). the probability we want to find is
p[x= +1|y >0], which can be determined using bayes‚Äô theorem.
p[x= +1|y >0] =p[y >0|x= +1]p[x= +1]
p[y >0].
the denominator can be found by using the law of total probability:
p[y >0] =p[y >0|x= +1]p[x= +1]
+p[y >0|x=‚àí1]p[x=‚àí1]
= 1‚àí1
2(œÜ(+1) + œÜ( ‚àí1))
=1
2,
since œÜ(+1) + œÜ( ‚àí1) = œÜ(+1) + 1 ‚àíœÜ(+1) = 1. therefore,
p[x= +1|y >0] = 1 ‚àíœÜ(‚àí1)
= 0.8413.
the implication is that if y > 0, the probability p[x= +1|y > 0] = 0 .8413. the
complement of this result gives p[x=‚àí1|y >0] = 1 ‚àí0.8413 = 0 .1587.
practice exercise 5.8 . find p[y > y ], where
x‚àºuniform[1 ,2], y|x‚àºexponential( x).
solution . the tricky part of this problem is the tendency to confuse the two variables
xandy. once you understand their roles the problem becomes easy. first notice that
y|x‚àºexponential( x) is a conditional distribution. it says that given x=x, the
probability distribution of yis exponential, with the parameter x. thus, we have that
fy|x(y|x) =xe‚àíxy.
why? recall that if y‚àºexponential( Œª) then fy(y) =Œªe‚àíŒªy. now if we replace Œª
with x, we have xe‚àíxy. so the role of xin this conditional density function is as a
parameter.
2745.4. conditional expectation
given this property, we can compute the conditional probability:
p[y > y |x=x] =z‚àû
yfy|x(y‚Ä≤|x)dy‚Ä≤
=z‚àû
yxe‚àíxy‚Ä≤dy‚Ä≤=
‚àíe‚àíxy‚Ä≤‚àû
y‚Ä≤=y=e‚àíxy.
finally, we can compute the marginal probability:
p[y > y ] =z
œâxp[y >0|x=x‚Ä≤]fx(x‚Ä≤)dx‚Ä≤
=z1
0e‚àíx‚Ä≤ydx‚Ä≤
=1
ye‚àíx‚Ä≤yx‚Ä≤=1
x‚Ä≤=0=1
y 
1‚àíe‚àíy
.
we can double-check this result by noting that the problem asks about the probability
p[y > y ]. thus, the answer must be a function of ybut not of x.
5.4 conditional expectation
5.4.1 definition
when dealing with two dependent random variables, at times we would like to determine
the expectation of a random variable when the second random variable takes a particular
state. the conditional expectation is a formal way of doing so.
definition 5.18. theconditional expectation ofxgiven y=yis
e[x|y=y] =x
xxpx|y(x|y) (5.25)
for discrete random variables, and
e[x|y=y] =z‚àû
‚àí‚àûxfx|y(x|y)dx (5.26)
for continuous random variables.
there are two points to note here. first, the expectation of e[x|y=y] is taken with respect
tofx|y(x|y). we assume that the random variable yis already fixed at the state y=y.
thus, the only source of randomness is x. secondly, since the expectation e[x|y=y] has
eliminated the randomness of x, the resulting function is in y.
275chapter 5. joint distributions
what is conditional expectation?
¬àe[x|y=y] is the expectation using fx|y(x|y).
¬àthe integration is taken w.r.t. x, because y=yis given and fixed.
5.4.2 the law of total expectation
theorem 5.9. the law of total expectation states that
e[x] =x
ye[x|y=y]py(y),ore[x] =z‚àû
‚àí‚àûe[x|y=y]fy(y)dy. (5.27)
proof . we will prove the discrete case only, as the continuous case can be proved by replacing
summation with integration.
e[x] =x
xxpx(x) =x
xx x
ypx,y(x, y)!
=x
xx
yxpx|y(x|y)py(y)
=x
y x
xxpx|y(x|y)!
py(y) =x
ye[x|y=y]py(y).
‚ñ°
figure 5.11 illustrates the idea behind the proof. essentially, we decompose the expec-
tation e[x] into ‚Äúsubexpectations‚Äù e[x|y=y]. the probability of each subexpectation is
py(y). by summing the subexpectation multiplied by py(y), we obtain the overall expecta-
tion.
figure 5.11: the expectation e[x]can be decomposed into a set of subexpectations. this gives us
e[x] =p
ye[x|y=y]py(y).
2765.4. conditional expectation
what is the law of total expectation?
¬àthe law of total expectation is a decomposition rule.
¬àit decomposes e[x] into smaller/easier conditional expectations.
this law can also be written in a more compact form.
corollary 5.1. letxandybe two random variables. then
e[x] =ey
ex|y[x|y]
. (5.28)
proof . the previous theorem states that e[x] =p
ye[x|y=y]py(y). if we treat e[x|y=
y] as a function of y, for instance h(y), then
e[x] =x
ye[x|y=y]py(y) =x
yh(y)py(y) =e[h(y)] =e[e[x|y]].
‚ñ°
example 5.21 . suppose there are two classes of cars. let xbe the speed of a car
andcbe the class. when c= 1, we know that x‚àºgaussian( ¬µ1, œÉ1). we know that
p[c= 1] = p. when c= 2, x‚àºgaussian( ¬µ2, œÉ2). also, p[c= 2] = 1 ‚àíp. if you see
a car on the freeway, what is its average speed?
solution . the problem has given us everything we need. in particular, we know that
the conditional pdfs are:
fx|c(x|1) =1p
2œÄœÉ2
1exp
‚àí(x‚àí¬µ1)2
2œÉ2
1
,
fx|c(x|2) =1p
2œÄœÉ2
2exp
‚àí(x‚àí¬µ2)2
2œÉ2
2
.
therefore, conditioned on c, we have two expectations:
e[x|c= 1] =z‚àû
‚àí‚àûx fx|c(x|1)dx=¬µ1,
e[x|c= 2] =z‚àû
‚àí‚àûx fx|c(x|2)dx=¬µ2.
the overall expectation e[x] is
e[x] =2x
c=1e[x|c=c]pc(c)
=e[x|c= 1]p[c= 1] + e[x|c= 2]p[c= 2]
=p¬µ1+ (1‚àíp)¬µ2.
277chapter 5. joint distributions
practice exercise 5.9 . consider a joint pmf given by the following table. find
e[x|y= 102] ande[x|y= 104].
y1040 01
121
181
36
102 5
125
185
360 0
0.01 0.1 1 10 100
x
solution . to find the conditional expectation, we first need to know the conditional
pmf.
px|y(x|102) =1
21
31
60 0
,
px|y(x|104) =
0 01
21
31
6
.
therefore, the conditional expectations are
e[x|y= 102] = (10‚àí2)1
2
+ (10‚àí1)1
3
+ (1)1
6
=123
600,
e[x|y= 104] = (1)1
2
+ (10)1
3
+ (100)1
6
=123
6.
from the conditional expectations we can also find e[x]:
e[x] =e[x|y= 102]py(102)
+e[x|y= 104]py(104)
=123
6005
6
+123
61
6
= 3.5875.
example 5.22 . consider two random variables xandy. the random variable x
is gaussian-distributed with x‚àºgaussian( ¬µ, œÉ2). the random variable yhas a
conditional distribution y|x‚àºgaussian( x, x2). find e[y].
solution . the notation y|x‚àºgaussian( x, x2) means that given the variable x,
the other variable yhas a conditional distribution gaussian( x, x2). that is, the
variable yis a gaussian with mean xand variance x2. how can the mean be a
random variable xand the variance be another random variable x2? because xis
the conditional variable. y|xmeans that you have already chosen one state of x.
given that particular state, the distribution of yfollows fy|x. therefore, for this
2785.4. conditional expectation
problem, we know the pdfs:
fx(x) =1‚àö
2œÄœÉ2exp
‚àí(x‚àí¬µ)2
2œÉ2
,
fy|x(y|x) =1‚àö
2œÄx2exp
‚àí(y‚àíx)2
2x2
.
the conditional expectation of ygiven xis
e[y|x=x] =z‚àû
‚àí‚àûy1‚àö
2œÄx2exp
‚àí(y‚àíx)2
2x2
dy
=e[gaussian( x, x2)] =x.
the last equality holds because we are computing the expectation of a gaussian ran-
dom variable with mean x. finally, applying the law of total expectation, we can show
that
e[y] =z‚àû
‚àí‚àûe[y|x=x]fx(x)dx
=z‚àû
‚àí‚àûx1‚àö
2œÄœÉ2exp
‚àí(x‚àí¬µ)2
2œÉ2
dx
=e[gaussian( ¬µ, œÉ2)] =¬µ,
where the last equality is based on the fact that it is the mean of a gaussian.
practice exercise 5.10 . find e[sin(x+y)], if x‚àºgaussian(0 ,1), and y|x‚àº
uniform[ x‚àíœÄ, x+œÄ].
solution . we know that the conditional density is
fy|x(y|x) =1
2œÄ, x ‚àíœÄ‚â§y‚â§x+œÄ.
therefore, we can compute the probability
e[sin(x+y)|x=x] =zx+œÄ
x‚àíœÄsin(x+y)fy|x(y|x)dy
=1
2œÄzx+œÄ
x‚àíœÄsin(x+y)dy
| {z }
=0= 0.
hence, the overall expectation is
e[sin(x+y)] =z1
0e[sin(x+y)|x=x]| {z }
=01‚àö
2œÄe‚àíx2
2dx= 0.
279chapter 5. joint distributions
5.5 sum of two random variables
one typical problem we encounter in engineering is to determine the pdf of the sum of
two random variables xandy, i.e., x+y. such a problem arises naturally when we want
to evaluate the average of many random variables, e.g., the sample mean of a collection of
data points. this section will discuss a general principle for determining the pdf of a sum
of two random variables.
5.5.1 intuition through convolution
first, consider two random variables, xandy, both discrete uniform random variables
in the range of 0 ,1,2,3. that is, px(x) =py(y) = [1 /4,1/4,1/4,1/4]. since this is such a
simple problem we can enumerate all the possible cases of the sum z=x+y. the resulting
probabilities are shown in the following table.
z=x+ycases, written in terms of (x, y) probability
0 (0,0) 1/16
1 (0,1), (1,0) 2/16
2 (1,1), (2,0), (0,2) 3/16
3 (3,0), (2,1), (1,2), (0,3) 4/16
4 (3,1), (2,2), (1,3) 3/16
5 (3,2), (2,3) 2/16
6 (3,3) 1/16
clearly, the pmf of zis not fz(z) =fx(x)+fy(y). (caution! do not write this.) the
pmf of zlooks like a triangle distribution. how can we get to this triangle distribution
from two uniform distributions? the key is the idea of convolution. let us start with the
pmf of x, which is px(x). let us also flip py(y) over the y-axis. as we shift the flipped py,
we multiply and add the pmf values as shown in figure 5.12 . this gives us
pz(0) =p[x+y= 0]
=p[(x, y) = (0 ,0)]
=px(0)py(0)
=1
16.
now, if we shift towards the right by 1, we have
pz(1) =p[x+y= 1]
=p[(x, y) = (0 ,1)‚à™(0,1)]
=px(0)py(1) + px(1)py(0) =2
16.
by continuing our argument, you can see that we will obtain the same pmf as the one
shown in the table.
2805.5. sum of two random variables
figure 5.12: when summing two random variables xandy, we are effectively taking the convolutions
of the two respective pmf / pdfs.
5.5.2 main result
we can show that for any arbitrary random variable xandy, the sum z=x+yhas a
distribution that is the convolution of two individual pdfs.
theorem 5.10. letxandybe two independent random variables with pdfs fx(x)
andfy(y)respectively. let z=x+y. the pdf of zis given by
fz(z) = (fx‚àófy)(z) =z‚àû
‚àí‚àûfx(z‚àíy)fy(y)dy, (5.29)
where ‚Äú ‚àó‚Äù denotes the convolution.
proof . we begin by analyzing the cdf of z. the cdf of zis
fz(z) =p[z‚â§z] =p[x+y‚â§z].
we now draw a picture to illustrate the line under which we want to integrate. as shown in
figure 5.13 , the equation x+y‚â§zdefines a straight line in the xyplane. you can think
of it as y‚â§ ‚àíx+z, so that the slope is ‚àí1 and the y-intercept is z.
now, shall we take the upper half of the triangle or the lower half? since the equation
isy‚â§ ‚àíx+z, a value of yhas to be less than that of the line. another easy way to check
is to assume z >0 so that we have a positive y-intercept. then we check where the origin
281chapter 5. joint distributions
figure 5.13: the shaded region highlights the set x+y‚â§z. to integrate the pdf over this region,
we first take the inner integration over dxand then take the outer integration over dy.
(0,0) belongs. in this case, if z >0, the origin (0 ,0) will satisfy the equation y‚â§ ‚àíx+z,
and so it must be included. thus, we conclude that the area is below the line.
once we have determined the area to be integrated, we can write down the integration:
p[x+y‚â§z] =z‚àû
‚àí‚àûzz‚àíy
‚àí‚àûfx,y(x, y)dx dy
=z‚àû
‚àí‚àûzz‚àíy
‚àí‚àûfx(x)fy(y)dx dy, (independence)
where the integration limits are just a rewrite of x+y‚â§z(in this case since we are
integrating xfirst we have x‚â§ ‚àíy+z). then, by the fundamental theorem of calculus,
we can show that
fz(z) =d
dzfz(z) =d
dzz‚àû
‚àí‚àûzz‚àíy
‚àí‚àûfx(x)fy(y)dx dy
=z‚àû
‚àí‚àûd
dzzz‚àíy
‚àí‚àûfx(x)fy(y)dx
dy
=z‚àû
‚àí‚àûfx(z‚àíy)fy(y)dy= (fx‚àófy)(z),
where ‚Äú ‚àó‚Äù denotes the convolution.
how is convolution related to random variables?
¬àif you sum xandy, the resulting pdf is the convolution of fxandfy.
¬àe.g., convolving two uniform random variables gives you a triangle pdf.
5.5.3 sum of common distributions
theorem 5.11 (sum of two poissons) .letx1‚àºpoisson (Œª1)andx2‚àºpoisson (Œª2).
then
x1+x2‚àºpoisson (Œª1+Œª2). (5.30)
2825.5. sum of two random variables
proof . let us apply the convolution principle.
py(k) =p[x1+x2=k]
=p[x1=‚Ñì‚à©x2=k‚àí‚Ñì]
=kx
‚Ñì=0Œª‚Ñì
1e‚àíŒª1
‚Ñì!¬∑Œªk‚àí‚Ñì
2e‚àíŒª2
(k‚àí‚Ñì)!
=e‚àí(Œª1+Œª2)kx
‚Ñì=0Œª‚Ñì
1
‚Ñì!¬∑Œªk‚àí‚Ñì
2
(k‚àí‚Ñì)!
=e‚àí(Œª1+Œª2)¬∑1
k!kx
‚Ñì=0k!
‚Ñì!(k‚àí‚Ñì)!Œª‚Ñì
1Œªk‚àí‚Ñì
2
| {z }
=pk
‚Ñì=0(k
‚Ñì)Œª‚Ñì
1Œªk‚àí‚Ñì
2
=(Œª1+Œª2)k
k!e‚àí(Œª1+Œª2),
where the last step is based on the binomial identitypk
‚Ñì=0 k
‚Ñì
a‚Ñìbk‚àí‚Ñì= (a+b)k.
‚ñ°
theorem 5.12 (sum of two gaussians ).letx1andx2be two gaussian random
variables such that
x1‚àºgaussian (¬µ1, œÉ2
1)and x 2‚àºgaussian (¬µ2, œÉ2
2).
then
x1+x2‚àºgaussian (¬µ1+¬µ2, œÉ2
1+œÉ2
2). (5.31)
proof . let us apply the convolution principle.
fz(z) =z‚àû
‚àí‚àûfx(t)fy(z‚àít)dt
=z‚àû
‚àí‚àû1‚àö
2œÄœÉ2exp
‚àí(t‚àí¬µ1)2
2œÉ2
¬∑1‚àö
2œÄœÉ2exp
‚àí(z‚àít‚àí¬µ2)2
2œÉ2
dt
=1‚àö
2œÄœÉ2z‚àû
‚àí‚àû1‚àö
2œÄœÉ2exp
‚àí(t‚àí¬µ1)2+ (z‚àít‚àí¬µ2)2
2œÉ2
dt.
we now complete the square:
(t‚àí¬µ1)2+ (z‚àít‚àí¬µ2)2= [t2‚àí2¬µ1t+¬µ2
1] + [t2+ 2t(¬µ2‚àíz) + (¬µ2‚àíz)2]
= 2t2‚àí2t(¬µ1‚àí¬µ2+z) +¬µ2
1+ (¬µ2‚àíz)2
= 2
t2‚àí2t¬∑¬µ1‚àí¬µ2+z
2
+¬µ2
1+ (¬µ2‚àíz)2
= 2
t‚àí¬µ1‚àí¬µ2+z
22
‚àí2¬µ1‚àí¬µ2+z
22
+¬µ2
1+ (¬µ2‚àíz)2.
283chapter 5. joint distributions
the last term can be simplified to
‚àí2¬µ1‚àí¬µ2+z
22
+¬µ2
1+ (¬µ2‚àíz)2
=‚àí¬µ2
1‚àí2¬µ1(¬µ2‚àíz) + (¬µ2‚àíz)2
2+¬µ2
1+ (¬µ2‚àíz)2
=¬µ2
1+ 2¬µ1(¬µ2‚àíz) + (¬µ2‚àíz)2
2=(¬µ1+¬µ2‚àíz)2
2.
substituting these into the integral, we can show that
fz(z) =1‚àö
2œÄœÉ2z‚àû
‚àí‚àû1‚àö
2œÄœÉ2exp(
‚àí2
t‚àí¬µ1‚àí¬µ2+z
22+(¬µ1+¬µ2‚àíz)2
2
2œÉ2)
dt
=1‚àö
2œÄœÉ2exp
‚àí(¬µ1+¬µ2‚àíz)2
2(2œÉ2)z‚àû
‚àí‚àû1‚àö
2œÄœÉ2exp(
‚àí
t‚àí¬µ1‚àí¬µ2+z
22
œÉ2)
dt
| {z }
=1‚àö
2
=1p
2œÄ(2œÉ)2exp
‚àí(¬µ1+¬µ2‚àíz)2
2(2œÉ2)
.
therefore, we have shown that the resulting distribution is a gaussian with mean ¬µ1+¬µ2
and variance 2 œÉ2. ‚ñ°
practice exercise 5.11 . let xandybe independent, and let
fx(x) =(
xe‚àíx, x ‚â•0,
0, x < 0,and fy(y) =(
ye‚àíy, y ‚â•0,
0, y < 0.
find the pdf of z=x+y.
solution . using the results derived above, we see that
fz(z) =z‚àû
‚àí‚àûfx(z‚àíy)fy(y)dy
=zz
‚àí‚àûfx(z‚àíy)fy(y)dy,
where the upper limit zcame from the fact that x‚â•0. therefore, since z=x+y, we
must have z‚àíy=x‚â•0 and so z‚â•y. this is portrayed graphically in figure 5.14 .
substituting the pdfs into the integration yields
fz(z) =zz
0(z‚àíy)e‚àí(z‚àíy)ye‚àíydy=z3
6e‚àíz, z‚â•0.
forz <0,fz(z) = 0.
the functions of two random variables are not limited to summation. the following
example illustrates the case of the product of two random variables.
2845.5. sum of two random variables
figure 5.14: [left] the outer integral goes from 0 to zbecause the triangle stops at y=z. [right] if
the triangle is unbounded, then the integral goes from ‚àí‚àû to‚àû.
example 5.23 . let xandybe two independent random variables such that
fx(x) =(
2x, if 0‚â§x‚â§1,
0, otherwise ,and fy(y) =(
1, if 0‚â§y‚â§1,
0, otherwise .
letz=xy. find fz(z).
solution . the cdf of zcan be evaluated as
fz(z) =p[z‚â§z] =p[xy‚â§z] =z‚àû
‚àí‚àûzz
y
‚àí‚àûfx(x)fy(y)dx dy.
taking the derivative yields
fz(z) =d
dzfz(z) =d
dzz‚àû
‚àí‚àûzz
y
‚àí‚àûfx(x)fy(y)dx dy
(a)=z‚àû
‚àí‚àû1
yfxz
y
fy(y)dy,
where (a) holds by the fundamental theorem of calculus. the upper and lower limit of
this integration can be determined by noting that
0‚â§z
y=x‚â§1,
which implies that z‚â§y. since y‚â§1, we have that z‚â§y‚â§1. therefore, the pdf is
fz(z) =z1
z1
yfxz
y
fy(y)dy
=z1
z2z
y2dy= 2(1 ‚àíz), z‚â•0.
forz <0,fz(z) = 0.
285chapter 5. joint distributions
closing remark . for some random variables, summing two i.i.d. copies remain the same
random variable (but with different parameters). for other random variables, summing
two i.i.d. copies gives a different random variable. table 5.1 summarizes some of the most
commonly used random variable pairs.
x1 x2 sum x1+x2
bernoulli( p) bernoulli( p) binomial(2 , p)
binomial( n, p) binomial( m, p) binomial( m+n, p)
poisson( Œª1) poisson( Œª2) poisson( Œª1+Œª2)
exponential( Œª) exponential( Œª) erlang(2 , Œª)
gaussian( ¬µ1, œÉ2
1) gaussian( ¬µ2, œÉ2
2) gaussian( ¬µ1+¬µ2, œÉ2
1+œÉ2
2)
table 5.1: common distributions of the sum of two random variables.
5.6 random vectors and covariance matrices
we now enter the second part of this chapter. in the first part, we were mainly interested
in a pair of random variables. in the second part, however, we will study vectors of n
random variables. to understand a vector of random variables, we will not drill down to
the integrations of the pdfs (which you would certainly not enjoy). instead, we will blend
linear algebra tools and probabilistic tools to learn a few practical data analysis techniques.
5.6.1 pdf of random vectors
joint distributions can be generalized to more than two random variables. the most conve-
nient way is to consider a vector of random variables and their corresponding states.
x=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞x1
x2
...
xnÔ£π
Ô£∫Ô£∫Ô£∫Ô£ªand x=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞x1
x2
...
xnÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª.
our notation here is unconventional since bold upper case letters usually represent matrices.
here, xdenotes a vector, specifically a random vector. its state is a vector x. in this chapter,
we will use the following notational convention: xandyrepresent random vectors while
arepresents a matrix.
one way to think about xis to imagine that if you put your hand into the sample
space, you will pick up a vector x. this random realization xhasnentries, and so you
need to specify the probability of getting all these entries simultaneously . accordingly, we
should expect that xis characterized by an n-dimensional pdf
fx(x) =fx1,x2,...,x n(x1, x2, . . . , x n).
2865.6. random vectors and covariance matrices
essentially, this pdf tells us the probability density for random variable x1=x1, random
variable x2=x2, etc. it is a coordinate-wise description. for example, if xcontains three
elements such that x= [x1, x2, x3]t, and if the state we are looking at is x= [3,1,7]t,
then fx(x) is the probability density such that this 3d coordinate ( x1, x2, x3) takes the
value [3 ,1,7]t.
to compute the probability, we can integrate fx(x) with respect to x. letabe the
event. then
p[x‚àà a] =z
afx(x)dx
=z
¬∑¬∑¬∑z
afx1,...,x n(x1, . . . , x n)dx1. . . dx n.
if the random coordinates x1, . . . , x nareindependent , the pdf can be written as a prod-
uct of nindividual pdfs:
fx1,...,x n(x1, . . . , x n) =fx1(x1)fx2(x2)¬∑¬∑¬∑fxn(xn),and so
p[x‚àà a] =z
¬∑¬∑¬∑z
afx1(x1)fx2(x2)¬∑¬∑¬∑fxn(xn)dx1¬∑¬∑¬∑dxn.
however, this does not necessarily simplify the calculation unless ais separable, e.g., a=
[a1, b1]√ó[a2, b2]√ó ¬∑¬∑¬∑ √ó [an, bn]. in this case the integration becomes
p[x‚àà a] =ny
i=1"zbi
aifxi(xi)dxi#
,
which is obviously manageable.
example 5.24 . letx= [x1, . . . , x n]tbe a vector of zero-mean unit variance gaus-
sian random vectors. let a= [‚àí1,2]n. then
p[x‚àà a] =z
afx(x)dx
=z
¬∑¬∑¬∑z
afx1,¬∑¬∑¬∑,xn(x1, . . . , x n)dx1¬∑¬∑¬∑dxn
=z2
‚àí1fx1(x1)dx1n
= [œÜ(2) ‚àíœÜ(‚àí1)]n,
where œÜ( ¬∑) is the standard gaussian cdf.
as you can see from the definition of a vector random variable, computing the proba-
bility typically involves integrating a high-dimensional function, which is tedious. however,
the good news is that in practice we seldom need to perform such calculations. often we are
more interested in the mean and the covariance of the random vectors because they usually
carry geometric meanings. the next subsection explores this topic.
287chapter 5. joint distributions
5.6.2 expectation of random vectors
letx= [x1, . . . , x n]tbe a random vector. we define the expectation of a random vector
as follows.
definition 5.19. letx= [x1, . . . , x n]tbe a random vector. the expectation is
¬µdef=e[x] =Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞e[x1]
e[x2]
...
e[xn]Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª. (5.32)
the resulting vector is called the mean vector . since the mean vector is a vector of
individual elements, we need to compute the marginal pdfs before computing the expec-
tations:
e[x] =Ô£Æ
Ô£ØÔ£∞e[x1]
...
e[xn]Ô£π
Ô£∫Ô£ª=Ô£Æ
Ô£ØÔ£∞r
œâx1fx1(x1)dx1
...r
œâxnfxn(xn)dxnÔ£π
Ô£∫Ô£ª,
where the marginal pdf is determined by
fxn(xn) =z
œâfx\n(x\n)dx\n.
in the equation above, x\n= [x1, . . . , x n‚àí1, xn+1, . . . , x n]tcontains all the elements with-
outxn. for example, if the pdf is fx1,x2,x3(x1, x2, x3), then
e[x1] =z
x1z
fx1,x2,x3(x1, x2, x3)dx2dx3
| {z }
fx1(x1)dx1.
again, this will become tedious when there are many variables.
while the definition of the expectation may be challenging to understand, some prob-
lems using it are straightforward. we will first demonstrate the case of independent poisson
random variables, and then we will discuss joint gaussians.
example 5.25 . letx= [x1, . . . , x n]tbe a random vector such that xnare inde-
pendent poissons with xn‚àºpoisson( Œªn). then
e[x] =Ô£Æ
Ô£ØÔ£∞e[x1]
...
e[xn]Ô£π
Ô£∫Ô£ª=Ô£Æ
Ô£ØÔ£ØÔ£∞p‚àû
k=0k¬∑Œªk
1e‚àíŒª1
k!...p‚àû
k=0k¬∑Œªk
ne‚àíŒªn
k!Ô£π
Ô£∫Ô£∫Ô£ª=Ô£Æ
Ô£ØÔ£∞Œª1
...
ŒªnÔ£π
Ô£∫Ô£ª.
on computers, computing the mean vector can be done using built-in commands such
asmean in matlab and np.mean in python. however, caution is needed when performing
the calculation. in matlab, mean computes along first dimension (rows index). thus, if we
2885.6. random vectors and covariance matrices
have an n√ó2 array, applying mean will give us a 1 √ó2 vector. to obtain the column mean
vector of size n√ó1, we need to specify the direction as mean(x,2) . similarly, in python,
when calling np.mean , we need to specify the axis.
% matlab code to compute a mean vector
x = randn(100,2);
mx = mean(x,2);
# python code to compute a mean vector
import numpy as np
import scipy.stats as stats
x = stats.multivariate_normal.rvs([0,0],[[1,0],[0,1]],100)
mx = np.mean(x,axis=1)
5.6.3 covariance matrix
definition 5.20. thecovariance matrix of a random vector x= [x1, . . . , x n]tis
œÉdef= cov( x) =Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞var[x1] cov( x1, x2)¬∑¬∑¬∑ cov(x1, xn)
cov[x2, x1] var[ x2]¬∑¬∑¬∑ cov(x2, xn)
............
cov(xn, x1) cov( xn, x2)¬∑¬∑¬∑ var[xn]Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª. (5.33)
a more compact way of writing the covariance matrix is
œÉ= cov( x) =e[(x‚àí¬µ)(x‚àí¬µ)t],
where ¬µ=e[x] is the mean vector. the notation abtmeans the outer product , defined
as
abt=Ô£Æ
Ô£ØÔ£∞a1
...
anÔ£π
Ô£∫Ô£ªb1¬∑¬∑¬∑bn
=Ô£Æ
Ô£ØÔ£∞a1b1a1b2¬∑¬∑¬∑ a1bn
............
anb1anb2¬∑¬∑¬∑anbnÔ£π
Ô£∫Ô£ª.
it is easy to show that cov( x) = cov( x)t, i.e., they are symmetric.
theorem 5.13. if the coordinates x1, . . . , x nare independent, then the covariance
matrix cov(x) =œÉis a diagonal matrix:
œÉ= cov( x) =Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞var[x1] 0 ¬∑¬∑¬∑ 0
0 var[ x2]¬∑¬∑¬∑ 0
............
0 0 ¬∑¬∑¬∑ var[xn]Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
289chapter 5. joint distributions
proof . if all xi‚Äôs are independent, then cov( xi, xj) = 0 for all iÃ∏=j. substituting this
into the definition of the covariance matrix, we obtain the result.
‚ñ°
if we ignore the mean vector ¬µ, we obtain the autocorrelation matrix r.
definition 5.21. letx= [x1, . . . , x n]tbe a random vector. the autocorrelation
matrix is
r=e[xxt] =Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞e[x1x1]e[x1x2]¬∑¬∑¬∑e[x1xn]
e[x2x1]e[x2x2]¬∑¬∑¬∑e[x2xn]
............
e[xnx1]e[xnx2]¬∑¬∑¬∑e[xnxn]Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª. (5.34)
we state without proof that
œÉ=r‚àí¬µ¬µt,
which corresponds to the single-variable case where œÉ2=e[x2]‚àí¬µ2.
on computers, computing the covariance matrix is done using built-in commands cov
in matlab and np.cov in python. like the mean vectors, when computing the covariance,
we need to specify the direction. for example, for an n√ó2 data matrix x, the covariance
needs to be a 2 √ó2 matrix. if we compute the covariance along the wrong direction, we will
obtain an n√ónmatrix, which is incorrect.
% matlab code to compute covariance matrix
x = randn(100,2);
covx = cov(x);
# python code to compute covariance matrix
import numpy as np
import scipy.stats as stats
x = stats.multivariate_normal.rvs([0,0],[[1,0],[0,1]],100)
covx = np.cov(x,rowvar=false)
print(covx)
5.6.4 multidimensional gaussian
with the above tools in hand, we can now define a high-dimensional gaussian. the pdf of
a high-dimensional gaussian is defined as follows.
definition 5.22. ad-dimensional joint gaussian has the pdf
fx(x) =1p
(2œÄ)d|œÉ|exp
‚àí1
2(x‚àí¬µ)tœÇ‚àí1(x‚àí¬µ)
, (5.35)
where ddenotes the dimensionality of the vector x.
2905.6. random vectors and covariance matrices
the mean vector and the covariance matrix of a joint gaussian is readily available from the
definition.
e[x] =¬µ and cov( x) =œÉ.
it is easy to show that if xis a scalar x, then d= 1,¬µ=¬µ, and œÉ=œÉ2. substituting
these into the above definition returns us the familiar 1d gaussian.
thed-dimensional gaussian is a generalization of the 1d gaussian(s). suppose that xi
andxjareindependent for all iÃ∏=j. then e[xixj] =e[xi]e[xj] and hence cov( xi, xj) =
0. consequently, the covariance matrix œÉis a diagonal matrix:
œÉ=Ô£Æ
Ô£ØÔ£∞œÉ2
1¬∑¬∑¬∑ 0
.........
0¬∑¬∑¬∑œÉ2
dÔ£π
Ô£∫Ô£ª,
where œÉ2
i= var[ xi]. when this occurs, the exponential term in the gaussian pdf is
(x‚àí¬µ)tœÇ‚àí1(x‚àí¬µ) =Ô£Æ
Ô£ØÔ£∞x1‚àí¬µ1
...
xd‚àí¬µdÔ£π
Ô£∫Ô£ªtÔ£Æ
Ô£ØÔ£∞œÉ2
1¬∑¬∑¬∑ 0
.........
0¬∑¬∑¬∑œÉ2
dÔ£π
Ô£∫Ô£ª‚àí1Ô£Æ
Ô£ØÔ£∞x1‚àí¬µ1
...
xd‚àí¬µdÔ£π
Ô£∫Ô£ª=dx
i=1(xi‚àí¬µi)2
œÉ2
i.
moreover, the determinant |œÉ|is
|œÉ|=Ô£Æ
Ô£ØÔ£∞œÉ2
1¬∑¬∑¬∑ 0
.........
0¬∑¬∑¬∑œÉ2
dÔ£π
Ô£∫Ô£ª=dy
i=1œÉ2
i.
substituting these results into the joint gaussian pdf, we obtain
fx(x) =ny
i=11p
(2œÄ)œÉ2
iexp
‚àí(x‚àí¬µi)2
2œÉ2
i
,
which is a product of individual gaussians.
the gaussian has different offsets and orientations for different choices of ¬µandœÇ.
figure 5.15 shows a few examples. note that for œÉto be valid œÉhas to be ‚Äúsymmetric
positive semi-definite‚Äù, the meaning of which will be explained shortly.
generating random numbers from a multidimensional gaussian can be done by calling
built-in commands. in matlab, we use mvnrnd . in python, we have a similar command.
% matlab code to generate random numbers from multivariate gaussian
mu = [0 0];
sigma = [.25 .3; .3 1];
x = mvnrnd(mu,sigma,100);
# python code to generate random numbers from multivariate gaussian
import numpy as np
import scipy.stats as stats
x = stats.multivariate_normal.rvs([0,0],[[0.25,0.3],[0.3,1.0]],100)
291chapter 5. joint distributions
-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345
-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345
-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345
(¬µ,œÉ) =0
2
,5 0
0 0.5
(¬µ,œÉ) =1
2
,1‚àí0.5
‚àí0.5 1
(¬µ,œÉ) =0
0
,2 1 .9
1.9 2
figure 5.15: visualization of 2d gaussians with different means and covariances.
to display the data points and overlay with the contour, we can use matlab com-
mands such as contour . the resulting plot looks like the one shown in figure 5.16 . in
python the corresponding command is plt.contour . to set up the plotting environment
we use the commands np.meshgrid . the grid points are used to evaluate the pdf values,
thus giving us the contour.
% matlab code: overlay random numbers with the gaussian contour.
x = mvnrnd([0 0],[.25 .3; .3 1],1000);
x1 = -2.5:.01:2.5;
x2 = -3.5:.01:3.5;
[x1,x2] = meshgrid(x1,x2);
f = mvnpdf([x1(:) x2(:)],[0 0],[.25 .3; .3 1]);
f = reshape(f,length(x2),length(x1));
figure(1);
scatter(x(:,1),x(:,2),‚Äôrx‚Äô, ‚Äôlinewidth‚Äô, 1.5); hold on;
contour(x1,x2,f,[.001 .01 .05:.1:.95 .99 .999], ‚Äôlinewidth‚Äô, 2);
# python code: overlay random numbers with the gaussian contour.
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
x = stats.multivariate_normal.rvs([0,0],[[0.25,0.3],[0.3,1.0]],1000)
x1 = np.arange(-2.5, 2.5, 0.01)
x2 = np.arange(-3.5, 3.5, 0.01)
x1, x2 = np.meshgrid(x1,x2)
xpos = np.empty(x1.shape + (2,))
xpos[:,:,0] = x1
xpos[:,:,1] = x2
f = stats.multivariate_normal.pdf(xpos,[0,0],[[0.25,0.3],[0.3,1.0]])
plt.scatter(x[:,0],x[:,1])
plt.contour(x1,x2,f)
2925.7. transformation of multidimensional gaussians
-2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5
x-3-2-10123y
figure 5.16: 1000 random numbers drawn from a 2d gaussian, overlaid with the contour plot.
5.7 transformation of multidimensional gaussians
as we have seen in figure 5.15 , the shape and orientation of a multidimensional gaussian
are determined by the mean vector ¬µand the covariance matrix œÉ. this means that if we
can somehow transform the mean vector and the covariance matrix, we will get another
gaussian. a few practical questions are:
¬àhow do we shift and rotate a gaussian random variable?
¬àif we have an arbitrary gaussian, how do we go back to zero-mean unit-variance
gaussian?
¬àhow do we generate random vectors according to a predefined gaussian?
these questions come up frequently in data analysis. answering the first two questions will
help us transform gaussians back and forth, while answering the last question will help us
with generating random samples.
5.7.1 linear transformation of mean and covariance
suppose we have an arbitrary (not necessarily a gaussian) random vector x= [x1, . . . , x n]t
with mean ¬µxand covariance œÉx. entries of xare not necessarily independent. let
a‚ààrn√ónbe a transformation, and let y=ax. that is,
y=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞y1
y2
...
ynÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞a11a12¬∑¬∑¬∑ a1n
a21a22¬∑¬∑¬∑ a2n
............
an1an2¬∑¬∑¬∑annÔ£π
Ô£∫Ô£∫Ô£∫Ô£ªÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£∞x1
x2
...
xnÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª=ax.
then we can show the following result.
293chapter 5. joint distributions
theorem 5.14. the mean vector and covariance matrix of y=ax are
¬µy=a¬µx, œÉy=aœÉxat. (5.36)
proof . we first show the mean. consider the nth element of y:
e[yn] =e"nx
k=1ankxk#
=nx
k=1anke[xk].
therefore,
¬µy=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞e[y1]
e[y2]
...
e[yn]Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞pn
k=1a1ke[xk]pn
k=1a2ke[xk]
...pn
k=1anke[xk]Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞a11a12¬∑¬∑¬∑ a1n
a21a22¬∑¬∑¬∑ a2n
............
an1an2¬∑¬∑¬∑annÔ£π
Ô£∫Ô£∫Ô£∫Ô£ªÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£∞e[x1]
e[x2]
...
e[xn]Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª=a¬µx.
the covariance matrix follows from the fact that
œÉy=e[(y‚àí¬µy)(y‚àí¬µy)t]
=e[(ax‚àía¬µx)(ax‚àía¬µx)t]
=e[a(x‚àí¬µx)(x‚àí¬µx)tat]
=ae[(x‚àí¬µx)(x‚àí¬µx)t]at
=aœÉxat.
‚ñ°
what if we shift the random vector by defining y=x+b? we state the following
result without proof (try proving it as an exercise).
theorem 5.15. the mean vector and covariance matrix of y=x+bare
¬µy=¬µx+b, œÉy=œÉx. (5.37)
for a gaussian random vector, the linear transformations either shifts the gaussian or
rotates the gaussian, as shown in figure 5.17 :
¬àif we add btox, the resulting operation is a translation.
¬àif we multiply abyx, then the resulting operation is a rotation and scaling.
2945.7. transformation of multidimensional gaussians
figure 5.17: transforming a gaussian. [left] translation by a vector b. [right] rotation and scaling by
a matrix x.
how to rotate, scale, and translate a gaussian random variable
¬àwe rotate and scale a gaussian by y=ax.
¬àwe translate a gaussian by y=x+b.
5.7.2 eigenvalues and eigenvectors
as our next step, we need to understand eigendecomposition . you can easily find relevant
background in any undergraduate linear algebra textbook. here we provide a summary for
completeness.
when applying a matrix ato a vector x, a typical engineering question is: what x
would be invariant to a? or in other words, for what xcan we make sure that ax=Œªx,
for some scalar Œª? if we can find such a vector x, we say that xis the eigenvector ofa.
eigenvectors are useful for seeking principal components of datasets or finding efficient signal
representations. they are defined as follows:
definition 5.23. given a square matrix a‚ààrn√ón, the vector u‚ààrn(with uÃ∏=0)
is called the eigenvector ofaif
au=Œªu, (5.38)
for some Œª‚ààr. the scalar Œªis called the eigenvalue associated with u.
ann√ónmatrix has neigenvectors and neigenvalues. therefore, the above equation can
be generalized to
aui=Œªiui,
fori= 1, . . . , n , or more compactly as au = Œªu. the eigenvalues Œª1, . . . , Œª nare not
necessarily distinct. there are matrices with identical eigenvalues, the identity matrix being
a trivial example. on the other hand, not all square matrices have eigenvectors. for example,
the matrix0 1
0 0
does not have an eigenvalue. matrices that have eigenvalues must be
diagonalizable .
295chapter 5. joint distributions
there are a number of equivalent conditions for Œªto be an eigenvalue:
‚Ä¢ there exists uÃ∏= 0 such that au=Œªu;
‚Ä¢ there exists uÃ∏= 0 such that ( a‚àíŒªi)u=0;
‚Ä¢ (a‚àíŒªi) is not invertible;
‚Ä¢ det(a‚àíŒªi) = 0.
we are mostly interested in symmetric matrices. if ais symmetric, then all the eigen-
values are real, and the following result holds.
theorem 5.16. ifais symmetric, all the eigenvalues are real, and there exists u
such that utu=ianda=uŒªut. then
Ô£Æ
Ô£∞| | |
a1a2¬∑¬∑¬∑an
| | |Ô£π
Ô£ª
| {z }
a=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞| | |
u1u2¬∑¬∑¬∑un
| | |Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
| {z }
uÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£∞Œª1
Œª2
...
ŒªnÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª
| {z }
ŒªÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£∞‚Äîut
1‚Äî
‚Äîut
2‚Äî
...
‚Äîut
n ‚ÄîÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª
| {z }
ut.
(5.39)
we call such a decomposition the eigendecomposition . in matlab, we can compute the
eigenvalues of a matrix by using the eigcommand. in python, the corresponding command
isnp.linalg.eig . note that in our demonstration below we symmetrize the matrix. this
step is needed, for otherwise the eigenvalues will contain complex numbers.
% matlab code to perform eigendecomposition
a = randn(100,100);
a = (a + a‚Äô)/2; % symmetrize because a is not symmetric
[u,s] = eig(a); % eigendecomposition
s = diag(s); % extract eigenvalue
# python code to perform eigendecomposition
import numpy as np
a = np.random.randn(100,100)
a = (a + np.transpose(a))/2
s, u = np.linalg.eig(a)
s = np.diag(s)
the condition that utu=iis the result of an orthonormal matrix. equivalently,
ut
iuj= 1 if i=jandut
iuj= 0 if iÃ∏=j. since {ui}n
i=1is orthonormal, it can serve as a
basis of any vector in rn:
x=nx
j=1Œ±juj,
where Œ±j=ut
jxis called the basis coefficient . basis vectors are useful in that they can
provide alternative representations of a vector.
2965.7. transformation of multidimensional gaussians
figure 5.18: the center and the radius of the ellipse is determined by ¬µandœÇ.
the geometry of the joint gaussian is determined by its eigenvalues and eigenvectors.
consider the eigendecomposition of œÉ:
œÉ=uŒªut
=Ô£Æ
Ô£∞| | |
u1u2¬∑¬∑¬∑ud
| | |Ô£π
Ô£ªÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£∞Œª10¬∑¬∑¬∑ 0
0Œª2¬∑¬∑¬∑ 0
............
0¬∑¬∑¬∑ ¬∑¬∑¬∑ ŒªdÔ£π
Ô£∫Ô£∫Ô£∫Ô£ªÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£∞‚àíut
1‚àí
‚àíut
2‚àí
...
‚àíut
d‚àíÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª,
for some unitary matrix uand diagonal matrix Œª. the columns of uare called the eigen-
vectors, and the entries of Œªare called the eigenvalues. since œÉis symmetric, all Œªi‚Äôs are
real. in addition, since œÉis positive semi-definite, all Œªi‚Äôs are non-negative. accordingly, the
volume defined by the multidimensional gaussian is always a convex object, e.g., an ellipse
in 2d or an ellipsoid in 3d.
the orientation of the axes is defined by the column vectors ui. in the case of d= 2,
the major axis is defined by u1and the minor axis is defined by u2. the corresponding radii
of each axis are specified by the eigenvalues Œª1andŒª2.figure 5.18 provides an illustration.
5.7.3 covariance matrices are always positive semi-definite
the following subsection about positive semi-definite matrices can be skipped if it is your
first time reading the book.
now that we understand eigendecomposition, what can we do with it? here is one practical
problem. given a matrix œÉ, how do you know whether this œÉis valid? for example, if we
give you a singular matrix, then œÉ‚àí1may not exist. checking the validity of œÉrequires the
concept of positive semi-definite .
given a square matrix a‚ààrn√ón, it is important to check the positive semi-definiteness
ofa. there are two practical scenarios where we need positive semi-definiteness. (1) if
we are estimating the covariance matrix œÉfrom a dataset, we need to ensure that œÉ=
e[(x‚àí¬µ)(x‚àí¬µ)t] is positive semi-definite because all covariance matrices are positive
297chapter 5. joint distributions
semi-definite. otherwise, the matrix we estimate is not a legitimate covariance matrix. (2)
if we solve an optimization problem involving a function f(x) =xtax, then having a
being positive semi-definite, we can guarantee that the problem is convex. convex problems
ensure that a local minimum is also global, and convex problems can be solved efficiently
using known algorithms.
definition 5.24 (positive semi-definite ).a matrix a‚ààrn√ónis positive semi-
definite if
xtax‚â•0 (5.40)
for any x‚ààrn.aispositive definite ifxtax>0for any x‚ààrn.
using eigendecomposition, it is not difficult to show that positive semi-definiteness is equiv-
alent to having non-negative eigenvalues.
theorem 5.17. a matrix a‚ààrn√ónispositive semi-definite if and only if
Œªi(a)‚â•0 (5.41)
for all i= 1, . . . , n , where Œªi(a)denotes the ith eigenvalue of a.
proof . by the definitions of eigenvalue and eigenvector, we have that
aui=Œªiui,
where Œªiis the eigenvalue and uiis the corresponding eigenvector. if ais positive semi-
definite, then ut
iaui‚â•0 since uiis a particular vector in rn. so we have
0‚â§ut
iaui=Œª‚à•ui‚à•2,
and hence Œªi‚â•0. conversely, if Œªi‚â•0 for all i, then since a=pn
i=1Œªiuiut
iwe can
conclude that
xtax=xt nx
i=1Œªiuiut
i!
x=nx
i=1Œªi(ut
ix)2‚â•0.
‚ñ°
the following corollary shows that if a‚ààrn√ónis positive definite, it must be invert-
ible. being invertible also means that the columns of aare linearly independent.
corollary 5.2. if a matrix a‚ààrn√ónispositive definite (but not semi-definite),
thenamust be invertible, i.e., there exists a‚àí1‚ààrn√ónsuch that
a‚àí1a=aa‚àí1=i. (5.42)
the next theorem tells us that the covariance matrix is always positive semi-definite.
2985.7. transformation of multidimensional gaussians
theorem 5.18. the covariance matrix cov(x) =œÉissymmetric positive semi-
definite , i.e.,
œÉt=œÉ,and vtœÉv‚â•0,‚àÄv‚ààrd.
proof . symmetry follows immediately from the definition, because cov( xi, xj) = cov( xj, xi).
the positive semi-definiteness comes from the fact that
vtœÉv=vte[(x‚àí¬µ)(x‚àí¬µ)t]v
=e[vt(x‚àí¬µ)(x‚àí¬µ)tv]
=e[btb] =e[‚à•b‚à•2]‚â•0,
where b= (x‚àí¬µ)tv. ‚ñ°
end of the discussion.
5.7.4 gaussian whitening
besides checking positive semi-definiteness, another typical problem we encounter is how to
generate random samples according to some gaussian distributions.
from gaussian (0,i)to gaussian (¬µ,œÉ). if we are given zero-mean unit-variance gaus-
sianx‚àºgaussian( 0,i), how do we generate y‚àºgaussian( ¬µ,œÉ) from x?
the idea is to define a transformation
y=œÉ1
2x+¬µ,
where œÉ1
2=uŒª1
2ut. then the mean of yis
e[y] =e[œÉ1
2x+¬µ] =œÉ1
2e[x] +¬µ=œÉ1
20+¬µ=¬µ,
and the covariance matrix is
e[(y‚àí¬µ)(y‚àí¬µ)t] =e[(œÉ1
2x+¬µ‚àí¬µ)(œÉ1
2x+¬µ‚àí¬µ)t]
=e[(œÉ1
2x)(œÉ1
2x)t] =œÉ1
2e[xxt]œÉ1
2
=œÉ1
2iœÇ1
2=œÉ.
the following theorem summarizes this result.
theorem 5.19. letxbex‚àºgaussian (0,i). consider a mean vector ¬µand a
covariance matrix œÉwith eigendecomposition œÉ=uŒªut. if
y=œÉ1
2x+¬µ, (5.43)
where œÉ1
2=uŒª1
2ut, then y‚àºgaussian (¬µ,œÉ).
299chapter 5. joint distributions
therefore, the two steps for doing this gaussian whitening are:
¬àstep 1: generate samples {x1, . . . ,xn}that are distributed according to gaussian( 0,i).
¬àstep 2: define ynwhere
yn=œÉ1
2xn+¬µ.
these two steps are portrayed in figure 5.19 .
figure 5.19: generating an arbitrary gaussian from gaussian (0,i).
example 5.26 . consider a set of n= 1000 i.i.d. gaussian( 0,i) data points as shown
infigure 5.20 , for example,
x1=0.5377
1.8399
,x2=‚àí2.2588
0.8622
, . . . , x1000=0.3188
‚àí1.3077
.
-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345
-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345
(a) before (b) after
figure 5.20: generating arbitrary gaussian random variables from gaussian (0,i).
transform these data points so that the new distribution is a gaussian with
¬µ=1
‚àí2
and œÉ=3‚àí0.5
‚àí0.5 1
.
3005.7. transformation of multidimensional gaussians
solution . to perform the transformation, we first perform eigendecomposition of œÉ=
uŒªut. then œÉ1
2=uŒª1
2ut. for our problem, we compute
œÉ1
2=1.722 ‚àí0.1848
‚àí0.1848 0 .9828
.
multiplying this matrix to yield yn=œÉ1
2xn+¬µ, we obtain
y1=1.5870
‚àí0.2971
,y2=‚àí3.0495
‚àí0.7351
, . . . , y1000=1.7907
‚àí3.3441
.
in matlab, the above whitening procedure can be realized using the following com-
mands.
% matlab code to perform the whitening
x = mvnrnd([0,0],[1 0; 0 1],1000);
sigma = [3 -0.5; -0.5 1];
mu = [1; -2];
y = sigma^(0.5)*x‚Äô + mu;
the python implementation is similar, although one needs to be careful with the
more complicated syntax. for example, sigma^(0.5) in matlab does the eigen-based
matrix power automatically, whereas in python we need to call a specific built-in command
fractional_matrix_power . in matlab, broadcasting a vector to a matrix can be rec-
ognized. in python, we need to call repmat explicitly to control the shape of the mean
vectors.
# python code to perform the whitening
import numpy as np
import scipy.stats as stats
from scipy.linalg import fractional_matrix_power
x = np.random.multivariate_normal([0,0],[[1,0],[0,1]],1000)
mu = np.array([1,-2])
sigma = np.array([[3, -0.5],[-0.5, 1]])
sigma2 = fractional_matrix_power(sigma,0.5)
y = np.dot(sigma2, x.t) + np.matlib.repmat(mu,1000,1).t
from gaussian (¬µ,œÉ)to gaussian (0,i). the reverse direction can be done as follows.
supposing that we have y‚àºgaussian( ¬µ,œÉ), we define
x=œÉ‚àí1
2(y‚àí¬µ). (5.44)
then
e[x] =e[œÉ‚àí1
2(y‚àí¬µ)]
=œÉ‚àí1
2(e[y]‚àí¬µ) =0.
301chapter 5. joint distributions
the covariance is
cov(x) =e[(x‚àí¬µx)(x‚àí¬µx)t]
=e[xxt]
=eh
œÉ‚àí1
2(y‚àí¬µ)(y‚àí¬µ)tœÇ‚àít
2i
=œÉ‚àí1
2e
(y‚àí¬µ)(y‚àí¬µ)t
œÉ‚àít
2
=œÉ‚àí1
2œÉœÇ‚àí1
2=i.
the following theorem summarizes this result.
theorem 5.20. letybe a gaussian y‚àºgaussian (¬µ,œÉ). if
x=œÉ‚àí1
2(y‚àí¬µ), (5.45)
thenx‚àºgaussian (0,i).
thus the two steps of doing this reversed gaussian whitening are:
¬àstep 1: assuming that y1, . . . ,ynare distributed as gaussian( ¬µ,œÉ), estimate ¬µ
andœÇ.
¬àstep 2: define xnwhere
xn=œÉ1
2(yn‚àí¬µ). (5.46)
these two steps are shown pictorially in figure 5.21 .
figure 5.21: converting an arbitrary gaussian back to gaussian (0,i).
in practice, if we are given {yn}n
n=1, we need to estimate ¬µandœÇ. the estimations
are quite straightforward.
b¬µ=1
nnx
n=1yn,
bœÇ=1
nnx
n=1(yn‚àíb¬µ)(yn‚àíb¬µ)t.
3025.8. principal-component analysis
on computers, these can be obtained using the command mean and cov. once we have
calculated b¬µandbœÇ, we can define xnas
xn=bœÇ‚àí1
2(yn‚àíb¬µ).
on computers, the codes for the whitening procedure that uses the estimated mean
and covariance are shown below.
% matlab code to perform whitening
y = mvnrnd([1; -2],[3 -0.5; -0.5 1],100);
my = mean(y);
covy = cov(y);
x = covy^(-0.5)*(y-my)‚Äô;
# python code to perform whitening
import numpy as np
import scipy.stats as stats
from scipy.linalg import fractional_matrix_power
y = np.random.multivariate_normal([1,-2],[[3,-0.5],[-0.5,1]],100)
my = np.mean(y,axis=0)
covy = np.cov(y,rowvar=false)
covy2 = fractional_matrix_power(covy,-0.5)
x = np.dot(covy2, (y-np.matlib.repmat(my,100,1)).t)
5.8 principal-component analysis
we have studied the covariance matrix œÉin some depth. it has many other uses besides
transforming gaussian random variables, and in this section we present one of them, called
theprincipal-component analysis (pca). pca is a widely used tool for dimension reduc-
tion. instead of using nfeatures to describe a data point, pca allows us to use the leading
pprincipal components to describe the same data point. in many problems in machine
learning, this makes the learning task easier and the inference task more efficient.
5.8.1 the main idea: eigendecomposition
pca can be summarized in one sentence:
the key idea of pca is the eigendecomposition of the covariance matrix œÉ.
this is a condensed summary of pca: it is just the eigendecomposition of the co-
variance. however, before we discuss the computational procedure, we will explain why we
would want to perform the eigendecomposition of the covariance matrix.
303chapter 5. joint distributions
consider a set of data points {x(1), . . . ,x(n)}, where each x(n)‚ààrdis ad-dimensional
vector. the dimension dis often high. for example, if we have an image of size 1024 √ó1024√ó3,
then d= 3,145,728 ‚Äî not a huge number, but enough to make you feel dizzy. the goal
of pca is to find a low-dimensional representation inrpwhere p‚â™d. if we can find
this low-dimensional representation, we can represent the d-dimensional input using only p
coefficients. since p‚â™d, we can ‚Äúcompress‚Äù the data by using a compact representation. in
modern data science, such a dimension reduction scheme is useful for handling large-scale
datasets.
mathematically, we define a set of basis vector v1, . . . ,vp, where each vi‚ààrd. our
goal is to approximate an input data point x(n)‚ààrdby these basis vectors:
x(n)‚âàpx
i=1Œ±ivi,
where {Œ±i}p
i=1are called the representation coefficients . the representation described by
this equation is a linear representation. linear representation is extremely common in prac-
tice. for example, a data point x(n)= [7,1,4]tcan be represented as
Ô£Æ
Ô£∞7
1
4Ô£π
Ô£ª
|{z}
x(n)= 3|{z}
Œ±1Ô£Æ
Ô£∞1
‚àí1
0Ô£π
Ô£ª
|{z}
v1+ 4|{z}
Œ±2Ô£Æ
Ô£∞1
1
1Ô£π
Ô£ª
|{z}
v2.
therefore, the 3-dimensional input x(n)can now be represented by two coefficients Œ±1= 3
andŒ±2= 4. this is called dimensionality reduction .
pictorially, if we have already determined the basis vectors, we can compute the co-
efficients for every data point in the dataset. however, not all basis vectors are good. as
illustrated in figure 5.22 , an elongated dataset will be of the greatest benefit if the basis
vectors are oriented according to the data geometry. if we can find such basis vectors, then
the data points will have a large coefficient and a small coefficient, corresponding to the
major and the minor axes. dimensionality reduction can thus be achieved by, for example,
only keeping the larger coefficients.
figure 5.22: pca aims at finding a low-dimensional representation of a high-dimensional dataset. in
this figure, the 2d data points can be well represented by the 1d space spanned by v1.
the challenge here is that, given the dataset {x(1), . . . ,x(n)}, we need to determine
both the basis vectors {vi}p
i=1and the coefficients {Œ±i}p
i=1. fortunately, this can be formu-
lated as an eigendecomposition problem.
3045.8. principal-component analysis
to see how this problem can be thus formulated, we consider the simplest case as
illustrated in figure 5.22 , where we want to find theleading principal component. that is,
we find ( Œ±,v) such that x‚âàŒ±v. this amounts to solving the optimization problem
(bv,bŒ±) = argmin
‚à•v‚à•2=1,Œ±Ô£Æ
Ô£∞|
x
|Ô£π
Ô£ª‚àíŒ±Ô£Æ
Ô£∞|
v
|Ô£π
Ô£ª2
.
the notation ‚Äúargmin‚Äù means the argument that minimizes the function. the equation
says that we find the ( Œ±,v) that minimizes the distance between xandŒ±v. the constraint
‚à•v‚à•2= 1 limits the search to within a unit circle; otherwise our solution will not be unique.
solving the optimization problem is not difficult. if we take the derivative w.r.t. Œ±and
set it to zero, we have that
2vt(x‚àíŒ±v) = 0 ‚áí Œ±=vtx.
substituting Œ±=xtvinto the objective function again, we show that
argmin
‚à•v‚à•2=1‚à•x‚àíŒ±v‚à•2= argmin
‚à•v‚à•2=1
xtx‚àí2Œ±xtv+Œ±2vtv
,‚à•v‚à•2= 1
= argmin
‚à•v‚à•2=1
‚àí2Œ±xtv+Œ±2
, dropxtx
= argmin
‚à•v‚à•2=1
‚àí2(xtv)xtv+ (xtv)2
, substitute Œ±=xtv
= argmax
‚à•v‚à•2=1
vtxxtv
, change min to max .
let us pause for a second. we have shown that if we have onedata point x, the leading
principal component vcan be determined by maximizing vtxxtv. what have we gained?
we have transformed the original optimization, which contains two variables ( v, Œ±), to a new
optimization that contains one variable v. thus if we know how to solve the one-variable
problem we are done.
however, there is one more issue we need to address before we discuss how to solve
for the problem. the issue is that the formulation is about one data sample , not the entire
dataset. to include all the samples, we need to assume that xis a realization of a random
vector x. then the above optimization can be formulated in the expectation sense as
argmin
‚à•v‚à•2=1e‚à•x‚àíŒ±v‚à•2= argmax
‚à•v‚à•2=1vte
xxt
v
= argmax
‚à•v‚à•2=1vtœÉv,
where œÉdef=e[xtx].1therefore, if we can maximize vtœÉvwe will be able to determine
the principal component.
now comes the main result. the following theorem shows that the maximization is
equivalent to eigendecomposition. the proof requires lagrange multipliers, which are beyond
the scope of this book.
1here we assume that xis zero-mean, i.e., e[x] = 0. if it is not, then we can subtract the mean by
considering argmax
‚à•v‚à•2=1vte
(x‚àí¬µ)(x‚àí¬µ)t
v.
305chapter 5. joint distributions
theorem 5.21. letœÉbe ad√ódmatrix with eigendecomposition œÉ=usut. then
the optimization
bv=argmax
‚à•v‚à•2=1vtœÉv (5.47)
has a solution bv=u1, i.e., the first column of the eigenvector matrix u.
the following proof requires an understanding of lagrange multipliers and constrained
optimizations. it is not essential for understanding this chapter.
we want to prove that the solution to the problem
bv= argmax
‚à•v‚à•2=1vtœÉv
is the eigenvector of the matrix œÉ. to show that, we first write down the lagrangian:
l(v, Œª) =vtœÉv‚àíŒª(‚à•v‚à•2‚àí1)
taking the derivative w.r.t. vand setting to zero yields
‚àávl(v, Œª) = 2œÉv‚àí2Œªv=0.
this is equivalent to œÉv=Œªv. so if œÉ=usut, then by letting v=uiandŒª=siwe can
satisfy the condition since œÉui=usutui=use i=siui.
end of the proof.
this theorem can be extended to the second (and other) principal components of
the covariance matrix. in fact, given the covariance matrix œÉwe can follow the procedure
outlined in figure 5.23 to determine the principal components. the eigendecomposition of a
d√ódmatrix œÉwill give us a d√ódeigenvector matrix uand an eigenvalue matrix s. to keep
thepleading eigenvectors, we truncate the umatrix to only use the first peigenvectors. here,
we assume that the eigenvectors are ordered according to the magnitude of the eigenvalues,
from large to small.
in practice, if we are given a dataset {x(1), . . . ,x(n)}, we can first estimate the covari-
ance matrix œÉby
bœÇ=1
nnx
n=1(x(n)‚àíb¬µ)(x(n)‚àíb¬µ)t,
where b¬µ=1
npn
n=1x(n)is the mean vector. afterwards, we can compute the eigendecom-
position of bœÉby
[u,s] = eig( bœÇ).
on a computer, the principal components are obtained through eigendecomposition.
a matlab example and a python example are shown below. we explicitly show the two
principal components in this example. the magnitudes of these two vectors are determined
by the eigenvalues diag(s) .
3065.8. principal-component analysis
figure 5.23: the principal components are the eigenvectors of the covariance matrix. in this figure œÉ
denotes the covariance matrix, u1, . . . ,updenote the pleading eigenvectors, and sdenotes the diagonal
of the eigenvalue matrix.
% matlab code to perform the principal-component analysis
x = mvnrnd([0,0],[2 -1.9; -1.9 2],1000);
covx = cov(x);
[u,s] = eig(covx);
u(:,1) % principle components
u(:,2) % principle components
# python code to perform the principal-component analysis
import numpy as np
x = np.random.multivariate_normal([1,-2],[[3,-0.5],[-0.5,1]],1000)
covx = np.cov(x,rowvar=false)
s, u = np.linalg.eig(covx)
print(u)
example 5.27 . suppose we have a dataset containing n= 1000 samples, drawn from
an unknown distribution. the first few samples are
x1=0.5254
‚àí0.6930
,x2=‚àí0.4040
0.3724
, . . . , x1000=1.4165
‚àí1.5463
.
we can compute the mean and covariance using matlab commands mean andcov.
this will return us
b¬µ=0.0561
‚àí0.0303
andbœÇ=2.0460 ‚àí1.9394
‚àí1.9394 2 .0426
.
applying eigendecomposition on bœÇ, we show that
[u,s] = eig( bœÇ),
=‚áíu=‚àí0.7068 ‚àí0.7074
‚àí0.7074 0 .7068
and s=0.1049 0
0 3 .9837
.
307chapter 5. joint distributions
therefore, we have obtained two principal components
u1=‚àí0.7068
‚àí0.7074
and u2=‚àí0.7074
0.7068
.
as seen in the figure below, these two principal components make sense. the vector
u1is the orange line and is the minor axis. the vector u2is the blue line and is the
major axis. again, the ordering of the vectors is determined by the eigenvalues. since
u2has a larger eigenvalue (=3.9837), it is the leading principal component.
-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345
figure 5.24: to determine the representation coefficients, we solve an inverse problem by finding the
vector Œ±in the equation x(n)=upŒ±(n).
why do we call our method principal component analysis ? the analysis part comes
from the fact that we can compress a data vector x(n)from a high dimension dto a low
dimension p. defining up= [u1, . . . ,up], a matrix containing the pleading eigenvectors of
the matrix u, we solve the inverse problem:
x(n)=upŒ±(n),
where the goal is to determine the coefficient vector Œ±(n)‚ààrp. since upis an orthonormal
matrix (i.e., ut
pup=i), it follows that
ut
px(n)=ut
pup|{z}
=iŒ±(n),
3085.8. principal-component analysis
as illustrated in figure 5.24 . hence,
Œ±(n)=ut
px(n).
this equation is a projection operation that projects a data point x(n)onto the space
spanned by the pleading principal components. repeating the procedure for all the data
points x(1), . . . ,x(n)in the dataset, we have compressed the dataset.
example 5.28 . using the example above, we can show that
Œ±(1)=utx(1)=0.1189
‚àí0.8615
,Œ±(2)=0.0221
0.5491
, . . . , Œ±(1000)=0.0927
‚àí2.0950
.
the principal-component analysis says that since the leading components represent the
data, we only need to keep the blue-colored values because they are the coefficients
associated with the leading principal component.
5.8.2 the eigenface problem
as a concrete example of pca, we consider a computer vision problem called the eigen-
face problem. in 2001, researchers at yale university published the yale database, and
a few years later they extended it to a larger one ( http://vision.ucsd.edu/ ~leekc/
extyaledatabase/extyaleb.html ). the dataset, now known as the yale face dataset, con-
tains 16,128 images of 28 human subjects under nine poses and 64 illumination conditions.
the sizes of the images are d= 168 √ó192 = 32,256 pixels. treating these n=16,128 images
as vectors in r32,256√ó1, we have 16,128 of these vectors. let us call them {x(1), . . . ,x(n)}.
following the procedure we described above, we estimate the covariance matrix by
computing
bœÇ=e[(x‚àíb¬µ)(x‚àíb¬µ)t]‚âà1
nnx
n=1(x(n)‚àíb¬µ)(x(n)‚àíb¬µ)t, (5.48)
where b¬µ=e[x]‚âà1
npn
n=1x(n)is the mean vector. note that the size of b¬µis 32,256 √ó1
and the size of bœÉis 32,256 √ó32,256.
figure 5.25: the extended yale face database b.
once we obtain an estimate of the covariance matrix, we can perform an eigendecom-
position to get
[u,s] = eig( bœÇ).
the columns of u, i.e.,{ui}d
i=1, are the eigenvectors of bœÇ. these eigenvectors are the basis
of a testing face image.
309chapter 5. joint distributions
figure 5.26: given a face image, the learned basis vectors (from the eigendecomposition of the covari-
ance matrix) can be used to compress the image xinto a feature vector Œ±where the dimension of Œ±is
significantly lower than that of x.
with the basis vectors u1, . . . ,upwe can project every image in the dataset using a
low-dimensional representation. specifically, for an image xwe compute the coefficients
Œ±i=ut
ix, i = 1, . . . , p
or more compactly Œ±=utx. note that the dimension of xisd√ó1 (which in our case is
d= 32,526), and the dimensions of Œ±can be as few as p= 100. therefore, we are using a
100-dimensional vector to represent a 32,526-dimensional data. this is a huge dimensionality
reduction.
the process repeats for all the samples x(1), . . . ,x(n). this gives us a collection of rep-
resentation coefficients Œ±(1), . . . ,Œ±(n), where each Œ±(n)is 100-dimensional (see figure 5.26 ).
notice that the basis vectors uiappear more or less ‚Äúface images,‚Äù but they are the features
of the faces. pca says that a real face can be written as a linear combination of these basis
vectors.
how to solve the eigenface problem
¬àcompute the covariance matrix of all the images.
¬àapply eigendecomposition to the covariance matrix.
¬àproject onto the basis vectors and find the coefficients.
¬àthe coefficients are the low-dimensional representation of the images.
¬àwe use the coefficients to perform downstream tasks, such as classification.
3105.8. principal-component analysis
5.8.3 what cannot be analyzed by pca?
pca is a dimension reduction tool. it compresses a raw data vector x‚ààrdinto a smaller
feature vector Œ±‚ààrp. the advantage is that the downstream learning problems are much
easier because p‚â™d. for example, classification using Œ±is more efficient than classification
using xsince there is very little information loss from xtoŒ±.
there are three limitations of pca:
¬àpcs fails when the raw data are not orthogonal . the basis vectors uireturned
by pca are orthogonal , meaning that ut
iuj= 0 as long as iÃ∏=j. as a result, if
the data intrinsically have this orthogonality property, then pca will work very well.
however, if the data live in a space such as a donut shape as illustrated in figure 5.27 ,
then pca will fail. here, by failure, we mean that pis not much smaller than d. to
handle datasets behaving like figure 5.27 we need advanced tools. one of these is the
kernel-pca. the idea is to apply a nonlinear transformation to the data before you
run pca.
figure 5.27: [left] pca works when the data has redundant dimensions or is living on orthogonal
spaces. [right] pca fails when the data does not have easily decomposable spaces.
¬àbasis vectors returned by pca are not interpretable . a temptation with pca is to
think that the basis vectors uioffer meaningful information because they are the ‚Äúprin-
cipal components‚Äù. however, since pca is the eigendecomposition of the covariance
matrix, which is purely a mathematical operation, there is no guarantee that the basis
vectors contain any semantic meaning. if we look at the basis vectors shown in fig-
ure 5.26 , there is almost no information one can draw. therefore, in the data-science
literature alternative methods such as non-negative matrix factorization and the more
recent deep neural network embedding are more attractive because the feature vectors
sometimes (not always) have meanings.
¬àpca does not return you the most influential ‚Äúcomponent‚Äù . imagine that you
are analyzing medical data for research on a disease, in which each data vector x(n)
contains height, weight, bmi, blood pressure, etc. when you run pca on the dataset,
you will obtain some ‚Äúprincipal components‚Äù. however, these principal components
will likely have everything, e.g., the height entry of the principal component will have
some values, the weight will have some values, etc. if you have found a principal
component, it does not mean that you have identified the leading risk factor of the
disease. if you want to identify the leading risk factor of the disease, e.g., whether
the height or weight is more important, you need to resort to advanced tools such as
variable selection or the lasso type of regression analysis (see chapter 7).
311chapter 5. joint distributions
closing remark . pcas are powerful computational tools based on the simplest concept of
covariance matrices because, as our derivation showed, covariance matrices encode the ‚Äúvari-
ation‚Äù of the data. therefore, by finding a vector that aligns with the maximum variation
of the data, we can find the principal component.
5.9 summary
as you were reading this chapter, you may have felt that the first and second parts discuss
distinctly different subjects, and in fact many books treat them as separate topics. we take
a different approach. we think that they are essentially the same thing if you understand
the following chain of distributions:
fx(x)|{z}
one variable=‚áífx1,x2(x1, x2)|{z }
two variables=‚áí ¬∑¬∑¬∑ =‚áífx1,...,x n(x1, . . . , x n)| {z }
nvariables.
the first part exclusively deals with two variables. the generalization from two variables to
nvariables is straightforward for pdfs and cdfs:
¬àpdf: fx1,x2(x1, x2) =‚áífx1,...,x n(x1, . . . , x n).
¬àcdf: fx1,x2(x1, x2) =‚áífx1,...,x n(x1, . . . , x n).
the joint expectation can also be generalized from two variables to nvariables:
var[x2
1] cov( x1, x2)
cov(x2, x1) var[ x2
2]
=‚áíÔ£Æ
Ô£ØÔ£∞var[x2
1]¬∑¬∑¬∑ cov(x1, xn)
.........
cov(xn, x1)¬∑¬∑¬∑ var[x2
n]Ô£π
Ô£∫Ô£ª.
conditional pdfs and conditional expectations are powerful tools for decomposing
complex events into simpler events. specifically, the law of total expectation,
e[x] =z
e[x|y=y]fy(y)dy=ey[ex|y[x|y]],
is instrumental for evaluating variables defined through conditional relationships. the idea
is also extendable to more random variables, such as
e[x1] =z z
e[x1|x2=x2, x3=x3]fx2,x3(x2, x3)dx2dx3,
where e[x1|x2=x2, x3=x3] can be evaluated through
e[x1|x2=x2, x3=x3] =z
x1fx1|x2,x3(x1|x2, x3)dx1.
this type of chain relationship can generalize to other high-order cases.
it is important to remember that for any high-dimensional random variables, the char-
acterization is always made by the pdf fx(x) (or the cdf). we did not go into the details
3125.10. references
of analyzing fx(x) but have only discussed the mean vector e[x] =¬µand the covariance
matrix cov( x) =œÉ. we have been focusing exclusively on the high-dimensional gaussian
random variables
fx(x) =1p
(2œÄ)d|œÉ|exp
‚àí1
2(x‚àí¬µ)tœÇ(x‚àí¬µ)
,
because they are ubiquitous in data science today. we discussed the linear transformations
from a zero-mean unit-variance gaussian to another gaussian, and vice versa.
5.10 references
joint distributions and correlation
5-1 dimitri p. bertsekas and john n. tsitsiklis, introduction to probability , athena sci-
entific, 2nd edition, 2008. chapters 2.5, 3.4, 4.2.
5-2 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. chapters 5.1 ‚Äì 5.6.
5-3 athanasios papoulis and s. unnikrishna pillai, probability, random variables and
stochastic processes , mcgraw-hill, 4th edition, 2001. chapters 6.1 ‚Äì 6.4.
5-4 john a. gubner, probability and random processes for electrical and computer en-
gineers , cambridge university press, 2006. chapters 7.1 ‚Äì 7.2.
5-5 sheldon ross, a first course in probability , prentice hall, 8th edition, 2010. chapters
6.1 ‚Äì 6.3.
5-6 henry stark and john woods, probability and random processes with applications
to signal processing , prentice hall, 3rd edition, 2001. chapter 2.6.
conditional distributions and expectations
5-7 dimitri p. bertsekas and john n. tsitsiklis, introduction to probability , athena sci-
entific, 2nd edition, 2008. chapters 2.6, 3.5, 3.6, 4.3.
5-8 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. chapter 5.7.
5-9 athanasios papoulis and s. unnikrishna pillai, probability, random variables and
stochastic processes , mcgraw-hill, 4th edition, 2001. chapters 6.6 ‚Äì 6.7.
5-10 john a. gubner, probability and random processes for electrical and computer en-
gineers , cambridge university press, 2006. chapters 7.3 ‚Äì 7.5.
5-11 sheldon ross, a first course in probability , prentice hall, 8th edition, 2010. chapters
7.5 ‚Äì 7.6.
5-12 henry stark and john woods, probability and random processes with applications
to signal processing , prentice hall, 3rd edition, 2001. chapter 4.2.
313chapter 5. joint distributions
sum of random variables
5-13 dimitri p. bertsekas and john n. tsitsiklis, introduction to probability , athena sci-
entific, 2nd edition, 2008. chapter 4.5.
5-14 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. chapter 7.1.
5-15 henry stark and john woods, probability and random processes with applications
to signal processing , prentice hall, 3rd edition, 2001. chapters 3.3 and 3.4.
vector random variables
5-16 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. chapters 6.1 ‚Äì 6.6.
5-17 john a. gubner, probability and random processes for electrical and computer en-
gineers , cambridge university press, 2006. chapters 8.1 ‚Äì 8.3, 9.
5-18 henry stark and john woods, probability and random processes with applications
to signal processing , prentice hall, 3rd edition, 2001. chapters 5.1 ‚Äì 5.6.
principal-component analysis
pca is often taught in machine learning courses. for first-time readers, we suggest reviewing
the linear algebraic tools in moon and stirling. then, the tutorial by shlens and the chapter
in bishop would be sufficient to cover most of the materials. more advanced topics, such as
kernel pca, can be found in the following references.
5-19 todd k. moon and wynn c. stirling, mathematical methods and algorithms for signal
processing , prentice-hall, 2000. chapter 7.
5-20 christopher bishop, pattern recognition and machine leanring , springer, 2006. chap-
ter 12.
5-21 jonathon shlens (2014) ‚Äúa tutorial on principal component analysis‚Äù, https://
arxiv.org/pdf/1404.1100.pdf
5-22 paul honeine (2014), ‚Äúan eigenanalysis of data centering in machine learning‚Äù, https:
//arxiv.org/pdf/1407.2904.pdf
5-23 quan wang (2012), ‚Äúkernel principal component analysis and its applications‚Äù,
https://arxiv.org/abs/1207.3538
5-24 sch¬® olkopf et al. (2005), ‚Äúkernel principal component analysis‚Äù, https://link.springer.
com/chapter/10.1007/bfb0020217
5.11 problems
exercise 1. (video solution)
alex and bob each flips a fair coin twice. use ‚Äú1‚Äù to denote heads and ‚Äú0‚Äù to denote tails.
letxbe the maximum of the two numbers alex gets, and let ybe the minimum of the
two numbers bob gets.
3145.11. problems
(a) find and sketch the joint pmf px,y(x, y).
(b) find the marginal pmf px(x) and py(y).
(c) find the conditional pmf px|y(x|y). does px|y(x|y) =px(x)? why or why not?
exercise 2.
two fair dice are rolled. find the joint pmf of xandywhen
(a)xis the larger value rolled, and yis the sum of the two values.
(b)xis the smaller, and yis the larger value rolled.
exercise 3.
the amplitudes of two signals xandyhave joint pdf
fxy(x, y) =e‚àíx/2ye‚àíy2
forx >0, y > 0.
(a) find the joint cdf.
(b) find p(x1/2> y).
(c) find the marginal pdfs.
exercise 4. (video solution)
find the marginal cdfs fx(x) and fy(y) and determine whether or not xandyare
independent, if
fxy(x, y) =Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥x‚àí1‚àíe‚àíy‚àíe‚àíxy
y,if 1‚â§x‚â§2, y‚â•0
1‚àíe‚àíy‚àíe‚àí2y
y, ifx >2, y‚â•0,
0, otherwise .
exercise 5. (video solution)
(a) find the marginal pdf fx(x) if
fxy(x, y) =exp{‚àí|y‚àíx| ‚àíx2/2}
2‚àö
2œÄ.
(b) find the marginal pdf fy(y) if
fxy(x, y) =4e‚àí(x‚àíy)2/2
y2‚àö
2œÄ.
315chapter 5. joint distributions
exercise 6. (video solution)
letx, y be two random variables with joint cdf
fx,y(x, y) =y+e‚àíx(y+1)
y+ 1.
show that
‚àÇ2
‚àÇx‚àÇyfx,y(x, y) =‚àÇ2
‚àÇy‚àÇxfx,y(x, y).
what is the implication of this result?
exercise 7. (video solution)
letxandybe two random variables with joint pdf
fx,y(x, y) =1
2œÄe‚àí1
2(x2+y2).
(a) find the pdf of z= max( x, y).
(b) find the pdf of z= min( x, y).
you may leave your answers in terms of the œÜ( ¬∑) function.
exercise 8.
the random vector ( x, y) has a joint pdf
fxy(x, y) = 2 e‚àíxe‚àí2y
forx >0, y > 0. find the probability of the following events:
(a){x+y‚â§8}.
(b){x‚àíy‚â§10}.
(c){x2< y}.
exercise 9.
letxandybe zero-mean, unit-variance independent gaussian random variables. find the
value of rfor which the probability that ( x, y) falls inside a circle of radius ris 1/2.
exercise 10.
the input xto a communication channel is +1 or ‚àí1 with probabilities pand 1 ‚àíp,
respectively. the received signal yis the sum of xand noise n, which has a gaussian
distribution with zero mean and variance œÉ2= 0.25.
(a) find the joint probability p(x=j, y‚â§y).
(b) find the marginal pmf of xand the marginal pdf of y.
(c) suppose we are given that y >0. which is more likely, x= 1 or x=‚àí1?
3165.11. problems
exercise 11. (video solution)
let
fx,y(x, y) =(
ce‚àíxe‚àíy,if 0‚â§y‚â§x <‚àû,
0,otherwise .
(a) find c.
(b) find fx(x) and fy(y).
(c) find e[x] ande[y], var[ x] and var[ y].
(d) find e[xy], cov( x, y) and œÅ.
exercise 12. (video solution)
in class, we have used the cauchy-schwarz inequality to show that ‚àí1‚â§œÅ‚â§1. this exercise
asks you to prove the cauchy-schwarz inequality:
(e[xy])2‚â§e[x2]e[y2].
hint: consider the expectation e[(tx+y)2]. note that this is a quadratic equation in tand
e[(tx+y)2]‚â•0 for all t. consider the discriminant of this quadratic equation.
exercise 13. (video solution)
let Œ∏ ‚àºuniform[0 ,2œÄ].
(a) if x= cos Œ∏, y= sin Œ∏. are xandyuncorrelated?
(b) if x= cos(Œ∏ /4),y= sin(Œ∏ /4). are xandyuncorrelated?
exercise 14. (video solution)
letxandyhave a joint pdf
fx,y(x, y) =c(x+y),
for 0‚â§x‚â§1 and 0 ‚â§y‚â§1.
(a) find c,fx(x),fy(y), and e[y].
(b) find fy|x(y|x).
(c) find p[y > x |x > 1/2].
(d) find e[y|x=x].
(e) find e[e[y|x]], and compare with the e[y] computed in (a).
exercise 15. (video solution)
use the law of total expectation to compute the following:
317chapter 5. joint distributions
1.e[sin(x+y)], where x‚àº n(0,1), and y|x‚àºuniform[ x‚àíœÄ, x+œÄ]
2.p[y < y ], where x‚àºuniform[0 ,1], and y|x‚àºexponential( x)
3.e[xey], where x‚àºuniform[ ‚àí1,1], and y|x‚àº n(0, x2)
exercise 16.
lety=x+n, where xis the input, nis the noise, and yis the output of a system. assume
that xandnare independent random variables. it is given that e[x] = 0, var[ x] =œÉ2
x,
e[n] = 0, and var[ n] =œÉ2
n.
(a) find the correlation coefficient œÅbetween the input xand the output y.
(b) suppose we estimate the input xby a linear function g(y) =ay. find the value of
athat minimizes the mean squared error e[(x‚àíay)2].
(c) express the resulting mean squared error in terms of Œ∑=œÉ2
x/œÉ2
n.
exercise 17. (video solution)
two independent random variables xandyhave pdfs
fx(x) =(
e‚àíx, x ‚â•0,
0, x < 0,fy(y) =(
0, y > 0,
ey, y ‚â§0.
find the pdf of z=x‚àíy.
exercise 18.
letxandybe two independent random variables with densities
fx(x) =(
xe‚àíx, x ‚â•0,
0, x < 0,and fy(y) =(
ye‚àíy, y ‚â•0,
0, y < 0.
find the pdf of z=x+y.
exercise 19.
the random variables xandyhave the joint pdf
fxy(x, y) =e‚àí(x+y)
for 0 < y < x < 1. find the pdf of z=x+y.
exercise 20.
the joint density function of xandyis given by
fxy(x, y) =e‚àí(x+y)
forx >0, y > 0. find the pdf of the random variable z=x/y .
318chapter 6
sample statistics
when we think about probability, the first thing that likely comes to mind is flipping a coin,
throwing a die, or playing a card game. these are excellent examples of the subject. however,
they seldom fit in the context of modern data science, which is concerned with drawing
conclusions from data. in our opinion, the power of probability is its ability to summarize
microstates using macro descriptions . this statement will take us some effort to elaborate.
we study probability because we want to analyze the uncertainties. however, when we
have many data points, analyzing the uncertainties of each data point (the microstates)
is computationally very difficult. probability is useful here because it allows us to bypass
the microstates and summarize the macro behavior. instead of reporting the states of each
individual, we report their sample average. instead of offering the worst-case guarantee,
we offer a probabilistic guarantee. you ask: so what? if we can offer you a performance
guarantee at 99.99% confidence but one-tenth of the cost of a 100% performance guarantee,
would you consider our offer? the goal of this chapter is to outline the concepts of these
probabilistic arguments.
the significance of sample average
imagine that you have a box containing many tiny magnets. (you can also think of a dataset
containing two classes of labels.) in condensed matter physics, these are known as the spin
glasses . the orientations of the magnets depend on the magnetic field. under an extreme
condition where the magnetic field is strong, all magnets will point in the same direction.
when the magnetic field is not as strong, some will align with the field but some will not,
as we show in figure 6.1 .
if we try to study every single magnet in this box, the correlation of the magnets will
force us to consider a joint distribution, since if one magnet points to the right it is likely
that another magnet will also point to the right. the simultaneous description of all magnets
is modeled through a joint probability distribution
fx1,x2,...,x n(x1, x2, . . . , x n).
like any joint pdf, this pdf tells us the probability density that the magnets will take
a collection of states simultaneously. if nis large (say, on the order of millions), this joint
distribution will be very complicated.
319chapter 6. sample statistics
figure 6.1: imagine that we have a box of magnets and we want to measure their orientation angles.
the data points have individual randomness and correlations. studying each one individually could be
computationally infeasible, as we need to estimate the joint pdf fx1,...,x n(x1, . . . , x n)across all the
data points. probability offers a tool to summarize these individual states using a macro description.
for example, we can analyze the sample average xnof the data points and derive conclusions from
the pdf of xn, i.e., fxn(x). the objective of this chapter is to present a few probabilistic tools to
analyze macro descriptions, such as the sample average.
since the joint pdf is very difficult to obtain computationally, physicists proposed
to study the sample statistics. instead of looking at the individual states, they look at the
sample average of the states. if we define x1, . . . , x nas the states of the magnets, then
the sample average is
xn=1
nnx
n=1xn.
since each magnet is random, the sample average is also random, and therefore it is granted
a pdf:
fxn(x).
thus, xnhas a pdf, a mean, a variance, and so on.
we call xna sample statistic. it is called a statistic because it is a summary of the
microstates, and a sample statistic because the statistic is based on random samples, not on
the underlying theoretical distributions. we are interested in knowing the behavior of xn
because it is the summary of the observations. if we know the pdf of xn, we will know
the mean, the variance, and the value of xnwhen the magnetic field increases or decreases.
why study the sample average xn?
¬àanalyzing individual variables is not feasible because the joint pdf can be ex-
tremely high-dimensional.
¬àsample average is a macro description of the data.
¬àif you know the behavior of the sample average, you know most of the data.
probabilistic guarantee versus worst-case guarantee
besides the sample average, we are also interested in the difference between a probabilistic
guarantee and a deterministic guarantee.
320consider the birthday paradox (see chapter 1 for details). suppose there are 50 stu-
dents in a room. what is the probability that at least two students have the same birthday?
a naive thought would suggest that we need 366 students to guarantee a pair of the same
birthday because there are 365 days. so, with only 50 students, it would seem unlikely to
have a pair with the same birthday. however, it turns out that with just 50 students, the
probability of having at least one pair with the same birthday is more than 97%. figure 6.2
below shows a calculation by a computer, where we plot the estimated probability as a func-
tion of the number of students. what is more surprising is that with as few as 23 students,
the probability is greater than 50%. there is no need for there to be 365 students in order
to offer a guarantee.
0 10 20 30 40 50 60 70 80 90 100
number of people00.10.20.30.40.50.60.70.80.91probability
figure 6.2: the birthday paradox asks the question of how many people we need to ask in order to have
at least two of them having the same birthday. while we tend to think that the answer is 366 (because
there are 365 days), the actual probability, as we have calculated (see chapter 1), is more than 97%,
even if we have only asked 50 people. the curve above shows the probability of having at least one pair
of people having the same birthday as a function of the number of people. the plot highlights the gap
between the worst-case performance and an average-case performance.
why does this happen? certainly, we can trace back to the formulae in chapter 1 and
argue through the lens of combinations and permutations. however, the more important
message is about the difference between the worst-case guarantee and the average-case
guarantee .
worst case versus average case
¬àworst-case guarantee: you need to ensure that the worst one is protected. this
requires an exhaustive search until hitting 100%. it is a deterministic guarantee.
¬àaverage-case guarantee: you guarantee that with a high probability (e.g., 99.99%),
the undesirable event does not happen. this is a probabilistic guarantee.
is there a difference between 99.99% and 100%? if the probability is 99.99%, there is
one failure every 10,000 trials on average. you are unlikely to fail, but it is still possible.
a 100% guarantee says that no matter how many trials you make you will not fail. the
99.99% guarantee is much weaker (yes, much weaker, not just a little bit weaker) than the
deterministic guarantee. however, in practice, people might be willing to pay for the risk in
exchange for efficiency. this is the principle behind insurance. automobile manufacturing
321chapter 6. sample statistics
also uses this principle ‚Äî your chance of purchasing a defective car is non-zero, but if the
manufacturer can sell enough cars to compensate for the maintenance cost of fixing your
car, they might be willing to offer a limited warranty in exchange for a lower selling price.
how do we analyze the probabilistic guarantee, e.g., for the sample average? remember
that the sample average xnis a random variable. since it is a random variable, it has a
mean, variance, and pdf.1to measure the probabilistic guarantee, we consider the event
bdef={|xn‚àí¬µ| ‚â•œµ},
where ¬µ=e[xn] is the true population mean, and œµ >0 is a very small number. this
probability is illustrated in figure 6.3 , assuming that xnhas the pdf of a gaussian. the
probability of bis the two tails under the pdf. therefore, bis abadevent because in
principle xnshould be close to ¬µ. the probability p[b] measures situations where xn
stays very far from ¬µ. if we can show that p[b] is small (e.g., <0.01%), then we can say
that we have obtained a probabilistic guarantee at 99.99%.
figure 6.3: the probabilistic guarantee of a sample average xnis established by computing the
probability of the tails. in this example, we assume that fxn(x)take a gaussian shape, and we define
œµ= 1. anything belonging to |xn‚àí¬µ| ‚â•œµis called a undesired event b. if the probability of a
undesired event is small, we say that we can offer a probabilistic guarantee.
the moment we compute p[|xn‚àí¬µ| ‚â•œµ], we enter the race of probabilistic guarantee
(e.g., 99.99%). why? if the probability p[|xn‚àí¬µ| ‚â•œµ] is less than 0.01%, it still does not
exclude the possibility that something bad will happen once every 10,000 trials on average.
the chance is low, but it is still possible. we will learn some mathematical tools for analyzing
this type of probabilistic guarantee.
plan for this chapter
with these two main themes in mind, we now discuss the organization of this chapter. there
are four sections: two for mathematical tools and two for main results.
¬àmoment-generating functions : we have seen in chapter 5 that the pdf of a sum of
two random variables x+yis the convolution of the two pdfs fx‚àófy. convolutions
are non-trivial, especially when we have more random variables to sum. the moment-
generating functions provide a convenient way of summing nrandom variables. they
are the transform domain techniques (e.g., fourier transforms). since convolutions in
1not all random variables have mean and variance, e.g., a cauchy random variable, but most of them
do.
322time are multiplications in frequency, the moment-generating functions allow us to
multiply pdfs in the transformed space. in this way, we can sum as many random
variables as we want. we will discuss this idea in section 6.1.
key concept 1: why study moment-generating functions?
moment-generating functions help us determine the pdf of x1+x2+¬∑¬∑¬∑+xn.
¬àprobability inequalities : when analyzing sample statistics such as xn, evaluating the
exact probability could be difficult because it requires integrating the pdfs. however,
if our ultimate goal is to estimate the probability, deriving an upper bound might be
sufficient to achieve the goal. the probability inequalities are designed for this purpose.
in section 6.2, we discuss several of the most basic probability inequalities. we will
use some of them to prove the law of large numbers.
key concept 2: how can probability inequalities be useful?
probability inequalities help us upper-bound the bad event p[|xn‚àí¬µ| ‚â•œµ].
¬àlaw of large numbers : this is the first main result of the chapter. the law of large
numbers says that the sample average xnconverges to the population mean ¬µwhen
the number of samples grows to infinity. the law of large numbers comes in two
versions: the weak law of large numbers and the strong law of large numbers. the
difference is the type of convergence they guarantee. the weak law is based on con-
vergence in probability , whereas the strong law is based on almost sure convergence .
we will discuss these types of convergence in section 6.3.
key concept 3: what is the law of large numbers?
there is a weak law and a strong law of large numbers. the weak law of large
numbers says that xnconverges to the true mean ¬µ, asngrows:
lim
n‚Üí‚àûp[|xn‚àí¬µ|> œµ] = 0.
¬àcentral limit theorem : the central limit theorem says that the probability of
xncan be approximated by the probability of a gaussian. you can also think of
this as saying that the pdf of xnis converging to a distribution that can be well
approximated by a bell-shaped gaussian. if we have many random variables and their
sum is becoming a gaussian, we can ignore the individual pdfs and focus on the
gaussian. thus it explains why gaussian is so popular. we will discuss this theorem
in detail in section 6.4.
key concept 4: what is the central limit theorem?
the cdf of xncan be approximated by the cdf of a gaussian, as ngrows.
323chapter 6. sample statistics
6.1 moment-generating and characteristic functions
consider two independent random variables xandywith pdfs fx(x) and fy(y), respec-
tively. let z=x+ybe the sum of the two random variables. we know from chapter 5
that the pdf of z,fz, is the convolution of fxandfy. however, we think you will agree
that convolutions are not easy to compute. especially when the sum involves more random
variables, computing the convolution would be tedious. so how should we proceed in this
case? one approach is to use some kind of ‚Äúfrequency domain‚Äù method that transforms
the pdfs to another domain and then perform multiplication instead of the convolution
to make the calculations easy or at least easier. the moment-generating functions and the
characteristic functions are designed for this purpose.
6.1.1 moment-generating function
definition 6.1. for any random variable x, themoment-generating function (mgf)
mx(s)is
mx(s) =e
esx
. (6.1)
the definition says that the moment-generating function (mgf) is the expectation of the
random variable taken to the power esxfor some s. effectively, it is the expectation of a
function of random variables. the meaning of the expectation can be seen by writing out
the definition. for the discrete case, the mgf is
mx(s) =x
x‚ààœâesxpx(x), (6.2)
whereas in the continuous case, the mgf is
mx(s) =z‚àû
‚àí‚àûesxfx(x)dx. (6.3)
the continuous case should remind us of the definition of a laplace transform. for any
function f(t), the laplace transform is
l[f](s) =z‚àû
‚àí‚àûf(t)estdt.
from this perspective, we can interpret the mgf as the laplace transform of the pdf.
the argument sof the output can be regarded as the coordinate in the laplace space. if
s=‚àíjœâ, then mx(jœâ) becomes the fourier transform of the pdf.
example 6.1 . consider a random variable xwith three states 0 ,1,2 and with prob-
ability masses2
6,3
6,1
6respectively. find the mgf.
3246.1. moment-generating and characteristic functions
solution . the moment-generating function is
mx(s) =e[esx] =es0¬∑2
6+es1¬∑3
6+es2¬∑1
6
=1
3+es
2+e2s
6.
practice exercise 6.1 . find the mgf for a poisson random variable.
solution . the mgf of poisson random variable can be found as
mx(s) =e[esx] =‚àûx
x=0esxŒªxe‚àíŒª
x!=‚àûx
x=0(Œªes)x
x!e‚àíŒª=eŒªese‚àíŒª.
practice exercise 6.2 . find the mgf for an exponential random variable.
solution . the mgf of an exponential random variable can be found as
mx(s) =e[esx] =z‚àû
0esxŒªe‚àíŒªxdx=z‚àû
0Œªe(s‚àíŒª)xdx=Œª
Œª‚àís, ifŒª > s.
why are moment-generating functions so called? the following theorem reveals the
reason.
theorem 6.1. the mgf has the properties that
¬àmx(0) = 1 ,
¬àd
dsmx(s)|s=0=e[x],d2
ds2mx(s)|s=0=e[x2],
¬àdk
dskmx(s)|s=0=e[xk], for any positive integer k.
proof . the first property can be proved by noting that
mx(0) =e[e0x] =e[1] = 1 .
the third property holds because
dk
dskmx(s) =z‚àû
‚àí‚àûdk
dskesxfx(x)dx=z‚àû
‚àí‚àûxkesxfx(x)dx.
setting s= 0 yields
dk
dskmx(s)|s=0=z‚àû
‚àí‚àûxkfx(x)dx=e[xk].
the second property is a special case of the third property.
‚ñ°
325chapter 6. sample statistics
the theorem tells us that if we take the derivative of the mgf and set s= 0, we will
obtain the moment. the order of the moment depends on the order of the derivative. as a
result, the mgf can ‚Äúgenerate moments‚Äù by taking derivatives. this happens because of
the exponential function esx. sinced
dsesx=xesx, the variable xappears whenever we take
the derivative.
practice exercise 6.3 . let xbe a bernoulli random variable with parameter p.
find the first two moments using mgf.
solution . the mgf of a bernoulli random variable is
mx(s) =e[esx]
=es0px(0) + es1px(1)
= (1)(1 ‚àíp) + (es)(p)
= 1‚àíp+pes.
the first and the second moment, using the derivative approach, are
e[x] =d
dsmx(s)
s=0=d
ds
1‚àíp+pes
s=0=pes
s=0=p,
e[x2] =d2
ds2mx(s)
s=0=d2
ds2
1‚àíp+pes
s=0=pes
s=0=p.
to facilitate our discussions of mgf, we summarize a few mgfs in the table below.
distribution pmf / pdf e[x] var[ x] mx(s)
bernoulli px(1) = pandpx(0) = 1 ‚àíp p p (1‚àíp) 1 ‚àíp+pes
binomial px(k) = n
k
pk(1‚àíp)n‚àíknp np (1‚àíp) (1 ‚àíp+pes)n
geometric px(k) =p(1‚àíp)k‚àí11
p1‚àíp
p2pes
1‚àí(1‚àíp)es
poisson px(k) =Œªke‚àíŒª
k!Œª Œª eŒª(es‚àí1)
gaussian fx(x) =1‚àö
2œÄœÉ2exp
‚àí(x‚àí¬µ)2
2œÉ2
¬µ œÉ2exp
¬µs+œÉ2s2
2
exponential fx(x) =Œªexp{‚àíŒªx}1
Œª1
Œª2Œª
Œª‚àís
uniform fx(x) =1
b‚àíaa+b
2(b‚àía)2
12esb‚àíesa
s(b‚àía)
table 6.1: moment-generating functions of common random variables.
3266.1. moment-generating and characteristic functions
6.1.2 sum of independent variables via mgf
mgfs are most useful when analyzing the pdf of a sum of two random variables. the
following theorem highlights the result.
theorem 6.2. letxandybe independent random variables. let z=x+y. then
mz(s) =mx(s)my(s). (6.4)
proof . by the definition of mgf, we have that
mz(s) =eh
es(x+y)i(a)=e
esx
e
esy
=mx(s)my(s),
where (a) is valid because xandyare independent.
‚ñ°
corollary 6.1. consider independent random variables x1, . . . , x n. let z=pn
n=1xn
be the sum of random variables. then the mgf of zis
mz(s) =ny
n=1mxn(s). (6.5)
if these random variables are further assumed to be identically distributed, the mgf is
mz(s) = (mx1(s))n. (6.6)
proof . this follows immediately from the previous theorem:
mz(s) =e[es(x1+¬∑¬∑¬∑+xn)] =e[esx1]e[esx2]¬∑¬∑¬∑e[esxn] =ny
n=1mxn(s).
if the random variables x1, . . . , x nare i.i.d., then the product simplifies to
ny
n=1mxn(s) =ny
n=1mx1(s) = (mx1(s))n.
‚ñ°
theorem 6.3 (sum of bernoulli = binomial ).letx1, . . . , xnbe a sequence of
i.i.d. bernoulli random variables with parameter p. let z=x1+¬∑¬∑¬∑+xnbe the sum.
then zis a binomial random variable with parameters (n, p).
proof . let us consider a sequence of i.i.d. bernoulli random variables xn‚àºbernoulli( p)
forn= 1, . . . , n . let z=x1+¬∑¬∑¬∑+xn. the moment-generating function of zis
mz(s) =e[es(x1+¬∑¬∑¬∑+xn)] =ny
n=1e[esxn]
=ny
n=1 
pes1+ (1‚àíp)es0
= (pes+ (1‚àíp))n.
327chapter 6. sample statistics
now, let us check the moment-generating function of a binomial random variable: if z‚àº
binomial( n, p), then
mz(s) =e[esz] =nx
n=0eskn
k
pk(1‚àíp)n‚àík
=nx
n=0n
k
(pes)k(1‚àíp)n‚àík= (pes+ (1‚àíp))n,
where the last equality holds becausepn
n=0 n
k
akbn‚àík= (a+b)n. therefore, the two
moment-generating functions are identical. ‚ñ°
theorem 6.4 (sum of binomial = binomial ).letx1, . . . , xnbe a sequence of
i.i.d. binomial random variables with parameters (n, p). let z=x1+¬∑¬∑¬∑+xnbe the
sum. then zis a binomial random variable with parameters (nn, p ).
proof . the mgf of a binomial random variable is
mxi(s) = (pes+ (1‚àíp))n.
if we have nof these random variables, then z=x1+¬∑¬∑¬∑+xnwill have the mgf
mz(s) =ny
i=1mxi(s) = (pes+ (1‚àíp))nn.
note that this is just the mgf of another binomial random variable with parameter ( nn, p ).
‚ñ°
theorem 6.5 (sum of poisson = poisson ).letx1, . . . , xnbe a sequence of
i.i.d. poisson random variables with parameter Œª. let z=x1+¬∑¬∑¬∑+xnbe the sum.
then zis a poisson random variable with parameters nŒª.
proof . the mgf of a poisson random variable is
mx(s) =e[esx] =‚àûx
k=0eskŒªk
k!e‚àíŒª
=e‚àíŒª‚àûx
k=0(Œªes)k
k!
=e‚àíŒªeŒªes=eŒª(es‚àí1).
assume that we have a sum of ni.i.d. poisson random variables. then, by the main theorem,
we have that
mz(s) = [mx(s)]n=enŒª(es‚àí1).
therefore, the resulting random variable zis a poisson with parameter nŒª. ‚ñ°
3286.1. moment-generating and characteristic functions
theorem 6.6 (sum of gaussian = gaussian ).letx1, . . . , xnbe a sequence of
independent gaussian random variables with parameters (¬µ1, œÉ2
1), . . . , (¬µn, œÉ2
n). let
z=x1+¬∑¬∑¬∑+xnbe the sum. then zis a gaussian random variable:
z=gaussiannx
n=1¬µn,nx
n=1œÉ2
n
. (6.7)
proof . we skip the proof of the mgf of a gaussian. it can be shown that
mx(s) = exp
¬µs+œÉ2s2
2
.
when we have a sequence of gaussian random variables, then
mz(s) =e[es(x1+¬∑¬∑¬∑+xn)]
=mx1(s)¬∑¬∑¬∑mxn(s)
=
exp
¬µ1s+œÉ2
1s2
2
¬∑¬∑¬∑
exp
¬µns+œÉ2
ns2
2
= exp( nx
n=1¬µn!
s+ nx
n=1œÉ2
n!
s2
2)
.
therefore, the resulting random variable zis also a gaussian. the mean and variance of z
arepn
n=1¬µnandpn
n=1œÉ2
n, respectively.
‚ñ°
6.1.3 characteristic functions
moment-generating functions are the laplace transforms of the pdfs. however, since the
laplace transform is defined on the entire right half-plane, not all pdfs can be transformed.
one way to mitigate this problem is to restrict sto the imaginary axis, s=jœâ. this will
give us the characteristic function .
definition 6.2 (usual definition) .thecharacteristic function of a random variable
xis
œÜx(jœâ) =e[ejœâx]. (6.8)
however, we note that since œâcan take any value in ( ‚àí‚àû,‚àû), it does not matter if we
consider e[e‚àíjœâx] ore[ejœâx]. this leads to the following equivalent definition of the char-
acteristic function:
definition 6.3 (alternative definition (for this book)) .thecharacteristic function
of a random variable xis
œÜx(jœâ) =e[e‚àíjœâx]. (6.9)
329chapter 6. sample statistics
if we follow this definition, we see that the characteristic function can be written as
œÜx(jœâ) =e[e‚àíjœâx] =z‚àû
‚àí‚àûe‚àíjœâxfx(x)dx. (6.10)
this is exactly the fourier transform of the pdf. the reason for introducing this alternative
characteristic function is that e[e‚àíjœâx] is the fourier transform of fx(x) bute[ejœâx] is the
inverse fourier transform of fx(x). the former is more convenient (in terms of notation)
for students who have taken a course in signals and systems. however, we should stress that
the usual way of defining the characteristic function is e[ejœâx].
a list of common fourier transforms is shown in the table below. additional identities
can be found in standard signals and systems textbooks.
fourier transforms
f(t)‚Üê‚Üíf(œâ) f(t)‚Üê‚Üíf(œâ)
1.e‚àíatu(t)‚Üê‚Üí1
a+jœâ,a >0 10. sinc2(wt
2)‚Üê‚Üí2œÄ
w‚àÜ(œâ
2w)
2.eatu(‚àít)‚Üê‚Üí1
a‚àíjœâ,a >0 11. e‚àíatsin(œâ0t)u(t)‚Üê‚Üíœâ0
(a+jœâ)2+œâ2
0
3.e‚àía|t|‚Üê‚Üí2a
a2+œâ2,a >0 12. e‚àíatcos(œâ0t)u(t)‚Üê‚Üía+jœâ
(a+jœâ)2+œâ2
0
4.a2
a2+t2‚Üê‚ÜíœÄae‚àía|œâ|,a >0 13. e‚àít2
2œÉ2‚Üê‚Üí‚àö
2œÄœÉe‚àíœÉ2œâ2
2
5.te‚àíatu(t)‚Üê‚Üí1
(a+jœâ)2,a >0 14. Œ¥(t)‚Üê‚Üí1
6.tne‚àíatu(t)‚Üê‚Üín!
(a+jœâ)n+1,a >0 15. 1 ‚Üê‚Üí2œÄŒ¥(œâ)
7.rect(t
œÑ)‚Üê‚ÜíœÑsinc(œâœÑ
2) 16. Œ¥(t‚àít0)‚Üê‚Üíe‚àíjœât0
8. sinc( wt)‚Üê‚ÜíœÄ
wrect(w
2w) 17. ejœâ0t‚Üê‚Üí2œÄŒ¥(œâ‚àíœâ0)
9.‚àÜ(t
œÑ)‚Üê‚ÜíœÑ
2sinc2(œâœÑ
4) 18. f(t)ejœâ0t‚Üê‚Üíf(œâ‚àíœâ0)
table 6.2: fourier transform pairs of commonly used functions.
example 6.2 . let xbe a random variable with pdf fx(x) =Œªe‚àíŒªxforx‚â•0. find
the characteristic function.
solution . the fourier transform pair is
Œªe‚àíŒªx‚àí‚ÜíŒª¬∑ f
e‚àíŒªx
=Œª¬∑1
Œª+jœâ.
therefore, the characteristic function is œÜ x(jœâ) =Œª
Œª+jœâ.
3306.1. moment-generating and characteristic functions
example 6.3 . let xandybe independent, and let
fx(x) =(
Œªe‚àíŒªx, x ‚â•0,
0, x < 0,fy(y) =(
Œªe‚àíŒªy, y ‚â•0,
0, y < 0.
find the pdf of z=x+y.
solution . the characteristic function of xandycan be found from the fourier table:
œÜx(jœâ) =Œª
Œª+jœâand œÜ y(jœâ) =Œª
Œª+jœâ.
therefore, the characteristic function of zis
œÜz(jœâ) = œÜ x(jœâ)œÜy(jœâ) =Œª2
(Œª+jœâ)2.
by inverse fourier transform, we have that
fz(z) =f‚àí1Œª2
(Œª+jœâ)2
=Œª2ze‚àíŒªz, z‚â•0.
why œÜx(jœâ)but not mx(s)?as we said, the function is not always defined. recall
that the expectation e[x] exists only when fx(x) is absolutely integrable, or e[|x|]<‚àû.
for a characteristic function, the expectation is valid because e[|ejœâx|] =e[1] = 1. however,
for a function, e[|esx|] could be unbounded. to see a counterexample, we consider the
cauchy distribution.
theorem 6.7. consider the cauchy distribution with pdf
fx(x) =1
œÄ(x2+ 1). (6.11)
the mgf of xis undefined but the characteristic function is well defined.
proof . the mgf is
mx(s) =z‚àû
‚àí‚àûesx 1
œÄ(x2+ 1)dx‚â•z‚àû
1esx 1
œÄ(x2+ 1)dx
‚â•z‚àû
1(sx)3
6œÄ(x2+ 1)dx, because esx‚â•(sx)3
6
‚â•z‚àû
1(sx)3
6œÄ(2x2)dx=s3
12œÄz‚àû
1x dx =‚àû.
therefore, the mgf is undefined. on the other hand, by the fourier table we know that
œÜx(jœâ) =f1
œÄ(x2+ 1)
=e‚àí|œâ|.
‚ñ°
331chapter 6. sample statistics
example 6.4 . let x0, x1, . . .be a sequence of independent random variables with
pdf
fxk(x) =ak
œÄ(a2
k+x2), a k=1
2k+1fork= 0,1, . . . .
find the pdf of y, where y=p‚àû
k=0xk.
solution . from the fourier transform table, we know that
ak
œÄ(a2
k+x2)=1
akœÄ¬∑a2
k
(a2
k+x2)f‚Üê‚Üí1
akœÄ¬∑œÄake‚àíak|œâ|=e‚àíak|œâ|.
the characteristic function of yis
œÜy(jœâ) =‚àûy
k=0œÜxk(jœâ) = exp(
‚àí|œâ|‚àûx
k=0ak)
.
sincep‚àû
k=0ak=p‚àû
k=01
2k+1=1
2+1
4+¬∑¬∑¬∑= 1, the characteristic function becomes
œÜy(jœâ) =e‚àí|œâ|. the inverse fourier transform gives us
e‚àí|œâ|=1
œÄ¬∑œÄe‚àí|œâ|f‚Üê‚Üí1
œÄ¬∑1
1 +x2.
therefore the pdf of yis
fy(y) =1
œÄ(1 +y2).
example 6.5 . two random variables xandyhave the pdfs
fx(x) =(
e‚àíx, x ‚â•0,
0, x < 0,and fy(y) =(
e‚àíy, y ‚â•0,
0, y < 0.
find the pdf of z= max( x, y)‚àímin(x, y).
solution . we first show that
z= max( x, y)‚àímin(x, y) =|x‚àíy|.
suppose x > y , then max( x, y) =xand min( x, y) =y. soz=x‚àíy. ifx < y ,
then max( x, y) =yand min( x, y) =x. soz=y‚àíx. combining the two cases
gives us z=|x‚àíy|. now, consider the fourier transform of the pdfs:
e‚àíxf‚Üê‚Üí1
1 +jœâ.
3326.2. probability inequalities
letu=x‚àíy, and let z=|u|. the characteristic function is
œÜu(jœâ) =e[e‚àíjœâ(x‚àíy)] =e[e‚àíjœâx]e[ejœây]
=1
1 +jœâ¬∑1
1‚àíjœâ=1
1 +œâ2f‚Üê‚Üí fu(u) =1
2e‚àí|u|.
with the pdf of u, we can find the cdf of z:
fz(z) =p[z‚â§z] =p[|u| ‚â§z]
=zz
‚àízfu(u)du
=zz
‚àíz1
2e‚àí|u|du
= 2zz
01
2e‚àíudu= 1‚àíe‚àíz.
hence, the pdf is
fz(z) =d
dzfz(z) =e‚àíz.
closing remark . moment-generating functions and characteristic functions are useful
mathematical tools. in this section, we have confined our discussion to using them to com-
pute the sum of two random variables. later sections and chapters will explain further uses
for these functions. for example, we use the mgfs when proving chernoff‚Äôs bound and
proving the central limit theorem.
6.2 probability inequalities
moment-generating functions and characteristic functions are powerful tools for handling the
sum of random variables. we now introduce another set of tools, known as the probability
inequalities , that allow us to do approximations. we will highlight a few basic probability
inequalities in this section.
6.2.1 union bound
the first inequality is the union bound we had introduced when we discussed the axioms of
probabilities. the union bound states the following:
theorem 6.8 (union bound ).leta1, . . . , a nbe a collection of sets. then
p"n[
n=1an#
‚â§nx
n=1p[an]. (6.12)
333chapter 6. sample statistics
proof . we can prove this by induction. first, if n= 2,
p[a1‚à™a2] =p[a1] +p[a2]‚àíp[a1‚à©a2]‚â§p[a1] +p[a2],
because p[a1‚à©a2] is a probability and so it must be non-negative. thus we have proved
the base case. assume that the statement is true for n=k. we need to prove that the
statement is also true for n=k+ 1. to this end, we note that
p"k+1[
n=1an#
=p" k[
n=1an!
‚à™ak+1#
=p"k[
n=1an#
+p[ak+1]‚àíp" k[
n=1an!
‚à©ak+1#
‚â§p"k[
n=1an#
+p[ak+1].
then, according to our hypothesis for n=k, it follows that
p"k[
n=1an#
‚â§kx
n=1p[an].
putting these together,
p"k+1[
n=1an#
‚â§kx
n=1p[an] +p[ak+1] =k+1x
n=1p[an].
therefore, by the principle of induction, we have proved the statement.
‚ñ°
remark . the tightness of the union bound depends on the amount of overlapping between
the events a1, . . . , a n, as illustrated in figure 6.4 . if the events are disjoint, the union bound
is tight. if the events are overlapping significantly, the union is loose. the idea of the union
bound is the principle of divide and conquer. we decompose the system into smaller events
for a system of nvariables and use the union bound to upper-limit the overall probability. if
the probability of each event is small, the union bound tells us that the overall probability
of the system will also be small.
figure 6.4: conditions under which the union bound is loose or tight. [left] the union bound is loose
when the sets are overlapping. [right] the union bound is tight when the sets are (nearly) disjoint.
3346.2. probability inequalities
example 6.6 . let x1, . . . , x nbe a sequence of i.i.d. random variables with cdf
fxn(x) and let z= min( x1, . . . , x n). find an upper bound on the cdf.
solution . note that z= min( x1, . . . , x n)‚â§zis equivalent to at least one of the
xn‚Äôs being less than z. thus, we have that
z= min( x1, . . . , x n)‚â§z‚áîx1‚â§z‚à™ ¬∑¬∑¬∑ ‚à™ xn‚â§z.
substituting this result into the cdf,
fz(z) =p[z‚â§z]
=p[min( x1, . . . , x n)‚â§z]
=p[x1‚â§z‚à™ ¬∑¬∑¬∑ ‚à™ xn‚â§z]
‚â§p[x1‚â§z] +¬∑¬∑¬∑+p[xn‚â§z]
=n¬∑fx(z).
6.2.2 the cauchy-schwarz inequality
the second inequality we study here is the cauchy-schwarz inequality , which we previously
mentioned in chapter 5. we review it for the sake of completeness.
theorem 6.9 (cauchy-schwarz inequality ).letxandybe two random variables.
then
e[xy]2‚â§e[x2]e[y2]. (6.13)
proof . let f(s) =e[(sx+y)2] for any real s. then
f(s) =e[(sx+y)2]
=e[s2x2+ 2sxy +y2]
=e[x2]s2+ 2e[xy]s+e[y2].
this is a quadratic equation, and f(s)‚â•0 for all sbecause e[(sx+y)2]‚â•0.
recall that for a quadratic equation œï(x) =ax2+bx+c, the function œï(x)‚â•0 if and
only if b2‚àí4ac‚â§0. substituting this result into our problem, we show that
(2e[xy])2‚àí4e[x2]e[y2]‚â§0.
this implies that
e[xy]2‚â§e[x2]e[y2],
which completes the proof.
‚ñ°
remark . as shown in chapter 5, the cauchy-schwarz inequality is useful in analyzing
e[xy]. for example, we can use the cauchy-schwarz inequality to prove that the correlation
coefficient œÅis bounded between ‚àí1 and 1.
335chapter 6. sample statistics
6.2.3 jensen‚Äôs inequality
our next inequality is jensen‚Äôs inequality . to motivate the inequality, we recall that
var[x] =e[x2]‚àíe[x]2.
since var[ x]‚â•0 for any x, it follows that
e[x2]|{z}
=e[g(x)]‚â•e[x]2
|{z}
=g(e[x]). (6.14)
jensen‚Äôs inequality is a generalization of the above result by recognizing that the inequality
does not only hold for the function g(x) =x2but also for any convex function g. the
theorem is stated as follows:
theorem 6.10 (jensen‚Äôs inequality ).letxbe a random variable, and let g:r‚Üír
be aconvex function. then
e[g(x)]‚â•g(e[x]). (6.15)
if the function gisconcave , then the inequality sign is flipped: e[g(x)]‚â§g(e[x]). the
way to remember this result is to remember that e[x2]‚àíe[x]2= var[ x]‚â•0.
now, what is a convex function? informally, a function gisconvex if, when we pick any
two points on the function and connect them with a straight line, the line will be above the
function for that segment. this definition is illustrated in figure 6.5 . consider an interval
[x, y], and the line segment connecting g(x) and g(y). if the function g(¬∑) is convex, then
the entire line segment should be above the curve.
figure 6.5: illustration of a convex function, a concave function, and a function that is neither convex
nor concave.
the definition of a convex function essentially follows the above picture:
definition 6.4. a function gisconvex if
g(Œªx+ (1‚àíŒª)y)‚â§Œªg(x) + (1 ‚àíŒª)g(y), (6.16)
for any 0‚â§Œª‚â§1.
here Œªrepresents a ‚Äúsweeping‚Äù constant that goes from xtoy. when Œª= 1 then Œªx+(1‚àíŒª)y
simplifies to x, and when Œª= 0 then Œªx+ (1‚àíŒª)ysimplifies to y.
3366.2. probability inequalities
the definition is easy to understand. the left-hand side g(Œªx+(1‚àíŒª)y) is the function
evaluated at any points in the interval [ x, y]. the right-hand side is the red straight line we
plotted in figure 6.5 . it connects the two points g(x) and g(y). convexity means that the
red line is entirely above the curve.
for twice-differentiable 1d functions, convexity can be described by the curvature of
the function. a function is convex if
g‚Ä≤‚Ä≤(x)‚â•0. (6.17)
this is self-explanatory because if the curvature is non-negative for all x, then the slope of
ghas to keep increasing.
example 6.7 . the following functions are convex or concave:
¬àg(x) = log xis concave, because g‚Ä≤(x) =1
xandg‚Ä≤‚Ä≤(x) =‚àí1
x2‚â§0 for all x.
¬àg(x) =x2is convex, because g‚Ä≤(x) = 2 xandg‚Ä≤‚Ä≤(x) = 2 is positive.
¬àg(x) =e‚àíxis convex, because g‚Ä≤(x) =‚àíe‚àíxandg‚Ä≤‚Ä≤(x) =e‚àíx‚â•0.
why is jensen inequality valid for a convex function? consider the illustration in
figure 6.6 . suppose we have a random variable xtaking some pdf fx(x). there is a
convex function g(¬∑) that maps the random variable xtog(x). since g(¬∑) is convex, a pdf
like the one we see in figure 6.6 will become skewed. (you can map the left tail to the new
left tail, the peak to the new peak, and the right tail to the new right tail.) as you can see
from the figure, the new random variable g(x) has a mean e[g(x)] that is greater than the
mapped old mean g(e[x]). jensen‚Äôs inequality captures this phenomenon by stating that
e[g(x)]‚â•g(e[x]) for any convex function g(¬∑).
figure 6.6: jensen‚Äôs inequality states that if there is a convex function g(¬∑)that maps a random variable
xto a new random variable g(x), the new mean e[g(x)]will be greater than the mapped old mean
g(e[x]).
proving jensen‚Äôs inequality is straightforward for a two-state discrete random variable.
define a random variable xwith states xandy. the probabilities for these two states are
p[x=x] =Œªandp[x=y] = 1‚àíŒª. then
e[x] =x
x‚Ä≤‚àà{x,y}x‚Ä≤px(x‚Ä≤) =Œªx+ (1‚àíŒª)y.
337chapter 6. sample statistics
now, let g(¬∑) be a convex function. we know from the expectation that
e[g(x)] =x
x‚Ä≤‚àà{x,y}g(x‚Ä≤)px(x‚Ä≤) =g(x)Œª+ (1‚àíŒª)g(y).
by convexity of the function g(¬∑), it follows that
g(Œªx+ (1‚àíŒª)y)| {z }
=g(e[x])‚â§Œªf(x) + (1 ‚àíŒª)g(y)| {z }
=e[g(x)],
where in the underbrace we substitute the definitions using the expectation. therefore,
for any two-state discrete random variables, the proof of jensen‚Äôs inequality follows directly
from the convexity. if the discrete random variable takes more than two states, we can prove
the theorem by induction. for continuous random variables, we can prove the theorem using
the following approach.
you may skip the proof of jensen‚Äôs inequality if this is your first time reading the book.
here we present an alternative proof of jensen‚Äôs inequality that does not require proof
by induction. the idea is to recognize that if the function gis convex we can find a tangent
linel(x) =ax+bat the point e[x] that is uniformly lower than g(x), i.e., g(x)‚â•l(x)
for all x. then we can prove the result with a simple geometric argument. figure 6.7
illustrates this idea.
figure 6.7: geometric illustration of the proof of jensen‚Äôs inequality. suppose g(¬∑)is a convex function.
for any point xong(¬∑), we can find a tangent line l(x) =ax+b. since the black curve is always
above the tangent, it follows that e[g(x)]‚â•e[l(x)]for any x. also, note that at a particular point
e[x], the black curve and the red line touch, and so we have l(e[x]) =g(e[x]).
proof of jensen‚Äôs inequality . consider l(x) as defined above. since gis convex, g(x)‚â•
l(x) for all x. therefore,
e[g(x)]‚â•e[l(x)]
=e[ax+b]
=ae[x] +b
=l(e[x]) =g(e[x]),
where the last equality holds because lis a tangent line to gwhere they meet at e[x].
‚ñ°
3386.2. probability inequalities
what are ( a, b) in the proof? by taylor expansion,
g(x)‚âàg(e[x]) +g‚Ä≤(e[x])(x‚àíe[x])
def=l(x).
therefore, if we want to be precise, then a=g‚Ä≤(e[x]) and b=g(e[x])‚àíg‚Ä≤(e[x])e[x].
the end of the proof.
example 6.8 . by jensen‚Äôs inequality, we have that
(a)e[x2]‚â•e[x]2, because g(x) =x2is convex.
(b)e1
x
‚â•1
e[x], because g(x) =1
xis convex.
(c)e[logx]‚â§loge[x], because g(x) = log xis concave.
6.2.4 markov‚Äôs inequality
our next inequality, markov‚Äôs inequality , is an elementary inequality that links probability
and expectation.
theorem 6.11 (markov‚Äôs inequality ).letx‚â•0be a non-negative random variable.
then, for any Œµ >0, we have
p[x‚â•Œµ]‚â§e[x]
Œµ. (6.18)
markov‚Äôs inequality concerns the tailof the random variable. as illustrated in figure 6.8 ,
p[x‚â•Œµ] measures the probability that the random variable takes a value greater than Œµ.
markov‚Äôs inequality asserts that this probability p[x‚â•Œµ] is upper-bounded by the ratio
e[x]/Œµ. this result is useful because it relates the probability and the expectation. in many
problems the probability p[x‚â•Œµ] could be difficult to evaluate if the pdf is complicated.
the expectation, on the other hand, is usually easier to evaluate.
proof . consider Œµp[x‚â•Œµ]. it follows that
Œµp[x‚â•Œµ] =z‚àû
ŒµŒµ fx(x)dx‚â§z‚àû
Œµxfx(x)dx,
where the inequality is valid because for any x‚â•Œµthe integrand (which is non-negative)
will always increase (or at least not decrease). it then follows that
z‚àû
Œµxfx(x)dx‚â§z‚àû
0xfx(x)dx=e[x].‚ñ°
a pictorial interpretation of markov‚Äôs inequality is shown in figure 6.9 . for x > 0, it
is not difficult to show that e[x] =r‚àû
01‚àífx(x)dx. then, in the cdf plot, we see that
Œµ¬∑p[x‚â•Œµ] is a rectangle covering the top left corner. this area is clearly smaller than the
area covered by the function 1 ‚àífx(x).
339chapter 6. sample statistics
figure 6.8: markov‚Äôs inequality provides an upper bound to the tail of a random variable. the inequality
states that the probability p[x‚â•Œµ]is upper bounded by the ratio e[x]/Œµ.
figure 6.9: the proof of markov‚Äôs inequality follows from the fact that Œµ¬∑p[x‚â•Œµ]occupies the
top left corner marked by the yellow rectangle. the expectation is the area above the cdf so that
e[x] =r‚àû
01‚àífx(x)dx. since the yellow rectangle is smaller than the orange shaded area, it follows
thatŒµ¬∑p[x‚â•Œµ]‚â§e[x], which is markov‚Äôs inequality.
practice exercise 6.4 . prove that if x > 0, then e[x] =r‚àû
01‚àífx(x)dx.
solution . we start from the right-hand side:
z‚àû
01‚àífx(x)dx=z‚àû
01‚àíp[x‚â§x]dx
=z‚àû
0p[x‚â•x]dx
=z‚àû
0z‚àû
xfx(t)dt dx
=z‚àû
0zt
0fx(t)dx dt
=z‚àû
0tfx(t)dt=e[x].
the change in the integration order is illustrated below.
3406.2. probability inequalities
how tight is markov‚Äôs inequality? it is possible to create a random variable such that
the equality is met (see exercise 6.14). however, in general, the estimate provided by the
upper bound is not tight. here is an example.
practice exercise 6.5 . let x‚àºuniform(0 ,4). verify markov‚Äôs inequality for p[x‚â•
2],p[x‚â•3] and p[x‚â•4].
solution . first, we observe that e[x] = 2. then
p[x‚â•2] = 0 .5,e[x]
2= 1,
p[x‚â•3] = 0 .25,e[x]
3= 0.67,
p[x‚â•4] = 0 ,e[x]
4= 0.5.
therefore, although the upper bounds are all valid, they are very loose.
if markov‚Äôs inequality is not tight, why is it useful? it turns out that while markov‚Äôs
inequality is not tight, its variations can be powerful. we will come back to this point when
we discuss chernoff‚Äôs bound.
6.2.5 chebyshev‚Äôs inequality
the next inequality is a simple extension of markov‚Äôs inequality. the result is known as
chebyshev‚Äôs inequality.
theorem 6.12 (chebyshev‚Äôs inequality ).letxbe a random variable with mean ¬µ.
then for any Œµ >0we have
p[|x‚àí¬µ| ‚â•Œµ]‚â§var[x]
Œµ2. (6.19)
the tail measured by chebyshev‚Äôs inequality is illustrated in figure 6.10 . since the
event |x‚àí¬µ| ‚â•Œµinvolves an absolute value, the probability measures the two-sided tail.
chebyshev‚Äôs inequality states that this tail probability is upper-bounded by var[ x]/Œµ2.
341chapter 6. sample statistics
figure 6.10: chebyshev‚Äôs inequality states that the two-sided tail probability p[|x‚àí¬µ| ‚â•Œµ]is upper-
bounded by var[x]/Œµ2
proof . we apply markov‚Äôs inequality to show that
p[|x‚àí¬µ| ‚â•Œµ] =p[(x‚àí¬µ)2‚â•Œµ2]
‚â§e[(x‚àí¬µ)2]
Œµ2=var[x]
Œµ2.
‚ñ°
an alternative form of chebyshev‚Äôs inequality is obtained by letting Œµ=kœÉ. in this
case, we have
p[|x‚àí¬µ| ‚â•kœÉ]‚â§œÉ2
k2œÉ2=1
k2.
therefore, if a random variable is ktimes the standard deviation away from the mean, then
the probability bound drops to 1 /k2.
practice exercise 6.6 . let x‚àºuniform(0 ,4). find the bound of chebyshev‚Äôs
inequality for the probability p[|x‚àí¬µ| ‚â•1].
solution . note that e[x] = 2 and œÉ2= 42/12 = 4 /3. therefore, we have
p[|x‚àí¬µ| ‚â•1]‚â§œÉ2
Œµ2=4
3,
which is a valid upper bound, but quite conservative.
practice exercise 6.7 . let x‚àºexponential(1). find the bound of chebyshev‚Äôs
inequality for the probability p[x‚â•Œµ].
solution . note that e[x] = 1 and œÉ2= 1. thus we have
p[x‚â•Œµ] =p[x‚àí¬µ‚â•Œµ‚àí¬µ]‚â§p[|x‚àí¬µ| ‚â•Œµ‚àí¬µ]
‚â§œÉ2
(Œµ‚àí¬µ)2=1
(Œµ‚àí1)2.
3426.2. probability inequalities
we can compare this with the exact probability, which is
p[x‚â•Œµ] = 1‚àífx(Œµ) =e‚àíŒµ.
again, the estimate given by chebyshev‚Äôs inequality is acceptable but too conservative.
corollary 6.2. letx1, . . . , x nbe i.i.d. random variables with mean e[xn] =¬µand
variance var[xn] =œÉ2. let xn=1
npn
n=1xnbe the sample mean. then
pxn‚àí¬µ> œµ
‚â§œÉ2
nœµ2. (6.20)
proof . we can first show that e[xn] =¬µand var[ xn] satisfies
var[xn] =1
n2nx
n=1var[xn] =œÉ2
n.
then by chebyshev‚Äôs inequality,
pxn‚àí¬µ> œµ
‚â§var[xn]
œµ2=œÉ2
nœµ2.
‚ñ°
the consequence of this corollary is that the upper bound œÉ2n/œµ2will converge to zero
asn‚Üí ‚àû . therefore, the probability of getting the event {xn‚àí¬µ> œµ}is vanishing.
it means that the sample average xnis converging to the true population mean ¬µ, in the
sense that the probability of failing is shrinking.
6.2.6 chernoff‚Äôs bound
we now introduce a powerful inequality or a set of general procedures that gives us some
highly useful inequalities. the idea is named for herman chernoff, although it was actually
due to his colleague herman rubin.
theorem 6.13 (chernoff‚Äôs bound ).letxbe a random variable. then, for any
Œµ‚â•0, we have that
p[x‚â•Œµ]‚â§e‚àíœÜ(Œµ), (6.21)
wherea
œÜ(Œµ) = max
s>0
sŒµ‚àílogmx(s)
, (6.22)
andmx(s)is the moment-generating function.
aœÜ(Œµ) is called the fenchel-legendre dual function of log mx. see references [6-14].
343chapter 6. sample statistics
proof . there are two tricks in the proof of chernoff‚Äôs bound. the first trick is a nonlinear
transformation. since esxis an increasing function for any s >0 and x, we have that
p[x‚â•Œµ] =p[esx‚â•esŒµ]
(a)
‚â§e[esx]
esŒµ
(b)=e‚àísŒµmx(s)
=e‚àísŒµ+log mx(s),
where the inequality (a) is due to markov‚Äôs inequality. step (b) just uses the definition of
mgf that e[esx] =mx(s).
now for the second trick. note that the above result holds for all s. that means it
must also hold for the sthat minimizes e‚àísŒµ+log mx(s). this implies that
p[x‚â•Œµ]‚â§min
s>0n
e‚àísŒµ+log mx(s)o
.
again, since exis increasing, the minimizer of the above probability is also the maximizer
of this function:
œÜ(Œµ) = max
s>0
sŒµ‚àílogmx(s)
.
thus, we conclude that p[x‚â•Œµ]‚â§e‚àíœÜ(Œµ).
‚ñ°
6.2.7 comparing chernoff and chebyshev
let‚Äôs consider an example of how chernoff‚Äôs bound can be useful.
suppose that we have a random variable x‚àºgaussian(0 , œÉ2/n). the number ncan
be regarded as the number of samples. for example, if y1, . . . , y narengaussian random
variables with mean 0 and variance œÉ2, then the average x=1
npn
n=1ynwill have mean
0 and variance œÉ2/n. therefore, as ngrows, the variance of xwill become smaller and
smaller.
first, since the random variable is gaussian, we can show the following:
lemma 6.1. letx‚àºgaussian (0,œÉ2
n)be a gaussian random variable. then, for any
Œµ >0,
p[x‚â•Œµ] = 1‚àíœÜ ‚àö
nŒµ
œÉ!
, (6.23)
where œÜis the standard gaussian‚Äôs cdf.
note that this is the exact result: if you tell me Œµ,n, and œÉ, then the probability p[x‚â•Œµ]
is exactly the one shown on the right-hand side. no approximation, no randomness.
3446.2. probability inequalities
proof . since xis gaussian, the probability is
p[x‚â•Œµ] =z‚àû
Œµ1p
2œÄ(œÉ2/n)exp
‚àíx2
2(œÉ2/n)
dx
= 1‚àízŒµ
‚àí‚àû1p
2œÄ(œÉ2/n)exp
‚àíx2
2(œÉ2/n)
dx
= 1‚àíz Œµ‚àö
œÉ2/n
‚àí‚àû1‚àö
2œÄexp
‚àíx2
2
dx
= 1‚àíœÜ 
Œµp
œÉ2/n!
= 1‚àíœÜ ‚àö
nŒµ
œÉ!
.
‚ñ°
let us compute the bound given by chebyshev‚Äôs inequality.
lemma 6.2. letx‚àºgaussian (0,œÉ2
n)be a gaussian random variable. then, for any
Œµ >0, chebyshev‚Äôs inequality implies that
p[x‚â•Œµ]‚â§œÉ2
nŒµ2. (6.24)
proof . we apply chebyshev‚Äôs inequality by assuming that ¬µ= 0:
p[x‚â•Œµ] =p[x‚àí¬µ‚â•Œµ‚àí¬µ]‚â§p[|x‚àí¬µ| ‚â•Œµ‚àí¬µ]
‚â§e[(x‚àí¬µ)2]
(Œµ‚àí¬µ)2=œÉ2
nŒµ2.
‚ñ°
we now compute chernoff‚Äôs bound.
theorem 6.14. letx‚àºgaussian (0,œÉ2
n)be a gaussian random variable. then, for
anyŒµ >0, chernoff‚Äôs bound implies that
p[x‚â•Œµ]‚â§exp
‚àíŒµ2n
2œÉ2
. (6.25)
proof . the mgf of a zero-mean gaussian random variable with variance œÉ2/nismx(s) =
expn
œÉ2s2
2no
. therefore, the function œÜcan be written as
œÜ(Œµ) = max
s>0
sŒµ‚àílogmx(s)
= max
s>0
sŒµ‚àíœÉ2s2
2n
.
to maximize the function we take the derivative and set it to zero. this yields
d
ds
sŒµ‚àíœÉ2s2
2n
= 0 ‚áí s‚àó=nŒµ
œÉ2.
345chapter 6. sample statistics
note that this s‚àóis a maximizer because sŒµ‚àíœÉ2s2
2nis a concave function.
substituting s‚àóintoœÜ(Œµ),
œÜ(Œµ) = max
s>0sŒµ‚àíœÉ2s2
2n
=s‚àóŒµ‚àíœÉ2(s‚àó)2
2n=nŒµ
œÉ2
Œµ‚àíœÉ2
2nnŒµ
œÉ22
=Œµ2n
2œÉ2,
and hence
p[x‚â•Œµ]‚â§e‚àíœÜ(Œµ)= exp
‚àíŒµ2n
2œÉ2
.
‚ñ°
figure 6.11 shows the comparison between the exact probability, the bound provided
by chebyshev‚Äôs inequality, and chernoff‚Äôs bound:
¬àexact :p[x‚â•Œµ] = 1‚àíœÜ‚àö
nŒµ
œÉ
.
¬àchebyshev :p[x‚â•Œµ]‚â§œÉ2
nŒµ2,
¬àchernoff :p[x‚â•Œµ]‚â§expn
‚àíŒµ2n
2œÉ2o
.
in this numerical experiment, we set Œµ= 0.1, and œÉ= 1. we vary the number n. as we can
see from the figure, the bound provided by chebyshev is valid but very loose. it does not
even capture the tail as ngrows. on the other hand, chernoff‚Äôs bound is reasonably tight.
however, one should note that the tightness of chernoff is only valid for large n. when n
is small, it is possible to construct random variables such that chebyshev is tighter.
the matlab code used to generate this plot is illustrated below.
% matlab code to compare the probability bounds
epsilon = 0.1;
sigma = 1;
n = logspace(1,3.9,50);
p_exact = 1-normcdf(sqrt(n)*epsilon/sigma);
p_cheby = sigma^2./(epsilon^2*n);
p_chern = exp(-epsilon^2*n/(2*sigma^2));
loglog(n, p_exact, ‚Äô-o‚Äô, ‚Äôcolor‚Äô, [1 0.5 0], ‚Äôlinewidth‚Äô, 2); hold on;
loglog(n, p_cheby, ‚Äô-‚Äô, ‚Äôcolor‚Äô, [0.2 0.7 0.1], ‚Äôlinewidth‚Äô, 2);
loglog(n, p_chern, ‚Äô-‚Äô, ‚Äôcolor‚Äô, [0.2 0.0 0.8], ‚Äôlinewidth‚Äô, 2);
what could go wrong if we insist on using chebyshev‚Äôs inequality? consider the fol-
lowing example.
example 6.9 . let x‚àºgaussian(0 , œÉ2/n). suppose that we want the probability to
be no greater than a confidence level of Œ±:
p[x‚â•Œµ]‚â§Œ±.
3466.2. probability inequalities
101102103
n10-1510-1010-5100probability
exact
chebyshev
chernoff
figure 6.11: comparison between chernoff‚Äôs bound and chebyshev‚Äôs bound. the random variable we
use is x‚àºgaussian (0, œÉ2/n). as ngrows, we show the probability bounds predicted by the two
methods.
letŒ±= 0.05,Œµ= 0.1, and œÉ= 1. find the nusing (i) chebyshev‚Äôs inequality and (ii)
chernoff‚Äôs inequality.
solution : (i) chebyshev‚Äôs inequality implies that
p[x‚â•Œµ]‚â§œÉ2
nŒµ2‚â§Œ±,
which means that
n‚â•œÉ2
Œ±Œµ2.
if we plug in Œ±= 0.05,Œµ= 0.1, and œÉ= 1, then n‚â•2000.
(ii) for chernoff‚Äôs inequality, it holds that
p[x‚â•Œµ]‚â§exp
‚àíŒµ2n
2œÉ2
‚â§Œ±,
which means that
n‚â• ‚àí2œÉ2
Œµ2logŒ±
plugging in Œ±= 0.05,Œµ= 0.1, and œÉ= 1, we have that n‚â•600. this is more than 3
times smaller than the one predicted by chebyshev‚Äôs inequality. which one is correct?
both are correct but chebyshev‚Äôs inequality is overly conservative. if n‚â•600 can
makep[x‚â•Œµ]‚â§Œ±, then certainly n‚â•2000 will work too. however, n‚â•2000 is too
loose.
347chapter 6. sample statistics
6.2.8 hoeffding‚Äôs inequality
chernoff‚Äôs bound can be used to derive many powerful inequalities. here we present an
inequality for bounded random variables. this result is known as hoeffding‚Äôs inequality.
theorem 6.15 (hoeffding‚Äôs inequality ).letx1, . . . , x nbe i.i.d. random variables
with 0‚â§xn‚â§1, ande[xn] =¬µ. then
pxn‚àí¬µ> œµ
‚â§2e‚àí2œµ2n, (6.26)
where xn=1
npn
n=1xn.
you may skip the proof of hoeffding‚Äôs inequality if this is your first time reading the book.
proof . (hoeffding‚Äôs inequality) first, we show that
p
xn‚àí¬µ > œµ
=p"
1
nnx
n=1xn‚àí¬µ > œµ#
=p"nx
n=1(xn‚àí¬µ)> nœµ#
=ph
espn
n=1(xn‚àí¬µ)‚â•esœµni
‚â§e[espn
n=1(xn‚àí¬µ)]
esœµn=e[es(xn‚àí¬µ)]
esœµn
.
letzn=xn‚àí¬µ. then ‚àí¬µ‚â§zn‚â§1‚àí¬µ. at this point we use hoeffding lemma (see
below) that e[eszn]‚â§es2
8because b‚àía= (1‚àí¬µ)‚àí(‚àí¬µ) = 1. thus,
p
xn‚àí¬µ > œµ
‚â§e[eszn]
esœµn
‚â§ 
es2
8
esœµ!n
=es2n
8‚àísœµn,‚àÄs.
this result holds for all s, and thus it holds for the sthat minimizes the right-hand side.
this implies that
p
xn‚àí¬µ > œµ
‚â§min
s
exps2n
8‚àísœµn
.
minimizing the exponent givesd
dsn
s2n
8‚àísœµno
=sn
4‚àíœµn= 0. thus we have s= 4œµ.
hence,
p
xn‚àí¬µ > œµ
‚â§exp(4œµ)2n
8‚àí(4œµ)œµn
=e‚àí2œµ2n.
by symmetry, p
xn‚àí¬µ <‚àíœµ
‚â§e‚àí2œµ2n. then by union bound we show that
p
|xn‚àí¬µ|> œµ
=p
xn‚àí¬µ > œµ
+p
xn‚àí¬µ <‚àíœµ
‚â§e‚àí2œµ2n+e‚àí2œµ2n
= 2e‚àí2œµ2n.‚ñ°
3486.2. probability inequalities
lemma 6.3 (hoeffding‚Äôs lemma ).leta‚â§x‚â§bbe a random variable with
e[x] = 0. then
mx(s)def=e
esx
‚â§exps2(b‚àía)2
8
. (6.27)
proof . since a‚â§x‚â§b, we can write xas a linear combination of aandb:
x=Œªb+ (1‚àíŒª)a,
where Œª=x‚àía
b‚àía. since exp( ¬∑) is a convex function, it follows that eŒªb+(1‚àíŒª)a‚â§Œªeb+(1‚àíŒª)ea.
(recall that his convex if h(Œªx+ (1‚àíŒª)y)‚â§Œªh(x) + (1 ‚àíŒª)h(y).) therefore, we have
esx‚â§Œªesb+ (1‚àíŒª)esa
=x‚àía
b‚àíaesb+b‚àíx
b‚àíaesa.
taking expectations on both sides of the equation,
e[esx]‚â§‚àía
b‚àíaesb+b
b‚àíaesa,
because e[x] = 0. now, if we let Œ∏=‚àía
b‚àía, then
‚àía
b‚àíaesb+b
b‚àíaesa=Œ∏esb+ (1‚àíŒ∏)esa
=esa
1‚àíŒ∏+Œ∏es(b‚àía)
=
1‚àíŒ∏+Œ∏es(b‚àía)
e‚àísŒ∏(b‚àía)
= (1‚àíŒ∏+Œ∏eu)e‚àíŒ∏u=e‚àíŒ∏u+log(1 ‚àíŒ∏+Œ∏eu),
where we let u=s(b‚àía). this can be simplified as e[esx]‚â§e[eœï(u)] by defining
œï(u) =‚àíŒ∏u+ log(1 ‚àíŒ∏+Œ∏eu).
the final step is to approximate œï(u). to this end, we use taylor approximation:
œï(u) =œï(0) + uœï‚Ä≤(0) +u2
2œï‚Ä≤‚Ä≤(Œæ),
for some Œæ‚àà[a, b]. since œï(0) = 0, œï‚Ä≤(0) = 0, and œï‚Ä≤‚Ä≤(u)‚â§1
4for all u, it follows that
œï(u) =u2
2œï‚Ä≤‚Ä≤(Œæ)‚â§u2
8=s2(b‚àía)2
8.‚ñ°
end of the proof.
349chapter 6. sample statistics
what is so special about the hoeffding‚Äôs inequality?
¬àsince hoeffding‚Äôs inequality is derived from chernoff‚Äôs bound, it inherits the
tightness. hoeffding‚Äôs inequality is much stronger than chebyshev‚Äôs inequality
in bounding the tail distributions.
¬àhoeffding‚Äôs inequality is one of the few inequalities that do not require e[x] and
var[x] on the right-hand side.
¬àa downside of the inequality is that boundedness is not always easy to satisfy.
for example, if xnis a gaussian random variable, hoeffding does not apply.
there are more advanced inequalities for situations like these.
interpreting hoeffding‚Äôs inequality . one way to interpret hoeffding‚Äôs inequality is to
write the equation as
pxn‚àí¬µ> œµ
‚â§2e‚àí2œµ2n
|{z}
Œ¥,
which is equivalent to
pxn‚àí¬µ‚â§œµ
‚â•1‚àíŒ¥.
this means that with a probability at least 1 ‚àíŒ¥, we have
xn‚àíœµ‚â§¬µ‚â§xn+œµ.
if we let Œ¥= 2e‚àí2œµ2n, this becomes
xn‚àír
1
2nlog2
Œ¥‚â§¬µ‚â§xn+r
1
2nlog2
Œ¥. (6.28)
this inequality is a confidence interval (see chapter 9). it says that with probability at
least 1 ‚àíŒ¥, the interval [ xn‚àíœµ,xn+œµ] includes the true population mean ¬µ.
there are two questions one can ask about the confidence interval:
¬àgiven nandŒ¥, what is the confidence interval? equation (6.28) tells us that if we
know n, to achieve a probability of at least 1 ‚àíŒ¥the confidence interval will follow
equation (6.28). for example, if n= 10,000 and Œ¥= 0.01,q
1
2nlog2
Œ¥= 0.016.
therefore, with a probability at least 99%, the true population mean ¬µwill be included
in the interval
xn‚àí0.16‚â§¬µ‚â§xn+ 0.16.
¬àif we want to achieve a certain confidence interval, what is the nwe need? if we are
given œµandŒ¥, the nwe need is
Œ¥‚â§2e‚àí2œµ2n‚áí n‚â•log2
Œ¥
2œµ2.
for example, if Œ¥= 0.01 and œµ= 0.01, the nwe need is n‚â•26,500.
when is hoeffding‚Äôs inequality used? hoeffding‚Äôs inequality is fundamental in modern
machine learning theory. in this field, one often wants to quantify how well a learning
3506.3. law of large numbers
algorithm performs with respect to the complexity of the model and the number of training
samples. for example, if we choose a complex model, we should expect to use more training
samples or overfit otherwise. hoeffding‚Äôs inequality provides an asymptotic description of
the training error, testing error, and the number of training samples. the inequality is
often used to compare the theoretical performance limit of one model versus another model.
therefore, although we do not need to use hoeffding‚Äôs inequality in this book, we hope you
appreciate its tightness.
closing remark . we close this section by providing the historic context of chernoff‚Äôs
inequality. herman chernoff, the discoverer of chernoff‚Äôs inequality, wrote the following
many years after the publication of the original paper in 1952.
‚Äúin working on an artificial example, i discovered that i was using the central limit
theorem for large deviations where it did not apply. this led me to derive the asymptotic
upper and lower bounds that were needed for the tail probabilities. [herman] rubin claimed
he could get these bounds with much less work, and i challenged him. he produced a rather
simple argument, using markov‚Äôs inequality, for the upper bound. since that seemed to be
a minor lemma in the ensuing paper i published (chernoff, 1952), i neglected to give him
credit. i now consider it a serious error in judgment, especially because his result is stronger
for the upper bound than the asymptotic result i had derived. ‚Äù ‚Äî herman chernoff, ‚Äúa
career in statistics,‚Äù in lin et al., past, present, and future of statistical science (2014),
p. 35.
6.3 law of large numbers
in this section, we present our first main result: the law of large numbers. we will discuss
two versions of the law: the weak law and the strong law. we will also introduce two forms
of convergence: convergence in probability and almost sure convergence.
6.3.1 sample average
the law of large numbers is a probabilistic statement about the sample average . suppose
that we have a collection of i.i.d. random variables x1, . . . , x n. the sample average of these
nrandom variables is defined as follows:
definition 6.5. thesample average of a sequence of random variables x1, . . . , x n
is
xn=1
nnx
n=1xn. (6.29)
if the random variables x1, . . . , x nare i.i.d. so that they have the same population
mean e[xn] =¬µ(forn= 1, . . . , n ), then by the linearity of the expectation,
e
xn
=1
nnx
n=1e[xn] =¬µ.
351chapter 6. sample statistics
therefore, the mean of xnis the population mean ¬µ.
the sample average, xn, plays an important role in statistics. for example, by sur-
veying 10,000 americans, we can find a sample average of their ages. since we never have
access to the true population mean, the sample average is an estimate, and since xnis only
an estimate, we need to ask how good the estimate is.
one reason we ask this question is that xnis a finite-sample ‚Äúapproximation‚Äù of ¬µ.
more importantly, the root of the problem is that xnitself is a random variable because
x1, . . . , x nare all random variables. since xnis a random variable, there is a pdf of
xn; there is a cdf of xn; there is e[xn]; and there is var[ xn]. since xnis a random
variable, it has uncertainty. to say that we are confident about xn, we need to ensure that
the uncertainty is within some tolerable range.
how do we control the uncertainty? we can compute the variance. if x1, . . . , x nare
i.i.d. random variables with the same variance var[ xn] =œÉ2(forn= 1, . . . , n ), then
var
xn
=1
n2nx
n=1var[xn] =1
n2nx
n=1œÉ2=œÉ2
n.
therefore, the variance will shrink to 0 as ngrows. in other words, the more samples we
use to construct the sample average, the less deviation the random variable xnwill have.
visualizing the sample average
to help you visualize the randomness of xn, we consider an experiment of drawing n
bernoulli random variables x1, . . . , x nwith parameter p= 1/2. since xnis bernoulli, it
follows that
e[xn] =p and var[ xn] =p(1‚àíp).
we construct a sample average xn=1
npn
n=1xn. since xnis a bernoulli random variable,
we know everything about xn. first, xnis a binomial random variable, since xnis the
sum of bernoulli random variables. second, the mean and variance of xnare respectively
¬µxndef=e[xn] =1
nnx
n=1e[xn] =p,
œÉ2
xndef= var[ xn] =1
n2nx
n=1var[xn] =p(1‚àíp)
n.
infigure 6.12 , we plot the random variables xn(the black crosses) for every n. you
can see that at each n, e.g., n= 100, there are many possible observations for xnbecause
xnitself is a random variable. as nincreases, we see that the deviation of the random
variables becomes smaller. in the same plot, we show the bounds ¬µ¬±3œÉxn, which are three
standard deviations from the mean. we can see clearly that the bounds provide a very good
envelope covering the random variables. as ngoes to infinity, we can see that the standard
deviation goes to zero, and so xnapproaches the true mean.
for your reference, the matlab code and the python code we used to generate the
plot are shown below.
% matlab code to illustrate the weak law of large numbers
nset = round(logspace(2,5,100));
3526.3. law of large numbers
102103104105
n0.30.40.50.60.7sample average
figure 6.12: the weak law of large numbers. in this plot, we assume that x1, . . . , x nare i.i.d. bernoulli
random variables with a parameter p. the black crosses in the plot are the sample averages
xn=1
npn
n=1xn. the red curves are the ideal bounds ¬µxn¬±3œÉxn, where ¬µxn=pand
œÉxn=p
p(1‚àíp)/n. as ngrows, we observe that the variance shrinks to zero. therefore, the
sample average is converging to the true population mean.
for i=1:length(nset)
n = nset(i);
p = 0.5;
x(:,i) = binornd(n, p, 1000,1)/n;
end
y = x(1:10:end,:)‚Äô;
semilogx(nset, y, ‚Äôkx‚Äô); hold on;
semilogx(nset, p+3*sqrt(p*(1-p)./nset), ‚Äôr‚Äô, ‚Äôlinewidth‚Äô, 4);
semilogx(nset, p-3*sqrt(p*(1-p)./nset), ‚Äôr‚Äô, ‚Äôlinewidth‚Äô, 4);
# python code to illustrate the weak law of large numbers
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
import numpy.matlib
p = 0.5
nset = np.round(np.logspace(2,5,100)).astype(int)
x = np.zeros((1000,nset.size))
for i in range(nset.size):
n = nset[i]
x[:,i] = stats.binom.rvs(n, p, size=1000)/n
nset_grid = np.matlib.repmat(nset, 1000, 1)
plt.semilogx(nset_grid, x,‚Äôko‚Äô);
plt.semilogx(nset, p + 3*np.sqrt((p*(1-p))/nset), ‚Äôr‚Äô, linewidth=6)
plt.semilogx(nset, p - 3*np.sqrt((p*(1-p))/nset), ‚Äôr‚Äô, linewidth=6)
353chapter 6. sample statistics
note the outliers for each ninfigure 6.12 . for example, at n= 102we see a point
located near 0.7 on the y-axis. this point is outside three standard deviations. is it normal?
yes. being outside three standard deviations only says that the probability of having this
outlier is small . it does not say that the outlier is impossible . having a small probability does
not exclude the possibility. by contrast, if you say that something will surely not happen you
mean that there is not even a small probability. the former is a weaker statement than the
latter. therefore, even though we establish a three standard deviation envelope, there are
points falling outside the envelope. as ngrows, the chance of having a bad outlier becomes
smaller. therefore, the greater the n, the smaller the chance we will get an outlier.
if the random variables xnare i.i.d., the above phenomenon is universal. below is an
example of the poisson case.
practice exercise 6.8 . let xn‚àºpoisson( Œª). define the sample average as xn=
1
npn
n=1xn. find the mean and variance of xn.
solution . since xnis poisson, we know that e[xn] =Œªand var[ xn] =Œª. so
e[xn] =1
nnx
n=1e[xn] =1
nnx
n=1Œª=Œª,
var[xn] =1
n2nx
n=1var[xn] =1
n2nx
n=1Œª=Œª
n.
therefore, as n‚Üí ‚àû , the variance var[ xn]‚Üí0.
6.3.2 weak law of large numbers (wlln)
the analysis of figure 6.12 shows us something important, namely that the convergence
in a probabilistic way is different from that in a deterministic way. we now describe one
fundamental result related to probabilistic convergence, known as the weak law of large
numbers.
theorem 6.16 (weak law of large numbers ).letx1, . . . , x nbe a set of i.i.d. ran-
dom variables with mean ¬µand variance œÉ2. assume e[x2]<‚àû. let xn=1
npn
n=1xn.
then for any Œµ >0,
lim
n‚Üí‚àûp
|xn‚àí¬µ|> Œµ
= 0. (6.30)
proof . by chebyshev‚Äôs inequality,
p
|xn‚àí¬µ|> Œµ
‚â§var[xn]
Œµ2=var[xn]
nŒµ2.
therefore, setting n‚Üí ‚àû we have
lim
n‚Üí‚àûp
|xn‚àí¬µ|> Œµ
= lim
n‚Üí‚àûvar[xn]
nŒµ2= 0.
‚ñ°
3546.3. law of large numbers
example 6.10 . consider a set of i.i.d. random variables x1, . . . , x nwhere
xn‚àºgaussian( ¬µ, œÉ2).
verify that the sample average xn=1
npn
n=1xnfollows the weak law of large num-
bers.
solution : since xnis a gaussian, the sample average xnis also a gaussian:
xn‚àºgaussian
¬µ,œÉ2
n
.
consider the probability p
|xn‚àí¬µ|> Œµ
for each n:
Œ¥ndef=p
|xn‚àí¬µ|> Œµ
=p
xn‚àí¬µ > Œµ
+p
xn‚àí¬µ <‚àíŒµ
= 1‚àíœÜ 
Œµ‚àö
n
œÉ!
+ œÜ 
‚àíŒµ‚àö
n
œÉ!
= 2œÜ 
‚àíŒµ‚àö
n
œÉ!
.
if we set œÉ= 1 and Œµ= 0.1, then
Œ¥1= 2œÜ
‚àí0.1¬∑1
1
= 0.9203, Œ¥ 5= 2œÜ 
‚àí0.1¬∑‚àö
5
1!
= 0.8231,
Œ¥10= 2œÜ 
‚àí0.1¬∑‚àö
10
1!
= 0.7518, Œ¥ 100= 2œÜ 
‚àí0.1¬∑‚àö
100
1!
= 0.3173,
Œ¥1000= 2œÜ 
‚àí0.1¬∑‚àö
1000
1!
= 0.0016.
as you can see, the the sequence Œ¥1, Œ¥2, . . . , Œ¥ n, . . .rapidly converges to 0 as ngrows.
in fact, since œÜ( z) is a increasing function for z <0 with œÜ( ‚àí‚àû) = 0, it follows that
lim
n‚Üí‚àûp
|xn‚àí¬µ|> Œµ
= lim
n‚Üí‚àû2œÜ 
‚àíŒµ‚àö
n
œÉ!
= 0.
the weak law of large numbers is portrayed graphically in figure 6.13 . in this figure
we draw several pdfs of the sample average xn. the shapes of the pdfs are getting
narrower as the variance of the random variable shrinks. since the pdfs become narrower,
the probability p[|xn‚àí¬µ|> Œµ] becomes more unlikely. at the limit when n‚Üí ‚àû , the
probability vanishes. the weak law of large numbers asserts that this happens for any set of
i.i.d. random variables. it says that the sequence of probability values Œ¥ndef=p[|xn‚àí¬µ|> Œµ]
355chapter 6. sample statistics
will converge to zero.
figure 6.13: the weak law of large numbers states that as nincreases, the variance of the sample
average xnshrinks. as a result, the probability p[|xn‚àí¬µ|> Œµ]decreases and eventually vanishes.
note that the convergence here is that of the sequence of probabilities p[|xn‚àí¬µ|> Œµ], which is just
a sequence of numbers.
what is the weak law of large numbers?
letxnbe the sample average of i.i.d. random variables x1, . . . , x n.
lim
n‚Üí‚àûp
|xn‚àí¬µ|> Œµ
= 0. (6.31)
¬àfor details, see theorem 6.16.
¬àthe wlln concerns the sequence of probability values Œ¥n=p[|xn‚àí¬µ|> Œµ].
¬àthe probabilities converge to zero as ngrows.
¬àit is weak because having a small probability does not exclude the possibility of
happening.
6.3.3 convergence in probability
the example above tells us that in order to show convergence, we need to first compute the
probability Œ¥nof each event and then take the limit of the sequence, e.g., the one shown in
the table below:
Œ¥1 Œ¥5 Œ¥10 Œ¥100 Œ¥1000 Œ¥10000
0.9203 0.8231 0.7518 0.3173 0.0016 1.5240 √ó10‚àí23
therefore, the convergence is the convergence of the probability . since {Œ¥1, Œ¥2, . . .}is a
sequence of real numbers (between 0 and 1), any convergence results for real numbers apply
here.
note that the convergence controls only the probabilities. probability means chance.
therefore, having the limit converging to zero only means that the chance of happening is
becoming smaller and smaller. however, at any n, there is still a chance that some bad
event can happen.
3566.3. law of large numbers
what do we mean by a bad event? assume that xnare fair coins. the sample average
xn= (1/n)pn
n=1xnis more or less equal to 1/2 as ngrows. however, even if nis a
large number, say n= 1000, we are still not certain that the sample average is exactly 1/2.
it is possible, though very unlikely, that we obtain 1000 heads or 1000 tails (so that the
sample average is ‚Äú1‚Äù or ‚Äú0‚Äù). the bottom line is: having a probability converging to zero
only means that for any tolerance level we can always find an nlarge enough so that the
probability is smaller than that tolerance.
the type of convergence described by the weak law of large numbers is known as the
convergence in probability .
definition 6.6. a sequence of random variables a1, . . . , a nconverges in probability
to a deterministic number Œ±if for every Œµ >0,
lim
n‚Üí‚àûp[|an‚àíŒ±|> Œµ] = 0. (6.32)
we write anp‚ÜíŒ±to denote convergence in probability.
the following two examples illustrate how to prove convergence in probability.
example 6.11 . let x1, . . . , x nbe i.i.d. random variables with xn‚àºuniform(0 ,1).
define an= min( x1, . . . , x n). show that anconverges in probability to zero.
solution . (without determining the pdf of an, we notice that as nincreases, the
value of anwill likely decrease. therefore, we should expect anto converge to zero.)
pick an Œµ >0. it follows that
p[|an‚àí0| ‚â•Œµ] =p[min( x1, . . . , x n)‚â•Œµ], because xn‚â•0
=p[x1‚â•Œµand¬∑¬∑¬∑andxn‚â•Œµ]
=p 
x1‚â•Œµ
¬∑¬∑¬∑p 
xn‚â•Œµ
= (1‚àíŒµ)n.
setting the limit of n‚Üí ‚àû , we conclude that
lim
n‚Üí‚àûp[|an‚àí0| ‚â•Œµ] = lim
n‚Üí‚àû(1‚àíŒµ)n= 0.
therefore, anconverges to zero in probability.
practice exercise 6.9 . let x‚àºexponential(1). by evaluating the cdf, we know
thatp[x‚â•x] =e‚àíx. let an=x/n . prove that anconverges to zero in probability.
solution . for any Œµ >0,
p[|an‚àí0| ‚â•Œµ] =p[an‚â•Œµ]
=p[x‚â•nŒµ]
=e‚àínŒµ.
357chapter 6. sample statistics
putting n‚Üí ‚àû on both sides of the equation gives us
lim
n‚Üí‚àûp[|an‚àí0| ‚â•Œµ] = lim
n‚Üí‚àûe‚àínŒµ= 0.
thus, anconverges to zero in probability.
example 6.12 . construct an example such that anconverges in probability to some-
thing, but e[an] does not converge to the same thing.
solution . consider a sequence of random variables ansuch that
p[an=Œ±] =Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥1‚àí1
n, Œ± = 0,
1
n, Œ± =n2,
0, otherwise .
the pdf of the random variable anis shown in figure 6.14 .
figure 6.14: probability density function of the random variable an.
we first show that anconverges in probability to zero. let Œµ >0 be a fixed
constant. since Œµ >0,p[an‚â•Œµ] =1
nfor any n >‚àöŒµ. therefore, we have that
lim
n‚Üí‚àûp[|an‚àí0| ‚â•Œµ] = lim
n‚Üí‚àûp[an‚â•Œµ]
= lim
n‚Üí‚àû1
n= 0.
hence, anconverges to 0 in probability.
however, e[an] does not converge to zero, because
e[an] = 0¬∑
1‚àí1
n
+n2¬∑1
n=n.
soe[an] goes to infinity as ngrows.
6.3.4 can we prove wlln using chernoff‚Äôs bound?
the following discussion of using chernoff‚Äôs bound to prove wlln can be skipped if this
is your first time reading the book.
3586.3. law of large numbers
in proving wlln we use chebyshev‚Äôs inequality. can we use chernoff‚Äôs inequality (or
hoeffding‚Äôs) to prove the result? yes, we can use them. however, notice that the task here is
to prove convergence, not to find the bestconvergence. finding the best convergence means
finding the fastest decay rate of the probability sequence. chernoff‚Äôs bound (and hoeffding‚Äôs
inequality) offers a better decay rate. however, chernoff‚Äôs bound needs to be customized for
individual random variables. for example, chernoff‚Äôs bound for gaussian is different from
chernoff‚Äôs bound for exponential. this result makes chebyshev the most convenient bound
because it only requires the variance to be bounded.
what if we insist on using chernoff‚Äôs bound in proving the wlln? we can do that for
specific random variables. let‚Äôs consider two examples. the first example is the gaussian
random variable where xn‚àº n(0, œÉ2). we know that xn‚àº n(0, œÉ2/n). chernoff‚Äôs bound
shows that
p
|xn‚àí¬µ|> Œµ
‚â§2 exp
‚àíŒµ2n
2œÉ2
,
taking the limit on both sides, we have
lim
n‚Üí‚àûp
|xn‚àí¬µ|> Œµ
= lim
n‚Üí‚àû2 exp
‚àíŒµ2n
2œÉ2
= 0.
note that the rate of convergence here is exponential. the rate of convergence offered by
chebyshev is only linear. of course, you may argue that since xnis gaussian we have
closed-form expressions about the probability, so we do not need chernoff‚Äôs bound. this is
a legitimate point, and so here is an example where we do not have a closed-form expression
for the probability.
consider a sequence of arbitrary i.i.d. random variables x1, . . . , x nwith 0 ‚â§xn‚â§1.
then hoeffding‚Äôs inequality tells us that
p
|xn‚àí¬µ|> Œµ
‚â§2 exp
‚àí2Œµ2n	
.
taking the limit on both sides, we have
lim
n‚Üí‚àûp
|xn‚àí¬µ|> Œµ
= lim
n‚Üí‚àû2 exp
‚àí2Œµ2n	
= 0.
again, we obtain a wlln result, this time for i.i.d. random variables x1, . . . , x nwith
0‚â§xn‚â§1.
as you can see from these two examples, wlln can be proved in multiple ways
depending on how general the random variables need to be.
end of the discussions.
6.3.5 does the weak law of large numbers always hold?
the following discussion of the failure of the weak law of large numbers can be skipped if
this is your first time reading the book.
359chapter 6. sample statistics
the weak law of large numbers does not always hold. recall that when we prove the
weak law of large numbers using chebyshev‚Äôs inequality, we implicitly require that the
variance var[ xn] is finite. (look at the condition that e[x2]<‚àû.) thus for distributions
whose variance is unbounded, chebyshev‚Äôs inequality does not hold. one example is the
cauchy distribution. the pdf of a cauchy distribution is
fx(x) =Œ≥
œÄ(Œ≥2+x2),
where Œ≥is a parameter. letting Œ≥= 1,
e[x2] =z‚àû
‚àí‚àûx2
œÄ(1 +x2)dx=1
œÄz‚àû
‚àí‚àû1‚àí1
1 +x2dx
=1
œÄz‚àû
‚àí‚àûdx‚àí1
œÄz‚àû
‚àí‚àû1
1 +x2dx=1
œÄ
x‚àítan‚àí1(x)‚àû
x=‚àí‚àû=‚àû.
since the second moment is unbounded, the variance of xwill also be unbounded.
a perceptive reader may observe that even if e[x2] is unbounded, it does not mean
that the tail probability is unbounded. this is correct. however, for cauchy distributions,
we can show that the sample average xndoes not converge to the mean when n‚Üí ‚àû
(and so the wlln fails). to see this, we note that the characteristic function of a cauchy
random variable xis
1
œÄ(1 +x2)‚Üîe‚àí|œâ|.
so for the sample average xn=1
npn
n=1xn, the characteristic function is
e[e‚àíjœâxn] =e[e‚àíjœâ
npn
n=1xn] =ny
n=1e[e‚àíjœâ
nxn] =h
e‚àí|œâ|
nin
=e‚àí|œâ|,
which remains a cauchy distribution with Œ≥= 1. therefore, we have that
p[|xn| ‚â§Œµ] =zŒµ
‚àí‚àû1
œÄ(1 +x2)dx
=z0
‚àí‚àû1
œÄ(1 +x2)dx+zŒµ
01
œÄ(1 +x2)dx=1
2+1
œÄtan‚àí1(Œµ).
thus no matter how many samples we have, p[|xn| ‚â§Œµ] will never converge to 1 (so
p[|xn|> Œµ] will never converge to 0). therefore, wlln does not hold.
end of the discussion.
6.3.6 strong law of large numbers
since there is a ‚Äúweak‚Äù law of large numbers, you will not be surprised to learn that there
is a strong law of large numbers. the strong law is more restrictive than the weak law. any
sequence satisfying the strong law will satisfy the weak law, but not vice versa. since the
strong law is ‚Äústronger‚Äù, the proof is more involved.
3606.3. law of large numbers
theorem 6.17 (strong law of large numbers ).letx1, . . . , x nbe a sequence of
i.i.d. random variables with common mean ¬µand variance œÉ2. assume e[x4]<‚àû.
letxn=1
npn
n=1xnbe the sample average. then
ph
lim
n‚Üí‚àûxn=¬µi
= 1. (6.33)
the strong law flips the order of limit and probability . as you can see, the difference
between the strong law and the weak law is the order of the limit and the probability. in the
weak law, the limit is outside the probability, whereas, in the strong law, the limit is inside
the probability. this switch in order makes the interpretation of the result fundamentally
different. in the final analysis, the weak law concerns the limit of a sequence of probabilities
(which are just real numbers between 0 and 1). however, the strong law concerns the limit
of a sequence of random variables. the strong law answers the question, what is the limiting
object of the sample average as ngrows?
the strong law concerns the limiting object, not a sequence of numbers . what
is the ‚Äúlimiting object‚Äù? if we denote xnas the sample average using nsamples, then
we know that x1is a random variable, x2is a random variable, and all xn‚Äôs are random
variables. so we have a sequence of random variables. as ngoes to infinity, we can ask about
the limiting object lim n‚Üí‚àûxn. however, even without any deep analysis, you should be
able to see that lim n‚Üí‚àûxnis another random variable. the strong law says that this
limiting object will ‚Äúsuccessfully‚Äù become a deterministic number ¬µ, after a finite number
of ‚Äúfailures‚Äù.
the strong law asserts that there are a finite number of failures . let us explain
‚Äúsuccess‚Äù and ‚Äúfailure‚Äù. xnis a random variable, so it fluctuates. however, as ngoes to
infinity, the strong law says that the number of times where xnÃ∏=¬µwill be zero. that
is, there is a finite number of times where xnÃ∏=¬µ(i.e., fail), and afterward, you will be
perfectly fine (i.e., success). yes, perfectly fine means 100%. the weak law only guarantees
99.99%.
a good example for differentiating the weak law and the strong law is an electronic
dictionary that improves itself every time you use it. the weak law says that if you use
the dictionary for a long period, the probability of making an error will become small. you
will still get an error once in a while, but the probability is very small. this is a 99.99%
guarantee, and it is the weak law. the strong law says that the number of failures is finite.
after you have gone through this finite number of failures, you will be completely free of
error. this is a 100% guarantee by the strong law. when will you hit this magical number?
the strong law does not say when; it only asserts the existence of this number. however, this
existence is already good enough in many ways. it gives a certificate of assurance, whereas
the weak law still has uncertainty.
strong law Ã∏=deterministic. if the strong law offers a 100% guarantee, does it mean
that it is a deterministic guarantee? no, the strong law is still a probabilistic statement
because we are still using p[¬∑] to measure an event. the event can include measure-zero
subsets, and the measure-zero subsets can be huge. for example, the set of rational numbers
on the real line is a measure-zero set when measuring the probability using an integration.
the strong law does not handle those measure-zero subsets.
361chapter 6. sample statistics
6.3.7 almost sure convergence
the discussion below can be skipped if this is your first time reading the book.
the type of convergence used by the strong law of large numbers is the almost sure
convergence . it is defined formally as follows.
definition 6.7. a sequence of random variables a1, . . . , a nconverges almost surely
toŒ±if
ph
lim
n‚Üí‚àûan=Œ±i
= 1. (6.34)
we write ana.s.‚ÜíŒ±to denote almost sure convergence.
to prove almost sure convergence, one needs to show that the sequence anwill demonstrate
anÃ∏=Œ±for a finite number of times. afterward, anneeds to demonstrate an=Œ±.
example 6.13 .aconstruct a sequence of events that converges almost surely.
solution . let x1, . . . , x nbe i.i.d. random variables such that xn‚àºuniform(0 ,1).
define an= min( x1, . . . , x n). since anis nonincreasing and is bounded below by
zero, it must have a limit. let us call this limit
adef= lim
n‚Üí‚àûan.
then we can show that
p[a‚â•œµ] =p[min( x1, x2, . . .)‚â•œµ]
(a)
‚â§p[min( x1, x2, . . . , x n)‚â•œµ]
(b)=p[x1‚â•œµandx2‚â•œµand¬∑¬∑¬∑andxn‚â•œµ]
= (1‚àíœµ)n,
where ( a) holds because there are more elements in ( x1, x2, . . .) than in
(x1, x2, . . . , x n). therefore, the minimum value of the former is less than the mini-
mum value of the latter. ( b) holds because if min( x1, x2, . . . , x n)‚â•œµ, then xn‚â•œµ
for all n.
sincep[a‚â•œµ]‚â§(1‚àíœµ)nfor any n, the statement still holds as n‚Üí ‚àû . thus,
p[a‚â•œµ]‚â§lim
n‚Üí‚àû(1‚àíœµ)n= 0.
this shows p[a‚â•œµ] = 0 for any positive œµ. sop[a > œµ ] = 0, and hence p[a= 0] = 1.
since ais the limit of an, we conclude that
ph
lim
n‚Üí‚àûan= 0i
=p[a= 0] = 1 .
3626.3. law of large numbers
soanconverges to 0 almost surely.
athis example is modified from bertsekas and tsitsiklis, introduction to probability , chapter 5.5.
example 6.14 .aconstruct an example where a sequence of events converges in prob-
ability but does not converge almost surely.
solution . consider a discrete time arrival process. the set of times is partitioned into
consecutive intervals of the form
i1={2,3},
i2={4,5,6,7},
i3={8,9,10, . . . , 15},
...
ik={2k,2k+ 1, . . . , 2k+1‚àí1}.
therefore, the length of each interval is |i1|= 2,|i2|= 4, . . . , |ik|= 2k.
during each interval, there is exactly one arrival. define ynas a binary random
variable such that for every n‚ààik,
yn=(
1, with probability1
|ik|,
0, with probability 1 ‚àí1
|ik|.
for example, if n‚àà {2,3}, then p[yn= 1] =1
2. ifn‚àà {4,5,6,7}, then p[yn= 1] =1
4.
in general, we have that
lim
n‚Üí‚àûp[yn= 1] = lim
n‚Üí‚àû1
|ik|= lim
n‚Üí‚àû1
2k= 0,
and hence
lim
n‚Üí‚àûp[yn= 0] = lim
n‚Üí‚àû1‚àí1
2k= 1.
therefore, ynconverges to 0 in probability.
however, when we carry out the experiment, there is exactly one arrival per
interval according to the problem conditions. since we have an infinite number of
intervals i1, i2, . . ., we will have an infinite number of arrivals in total. as a result,
yn= 1 for infinitely many times. we do not know which ynwill equal 1 and which
ynwill equal to 0. however, we know that there are infinitely many ynthat are equal
to 1. therefore, in the sequence y1, y2, . . . , y n, . . ., we must have that the tail of the
sequence is 1. (if ynstops being 1 after some n, then we will not have an infinite
number of arrivals in total.)
since yn= 1 when nis large enough, it follows that
ph
lim
n‚Üí‚àûyn= 1i
= 1.
363chapter 6. sample statistics
equivalently, we can say that the sequence ynwill never take the value 0 when nis
large enough. thus,
ph
lim
n‚Üí‚àûyn= 0i
= 0.
therefore, yndoes not converge to 0 almost surely.
athis example is modified from bertsekas and tsitsiklis, introduction to probability , chapter 5.5.
end of the discussions.
6.3.8 proof of the strong law of large numbers
the strong law of large numbers can be proved in several ways. we present a proof based on
bertsekas and tsitsiklis, introduction to probability , problems 5.16 and 5.17, which require
a finite fourth moment e[x4
n]<‚àû. an alternative proof that requires only e[xn]<‚àûis
from billingsley, probability and measure , theorem 22.1.
the proof of the strong law of large numbers is beyond the scope of this book. this
section is optional.
lemma 6.4. consider non-negative random variables x1, . . . , x n. assume that
e"‚àûx
n=1xn#
<‚àû. (6.35)
then xna.s.‚Üí0.
proof . let s=pn
n=1xn. note that sis a random variable, and our assumption is that
e[s]<‚àû. thus, we argue that s <‚àûwith probability 1. if not, then swill have a positive
probability of being ‚àû. but if this happens, we will have e[s] =‚àûbecause (by the law of
total expectation):
e[s] =e[s|s= infinite]| {z }
=‚àûp[s= infinite] + e[s|s= finite] p[s= finite] .
now, since sis finite, the sequence {x1, . . . , x n, . . .}must converge to zero. otherwise,
ifxnis converging to some constants c >0, then summing the tail of the sequence (which
contains infinitely many terms) gives infinity:
s=x1+¬∑¬∑¬∑|{z}
=finite+xn+¬∑¬∑¬∑+|{z}
=infinite.
since the probability of sbeing finite is 1, it follows that {x1, . . . , x n}is converging
to zero with probability 1.
‚ñ°
3646.3. law of large numbers
theorem 6.18 (strong law of large numbers ).letx1, . . . , x nbe a sequence of
i.i.d. random variables with common mean ¬µand variance œÉ2. assume e[x4
n]<‚àû.
letxn=1
npn
n=1xnbe the sample average. then
ph
lim
n‚Üí‚àûxn=¬µi
= 1. (6.36)
proof . we first prove the case where e[xn] = 0. to establish that xn‚Üí0 with probabil-
ity 1, we use the lemma to show that
e"‚àûx
n=1|xn|#
<‚àû.
but to show e[p‚àû
n=1|xn|]<‚àû, we note that |x| ‚â§1 +x4. therefore, e[p‚àû
n=1|xn|]‚â§
1 +e[p‚àû
n=1x4
n], and hence we just need to show that
e"‚àûx
n=1x4
n#
<‚àû.
let us expand the term e[x4
n] as follows:
e[x4
n] =1
n4nx
n1=1nx
n2=1nx
n3=1nx
n4=1e[xn1xn2xn3xn4].
there are five possibilities for e[xn1xn2xn3xn4]:
¬àall indices are different. then
e[xn1xn2xn3xn4] =e[xn1]e[xn2]e[xn3]e[xn4] = 0¬∑0¬∑0¬∑0 = 0 .
¬àone index is different from other three indices. for example, if n1is different from
n2, n3, n4, then
e[xn1xn2xn3xn4] =e[xn1]e[xn2xn3xn4] = 0¬∑e[xn2xn3xn4] = 0.
¬àtwo indices are identical. for example, if n1=n3, and n2=n4, then
e[xn1xn2xn3xn4] =e[xn1xn3]e[xn2xn4] =e[x2
n1x2
n2].
there are altogether 3 n(n‚àí1) of these cases: n(n‚àí1) comes from choosing n
followed by choosing n‚àí1, and 3 accounts for n1=n2Ã∏=n3=n4,n1=n3Ã∏=n2=n4,
andn1=n4Ã∏=n2=n3.
¬àtwo indices are identical, and two indices are different. for example, if n1=n3but
n2andn4are different. then
e[xn1xn2xn3xn4] =e[xn1xn3]e[xn2]e[xn4] =e[x2
n1]¬∑0¬∑0 = 0 .
365chapter 6. sample statistics
¬àall indices are identical. if n1=n2=n3=n4, then
e[xn1xn2xn3xn4] =e[x4
n1].
there are altogether ncases of this.
therefore, it follows that
e[x4
n] =ne[x4
1] + 3n(n‚àí1)e[x2
1x2
2]
n4.
since xy‚â§(x2+y2)/2, it follows that
e[x2
1x2
2]‚â§e[(x2
1)2+ (x2
2)2]/2 =e[x4
1+x4
2]/2 =e[x4
1].
substituting into the previous result,
e[x4
n]‚â§ne[x4
1] + 3n(n‚àí1)e[x4
1]
n4‚â§3n2
n4e[x4
1] =3
n2e[x4
1].
now, let us complete the proof.
e"‚àûx
n=1x4
n#
‚â§e"‚àûx
n=13
n2e[x4
1]#
<‚àû,
becausep‚àû
n=1(1/n2) is the bassel problem with a solution thatp‚àû
n=1(1/n2) =œÄ2/6.
consequently, we have shown that ehp‚àû
n=1x4
ni
<‚àû, which implies ep‚àû
n=1|xn|
<‚àû.
then, by the lemma, we have xnconverging to 0 with probability 1, which proves the result.
ife[xn] =¬µ, then just replace xnwith yn=xn‚àí¬µin the above arguments. then we
can show that ynconverges to 0 with probability 1, which is equivalent to xnconverging
to¬µwith probability 1.
end of the proof of strong law of large numbers.
6.4 central limit theorem
the law of large numbers tells us the mean of the sample average xn= (1/n)pn
n=1xn.
however, if you recall our experiment of throwing ndice and inspecting the pdf of the
sum of the numbers, you may remember that the convolution of an infinite number of
uniform distributions gives us a gaussian distribution. for example, we show a sequence
of experiments in figure 6.15 . in each experiment, we throw ndice and count the sum.
therefore, if each face of the die is denoted as xn, then the sum is x1+¬∑¬∑¬∑+xn. we plot
the pdf of the sum. as you can see in the figure, x1+¬∑¬∑¬∑+xnconverges to a gaussian.
this phenomenon is explained by the central limit theorem (clt) .
what does the central limit theorem say? let xnbe the sample average, and let
zn=‚àö
n
xn‚àí¬µ
œÉ
be the normalized variable. the central limit theorem is as follows:
3666.4. central limit theorem
figure 6.15: pictorial illustration of the central limit theorem. suppose we throw a die and record the
face. [left] if we only have one die, then the distribution of the face is uniform. [middle] if we throw
two dice, the distribution is the convolution of two uniform distributions. this will give us a triangle
distribution. [right] if we throw five dice, the distribution is becoming similar to a gaussian. the central
limit theorem says that as ngoes to infinity, the distribution of the sum will converge to a gaussian.
central limit theorem :
the cdf of znis converging pointwise to the cdf of gaussian(0,1).
note that we are very careful here. we are not saying that the pdf of znis converging to
the pdf of a gaussian, nor are we saying that the random variable znis converging to a
gaussian random variable. we are only saying that the values of the cdf are converging
pointwise. the difference is subtle but important.
to understand the difficulty and the core ideas, we first present the concept of conver-
gence in distribution.
6.4.1 convergence in distribution
definition 6.8. letz1, . . . , z nbe random variables with cdfs fz1, . . . , f znrespec-
tively. we say that a sequence of z1, . . . , z nconverges in distribution to a random
variable zwith cdf fzif
lim
n‚Üí‚àûfzn(z) =fz(z), (6.37)
for every continuous point zoffz. we write znd‚Üízto denote convergence in
distribution.
this definition involves many concepts, which we will discuss one by one. however, the
definition can be summarized in a nutshell as follows.
convergence in distribution = values of the cdf converge.
example 1 . (bernoulli ) consider flipping a fair coin ntimes. denote each coin flip as a
bernoulli random variable xn‚àºbernoulli( p), where n= 1,2, . . . , n . define znas the sum
367chapter 6. sample statistics
ofnbernoulli random variables, so that
zn=nx
n=1xn.
we know that the resulting random variable znis a binomial random variable with mean
npand variance np(1‚àíp). let us plot the pdf fzn(z) as shown in figure 6.16 .
figure 6.16: convergence in distribution. the convergence in distribution concerns the convergence of
the values of the cdf (not the pdf). in this figure, we let zn=x1+¬∑¬∑¬∑+xn, where xnis a
bernoulli random variable with parameter p. since a sum of bernoulli random variables is a binomial,
znis a binomial random variable with parameters (n, p). we plot the pdf of zn, which is a train of
delta functions, and compare it with the gaussian pdf. observe that the error, max z|fzn(z)‚àífz(z)|,
does notconverge to 0. the pdf of znis a binomial. a binomial is always a binomial. it will not turn
into a gaussian.
the first thing we notice in the figure is that as nincreases, the pdf of the binomial
has an envelope that is ‚Äúvery gaussian‚Äù. so one temptation is to say that the random
variable znis converging to another random variable z. in addition, we would think that
the pdfs converge in the sense that for allz,
fzn(z) =n
z
pz(1‚àíp)n‚àíz‚àí‚Üí fz(z) =1‚àö
2œÄœÉ2exp
‚àí(z‚àí¬µ)2
2œÉ2
,
where ¬µ=npandœÉ2=np(1‚àíp).
unfortunately this argument does not work, because fz(z) is continuous but fzn(z)
is discrete. the sample space of znand the sample space of zare completely different. in
fact, if we write fznas an impulse train, we observe that
fzn(z) =nx
i=0n
i
pi(1‚àíp)n‚àíiŒ¥(z‚àíi).
clearly, no matter how big the nis, the difference |fzn(z)‚àífz(z)|will never go to zero
for non-integer values of z. mathematically, we can show that
max
z|fzn(z)‚àífz(z)| Ã∏‚àí‚Üí 0,
asn‚Üí ‚àû .znis a binomial random variable regardless of n. it will not become a gaussian.
3686.4. central limit theorem
iffzn(z) is not converging to a gaussian pdf, how do we explain the convergence?
the answer is to look at the cdf. for discrete pdfs such as a binomial random variable,
the cdf is a staircase function. what we can show is that
fzn(z) =zx
i=0n
i
pi(1‚àíp)n‚àíi‚àí‚Üí fz(z) =zz
‚àí‚àû1‚àö
2œÄœÉ2exp
‚àí(t‚àí¬µ)2
2œÉ2
dt.
the difference between the pdf convergence and the cdf convergence is that the pdf
does not allow a meaningful ‚Äúdistance‚Äù between a discrete function and continuous function.
for cdf, the distance is well defined by taking the difference between the staircase function
and the continuous function. for example, we can compute
|fzn(z)‚àífz(z)|, for all continuous points zoffz,
and show that
max
z|fzn(z)‚àífz(z)| ‚àí‚Üí 0.
figure 6.17: convergence in distribution. this is the same as figure 6.16 , but this time we plot the
cdf of zn. the cdf is a staircase function. we compare it with the gaussian cdf. observe that the
error, max z|fzn(z)‚àífz(z)|, converges to zero as ngrows. convergence in distribution says that the
sequence of cdfs fzn(z)will converge to the limiting cdf fz(z), at all continuous points of fz(z).
we need to pay attention to the set of z‚Äôs. we do not evaluate all z‚Äôs but only the z‚Äôs
that are continuous points of fz. iffzis gaussian, this does not matter because all z‚Äôs
are continuous. however, for cdfs containing discontinuous points, our definition of con-
vergence in distribution will ignore these discontinuous points because they have a measure
zero.
example 2 . (poisson ) consider xn‚àºpoisson( Œª), and consider x1, . . . , x n. define zn=pn
n=1xn. it follows that e[zn] =pn
n=1e[xn] =nŒªand var[ zn] =pn
n=1var[xn] =nŒª.
moreover, we know that the sum of poissons remains a poisson. therefore, the pdf of zn
is
fzn(z) =‚àûx
k=0(nŒª)k
k!e‚àínŒªŒ¥(z‚àík) and fz(z) =1‚àö
2œÄœÉ2exp
‚àí(z‚àí¬µ)2
2œÉ2
,
369chapter 6. sample statistics
0 2 4 6 800.050.10.150.2
poisson
gaussian
0 5 10 15 2000.050.10.15
poisson
gaussian
0 20 40 60 80 10000.020.040.06
poisson
gaussian
0 2 4 6 800.20.40.60.81
poisson
gaussian
0 5 10 15 2000.20.40.60.81
poisson
gaussian
0 20 40 60 80 10000.20.40.60.81
poisson
gaussian
(a)n= 4 (b) n= 10 (c) n= 50
figure 6.18: convergence in distribution for a sum of poisson random variables. here we assume that
x1, . . . , x nare i.i.d. poisson with a parameter Œª. we let zn=pn
n=1xnbe the sum, and compute the
corresponding pdf (top row) and cdfs (bottom row). just as with the binomial example, the pdfs of
the poisson do not converge but the cdfs of the poisson converge to the cdf of a gaussian.
where ¬µ=nŒªandœÉ2=nŒª. again, fzndoes not converge to fz. however, if we compare
the cdf, we can see from figure 6.18 that the cdf of the poisson is becoming better
approximated by the gaussian.
interpreting ‚Äúconvergence in distribution‚Äù . after seeing two examples, you should
now have some idea of what ‚Äúconvergence in distribution‚Äù means. this concept applies to
the cdfs. when we write
lim
n‚Üí‚àûfzn(z) =fz(z), (6.38)
we mean that fzn(z) is converging to the value fz(z), and this relationship holds for all
the continuous z‚Äôs offz. it does not say that the random variable znis becoming another
random variable z.
znd‚àí‚Üízis equivalent to lim n‚Üí‚àûfzn(z) =fz(z).
example 3 . (exponential ) so far, we have studied the sum of discrete random variables.
now, let‚Äôs take a look at continuous random variables. consider xn‚àºexponential( Œª), and
letx1, . . . , x nbe i.i.d. copies. define zn=pn
n=1xn. then e[zn] =pn
n=1e[xn] =n/Œª
and var[ zn] =n
Œª2. how about the pdf of zn? using the characteristic functions, we know
that
fxn(x) =Œªe‚àíŒªx f‚Üê‚Üí œÜxn(jœâ) =Œª
Œª+jœâ.
3706.4. central limit theorem
therefore, the product is
œÜzn(jœâ) =ny
n=1œÜxn(jœâ) =Œªn
(Œª+jœâ)n=Œªn
(Œª+jœâ)n√ó(n‚àí1)!
(n‚àí1)!
=Œªn
(n‚àí1)!¬∑(n‚àí1)!
(Œª+jœâ)nf‚Üê‚ÜíŒªn
(n‚àí1)!zn‚àí1e‚àíŒªz=fzn(z).
this resulting pdf fzn(z) =Œªn
(n‚àí1)!zn‚àí1e‚àíŒªzis known as the erlang distribution . the
cdf of the erlang distribution is
fzn(z) =zz
‚àí‚àûfzn(t)dt
=zz
0Œªn
(n‚àí1)!tn‚àí1e‚àíŒªtdt
= gamma function( z, n),
where the last integral is known as the incomplete gamma function, evaluated at z.
given all these, we can now compare the pdf and the cdf of znversus z.figure 6.19
shows the pdfs and the cdfs of znfor various nvalues. in this experiment we set Œª= 1.
as we can see from the experiment, the erlang distribution‚Äôs pdf and cdf converge to
a gaussian. in fact, for continuous random variables such as exponential random variables,
we indeed have the random variable znconverging to the random variable z. this is quite
different from discrete random variables, where zndoes not converge to zbut only fzn
converges to fz.
0 2 4 6 800.050.10.150.20.25
sum of exponential
gaussian
0 5 10 15 2000.050.10.15
sum of exponential
gaussian
0 20 40 60 80 10000.020.040.06
sum of exponential
gaussian
0 2 4 6 800.20.40.60.81
sum of exponential
gaussian
0 5 10 15 2000.20.40.60.81
sum of exponential
gaussian
0 20 40 60 80 10000.20.40.60.81
sum of exponential
gaussian
(a)n= 4 (b) n= 10 (c) n= 50
figure 6.19: convergence in distribution for a sum of exponential random variables. here we assume
thatx1, . . . , x nare i.i.d. exponentials with a parameter Œª. we define zn=pn
n=1xnbe the sum.
it is known that the sum of exponentials is an erlang. we compute the corresponding pdf (top row)
and cdfs (bottom row). unlike the previous two examples, in this example we see that both pdfs and
cdfs of the erlang distribution are converging to a gaussian.
371chapter 6. sample statistics
isd‚àí‚Üístronger thanp‚àí‚Üí?convergence in distribution is actually weaker than con-
vergence in probability. consider a continuous random variable xwith a symmetric pdf
fx(x) such that fx(x) =fx(‚àíx). it holds that the pdf of ‚àíxhas the same pdf. if
we define the sequence zn=xifnis odd and zn=‚àíxifnis even, and let z=x,
then fzn(z) =fz(z) for every zbecause the pdf of xand‚àíxare identical. there-
fore, znd‚Üíz. however, znÃ∏p‚Üízbecause znoscillates between the random variables x
and‚àíx. these two random variables are different (although they have the same cdf)
because p[x=‚àíx] =p[{œâ:x(œâ) =‚àíx(œâ)}] =p[{œâ:x(œâ) = 0}] = 0.
6.4.2 central limit theorem
theorem 6.19 (central limit theorem ).letx1, . . . , x nbe i.i.d. random variables
of mean e[xn] =¬µand variance var[xn] =œÉ2. also, assume that e[|x3
n|]<‚àû. let
xn= (1/n)pn
n=1xnbe the sample average, and let zn=‚àö
n
xn‚àí¬µ
œÉ
. then
lim
n‚Üí‚àûfzn(z) =fz(z), (6.39)
where z=gaussian (0,1).
in plain words, the central limit theorem says that the sample average (which is
a random variable) has a cdf converging to the cdf of a gaussian. therefore, if we
want to evaluate probabilities associated with the sample average, we can approximate the
probability by the probability of a gaussian.
as we discussed above, the central limit theorem does not mean that the random
variable znis converging to a gaussian random variable, nor does it mean that the pdf
ofznis converging to the pdf of a gaussian. it only means that the cdf of znis
converging to the cdf of a gaussian. many people think that the central limit theorem
means ‚Äúsample average converges to gaussian‚Äù. this is incorrect for the above reasons.
however, it is not completely wrong. for continuous random variables where both pdf
and cdf are continuous, we will not run into situations where the pdf is a train of delta
functions. in this case, convergence in cdf can be translated to convergence in pdf.
the power of the central limit theorem is that the result holds for anydistribution
ofx1, . . . , x n. that is, regardless of the distribution of x1, . . . , x n, the cdf of xnis
approaching a gaussian.
summary of the central limit theorem
¬àx1, . . . , x nare i.i.d. random variables, with mean ¬µand variance œÉ2. they are
not necessarily gaussians.
¬àdefine the sample average as xn= (1/n)pn
n=1xn, and let zn=‚àö
n
x‚àí¬µ
œÉ
.
¬àthe central limit theorem says znd‚àí‚Üígaussian(0 ,1). equivalently, the the-
orem says that nxnd‚àí‚Üígaussian( ¬µ, œÉ2).
¬àso if we want to evaluate the probability of xn‚àà a for some set a, we can
3726.4. central limit theorem
approximate the probability by evaluating the gaussian:
p[xn‚àà a]‚âàz
a1p
2œÄ(œÉ2/n)exp
‚àí(y‚àí¬µ)2
2(œÉ2/n)
dy.
¬àclt does notsay that the pdf of xnis becoming a gaussian pdf.
¬àclt only says that the cdf of xnis becoming a gaussian cdf.
if the set ais an interval, we can use the standard gaussian cdf to compute the
probability.
corollary 6.3. letx1, . . . , x nbe i.i.d. random variables with mean ¬µand vari-
ance œÉ2. define the sample average as xn= (1/n)pn
n=1xn. then
p[a‚â§xn‚â§b]‚âàœÜ‚àö
nb‚àí¬µ
œÉ
‚àíœÜ‚àö
na‚àí¬µ
œÉ
, (6.40)
where œÜ(z) =rz
‚àí‚àû1‚àö
2œÄe‚àíx2
2dxis the cdf of the standard gaussian.
proof . by the central limit theorem, we know that xnd‚àí‚Üígaussian( ¬µ,œÉ2
n). therefore,
p[a‚â§xn‚â§b]‚âàzb
a1p
2œÄ(œÉ2/n)exp
‚àí(y‚àí¬µ)2
2(œÉ2/n)
dy
=z‚àö
nb‚àí¬µ
œÉ
‚àö
na‚àí¬µ
œÉ1‚àö
2œÄe‚àíy2
2dy= œÜ‚àö
nb‚àí¬µ
œÉ
‚àíœÜ‚àö
na‚àí¬µ
œÉ
.
‚ñ°
figure 6.20: the central limit theorem says that if we want to evaluate the probability p[a‚â§xn‚â§b],
where xn= (1/n)pn
n=1xnis the sample average of i.i.d. random variables x1, . . . , x n, we can
approximate the probability by integrating the gaussian pdf.
a graphical illustration of the clt is shown in figure 6.20 , where we use a binomial
random variable (which is the sum of i.i.d. bernoulli) as an example. the clt does not say
373chapter 6. sample statistics
that the binomial random variable is becoming a gaussian. it only says that the probability
covered by the binomial can be approximated by the gaussian.
the following proof of the central limit theorem can be skipped if this is your first time
reading the book.
proof of the central limit theorem . we now give a ‚Äúproof‚Äù of the central limit
theorem. technically speaking, this proof does not prove the convergence of the cdf as the
theorem claims; it only proves that the moment-generating function converges. the actual
proof of the cdf convergence is based on the berry-esseen theorem, which is beyond the
scope of this book. however, what we prove below is still useful because it gives us some
intuition about why gaussian is the limiting random variable we should consider in the first
place.
letzn=‚àö
n
xn‚àí¬µ
œÉ
. it follows that e[zn] = 0 and var[ zn] = 1. therefore, if we
can show that znis converging to a standard gaussian random variable z‚àºgaussian(0 ,1),
then by the linear transformation property of gaussian, y=œÉ‚àö
nz+¬µwill be gaussian( ¬µ, œÉ2/n).
our proof is based on analyzing the moment-generating function of zn. in particular,
mzn(s)def=e[eszn] =e
es‚àö
nxn‚àí¬µ
œÉ
=ny
n=1eh
es
œÉ‚àö
n(xn‚àí¬µ)i
.
expanding the exponential term using the taylor expansion (chapter 1.2),
ny
n=1eh
es
œÉ‚àö
n(xn‚àí¬µ)i
=ny
n=1e
1 +s
œÉ‚àö
n(xn‚àí¬µ) +s2
2œÉ2n(xn‚àí¬µ)2+o(xn‚àí¬µ)3
œÉ3n‚àö
n
=ny
n=1
1 +s
œÉ‚àö
ne[xn‚àí¬µ] +s2
2œÉ2ne
(xn‚àí¬µ)2
=
1 +s2
2nn
.
it remains to show that
1 +s2
2nn
‚Üíes2/2. if we can show that, we have shown that the
mgf of znis also the mgf of gaussian(0 ,1). to this end, we consider log(1 + x). by the
taylor approximation, we have that
log(1 + x)‚âàlog(1) +d
dxlogx|x=1
x+d2
dx2logx|x=1x2
2+o(x3).
therefore, we have log
1 +s2
2n
‚âàs2
2n‚àís4
4n2. asn‚Üí ‚àû , the limit becomes
lim
n‚Üí‚àûnlog
1 +s2
2n
‚âàs2
2‚àílim
n‚Üí‚àûs4
4n=s2
2,
and so taking the exponential on both sides yields lim n‚Üí‚àû
1 +s2
2nn
=es2
2. therefore,
we conclude that lim n‚Üí‚àûmzn(s) =es2
2, and so znis converging to a gaussian.
3746.4. central limit theorem
‚ñ°
limitation of our proof . the limitation of our proof lies in the issue of whether the
integration and the limit are interchangeable:
lim
n‚Üí‚àûmzn(s) = lim
n‚Üí‚àûz
fzn(z)eszdz
?=z
lim
n‚Üí‚àûfzn(z)
eszdz.
if they were, then proving lim n‚Üí‚àûmzn(s) =mz(s) is sufficient to claim fzn(z)‚Üífz(z).
however, we know that the latter is not true in general. for example, if fzn(z) is a train of
delta functions, then the limit and the integration are not interchangeable.
berry-esseen theorem . the formal way of proving the central limit theorem is to
prove the berry-esseen theorem. the theorem states that
sup
z‚ààrfzn(z)‚àífz(z)‚â§cŒ≤
œÉ3‚àö
n,
where Œ≤andcare universal constants. here, you can more or less treat the supremum
operator as the maximum. the left-hand side represents the worst-case error of the cdf
fzncompared to the limiting cdf fz. the right-hand side involves several constants c,
Œ≤, and œÉ, but they are fixed.
asngoes to infinity, the right-hand side will converge to zero. therefore, if we can
prove this result, then we have proved the actual central limit theorem. in addition,
we have found the rate of convergence since the right-hand side tells us that the error
drops at the rate of 1 /‚àö
n, which is not particularly fast but is sufficient for our purpose.
unfortunately, proving the berry-esseen theorem is not easy. one of the difficulties, for
example, is that one needs to deal with the infinite convolutions in the time domain or the
frequency domain.
interpreting our proof . if our proof is not completely valid, why do we mention it?
for one thing, it provides us with some useful intuition. for most of the (well-behaving)
random variables whose moments are finite, the exponential term in the moment-generating
function can be truncated to the second-order polynomial. since a second-order polynomial
is a gaussian, it naturally concludes that as long as we can perform such truncation the
truncated random variable will be gaussian.
to convince you that the gaussian mgf is the second-order approximation to other
mgfs, we use bernoulli as an example. let x1, . . . , x nbe i.i.d. bernoulli with a parame-
terp. then the moment-generating function of xn= (1/n)pn
n=1xnwould be:
mxn(s) =e[esx] =e[es1
npn
n=1xn] =ny
n=1e[es
nxn]
= (1‚àíp+pes
n)n‚âà
1‚àíp+p
1 +s
n+s2
2n2n
=
1 +sp
n+s2p
2n2n
.
375chapter 6. sample statistics
using the logarithmic approximation, it follows that
logmxn(s) =nlog
1 +sp
n+s2p
2n2
‚âànsp
n+s2p
2n2
‚àín
2sp
n+s2p
2n22
‚âàsp+s2p(1‚àíp)
2ndef= log my(s).
taking the exponential on both sides, we have that
my(s) = exp
sp+s2p(1‚àíp)
2n
,
which is the mgf of a gaussian random variable y‚àºgaussian
p,p(1‚àíp)
n
.
figure 6.21 shows several mgfs. in each of the subfigures we plot the exact mgf
mxn(s) = (1 ‚àíp+pes
n)nas a function of s. (the parameter pin this example is p= 0.5.) we
vary the number n, and we inspect how the shape of mxn(s) changes. on top of the exact
mgfs, we plot the gaussian approximations my(s) = expn
sp+s2p(1‚àíp)
2no
. according to
our calculation, this gaussian approximation is the second-order approximation to the exact
mgf. the figures show the effect of the second-order approximation. for example, in (a)
when n= 2 the gaussian is a quadratic approximation of the exact mgf. for (b) and (c),
asnincreases, the approximation improves.
-10 -5 0 5 1010-210-1100101102103104105
binomial mgf
gaussian mgf
-10 -5 0 5 1010-210-1100101102103104105
binomial mgf
gaussian mgf
-10 -5 0 5 1010-210-1100101102103104105
binomial mgf
gaussian mgf
(a)n= 2 (b) n= 4 (c) n= 10
figure 6.21: explanation of the central limit theorem using the function. in this set of plots, we show
the mgf of the random variable xn= (1/n)pn
n=1xn, where x1, . . . , x nare i.i.d. bernoulli random
variables. the exact mgf of xnis the binomial, whereas the approximated mgf is the gaussian. we
observe that as nincreases, the gaussian approximation to the exact mgf improves.
the reason why the second-order approximation works for gaussian is that when n
increases, the higher order moments of xnvanish and only the leading first two moments
survive. the mgfs are becoming flat because my(s) = expn
sp+s2p(1‚àíp)
2no
converges to
exp{sp}when n‚Üí ‚àû . taking the inverse laplace transform, my(s) = exp {sp}corresponds
to a delta function. this makes sense because as ngrows, the variance of the xshrinks.
end of the discussion.
3766.4. central limit theorem
6.4.3 examples
example 6.15 . prove the equivalence of a few statements.
¬à‚àö
n
xn‚àí¬µ
œÉd‚Üígaussian(0 ,1)
¬à‚àö
n(xn‚àí¬µ)d‚Üígaussian(0 , œÉ2)
¬à‚àö
nxnd‚Üígaussian( ¬µ, œÉ2)
solution . the proof is based on the linear transformation property of gaussian ran-
dom variables. for example, if the first statement is true, then the second statement
is also true because
lim
n‚Üí‚àûf‚àö
n(xn‚àí¬µ)(z) = lim
n‚Üí‚àûp[‚àö
n(xn‚àí¬µ)‚â§z]
= lim
n‚Üí‚àûp‚àö
nxn‚àí¬µ
œÉ
‚â§z
œÉ
=zz/œÉ
‚àí‚àû1‚àö
2œÄe‚àít2
2dt
=zz
‚àí‚àû1‚àö
2œÄœÉ2e‚àít2
2œÉ2dt.
the other results can be proved similarly.
example 6.16 . suppose xn‚àºpoisson(10) for n= 1, . . . , n , and let xnbe the
sample average. use the central limit theorem to approximate p[9‚â§xn‚â§11] for
n= 20.
solution . we first show that
e[xn] =e"
1
nnx
n=1xn#
=1
nnx
n=1e[xn] = 10 ,
var[xn] =1
n2nx
n=1var [xn] =1
nvar[xn] =10
20=1
2.
therefore, the central limit theorem implies that xnd‚àí‚Üígaussian 
10,1
2
. the
probability is
p[9‚â§xn‚â§11]‚âàœÜ 
11‚àí10p
1/2!
‚àíœÜ 
9‚àí10p
1/2!
= œÜ1‚àö
0.5
‚àíœÜ
‚àí1‚àö
0.5
= 0.9214‚àí0.0786 = 0 .8427.
377chapter 6. sample statistics
we can also do an exact calculation to verify our approximation. let sn=pn
n=1xnso that xn=sn
n. since a sum of poisson remains a poisson, it follows that
sn‚àºpoisson(10 n) = poisson(200) .
consequently,
p[9‚â§xn‚â§11] =p[180‚â§sn‚â§220]
=220x
‚Ñì=0200‚Ñìe‚àí200
‚Ñì!‚àí180x
‚Ñì=0200‚Ñìe‚àí200
‚Ñì!= 0.9247‚àí0.0822 = 0 .8425.
note that this is an exact calculation subject to numerical errors when evaluating the
finite sums. the proximity to the gaussian approximation shows the convenience of
the central limit theorem.
example 6.17 . suppose you have collected n= 100 data points from an unknown
distribution. the only thing you know is that the true population mean is ¬µ= 500
and the standard deviation is œÉ= 80. (note that this distribution is not necessarily a
gaussian.)
(a) find the probability that the sample mean will be inside the interval (490 ,510).
(b) find an interval such that 95% of the sample average is covered.
solution . to solve (a), we note that xnd‚Üígaussian
500,
80‚àö
1002
. therefore,
p[490‚â§xn‚â§510] = œÜ 
510‚àí500
80‚àö
100!
‚àíœÜ 
490‚àí500
80‚àö
100!
= œÜ(1 .25)‚àíœÜ(‚àí1.25) = 0 .7888.
to solve (b), we know that œÜ( x) = 0 .025 implies that x=‚àí1.96, and œÜ( x) = 0 .975
implies that x= +1 .96. so
y‚àí500
80‚àö
100=¬±1.96‚áí y= 484 .32 or y= 515 .68.
therefore, p[484.32‚â§xn‚â§515.68] = 0 .95.
6.4.4 limitation of the central limit theorem
if we recall the statement of the central limit theorem (berry-esseen), we observe that
the theorem states only that
lim
n‚Üí‚àûp‚àö
nxn‚àí¬µ
œÉ
‚â§Œµ
= lim
n‚Üí‚àûfzn(Œµ) =fz(Œµ) = œÜ( Œµ).
3786.4. central limit theorem
rearranging the terms,
lim
n‚Üí‚àûp
xn‚â§¬µ+œÉŒµ‚àö
n
= œÜ(Œµ).
this implies that the approximation is good only when the deviation Œµissmall .
let us consider an example to illustrate this idea. consider a set of i.i.d. exponential
random variables x1, . . . , x n, where xn‚àºexponential( Œª). let sn=x1+¬∑¬∑¬∑+xnbe
the sum, and let x=sn/nbe the sample average. then, according to chapter 6.4.1, sn
is an erlang distribution sn‚àºerlang( n, Œª) with a pdf
fsn(x) =Œªn
(n‚àí1)!xn‚àí1e‚àíŒªx.
practice exercise 6.10 . let sn‚àºerlang( n, Œª) with a pdf fsn(x). show that if
yn=asn+bfor any constants aandb, then
fyn(y) =1
afsny‚àíb
a
.
solution : this is a simple transformation of random variables:
fyn(y) =p[y‚â§y] =p[asn+b‚â§y] =p
sn‚â§y‚àíb
a
=zy‚àíb
a
‚àí‚àûfsn(x)dx.
hence, using the fundamental theorem of calculus,
fyn(y) =d
dyzy‚àíb
a
‚àí‚àûfsn(x)dx=1
afsny‚àíb
a
.
we are interested in knowing the statistics of xnand comparing it with a gaussian.
to this end, we construct a normalized variable
zn=xn‚àí¬µ
œÉ/‚àö
n,
where ¬µ=e[xn] =1
ŒªandœÉ2= var[ xn] =1
Œª2. then
zn=sn/n‚àí¬µ
œÉ/‚àö
n=sn‚àín¬µ
œÉ‚àö
n=Œª‚àö
nsn‚àí‚àö
n
using the result of the practice exercise, by mapping a=Œª‚àö
nandb=‚àí‚àö
n, it follows that
fzn(z) =‚àö
n
Œªfsn 
z+‚àö
n
Œª‚àö
n!
.
now we compare znwith the standard gaussian z‚àºgaussian(0 ,1). according to the
central limit theorem, the standard gaussian is a good approximation to the normalized
379chapter 6. sample statistics
sample average zn. to compare the two results, we conduct a numerical experiment. we
letŒª= 1 and we vary n. we plot the pdf fzn(z) as a function of z, for different n‚Äôs, in
figure 6.22 . in addition, we plot the pdf fz(z), which is the standard gaussian.
the plot in figure 6.22 shows that while the central limit theorem provides a good
approximation, the approximation is only good for values that are close to the mean. for
thetails, the gaussian approximation is not as good.
-1 0 1 2 3 4 510-610-410-2100
n = 1
n = 10
n = 100
n = 1000
gaussian
figure 6.22: clt fails at the tails. we note that x1, . . . , x nare i.i.d. exponential with a parameter
Œª= 1. we plot the pdfs of the normalized sample average zn=xn‚àí¬µ
œÉ/‚àö
nby varying n. we plot the pdf
of the standard gaussian z‚àºgaussian (0,1)on the same grid. note that the gaussian approximation
is good for values that are close to the mean. for the tails, the gaussian approximation is not very
accurate.
the limitation of the central limit theorem is attributable to the fact that gaussian
is a second-order approximation. if a random variable has a very large third moment, the
second-order approximation may not be sufficient. in this case, we need a much larger nto
drive the third moment to a small value and make the gaussian approximation valid.
when will the central limit theorem fail?
¬àthe central limit theorem fails when nis small.
¬àthe central limit theorem fails if the third moment is large. as an extreme
case, a cauchy random variable does not have a finite third moment. the central
limit theorem is not valid for this case.
¬àthe central limit theorem can only approximate the probability for input val-
ues near the mean. it does not approximate the tails, for which we need to use
chernoff‚Äôs bound.
6.5 summary
why do we need to study the sample average? because it is the summary of the dataset. in
machine learning, one of the most frequently asked questions is about the number of training
3806.6. references
samples required to train a model. the answer can be found by analyzing the average number
of successes and failures as the number of training samples grows. for example, if we define
fas the classifier that takes a data point xnand predicts a label f(xn), we hope that it
will match with the true label yn. if we define an error
en=(
1, f (xn) =yn correct classification ,
0, f (xn)Ã∏=yn incorrect classification ,
then enis a bernoulli random variable, and the total loss e=1
npn
n=1enwill be the
training loss. but what is1
npn
n=1en? it is exactly the sample average of en. therefore, by
analyzing the sample average ewe will learn something about the generalization capability
of our model.
how should we study the sample average? by understanding the law of large numbers
and the central limit theorem, as we have seen in this chapter.
¬àlaw of large numbers: xconverges to the true mean ¬µasngrows.
¬àcentral limit theorem: the cdf of xcan be approximated by the cdf of a
gaussian, as ngrows.
performance guarantee? the other topic we discussed in this chapter is the concept
of convergence type. there are essentially four types of convergence, ranked in the order of
restrictions.
¬àdeterministic convergence : a sequence of deterministic numbers converges to
another deterministic number. for example, the sequence 1 ,1
2,1
3,1
4, . . .converges
to 0 deterministically. there is nothing random about it.
¬àalmost sure convergence : randomness exists, and there is a probabilistic con-
vergence. almost sure convergence means that there is zero probability of failure
after a finite number of failures.
¬àconvergence in probability : the sequence of probability values converges, i.e.,
the chance of failure is going to zero. however, you can still fail even if your n
is large.
¬àconvergence in distribution : the probability values can be approximated by
the cdf of a gaussian.
6.6 references
moment-generating and characteristic functions
6-1 dimitri p. bertsekas and john n. tsitsiklis, introduction to probability , athena sci-
entific, 2nd edition, 2008. chapter 4.4.
381chapter 6. sample statistics
6-2 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. chapters 4.5 and 4.7.
6-3 athanasios papoulis and s. unnikrishna pillai, probability, random variables and
stochastic processes , mcgraw-hill, 4th edition, 2001. chapters 5.5 and 7.2.
6-4 henry stark and john woods, probability and random processes with applications
to signal processing , prentice hall, 3rd edition, 2001. chapters 4.5 and 4.7.
6-5 sheldon ross, a first course in probability , prentice hall, 8th edition, 2010. chapter
7.7.
6-6 john a. gubner, probability and random processes for electrical and computer en-
gineers , cambridge university press, 2006. chapter 4.3.
basic probability inequality
6-7 dimitri p. bertsekas and john n. tsitsiklis, introduction to probability , athena sci-
entific, 2nd edition, 2008. chapter 5.1.
6-8 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. chapters 6 and 8.
6-9 athanasios papoulis and s. unnikrishna pillai, probability, random variables and
stochastic processes , mcgraw-hill, 4th edition, 2001. chapter 7.4.
6-10 sheldon ross, a first course in probability , prentice hall, 8th edition, 2010. chapter
8.2.
6-11 larry wasserman, all of statistics , springer 2003. chapter 4.
concentration inequalities
6-12 larry wasserman, all of statistics , springer 2003. chapter 4.
6-13 martin wainwright, high-dimensional statistics , cambridge university press, 2019.
chapter 2.1.
6-14 stephane boucheron, gabor lugosi and pascal massart, concentration inequalities ,
oxford university press, 2013. chapters 2.1 and 2.2.
law of large numbers
6-15 dimitri p. bertsekas and john n. tsitsiklis, introduction to probability , athena sci-
entific, 2nd edition, 2008. chapters 5.2, 5.3, 5.5.
6-16 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. chapters 7.1, 7.2, 7.4
6-17 athanasios papoulis and s. unnikrishna pillai, probability, random variables and
stochastic processes , mcgraw-hill, 4th edition, 2001. chapter 7.4.
3826.7. problems
6-18 sheldon ross, a first course in probability , prentice hall, 8th edition, 2010. chapter
8.2, 8.4.
6-19 john a. gubner, probability and random processes for electrical and computer en-
gineers , cambridge university press, 2006. chapters 3.3, 14.1, 14.3.
6-20 larry wasserman, all of statistics , springer 2003. chapter 5.1 - 5.3.
6-21 patrick billingsley, probability and measure , wiley 1995. section 22.
central limit theorem
6-22 dimitri p. bertsekas and john n. tsitsiklis, introduction to probability , athena sci-
entific, 2nd edition, 2008. chapter 5.4.
6-23 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. chapter 7.3.
6-24 athanasios papoulis and s. unnikrishna pillai, probability, random variables and
stochastic processes , mcgraw-hill, 4th edition, 2001. chapter 7.4.
6-25 sheldon ross, a first course in probability , prentice hall, 8th edition, 2010. chapter
8.3.
6-26 john a. gubner, probability and random processes for electrical and computer en-
gineers , cambridge university press, 2006. chapters 5.6, 14.2.
6-27 larry wasserman, all of statistics , springer 2003. chapter 5.4.
6-28 patrick billingsley, probability and measure , wiley 1995. section 27.
6.7 problems
exercise 1. (video solution)
letx,y,zbe three independent random variables:
x‚àºbernoulli( p), y ‚àºexponential( Œ±), z ‚àºpoisson( Œª)
find the function for the following random variables.
(a)u=y+z
(b)u= 2z+ 3
(c)u=xy
(d)u= 2xy+ (1‚àíx)z
383chapter 6. sample statistics
exercise 2. (video solution)
two random variables xandyhave the joint pmf
p(x=n, y=m) =Œªn+m
1Œªm
2
(n+m)!m!e‚àí(Œª1+Œª2), m = 0,1,2, . . . , n ‚â• ‚àím.
letz=x+y. find the function mz(s) and the pmf of z.
exercise 3. (video solution)
letx0, x1, . . .be a sequence of independent random variables with pdf
fxk(x) =ak
œÄ(a2
k+x2), a k=1
2k+1,
fork= 0,1, . . .. find the pdf of y, where
y=‚àûx
k=0xk.
hint: you may find the characteristic function useful.
exercise 4.
the random variables xandyare independent and have pdfs
fx(x) =(
e‚àíx, x ‚â•0,
0, x < 0,and fy(y) =(
0, y > 0,
et, y ‚â§0.
find the pdf of z=x+y. (hint: use the characteristic function and the moment-
generating function.)
exercise 5.
a discrete random variable xhas a pmf
px(k) =1
2k, k = 1,2, . . . .
find the characteristic function œÜ x(jœâ).
exercise 6.
lett1, t2, . . .be i.i.d. random variables with pdf
ftk(t) =(
Œªe‚àíŒªt, t ‚â•0,
0, t < 0,
fork= 1,2,3, . . .. let sn=pn
k=1tk. find the pdf of sn.
exercise 7. (video solution)
in this exercise we will prove a variant of chebyshev when the variance œÉ2is unknown but
xis bounded between a‚â§x‚â§b.
3846.7. problems
(a) let Œ≥‚ààr. find a Œ≥that minimizes e[(x‚àíŒ≥)2]. hence, show that e[(x‚àíŒ≥)2]‚â•var[x]
for any Œ≥.
(b) let Œ≥= (a+b)/2. show that
e[(x‚àíŒ≥)2] =e[(x‚àía)(x‚àíb)] +(b‚àía)2
4.
(c) from (a) and (b), show that var[ x]‚â§(b‚àía)2
4.
(d) show that for any Œµ >0,
p[|x‚àí¬µ|> Œµ]‚â§(b‚àía)2
4Œµ2.
exercise 8.
the random variables xandyare independent with pdfs
fx(x) =1
œÄ(1 +x2)and fy(y) =1
œÄ(1 +y2),
respectively. find the pdf of z=x‚àíy. (hint: use the characteristic function.)
exercise 9.
a random variable xhas the characteristic function
œÜx(jœâ) =e‚àíjœâ/(1‚àíjœâ).
find the mean and variance of x.
exercise 10.
show that for any random variables xandy,
p[|x‚àíy|> œµ]‚â§1
œµ2e[(x‚àíy)2].
exercise 11.
letxbe an exponential random variable with a parameter Œª. let ¬µ=e[x] and œÉ2=
var[x]. compute p[|x‚àí¬µ| ‚â•kœÉ] for any k >1. compare this to the bound obtained by
chebyshev‚Äôs inequality.
exercise 12.
letx1, . . . , x nbe i.i.d. bernoulli with a parameter p. let Œ± >0 and define
œµ=s
1
2nlog2
Œ±
.
385chapter 6. sample statistics
letxn=1
npn
n=1xn. define an interval
i=
xn‚àíœµ,xn+œµ
.
use hoeffding‚Äôs inequality to show that
p[icontains p]‚â•1‚àíŒ±.
exercise 13.
letz‚àºgaussian(0 ,1). prove that for any œµ >0,
p[|z|> œµ]‚â§r
2
œÄe‚àíœµ2
2
œµ.
hint: note that œµp[|z|> œµ] = 2 œµp[z > œµ ], and then follow the procedure we used to prove
markov‚Äôs inequality.
exercise 14.
(a) give a non-negative random variable x‚â•0 such that markov‚Äôs inequality is met with
equality. hint: consider a discrete random variable.
(b) give a random variable xsuch that chebyshev‚Äôs inequality is met with equality.
exercise 15.
consider a random variable xsuch that
e[esx]‚â§es2œÉ2
2.
(a) show that for any t,
p[x‚â•t]‚â§exp
‚àít2
2œÉ2
.
hint: use chernoff‚Äôs bound.
(b) show that
e[x2]‚â§4œÉ2.
hint: first prove that e[x2] =r‚àû
0p[x2‚â•t]dt. then use part (a) above.
exercise 16.
letx1, . . . , x nbe i.i.d. uniform random variables distributed over [0 ,1]. suppose y1, . . . , y n
are defined as follows.
(a)yn=xn/n
(b)yn= (xn)n
(c)yn= max( x1, . . . , x n)
3866.7. problems
(d)yn= min( x1, . . . , x n)
for (a), (b), (c), and (d), show that ynconverges in probability to some limit. identify the
limit in each case.
exercise 17.
letŒªn=1
nforn= 1,2, . . .. let xn‚àºpoisson( Œªn). show that xnconverges in probability
to 0.
exercise 18.
lety1, y2, . . .be a sequence of random variables such that
yn=(
0, with probability 1 ‚àí1
n,
2n, with probability1
n.
does ynconverge in probability to 0?
exercise 19. (video solution)
a laplace random variable has a pdf
fx(x) =Œª
2e‚àíŒª|x|, Œª > 0,
and the variance is var[ x] =2
Œª2. let x1, . . . , x 500be a sequence of i.i.d. laplace random
variables. let
m500=x1+¬∑¬∑¬∑+x500
500.
(a) find e[x]. express your answer in terms of Œª.
(b) let Œª= 10. using chebyshev‚Äôs inequality, find a lower bound of
p[‚àí0.1‚â§m500‚â§0.1].
(c) let Œª= 10. using the central limit theorem, find the probability
p[‚àí0.1‚â§m500‚â§0.1].
you may leave your answer in terms of the œÜ( ¬∑) function.
exercise 20. (video solution)
letx1, . . . , x nbe a sequence of i.i.d. random variables such that xi=¬±1 with equal
probability. let
yn=1‚àönnx
i=1xi.
prove the central limit theorem for this particular sequence of random variables by showing
that
387chapter 6. sample statistics
(a)e[yn] = 0, var[ yn] = 1.
(b) the function of ynismyn(s)‚Üíes2
2asn‚Üí ‚àû .
exercise 21. (video solution)
letx1, . . . , x nbe a sequence of i.i.d. random variables with mean and variance
e[xn] =¬µand var[ xn] =œÉ2, n = 1, . . . , n.
the distribution of xnis, unknown. let
mn=1
nnx
n=1xn.
use the central limit theorem to estimate the probability p[mn>2¬µ].
388chapter 7
regression
starting with this chapter, we will discuss several combat skills ‚Äî techniques that we use
to do the actual data analysis. the theme of this topic is learning andinference , which are
both at the core of modern data science. the word ‚Äúlearning‚Äù can be broadly interpreted
as seeking the best model to explain the data, and the word ‚Äúinference‚Äù refers to prediction
and recovery. here, prediction means that we use the observed data to forecast or generalize
to unseen situations, whereas recovery means that we try to restore the missing data in our
current observations. in this chapter we will learn regression , one of the most widely used
learning and inference techniques.
regression is a process for finding the relationship between the inputs and the outputs.
in a regression problem, we consider a set of input data {x1, . . . , x n}and a set of output
data{y1, . . . , y n}. we call the set of these input-output pairs ddef={(x1, y1), . . . , (xn, yn)}
thetraining data . the true relationship between an xnand a ynis unknown. we do not
know, you do not know, only god knows. we denote this unknown relationship as a mapping
f(¬∑) that takes xnand maps it to yn,
yn=f(xn),
as illustrated in figure 7.1 .
figure 7.1: a regression problem is about finding the best approximation to the input-output relationship
of the data.
since we do not know f(¬∑), finding it from a set of finite number of data points d=
{(x1, y1), . . . , (xn, yn)}is infeasible ‚Äî there are infinitely many ways we can make yn=
f(xn) for every n= 1, . . . , n . the idea of regression is to add a structure to the problem.
instead of looking for f(¬∑), we find a proxy gŒ∏(¬∑). this proxy gŒ∏(¬∑) takes a certain parametric
form. for example, we can postulate that ( xn, yn) has a linear relationship, and so
gŒ∏(xn) = Œ∏1|{z}
parameterxn+ Œ∏0|{z}
parameter, n = 1, . . . , n.
389chapter 7. regression
this equation is a straight line with a slope Œ∏1and a y-intercept Œ∏0. we call Œ∏= [Œ∏1, Œ∏0]
theparameter of the model f(¬∑). to emphasize that the function we are using here is
parameterized byŒ∏, we denote the function by gŒ∏(¬∑).
of course, any model we choose is our guess . it will never be the true model. there is
always a difference between what our model tells us and what we have observed. we denote
this ‚Äúdifference‚Äù or ‚Äúerror‚Äù by enand define it as:
en=yn‚àígŒ∏(xn), n = 1, . . . , n.
the purpose of regression is to find the best Œ∏such that the error is minimized. for example,
consider a minimization of the sum-square error:
bŒ∏= argmin
Œ∏‚ààrdnx
n=1(yn‚àígŒ∏(xn))2
| {z }
training loss etrain(Œ∏).
the sum of the squared error is just one of the many possible ways we can define the training
lossetrain(Œ∏). we will discuss different ways to define the training loss in this chapter, but the
point should be evident. for a given dataset d={(x1, y1), . . . , (xn, yn)}, regression tries
to find a function gŒ∏(¬∑) such that the training loss is minimized. the optimization variable
is the parameter Œ∏. if the function gŒ∏(¬∑) is a linear function in Œ∏, we call the regression a
linear regression .
figure 7.2: a regression problem involves several steps: picking a model gŒ∏, defining the training loss
etrain(Œ∏), and solving the optimization to update Œ∏.
a summary of the regression process is shown in figure 7.2 . given the training data
d={(x1, y1), . . . , (xn, yn)}, the user picks a model gŒ∏(¬∑) to make a prediction. we com-
pare the predicted value gŒ∏(xn) with the observed value yn, and compute the training loss
etrain(Œ∏). the training loss etrain(Œ∏) is a function of the model parameter Œ∏. different model
parameters Œ∏give different training loss. we solve an optimization problem to find the best
model parameter. in practice, we often iterate the process for a few times until the training
loss is settled down.
390what is regression?
given the data points ( x1, y1), . . . , (xn, yn), regression is the process of finding
the parameter Œ∏of a function gŒ∏(¬∑) such that the training loss is minimized:
bŒ∏= argmin
Œ∏‚ààrdnx
n=1l(yn, gŒ∏(xn))
| {z }
training loss etrain(Œ∏), (7.1)
where l(¬∑,¬∑) is the loss between a pair of true observation ynand the prediction gŒ∏(xn).
one common choice of l(¬∑,¬∑) isl(gŒ∏(xn), yn) = (gŒ∏(xn)‚àíyn)2.
example 1. fitting the data
suppose we have a set of data points ( x1, y1), (x2, y2), . . . , ( xn, yn), where xn‚Äôs are the
inputs and yn‚Äôs are the outputs. these pairs of data points can be plotted in a scatter plot,
as shown in figure 7.3 . we want to find the curve that best fits the data.
to solve this problem, we first need to choose a model, for example
gŒ∏(xn) =Œ∏0+Œ∏1xn+Œ∏2x2
n+Œ∏3x3
n+Œ∏4x4
n.
we call the coefficients Œ∏= [Œ∏0, Œ∏1, Œ∏2, Œ∏3, Œ∏4] the regression coefficients . they can be found
by solving the optimization problem
minimize
Œ∏0,Œ∏1,Œ∏2,Œ∏3,Œ∏4nx
n=1
yn‚àí(Œ∏0+Œ∏1xn+Œ∏2x2
n+Œ∏3x3
n+Œ∏4x4
n)2
.
-1 -0.5 0 0.5 1-3-2-101234
data
fitted curve
figure 7.3: regression can be used to fit the dataset using curves. in this example, we use a fourth-th
order polynomial gŒ∏(x) =p4
p=0Œ∏pxp
nto fit a 50-point dataset.
this optimization asks for the best Œ∏= [Œ∏0, . . . , Œ∏ 4]tsuch that the training loss is
minimized. solving the minimization problem would require some effort, but if we imagine
that we have solved it we can find the best curve, which is gŒ∏(x) =p4
p=0Œ∏pxp
nwith the
optimal Œ∏plugged in. the red curve in figure 7.3 shows an example in which we have used
a fourth-order polynomial to fit a dataset comprising 50 data points. we will learn how to
solve the problem in this chapter.
391chapter 7. regression
example 2. predicting the stock market
imagine that you have bought some shares in the stock market. you have looked at the past
data, and you want to predict the price of the shares over the next few days. how would
you do it besides just eyeballing the data?
first, you would plot the data points on a graph. mathematically, we can denote these
data points as {x1, x2, . . . , x n}, where the indices n= 1,2, . . . , n can be treated as time
stamps. we assume a simple model to describe the relationship between the xn‚Äôs, say
xn‚âàaxn‚àí1+bxn‚àí2,
for some parameters Œ∏= (a, b).1this model assumes that the current value xncan be
approximated by a linear combination of two previous values xn‚àí1andxn‚àí2. therefore, if
we have x1andx2we should be able to predict x3, and if we have x2andx3we should be
able to predict x4, etc. the magic of this prediction comes from the parameters aandb. if
we know aandb, the prediction can be done by simply plugging in the numbers.
the regression problem here is to estimate the parameters aandbfrom the data. since
we are given a set of training data {x1, x2, . . . , x n}, we can check whether our predicted
valuebx3is close to the true x3, and whether our predicted value bx4is close to the true x4,
etc. this leads to the optimization
(ba,bb) = argmin
a,bnx
n=1
xn‚àí(axn‚àí1+bxn‚àí2)| {z }
=prediction2
,
where we use initial conditions that x0=x‚àí1= 0. the optimization problem requires us
to minimize the disparity between xnand the predicted value axn‚àí1+bxn‚àí2, for all n.
by finding the ( a, b) that minimizes this objective function, we will accomplish our goal of
estimating the best ( a, b).
figure 7.4 shows an example of predicting a random process using the above model.
if the parameters aandbare properly determined, we will obtain a reasonably well-fitted
curve to the data. a simple extrapolation to the future timestamp would suffice for the
forecast task.
plan for this chapter
what are the key ingredients of regression?
¬àlearning : formulate the regression problem as an optimization problem, and
solve it by finding the best parameters.
¬àinference : use the estimated parameters and models to predict the unseen data
points.
regression is too broad a topic to be covered adequately in a single chapter. accord-
ingly, we will present a few principles and a few practical algorithmic techniques that are
broadly applicable to many (definitely not all) regression tasks. these include the following.
1caution: if you lose money in the stock market by following this naive model, please do not cry. this
model is greatly oversimplified and probably wrong.
3920 0.2 0.4 0.6 0.8 1-1-0.500.511.5
data
best fit
candidatefigure 7.4: an autoregression model aims at learning the model parameters based on the previous
samples. this example illustrates fitting the data using the model xn=axn‚àí1+bxn‚àí2, forn= 1, . . . , n .
¬àtheprinciple of regression (section 7.1). we explain the formulation of a regression
problem via optimization. there are a few steps involved in developing this concept.
first, we will exclusively focus on linear models because these models are easier to
analyze than nonlinear models but are still rich enough for many practical problems.
we will discuss how to solve the linear regression problem and some applications of
the solutions. we then address the issue of outliers using a concept called the robust
linear regression .
¬àoverfitting (section 7.2). the biggest practical challenge of regression is overfitting .
overfitting occurs when a model fits too closely to the training samples so that it
fails to generalize . we will delve deeply into the roots of overfitting and show that
overfitting depends on three factors: the number of training samples n, the model
complexity d, and the magnitude of noise œÉ2.
¬àbias-variance trade-off (section 7.3). we will present one of the most fundamental
results in learning theory, known as the bias-variance trade-off. it applies to allregres-
sion problems, not just to linear models. understanding this trade-off will help you
understand the fundamental limits of your problem so that you know what to expect
from the model.
¬àregularization (section 7.4). in this section we discuss a technique for combatting
overfitting known as regularization . regularization is carried out by adding an extra
term to the regression objective function. by solving the modified optimization, the
regression solution is improved in two ways: (i) regularization makes the regression
solution less sensitive to noise perturbations, and (ii) it alleviates the fitting difficulty
when we have only a few training samples. we will discuss two regularization strategies:
theridge regression and the lasso regression .
much of this chapter deals with optimization. if this is your first time reading this
book, we encourage you to have a reference book on linear algebra at hand.
393chapter 7. regression
7.1 principles of regression
we start by recalling our discussion in the introduction. the purpose of regression can be
summarized in a simple statement:
given the data points ( x1, y1), . . . , (xn, yn), find the parameter Œ∏of a function gŒ∏(¬∑)
such that the training loss is minimized:
bŒ∏= argmin
Œ∏‚ààrdnx
n=1l(yn, gŒ∏(xn))
| {z }
training loss etrain(Œ∏), (7.2)
where l(¬∑,¬∑) is the loss between a pair of true observation ynand the prediction gŒ∏(xn).
when the context makes it clear, we will drop the subscript Œ∏ingŒ∏(¬∑) with the understanding
that the function g(¬∑) is parameterized by Œ∏.
as you can see, regression finds a function g(¬∑) that best approximates the input-output
relationship between xnandyn. there are two choices we need to make when formulating
a regression problem:
¬àfunction g(¬∑): what is the family of functions we want to use? this could be a line, a
polynomial, or a set of basis functions. if it is a polynomial, what is its order? we need
to make all these decisions before running the regression. a poor choice of function
family can lead to a poor regression result.
¬àloss ‚Äúl(¬∑,¬∑)‚Äù: how do we measure the closeness between ynandg(xn)? are we measur-
ing in terms of the squared error ( yn‚àíg(xn))2, or the absolute difference |yn‚àíg(xn)|,
or something else? again, a poor choice of distance function can create a false sense
of closeness because you might be optimizing for a wrong objective.
before we delve into the details, we need to discuss briefly the connection between
regression and probability. a regression problem can be solved without knowing probability,
so why is regression discussed in a book on probability?
this question is related to how much we know about the statistical model and what
kind of optimality we are seeking. a full answer requires some understanding of maximum
likelihood estimation and maximum a posteriori estimation, which will be explained in
chapter 8. as a quick preview of our results, we summarize the key ideas below:
how is regression related to probability?
¬àif you know the statistical relationship between xnandyn, then we can construct
a regression problem that maximizes the likelihood of the underlying distribu-
tion. such regression solution is optimal with respect to the likelihood.
¬àwe can construct a regression problem that can minimize the expectation of the
3947.1. principles of regression
squared error. this regression solution is mean-squared optimal .
¬àif you are a bayesian and you know the prior distribution of xn, then we can
construct a regression problem that maximizes the posterior distribution. the
solution to this regression problem is bayesian optimal .
¬àif you know nothing about the statistics of xnandyn, you can still run the
regression and get something, and this ‚Äúsomething‚Äù can be very useful. however,
you cannot claim statistical optimality of this ‚Äúsomething‚Äù.
see chapter 8 for additional discussion.
it is important to understand that a regression problem is at the intersection of op-
timization andstatistics . the need for optimization is clear because we need to minimize
the error. the statistical need is to generalize to unknown data. if there is no statistical
relationship between xnandyn(for all n), whatever model we obtain from the regression
will only work for the ntraining samples. the model will not generalize because knowing
xnwill not help us know yn. in other words, if there is no statistical relationship between
xnandyn, you can fit perfectly to the training data but you will fail miserably to fit the
testing data.
7.1.1 intuition: how to fit a straight line?
in this subsection we want to give you the basic idea of how regression is formulated. to
keep things simple, we will discuss how to fit data using a straight line.
consider a collection of data points d={(x1, y1), . . . , (xn, yn)}, where xn‚Äôs are the
inputs and yn‚Äôs are the observations, for example, in the table below.
n x n yn
1 0.6700 3.0237
2 0.3474 2.3937
3 0.6695 3.5548
.........
n‚àí1 0.2953 2.6396
n 0.6804 3.2536
let us consider the linear regression problem. the goal of linear regression is to find
thestraight line that best fits the datasets. all straight lines on a 2d graph are plots of the
equation
g(x) =ax+b,
where ais the slope of the line and bis the y-intercept of the line. we denote this line
byg(¬∑). note that this function gis characterized by two parameters ( a, b) because once
(a, b) are known the line is determined. if we change ( a, b), the line will change as well.
therefore, by finding the bestline we are essentially searching for the best ( a, b) such that
the training error is minimized.
the pictorial meaning of linear regression can easily be seen in figure 7.5 , which
shows n= 50 data points according to some latent distributions. given these 50 data
points, we construct several possible candidates for the regression model. these candidates
395chapter 7. regression
are characterized by the parameters ( a, b). for example, the parameters ( a, b) = (1 ,2) and
(a, b) = (‚àí2,3) represent two different straight lines in the candidate pool. the goal of the
regression is to find the best line from these candidates. note that since we limit ourselves
to straight lines, the candidate set will not include polynomials or trigonometric functions.
these functions are outside the family we are considering.
0 0.2 0.4 0.6 0.8 11.522.533.544.5
data
best fit
candidate
figure 7.5: the objective of least squares fitting (or linear regression) is to find a line that best fits the
dataset.
given these candidate functions, we need to measure the the training loss. this can
be defined in multiple ways, such as
¬àsum-squared loss etrain(Œ∏) =pn
n=1(yn‚àíg(xn))2.
¬àsum-absolute loss etrain(Œ∏) =pn
n=1|yn‚àíg(xn)|.
¬àcross-entropy loss etrain(Œ∏) =‚àípn
n=1(ynlogg(xn) + (1 ‚àíyn) log(1 ‚àíg(xn))).
¬àperceptual loss etrain(Œ∏) =pn
n=1max(‚àíyng(xn),0), when ynandg(xn) are binary
taking values ¬±1. this is a reasonable training error because if ynmatches with g(xn),
then yng(xn) = 1 and so max( ‚àíyng(xn),0) = 0. but if yndoes not match with g(xn),
then yng(xn) =‚àí1 and hence max( ‚àíyng(xn),0) = 1. thus, the loss captures the sum
of all the mismatched pairs.
choosing the loss function is problem-specific. it is also where probability enters the picture
because, without any knowledge about the distributions of xnandyn, there is no way to
choose the best training loss. you can still pick one, as we will do, but it will not be granted
any probabilistic guarantees.
among these possible choices of the training error, we are going to focus on the sum-
squared loss because it is convex anddifferentiable . this makes the computation easy,
since we can run any textbook optimization algorithm. the regression problem under the
sum-squared loss is:

ba,bb
= argmin
(a,b)nx
n=1
yn‚àí(axn+b)|{z}
=g(xn)2
. (7.3)
in this equation, the symbol ‚Äúargmin‚Äù means ‚Äúargument minimize‚Äù, which returns the ar-
gument that minimizes the cost function on the right. the interpretation of the equation is
3967.1. principles of regression
that we seek the ( a, b) that minimize the sumpn
n=1(yn‚àí(axn+b))2. since we are mini-
mizing the squared error, this linear regression problem is also known as the least squares
fitting problem. the idea is summarized in the following box.
what is linear least squares fitting?
¬àfind a line g(x) =ax+bthat best fits the training data {(xn, yn)}n
n=1.
¬àthe optimality criterion is to minimize the squared error
etrain(Œ∏) =nx
n=1
yn‚àíg(xn)2
, (7.4)
where Œ∏= (a, b) is the model parameter.
¬àthere exist other optimality criteria. squared error is convex and differentiable.
7.1.2 solving the linear regression problem
let‚Äôs consider how to solve the linear regression problem given by equation (7.3). the
problem is the following:
ba,bb
= argmin
(a,b)etrain(a, b). (7.5)
as with any two-dimensional optimization problem, the optimal point ( ba,bb) should
have a zero gradient, meaning that
‚àÇ
‚àÇaetrain(a, b) = 0 and‚àÇ
‚àÇbetrain(a, b) = 0 .
this should be familiar to you, even if you have only learned basic calculus. this pair of
equations says that, at a minimum point, the directional slopes should be zero no matter
which direction you are looking at.
the derivative with respect to ais
‚àÇ
‚àÇaetrain(a, b)
=‚àÇ
‚àÇanx
n=1
yn‚àí(axn+b)2
=‚àÇ
‚àÇa
y1‚àí(ax1+b)2
+
y2‚àí(ax2+b)2
+¬∑¬∑¬∑+
yn‚àí(axn+b)2
= 2
y1‚àí(ax1+b)
(‚àíx1) +¬∑¬∑¬∑+ 2
yn‚àí(axn+b)
(‚àíxn)
= 2 
‚àínx
n=1xnyn+anx
n=1x2
n+bnx
n=1xn!
.
397chapter 7. regression
similarly, the derivative with respect to bis
‚àÇ
‚àÇbetrain(a, b) =‚àÇ
‚àÇbnx
n=1
yn‚àí(axn+b)2
= 2
y1‚àí(ax1+b)
(‚àí1) +¬∑¬∑¬∑+ 2
yn‚àí(axn+b)
(‚àí1)
= 2 
‚àínx
n=1yn+anx
n=1xn+bnx
n=11!
.
setting these two equations to zero, we have that
2 
‚àínx
n=1ynxn+anx
n=1x2
n+bnx
n=1xn!
= 0,
2 
‚àínx
n=1yn+anx
n=1xn+bnx
n=11!
= 0.
rearranging the terms, the pair can be equivalently written as
Ô£Æ
Ô£ØÔ£ØÔ£∞np
n=1x2
nnp
n=1xn
np
n=1xn nÔ£π
Ô£∫Ô£∫Ô£ªa
b
=Ô£Æ
Ô£ØÔ£ØÔ£∞np
n=1xnyn
np
n=1ynÔ£π
Ô£∫Ô£∫Ô£ª.
therefore, if we can solve this system of linear equations, we will have the linear regression
solution.
remark . it is easy to see that the solution achieves the minimum instead of the maximum,
since the second-order derivatives are positive:
‚àÇ2
‚àÇa2etrain(a, b) =nx
n=1x2
n‚â•0 and‚àÇ2
‚àÇb2etrain(a, b) =nx
n=11>0.
the following theorem summarizes this intermediate result.
theorem 7.1. the solution of the problem equation (7.5)

ba,bb
=argmin
(a,b)nx
n=1
yn‚àí(axn+b)2
satisfies the equation
Ô£Æ
Ô£ØÔ£ØÔ£∞np
n=1x2
nnp
n=1xn
np
n=1xn nÔ£π
Ô£∫Ô£∫Ô£ªba
bb
=Ô£Æ
Ô£ØÔ£ØÔ£∞np
n=1xnyn
np
n=1ynÔ£π
Ô£∫Ô£∫Ô£ª. (7.6)
3987.1. principles of regression
matrix-vector form of linear regression
solving this linear regression requires some basic linear algebra. the regression can be
written as Ô£Æ
Ô£ØÔ£∞y1
...
ynÔ£π
Ô£∫Ô£ª
|{z}
y=Ô£Æ
Ô£ØÔ£∞x11
......
xn1Ô£π
Ô£∫Ô£ª
|{z}
xa
b
|{z}
Œ∏+Ô£Æ
Ô£ØÔ£∞e1
...
enÔ£π
Ô£∫Ô£ª
|{z}
e.
with x,y,Œ∏ande, we can write the linear regression problem compactly as
y=xŒ∏+e.
therefore, the training loss etrain(Œ∏) can be defined as
etrain(Œ∏) =‚à•y‚àíxŒ∏‚à•2
=Ô£Æ
Ô£ØÔ£∞y1
...
ynÔ£π
Ô£∫Ô£ª‚àíÔ£Æ
Ô£ØÔ£∞x11
......
xn1Ô£π
Ô£∫Ô£ªa
b2
=nx
n=1
yn‚àí(axn+b)2
.
now, taking the gradient with respect to Œ∏yields2
‚àáŒ∏etrain(Œ∏) =‚àáŒ∏
‚à•y‚àíxŒ∏‚à•2
=‚àí2xt(y‚àíxŒ∏).
equating this to zero, we obtain
xt(y‚àíxŒ∏) = 0 ‚áê‚áí xtxŒ∏=xty. (7.7)
equation (7.7) is called the normal equation .
the normal equation is a convenient way of constructing the system of linear equations.
using the 2-by-2 system shown in equation (7.6) as an example, we note that
xtx=
x1¬∑¬∑¬∑xn
1¬∑¬∑¬∑ 1Ô£Æ
Ô£ØÔ£∞x11
......
xn1Ô£π
Ô£∫Ô£ª=Ô£Æ
Ô£ØÔ£ØÔ£∞np
n=1x2
nnp
n=1xn
np
n=1xn nÔ£π
Ô£∫Ô£∫Ô£ª,
xty=
x1¬∑¬∑¬∑xn
1¬∑¬∑¬∑ 1Ô£Æ
Ô£ØÔ£∞y1
...
ynÔ£π
Ô£∫Ô£ª=Ô£Æ
Ô£ØÔ£ØÔ£∞np
n=1xnyn
np
n=1ynÔ£π
Ô£∫Ô£∫Ô£ª.
therefore, as long as you can construct the xmatrix, forming the 2-by-2 system in equa-
tion (7.6) is straightforward: start with y=xŒ∏and then multiply the matrix transpose
xtto both sides. the resulting system is what you need. there is nothing to memorize.
2this is a basic vector calculus result. for details, you may consult standard texts such as the university
of waterloo‚Äôs matrix cookbook. https://www.math.uwaterloo.ca/ ~hwolkowi/matrixcookbook.pdf
399chapter 7. regression
running linear regression on a computer
on a computer, solving the linear regression for a line is straightforward. let us look at the
matlab code first.
% matlab code to fit data points using a straight line
n = 50;
x = rand(n,1)*1;
a = 2.5; % true parameter
b = 1.3; % true parameter
y = a*x + b + 0.2*rand(size(x)); % synthesize training data
x = [x(:) ones(n,1)]; % construct the x matrix
theta = x\y(:); % solve y = x theta
t = linspace(0, 1, 200); % interpolate and plot
yhat = theta(1)*t + theta(2);
plot(x,y,‚Äôo‚Äô,‚Äôlinewidth‚Äô,2); hold on;
plot(t,yhat,‚Äôr‚Äô,‚Äôlinewidth‚Äô,4);
in this piece of matlab code, we need to define the data matrix x. here, x(:) is the
column vector that stores all the values ( x1, . . . , x n). the all-one vector ones(n,1) is the
second column in our xmatrix. the command x\y(:) is equivalent to solving the normal
equation
xtxŒ∏=xty.
the last few lines are used to plot the predicted curve. note that theta(1) andtheta(2)
are the entries of the solution Œ∏. the result of this program is exactly the plot shown in
figure 7.5 above.
in python, the program is quite similar. the command we use to solve the inversion
isnp.linalg.lstsq .
# python code to fit data points using a straight line
import numpy as np
import matplotlib.pyplot as plt
n = 50
x = np.random.rand(n)
a = 2.5 # true parameter
b = 1.3 # true parameter
y = a*x + b + 0.2*np.random.randn(n) # synthesize training data
x = np.column_stack((x, np.ones(n))) # construct the x matrix
theta = np.linalg.lstsq(x, y, rcond=none)[0] # solve y = x theta
t = np.linspace(0,1,200) # interpolate and plot
yhat = theta[0]*t + theta[1]
plt.plot(x,y,‚Äôo‚Äô)
plt.plot(t,yhat,‚Äôr‚Äô,linewidth=4)
4007.1. principles of regression
7.1.3 extension: beyond a straight line
regression is a powerful technique. although we have discussed its usefulness for fitting
straight lines, the same concept can fit other curves.
to generalize the regression formulation, we consider a d-dimensional regression coef-
ficient vector Œ∏= [Œ∏0, . . . , Œ∏ d‚àí1]t‚ààrdand a general linear model
gŒ∏(xn) =d‚àí1x
p=0Œ∏pœïp(xn).
here, the mappings {œïp(¬∑)}d‚àí1
p=0can be considered as a nonlinear transformation that takes
the input xnand maps it to another value. for example, œïp(¬∑) = (¬∑)pwill map an input x
to apth power xp.
we can now write the system of linear equations as
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞y1
y2
...
ynÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
y=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞œï0(x1)œï1(x1)¬∑¬∑¬∑ œïd‚àí1(x1)
œï0(x2)œï1(x2)¬∑¬∑¬∑ œïd‚àí1(x2)
... ¬∑¬∑¬∑......
œï0(xn)œï1(xn)¬∑¬∑¬∑œïd‚àí1(xn)Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
| {z }
xÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£∞Œ∏0
Œ∏1
...
Œ∏d‚àí1Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
Œ∏+Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞e1
e2
...
enÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
e. (7.8)
let us look at some examples.
example 7.1 . (quadratic fitting ) consider the linear regression problem using a
quadratic equation:
yn=ax2
n+bxn+c, n = 1, . . . , n.
express this equation in matrix-vector form.
solution . the matrix-vector expression is
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞y1
y2
...
ynÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞x2
1x11
x2
2x21
.........
x2
nxn1Ô£π
Ô£∫Ô£∫Ô£∫Ô£ªÔ£Æ
Ô£∞a
b
cÔ£π
Ô£ª+Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞e1
e2
...
enÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª.
this is again in the form of y=xŒ∏+e.
the matlab and python programs for example 7.1 are shown below. a numerical
example is illustrated in figure 7.6 .
% matlab code to fit data using a quadratic equation
n = 50;
x = rand(n,1)*1;
a = -2.5;
b = 1.3;
c = 1.2;
401chapter 7. regression
0 0.2 0.4 0.6 0.8 100.511.522.5
data
fitted curve
figure 7.6: example: our goal is to fit the dataset of 50 data points shown above. the model we use
isgŒ∏(xn) =ax2
n+bxn+c, forn= 1, . . . , n .
y = a*x.^2 + b*x + c + 1*rand(size(x));
n = length(x);
x = [ones(n,1) x(:) x(:).^2];
beta = x\y(:);
t = linspace(0, 1, 200);
yhat = theta(3)*t.^2 + theta(2)*t + theta(1);
plot(x,y, ‚Äôo‚Äô,‚Äôlinewidth‚Äô,2); hold on;
plot(t,yhat,‚Äôr‚Äô,‚Äôlinewidth‚Äô,6);
# python code to fit data using a quadratic equation
import numpy as np
import matplotlib.pyplot as plt
n = 50
x = np.random.rand(n)
a = -2.5
b = 1.3
c = 1.2
y = a*x**2 + b*x + c + 0.2*np.random.randn(n)
x = np.column_stack((np.ones(n), x, x**2))
theta = np.linalg.lstsq(x, y, rcond=none)[0]
t = np.linspace(0,1,200)
yhat = theta[0] + theta[1]*t + theta[2]*t**2
plt.plot(x,y,‚Äôo‚Äô)
plt.plot(t,yhat,‚Äôr‚Äô,linewidth=4)
the generalization to polynomials of arbitrary order is to replace the model with
gŒ∏(xn) =d‚àí1x
p=0Œ∏pxp,
4027.1. principles of regression
where p= 0,1, . . . , d ‚àí1 represent the orders of the polynomials and Œ∏0, . . . , Œ∏ d‚àí1are the
regression coefficients . in this case, the matrix system is
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞y1
y2
...
ynÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞1x1¬∑¬∑¬∑xd‚àí1
1
1x2¬∑¬∑¬∑xd‚àí1
2...¬∑¬∑¬∑......
1xn¬∑¬∑¬∑xd‚àí1
nÔ£π
Ô£∫Ô£∫Ô£∫Ô£ªÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£∞Œ∏0
Œ∏1
...
Œ∏d‚àí1Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª+Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞e1
e2
...
enÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª,
which again is in the form of y=xŒ∏+e.
example 7.2 . (legendre polynomial fitting ) let {lp(¬∑)}d‚àí1
p=0be a set of legendre
polynomials (see discussions below), and consider the linear regression problem using
yn=d‚àí1x
p=0Œ∏plp(x), n = 1, . . . , n.
express this equation in matrix-vector form.
solution . the matrix-vector expression is
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞y1
y2
...
ynÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞l0(x1)l1(x1)¬∑¬∑¬∑ ld‚àí1(x1)
l0(x2)l1(x2)¬∑¬∑¬∑ ld‚àí1(x2)
... ¬∑¬∑¬∑......
l0(xn)l1(xn)¬∑¬∑¬∑ld‚àí1(xn)Ô£π
Ô£∫Ô£∫Ô£∫Ô£ªÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£∞Œ∏0
Œ∏1
...
Œ∏d‚àí1Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª+Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞e1
e2
...
enÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª.
legendre polynomials are orthogonal polynomials. in conventional polynomials, the
functions {x, x2, x3, . . . , xp}are not orthogonal. as we increase p, the set of functions
{x, x2, x3, . . . , xp}will have redundancy, which will eventually result in the matrix xbeing
noninvertible.
thepth-order legendre polynomial is denoted by lp(x). using the legendre polyno-
mials as the building block of the regression problem, the model is expressed as
gŒ∏(x)def=d‚àí1x
p=0Œ∏plp(x)
=Œ∏0l0(x) +Œ∏1l1(x)|{z}
=x+Œ∏2l2(x)|{z}
=1
2(3x2‚àí1)+¬∑¬∑¬∑+Œ∏d‚àí1ld‚àí1(x),
where l0(¬∑),l1(¬∑) and l2(¬∑) are the legendre polynomials of order 0, 1 and 2, respectively.
as an example, the first few leading legendre polynomials are
l0(x) = 1 ,
l1(x) =x,
l2(x) =1
2(3x2‚àí1),
l3(x) =1
2(5x3‚àí3x).
403chapter 7. regression
the order of the legendre polynomials is always the same as that of the ordinary polyno-
mials. the shapes of these polynomials are shown in figure 7.7 (a).
-1 -0.5 0 0.5 1-1-0.500.51
l0(x)
l1(x)
l2(x)
l3(x)
l4(x)
-1 -0.5 0 0.5 1-2-101234
data
legendre basis
polynomial basis
(a) (b)
figure 7.7: (a) the first 5 leading legendre polynomials plotted in the range of ‚àí1‚â§x‚â§1. (b) fitting
the data using an ordinary polynomial and a legendre polynomial.
figure 7.7 (b) demonstrates a fitting problem using the legendre polynomials. you
can see that the fitting is just as good as that of the ordinary polynomials (which should
be the case). however, if we compare the coefficients, we observe that the magnitude of
the legendre coefficients is smaller (see table 7.1 ). in general, as the order of polynomials
increases and the noise grows, the ordinary polynomials will become increasingly difficult to
fit the data.
Œ∏4 Œ∏3 Œ∏2 Œ∏1 Œ∏0
ordinary polynomials 5.3061 3.3519 ‚àí3.6285 ‚àí1.8729 0.1540
legendre polynomials 1.2128 1.3408 0.6131 0.1382 0.0057
table 7.1: the regression coefficients of an ordinary polynomial and a legendre polynomial. note that
while both polynomials can fit the data, the legendre polynomial coefficients have smaller magnitudes.
calling legendre polynomials for regression is not difficult in matlab and python.
specifically, one can call legendrep in matlab and scipy.special.eval_legendre in
python.
% matlab code to fit data using legendre polynomials
n = 50;
x = 1*(rand(n,1)*2-1);
a = [-0.001 0.01 +0.55 1.5 1.2];
y = a(1)*legendrep(0,x) + a(2)*legendrep(1,x) + ...
+ a(3)*legendrep(2,x) + a(4)*legendrep(3,x) + ...
+ a(5)*legendrep(4,x) + 0.5*randn(n,1);
x = [legendrep(0,x(:)) legendrep(1,x(:)) ...
legendrep(2,x(:)) legendrep(3,x(:)) ...
4047.1. principles of regression
legendrep(4,x(:))];
beta = x\y(:);
t = linspace(-1, 1, 200);
yhat = beta(1)*legendrep(0,t) + beta(2)*legendrep(1,t) + ...
+ beta(3)*legendrep(2,t) + beta(4)*legendrep(3,t) + ...
+ beta(5)*legendrep(4,t);
plot(x,y,‚Äôko‚Äô,‚Äôlinewidth‚Äô,2,‚Äômarkersize‚Äô,10); hold on;
plot(t,yhat,‚Äôlinewidth‚Äô,6,‚Äôcolor‚Äô,[0.9 0 0]);
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import eval_legendre
n = 50
x = np.linspace(-1,1,n)
a = np.array([-0.001, 0.01, 0.55, 1.5, 1.2])
y = a[0]*eval_legendre(0,x) + a[1]*eval_legendre(1,x) + \
a[2]*eval_legendre(2,x) + a[3]*eval_legendre(3,x) + \
a[4]*eval_legendre(4,x) + 0.2*np.random.randn(n)
x = np.column_stack((eval_legendre(0,x), eval_legendre(1,x), \
eval_legendre(2,x), eval_legendre(3,x), \
eval_legendre(4,x)))
theta = np.linalg.lstsq(x, y, rcond=none)[0]
t = np.linspace(-1, 1, 50);
yhat = theta[0]*eval_legendre(0,t) + theta[1]*eval_legendre(1,t) + \
theta[2]*eval_legendre(2,t) + theta[3]*eval_legendre(3,t) + \
theta[4]*eval_legendre(4,t)
plt.plot(x,y,‚Äôo‚Äô,markersize=12)
plt.plot(t,yhat, linewidth=8)
plt.show()
the idea of fitting a set of data using the legendre polynomials belongs to the larger
family of basis functions . in general, we can use a set of basis functions to model the data:
gŒ∏(x)def=d‚àí1x
p=0Œ∏pœïp(x),
where {œïp(x)}d‚àí1
p=0are the basis functions and {Œ∏p}d‚àí1
p=0are the regression coefficients. the
constant Œ∏0is often called the biasof the regression.
choice of the œïp(x) can be extremely broad. one can choose the ordinary polynomials
œïp(x) =xpor the legendre polynomial œïp(x) =lp(x). other choices are also available:
¬àfourier basis: œïp(x) =ejœâpx, where œâpis the pth carrier frequency.
¬àsinusoid basis: œïp(x) = sin( œâpx), which is same as the fourier basis but taking the
imaginary part.
405chapter 7. regression
¬àgaussian basis: œïp(x) =1‚àö
2œÄœÉ2pexpn
‚àí(x‚àí¬µp)2
2œÉ2po
, where ( ¬µp, œÉp) are the model param-
eters.
evidently, by choosing different basis functions we have different ways to fit the data. there
is no definitive answer as to which functions are better. statistical techniques such as model
selections are available, but experience will tell you to align with one and not the other. it
is frequently more useful to have some domain knowledge rather than resorting to various
computational techniques.
how to fit data using basis functions
¬àconstruct this equation:
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞y1
y2
...
ynÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
y=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞œï0(x1)œï1(x1)¬∑¬∑¬∑ œïd‚àí1(x1)
œï0(x2)œï1(x2)¬∑¬∑¬∑ œïd‚àí1(x2)
... ¬∑¬∑¬∑......
œï0(xn)œï1(xn)¬∑¬∑¬∑œïd‚àí1(xn)Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
| {z }
xÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£∞Œ∏0
Œ∏1
...
Œ∏d‚àí1Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
Œ∏+Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞e1
e2
...
enÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
e, (7.9)
¬àthe functions œïp(x) are the basis functions, e.g., œïp(x) =xpfor ordinary poly-
nomials.
¬àyou can replace the polynomials with the legendre polynomials.
¬àyou can also replace the polynomials with other basis functions.
¬àsolve for Œ∏by
bŒ∏= argmin
Œ∏‚à•y‚àíxŒ∏‚à•2.
example 7.3 . (autoregressive model ) consider a two-tap autoregressive model:
yn=ayn‚àí1+byn‚àí2, n = 1,2, . . . , n
where we assume y0=y‚àí1= 0. express this equation in the matrix-vector form.
solution . the matrix-vector form of the equation is
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞y1
y2
...
ynÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
=y=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞y0 y‚àí1
y1 y0
......
yn‚àí1yn‚àí2Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
| {z }
=xa
b
|{z}
=Œ∏+Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞e1
e2
...
enÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª.
in general, we can append more previous samples to predict the future. the general
4067.1. principles of regression
expression is
yn=lx
‚Ñì=1Œ∏‚Ñìyn‚àí‚Ñì, n = 1,2, . . . , n,
where ‚Ñì= 1,2, . . . , l denote the previous lsamples of the data and {Œ∏1, . . . , Œ∏ l}are the
regression coefficients. if we do this we see that the matrix expression is
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞y1
y2
y3
y4
...
ynÔ£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
=y=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞y0 y‚àí1y‚àí2¬∑¬∑¬∑ y1‚àíl
y1 y0 y‚àí1¬∑¬∑¬∑ y2‚àíl
y2 y1 y0¬∑¬∑¬∑ y3‚àíl
y3 y2 y1¬∑¬∑¬∑ y4‚àíl
...............
yn‚àí1yn‚àí2yn‚àí3...yn‚àílÔ£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
| {z }
=xÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£∞Œ∏1
Œ∏2
...
Œ∏lÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
=Œ∏+Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞e1
e2
e3
e4
...
enÔ£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª.
observe the pattern associated with this matrix x. each column is a one-entry shifted
version of the previous column. this matrix is called a toeplitz matrix .
the matlab (and python) code for calling and using the toeplitz matrix is shown
below.
% matlab code for auto-regressive model
n = 500;
y = cumsum(0.2*randn(n,1)) + 0.05*randn(n,1); % generate data
l = 100; % use previous 100 samples
c = [0; y(1:400-1)];
r = zeros(1,l);
x = toeplitz(c,r); % toeplitz matrix
theta = x\y(1:400); % solve y = x theta
yhat = x*theta; % prediction
plot(y(1:400), ‚Äôko‚Äô,‚Äôlinewidth‚Äô,2);hold on;
plot(yhat(1:400),‚Äôr‚Äô,‚Äôlinewidth‚Äô,4);
# python code for auto-regressive model
import numpy as np
import matplotlib.pyplot as plt
from scipy.linalg import toeplitz
n = 500
y = np.cumsum(0.2*np.random.randn(n)) + 0.05*np.random.randn(n)
l = 100
c = np.hstack((0, y[0:400-1]))
r = np.zeros(l)
x = toeplitz(c,r)
theta = np.linalg.lstsq(x, y[0:400], rcond=none)[0]
yhat = np.dot(x, theta)
407chapter 7. regression
plt.plot(y[0:400], ‚Äôo‚Äô)
plt.plot(yhat[0:400],linewidth=4)
the plots generated by the above programs are shown in figure 7.8 (a). note that we
are doing an interpolation , because we are predicting the values within the training dataset.
0 100 200 300 400 500-6-4-202
0 100 200 300 400 500-6-4-202
(a) (b)
figure 7.8: autoregressive model on a simulated dataset, using l= 100 coefficients. (a) training data.
note that the model trains very well on this dataset. (b) testing data. when tested on future data, the
autoregressive model can still predict for a few samples but loses track when the time elapsed grows.
we now consider extrapolation . given the training data, we can find the regression
coefficients by solving the above linear equation. this gives us Œ∏. to predict the future
samples we need to return to the equation
byn=lx
‚Ñì=1Œ∏‚Ñì byn‚àí‚Ñì|{z}
=previous estimate, n = 1,2, . . . , n,
wherebyn‚àí‚Ñìare the previous estimates. for example, if we are given 100 days of stock prices,
then predicting the 101st day‚Äôs price should be based on the ldays before the 101st. a
simple for-loop suffices for such a calculation.
figure 7.8 (b) shows a numerical example of extrapolating data using the autoregressive
model. in this experiment we use n= 400 samples to train an autoregressive model of order
l= 100. we then predict the data for another 100 data points. as you can see from the
figure, the first few samples still look reasonable. however, as time increases, the model
starts to lose track of the real trend.
is there any way we can improve the autoregressive model? a simple way is to increase
the memory lso that we can use a long history to predict the future. this boils down to
the long-term running average of the curve, which works well in many cases. however, if
the testing data does not follow the same distribution as the training data (which is often
the case in the real stock market because unexpected news can change the stock price),
then even the long-term average will not be a good forecast. that is why data scientists on
wall street make so much money: they have advanced mathematical tools for modeling the
stock market. nevertheless, we hope that the autoregressive model provides you with a new
perspective for analyzing data.
the summary below highlights the main ideas of the autoregressive model.
4087.1. principles of regression
what is the autoregressive model?
¬àit solves this problem
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞y1
y2
y3
...
ynÔ£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
=y=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞y0 y‚àí1y‚àí2¬∑¬∑¬∑ y1‚àíl
y1 y0 y‚àí1¬∑¬∑¬∑ y2‚àíl
y2 y1 y0¬∑¬∑¬∑ y3‚àíl
...............
yn‚àí1yn‚àí2yn‚àí3...yn‚àílÔ£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
| {z }
=xÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£∞Œ∏1
Œ∏2
...
Œ∏lÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
=Œ∏+Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞e1
e2
e3
...
enÔ£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
=e. (7.10)
¬àthe number of taps in the past history would affect the memory and hence the
long-term forecast.
¬àsolve for Œ∏by
bŒ∏= argmin
Œ∏‚ààrd‚à•y‚àíxŒ∏‚à•2. (7.11)
7.1.4 overdetermined and underdetermined systems
the sub-section requires knowledge of some concepts in linear algebra that can be found
in standard references.a
acarl meyer, matrix analysis and applied linear algebra , siam, 2000.
let us now consider the theoretical properties of the least squares linear regression
problem, which is an optimization:
bŒ∏= argmin
Œ∏‚ààrd‚à•y‚àíxŒ∏‚à•2. (p1)
we observe that the objective value of this optimization problem can go to zero if and only
if the minimizer bŒ∏is the solution of the system of linear equations
findŒ∏such that y=xŒ∏. (p2)
we emphasize that problem (p1) and problem (p2) are two different problems. even if we
cannot solve problem (p2), problem (p1) is still well defined, but the objective value will
not go to zero. this subsection aims to draw the connection between the two problems and
discuss the respective solutions. we will start with problem (p2) by considering two shapes
of the matrix x.
overdetermined system
problem (p2) is called overdetermined ifx‚ààrn√ódis tall and skinny, i.e., n > d . this hap-
pens when you have more rows than columns, or equivalently when you have more equations
than unknowns. when n > d , problem (p2) has a unique solution bŒ∏= (xtx)‚àí1xtyif
409chapter 7. regression
and only if xtxis invertible, or equivalently if and only if the columns of xare linearly in-
dependent. a technical description of this is that xhas a full rank , denoted by rank( x) =d.
when rank( x) =d, problem (p1) has a unique global minimizer bŒ∏= (xtx)‚àí1xty, which
is the same as the unique solution of problem (p2).
figure 7.9: hierarchy of the solutions of an overdetermined system. an overdetermined system uses
a tall and skinny matrix x. the rank of a matrix xis defined as the largest number of independent
columns we can find in x. if rank (x) =d, the matrix xtxis invertible, and problem (p2) will have
a unique solution. if rank (x)< d, then the solution depends on whether the particular observation y
lives in the range space of x. if yes, problem (p2) will have infinitely many solutions because there is
a nontrivial null space. if no, problem (p2) will have no solution because the system is incompatible.
if the columns of xare linearly dependent so that xtxis not invertible, we say
thatxisrank-deficient (denoted as rank( x)< d). in this case, problem (p2) may not
have a solution. we say that it may not have a solution because it is still possible to have a
solution. it all depends on whether ycan be written as a linear combination of the linearly
independent columns of x.
if yes, we say that ylives in the range space ofx. the range space of xis defined
as the set of vectors {z|z=xŒ±,for some Œ±}. if rank( x) =d, allywill live in the range
space of x. but if rank( x)< d, only some of the ywill live in the range space of x.
when this happens, the matrix xmust have a nontrivial null space . the null space of x
is defined as the set of vectors {z|xz= 0}. a nontrivial null space will give us infinitely
many solutions to problem (p2). this is because if Œ±is the solution found in the range
space so that y=xŒ±, then we can pick any zfrom the null space such that xz= 0.
this will lead to another solution Œ±+zsuch that x(Œ±+z) =xŒ±+ 0 = y. since we have
infinitely many choices of such z‚Äôs, there will be infinitely many solutions to problem (p2).
although there are infinitely many solutions to problem (p2), all of them are the global
minimizers of problem (p1). they can make the objective value equal to zero because the
equality y=xŒ∏holds. however, the solutions to problem (p2) are not unique since the
objective function is convex but not strictly convex.
ifydoes not live in the range space of x, we say that problem (p2) is incompatible .
if a system of linear equations is incompatible, there is no solution. however, even when
this happens, we can still solve the optimization problem (p1), but the objective value will
not reach 0. the minimizer is a global minimizer because the objective function is convex,
4107.1. principles of regression
but the minimizer is not unique.
underdetermined system
problem (p2) is called underdetermined ifxis fat and short, i.e., n < d . this happens
when you have more columns than rows, or equivalently when you have more unknowns than
equations. in this case, xtxis not invertible, and so we cannot use bŒ∏= (xtx)‚àí1xty
as the solution. however, if rank( x) =n, then anyywill live in the range space of x. but
because xis fat and short, there exists a nontrivial null space. therefore, problem (p2)
will have infinitely many solutions, attributed to the vectors generated by the null space.
for this set of infinitely many solutions, the corresponding problem (p1) will have a global
minimizer, and the objective value will be zero. however, the minimizer is not unique. this
is the first case in figure 7.10 .
figure 7.10: hierarchy of the solutions of an underdetermined system. an underdetermined system uses
a fat and short matrix x. the rank of a matrix xis defined as the largest number of independent
columns we can find in x. if rank (x) =n, we will have infinitely many solutions. if rank (x)< n ,
then the solutions depends on whether the particular observation ylives in the range space of x. if yes,
problem (p2) will have infinitely many solutions because there is a nontrivial null space. if no, problem
(p2) will have no solution because the system is incompatible.
there are two other cases in figure 7.10 , which occur when rank( x)< n:
¬à(i) ifyis in the range space of x, problem (p2) will have infinitely many solutions.
since problem (p2) remains feasible, the objective function of problem (p1) will go
to zero.
¬à(ii) if yis not in the range space of x, the system in problem (p2) is incompatible
and there will be no solution. the objective value of problem (p1) will not go to zero.
if an underdetermined system has infinitely many solutions, we need to pick and choose.
one of the possible approaches is to consider the optimization
bŒ∏= argmin
Œ∏‚ààrd‚à•Œ∏‚à•2subject to xŒ∏=y. (p3)
this optimization is different from problem (p1), which is an unconstrained optimization.
our goal is to minimize the deviation between xŒ∏andy. problem (p3) is constrained . since
411chapter 7. regression
we assume that problem (p2) has infinitely many solutions, the constraint set y=xŒ∏
is feasible. among all the feasible choices, we pick the one that minimizes the squared
norm. therefore, the solution to problem (p3) is called the minimum-norm least squares.
theorem 7.2 below summarizes the solution. if ydoes not live in the range space of x,
then problem (p2) does not have a solution. therefore, the constraint in p3 is infeasible,
and hence the optimization problem does not have a minimizer.
theorem 7.2. consider the underdetermined linear regression problem where n < d :
bŒ∏=argmin
Œ∏‚ààrd‚à•Œ∏‚à•2subject to y=xŒ∏,
where x‚ààrn√ód,Œ∏‚ààrd, and y‚ààrn. if rank (x) =n, then the linear regression
problem will have a unique global minimum
bŒ∏=xt(xxt)‚àí1y. (7.12)
this solution is called the minimum-norm least-squares solution.
proof . the proof of the theorem requires some knowledge of constrained optimization.
consider the lagrangian of the problem:
l(Œ∏,Œª) =‚à•Œ∏‚à•2+Œªt(xŒ∏‚àíy),
where Œªis called the lagrange multiplier. the solution of the constrained optimization is
the stationary point of the lagrangian. to find the stationary point, we take the derivatives
with respect to Œ∏andŒª. this yields
‚àáŒ∏l= 2Œ∏+xtŒª= 0,
‚àáŒªl=xŒ∏‚àíy= 0.
the first equation gives us Œ∏=‚àíxtŒª/2. substituting it into the second equation, and
assuming that rank( x) =nso that xtxis invertible, we have
x
‚àíxtŒª/2
‚àíy= 0,
which implies that Œª=‚àí2(xxt)‚àí1y. therefore, Œ∏=xt(xxt)‚àí1y. ‚ñ°
the end of this subsection. please join us again.
7.1.5 robust linear regression
this subsection is optional for a first reading of the book.
the linear regression we have discussed so far is based on an important criterion,
namely the squared error criterion. we chose the squared error as the training loss because
4127.1. principles of regression
it is differentiable and convex. differentiability allows us to take the derivative and locate
the minimum point. convexity allows us to claim a global minimizer (also unique if the
objective function is strictly convex). however, such a nice criterion suffers from a serious
drawback: the issue of outliers .
consider figure 7.11 . infigure 7.11 (a), we show a regression problem for n= 50
data points. our basis functions are the ordinary polynomials in the fourth order. everything
looks fine in the figure. we intervene in the data by randomly altering a few of them so that
their values are off. there are only a handful of these outliers. we run the same regression
analysis again, but we observe (see figure 7.11 (b)) that our fitted curve has been distorted
quite significantly.
-1 -0.5 0 0.5 1-2-101234
data
fitted curve
-1 -0.5 0 0.5 1-2-1012345
data
fitted curve
(a) (¬∑)2without outlier (b) ( ¬∑)2with outlier
figure 7.11: linear regression using the squared error as the training loss suffers from outliers. (a) the
regression performs well when there is no outlier. (b) by adding only a few outliers, the regression curve
has already been distorted.
this occurs because of the squared error. by the definition of a squared error, our
training loss is
etrain(Œ∏) =nx
n=1
yn‚àígŒ∏(xn)2
.
without loss of generality, let us assume that one of these error terms is large because of an
outlier. then the training loss becomes
etrain(Œ∏) =
y1‚àígŒ∏(x1)2
| {z }
small+
y2‚àígŒ∏(x2)2
| {z }
small+
y3‚àígŒ∏(x3)2
| {z }
large+¬∑¬∑¬∑+
yn‚àígŒ∏(xn)2
| {z }
small.
here is the daunting fact: if one or a few of these individual error terms are large, the
square operation will amplify them. as a result, the error you see is not just large but large2.
moreover, since we put the squares to the small errors as well, we have small2instead of
small. when you try to weigh the relative significance between the outliers and the normal
data points, the outliers suddenly have a very large contribution to the error. since the goal
of linear regression is to minimize the total loss, the presence of the outliers will drive the
optimization solution to compensate for the large error.
413chapter 7. regression
one possible solution is to replace the squared error by the absolute error , such that
etrain(Œ∏) =nx
n=1yn‚àígŒ∏(xn).
this is a simple modification, but it is very effective. the reason is that the absolute error
keeps the small just as small, and keeps the large just as large. there is no amplification.
therefore, while the outliers still contribute to the overall loss, their contributions are less
prominent. (if you have a lot of strong outliers, even the absolute error will fail. if this
happens, you should go back to your data collection process and find out what has gone
wrong.)
when we use the absolute error as the training loss, the resulting regression problem is
theleast absolute deviation regression (or simply the robust regression ). the tricky thing
about the least absolute deviation is that the training loss is not differentiable. in other
words, we cannot take the derivative and find the optimal solution. the good news is that
there exists an alternative approach for solving this problem: using linear programming
(implemented via the simplex method ).
solving the robust regression problem
let us focus on the linear model
gŒ∏(xn) =xt
nŒ∏,
where xn= [œï0(xn), . . . , œï d‚àí1(xn)]t‚ààrdis the nth input vector for some basis functions
{œïp}d‚àí1
p=0, and Œ∏= [Œ∏0, . . . , Œ∏ d‚àí1]t‚ààrdis the parameter. substituting this into the training
loss, the optimization problem is
minimize
Œ∏‚ààrdnx
n=1yn‚àíxt
nŒ∏.
here is an important trick. the idea is to express the problem as an equivalent problem
minimize
Œ∏‚ààrd,u‚ààrnnx
n=1un
subject to un=|yn‚àíxt
nŒ∏|, n = 1, . . . , n.
there is a small but important difference between this problem and the previous one. in the
first problem, there is only one optimization variable Œ∏. in the new problem, we introduce an
additional variable u= [u1, . . . , u n]tand add a constraint un=|yn‚àíxt
nŒ∏|forn= 1, . . . , n .
we introduce uso that we can have some additional degrees of freedom. at the optimal
solution, unmust equal to |yn‚àíxt
nŒ∏|, and so the corresponding Œ∏is the solution of the
original problem.
now we note that x=|a|is equivalent to x‚â•aandx‚â• ‚àía. therefore, the constraint
can be equivalently written as
minimize
Œ∏‚ààrd,u‚ààrnnx
n=1un, (7.13)
subject to un‚â• ‚àí(yn‚àíxt
nŒ∏), n = 1, . . . , n
un‚â•(yn‚àíxt
nŒ∏), n = 1, . . . , n.
4147.1. principles of regression
in other words, we have rewritten the equality constraint as a pair of inequality constraints
by removing the absolute signs.
the optimization in equation (7.13) is in the form of a standard linear programming
problem. a linear programming problem takes the form of
minimize
x‚ààrkctx (7.14)
subject to ax‚â§b,
for some vectors c‚ààrk,b‚ààrm, and matrix a‚ààrm√ók. linear programming is a stan-
dard optimization problem that you can find in most optimization textbooks. on a com-
puter, if we know c,banda, solving the linear programming problem can be done using
built-in commands. for matlab, the command is linprog . for python, the command is
scipy.optimize.linprog . we will discuss a concrete example shortly.
% matlab command for linear programming
x = linprog(c, a, b);
# python command for linear programming
linprog(c, a, b, bounds=(none,none), method="revised simplex")
given equation (7.13), the question becomes how to convert it into the standard linear
programming format. this requires two steps. the first step uses the objective function :
nx
n=1un=d‚àí1x
p=0(0)(Œ∏p) +nx
n=1(1)(un)
=0 0 ¬∑¬∑¬∑ 0 1 1 ¬∑¬∑¬∑ 1
| {z }
=ct
Œ∏
u
.
therefore, the vector chasd0‚Äôs followed by n1‚Äôs.
the second step concerns the constraint . it can be shown that un‚â• ‚àí(yn‚àíxt
nŒ∏) is
equivalent to xt
nŒ∏‚àíun‚â§yn. written in the matrix form, we have
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞xt
1‚àí1 0 ¬∑¬∑¬∑ 0
xt
20‚àí1¬∑¬∑¬∑ 0
.........¬∑¬∑¬∑...
xt
n0 0 ¬∑¬∑¬∑ ‚àí 1Ô£π
Ô£∫Ô£∫Ô£∫Ô£ªÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£∞Œ∏
u1
...
unÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª‚â§Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞y1
y2
...
ynÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª,
which is equivalent to

x‚àíiŒ∏
u
‚â§y, (7.15)
where i‚ààrn√ónis the identity matrix.
similarly, the other constraint un‚â•(yn‚àíxt
nŒ∏) is equivalent to ‚àíxt
nŒ∏‚àíun‚â§ ‚àíyn.
written in the matrix form, we have
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞‚àíxt
1‚àí1 0 ¬∑¬∑¬∑ 0
‚àíxt
20‚àí1¬∑¬∑¬∑ 0
.........¬∑¬∑¬∑...
‚àíxt
n0 0 ¬∑¬∑¬∑ ‚àí 1Ô£π
Ô£∫Ô£∫Ô£∫Ô£ªÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£∞Œ∏
u1
...
unÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª‚â§Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞‚àíy1
‚àíy2
...
‚àíynÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª,
415chapter 7. regression
which is equivalent to
‚àíx‚àíiŒ∏
u
‚â§ ‚àíy
putting everything together, we have finally arrived at the linear programming problem
minimize
Œ∏‚ààrd,u‚ààrn0d1nŒ∏
u
subject to
x‚àíi
‚àíx‚àíi
Œ∏
u
‚â§
y
‚àíy
,
where 0d‚ààrdis an all-zero vector, and 1n‚ààrnis an all-one vector. it is this problem
that solves the robust linear regression.
let us look at how to implement linear programming to solve the robust regression
optimization. as an example, we continue with the polynomial fitting problem in which
there are outliers. we choose the ordinary polynomials as the basis functions. to construct
the linear programming problem, we need to define the matrix aand the vectors cand
baccording to the linear programming form. this is done using the following matlab
program.
% matlab code to demonstrate robust regression
n = 50;
x = linspace(-1,1,n)‚Äô;
a = [-0.001 0.01 0.55 1.5 1.2];
y = a(1)*legendrep(0,x) + a(2)*legendrep(1,x) + ...
a(3)*legendrep(2,x) + a(4)*legendrep(3,x) + ...
a(5)*legendrep(4,x) + 0.2*randn(n,1);
idx = [10, 16, 23, 37, 45];
y(idx) = 5;
x = [x(:).^0 x(:).^1 x(:).^2 x(:).^3 x(:).^4];
a = [x -eye(n); -x -eye(n)];
b = [y(:); -y(:)];
c = [zeros(1,5) ones(1,n)]‚Äô;
theta = linprog(c, a, b);
t = linspace(-1,1,200)‚Äô;
yhat = theta(1) + theta(2)*t(:) + ...
theta(3)*t(:).^2 + theta(4)*t(:).^3 + ...
theta(5)*t(:).^4;
plot(x,y, ‚Äôko‚Äô,‚Äôlinewidth‚Äô,2); hold on;
plot(t,yhat,‚Äôr‚Äô,‚Äôlinewidth‚Äô,4);
in this set of commands, the basis vectors are defined as xt
n= [œï4(xn), . . . , œï 0(xn)]t, for
n= 1, . . . , n . the matrix iis constructed by using the command eye(n) , which constructs
the identity matrix of size n√ón. the rest of the commands are self-explanatory. note that
the solution to the linear programming problem consists of both Œ∏andu. to squeeze Œ∏we
need to locate the first dentries. the remainder is u.
commands for python are similar, although we need to call np.hstack andnp.vstack
to construct the matrices and vectors. the main routine is linprog in the scipy.optimize
4167.1. principles of regression
library. note that for this particular example, the bounds are bounds=(none,none) , or
otherwise python will search in the positive quadrant.
# python code to demonstrate robust regression
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import eval_legendre
from scipy.optimize import linprog
n = 50
x = np.linspace(-1,1,n)
a = np.array([-0.001, 0.01, 0.55, 1.5, 1.2])
y = a[0]*eval_legendre(0,x) + a[1]*eval_legendre(1,x) + \
a[2]*eval_legendre(2,x) + a[3]*eval_legendre(3,x) + \
a[4]*eval_legendre(4,x) + 0.2*np.random.randn(n)
idx = [10,16,23,37,45]
y[idx] = 5
x = np.column_stack((np.ones(n), x, x**2, x**3, x**4))
a = np.vstack((np.hstack((x, -np.eye(n))), np.hstack((-x, -np.eye(n)))))
b = np.hstack((y,-y))
c = np.hstack((np.zeros(5), np.ones(n)))
res = linprog(c, a, b, bounds=(none,none), method="revised simplex")
theta = res.x
t = np.linspace(-1,1,200)
yhat = theta[0]*np.ones(200) + theta[1]*t + theta[2]*t**2 + \
theta[3]*t**3 + theta[4]*t**4
plt.plot(x,y,‚Äôo‚Äô,markersize=12)
plt.plot(t,yhat, linewidth=8)
plt.show()
the result of this experiment is shown in figure 7.12 . it is remarkable to see that the
robust regression result is almost as good as the result would be without outliers.
if robust linear regression performs so well, why don‚Äôt we use it all the time? why
is least squares regression still more popular? the answer has a lot to do with the com-
putational complexity and the uniqueness of the solution. linear programming requires an
algorithm for a solution. while we have very fast linear programming solvers today, the com-
putational cost of solving a linear program is still much higher than solving a least-squares
problem (which is essentially a matrix inversion).
the other issue with robust linear regression is the uniqueness of the solution. lin-
ear programming is known to have degenerate solutions when the constraint set (a high-
dimensional polygon) touches the objective function (which is a line) at one of its edges.
the least-squares fitting does not have this problem because the optimization surface is a
parabola. unless the matrix xtxis noninvertible, the solution is guaranteed to be the
unique global minimum. linear programming does not have this convenient property. we
can have multiple solutions Œ∏that give the same objective value. if you try to interpret your
result by inspecting the magnitude of the Œ∏‚Äôs, the nonuniqueness of the solution would cause
problems because your interpretation can be swiped immediately if the linear programming
gives you a nonunique solution.
417chapter 7. regression
-1 -0.5 0 0.5 1-2-1012345
-1 -0.5 0 0.5 1-2-1012345
(a) ordinary ( ¬∑)2regression with outliers (b) robust | ¬∑ |regression with outliers
figure 7.12: (a) ordinary linear regression using (¬∑)2as the training loss. in the absence of any outlier,
the regression performs well. (b) robust linear regression using | ¬∑ |as the training loss. note that even
in the presence of outliers, the robustness regression perform reasonably well.
end of this subsection. please join us again.
closing remark . the principle of linear regression is primarily to set up a function to fit
the data. this, in turn, is about finding a set of good basis functions and minimizing the
appropriate training loss. selecting the basis is usually done in several ways:
¬àthe problem forces you to choose certain basis functions. for example, suppose you
are working on a disease dataset. the variates are height, weight, and bmi. you do
not have any choice here because your goal is to see which factor contributes the most
to the cause of the disease.
¬àthere are known basis functions that work. for example, suppose you are working on
a speech dataset. physics tells us that fourier bases are excellent representations of
these sinusoidal functions. so it would make more sense to use the fourier basis than
the polynomials.
¬àsometimes the basis can be learned from the data. for example, you can run principal-
component analysis (pca) to extract the basis. then you can run the linear regression
to compute the coefficients. this is a data-driven approach and could apply to some
problems.
7.2 overfitting
the regression principle we have discussed in the previous section is a powerful technique
for data analysis. however, there are many ways in which things can fall apart. we have
seen the problem of outliers, where perturbations of one or a few data points would result
in a big change in the regression result, and we discussed some techniques to overcome the
4187.2. overfitting
outlier problem, e.g., using robust regression. in addition to outliers, there are other causes
of the failure of the regression.
in this section, we examine the relationship between the number of training samples
and the complexity of the model. for example, if we decide to use polynomials as the basis
functions and we have only n= 20 data points, what should be the order of the polynomials?
shall we use the 5th-order polynomial, or shall we use the 20th-order? our goal in this section
is to acquire an understanding of the general problem known as overfitting . then we will
discuss methods for mitigating overfitting in section 7.4.
7.2.1 overview of overfitting
imagine that we have a dataset containing n= 20 training samples. we know that the data
are generated from a fourth-order polynomial with legendre polynomials as the basis. on
top of these samples, we also know that a small amount of noise corrupts each sample, for
example, gaussian noise of standard deviation œÉ= 0.1.
we have two options here for fitting the data:
¬àoption 1: h(x) =p4
p=0Œ∏plp(x), which is a 4th-order polynomial.
¬àoption 2: g(x) =p50
p=0Œ∏plp(x), which is a 50th-order polynomial.
model 2 is more expressive because it has more degrees of freedom. let us fit the data using
these two models. figure 7.13 shows the results. however, what is going on with the 50th-
order polynomial? it has gone completely wild. how can the regression ever choose such a
terrible model?
-1 -0.5 0 0.5 1-3-2-10123
data
fitted curve
-1 -0.5 0 0.5 1-3-2-10123
data
fitted curve
(a) 4th-order polynomial (b) 50th-order polynomial
figure 7.13: fitting data using a 4th-order polynomial and a 50th-order polynomial.
here is an even bigger surprise: if we compute the training loss, we get
etrain(h) =1
nnx
n=1
yn‚àíh(xn)2
= 0.0063,
etrain(g) =1
nnx
n=1
yn‚àíg(xn)2
= 5.7811√ó10‚àí24.
419chapter 7. regression
thus, while model 2 looks wild in the figure, it has a much lower training loss than model 1.
so according to the training loss, model 2 fits better.
any sensible person at this point will object, since model 2 cannot possibly be better,
for the following reason. it is not because it ‚Äúlooks bad‚Äù, but because if you test the model
with an unseen sample it is almost certain that the testing error will explode. for example,
infigure 7.13 (a) if we look at x= 0, we would expect the predicted value to be close
toy= 0. however, figure 7.13 (b) suggests that the predicted value is going to negative
infinity. it would be hard to believe that the negative infinity is a better prediction than the
other one. we refer to this general phenomenon of fitting very well to the training data but
generalizing poorly to the testing data as overfitting .
what is overfitting?
overfitting means that a model fits too closely to the training samples so that it
fails to generalize.
overfitting occurs as a consequence of an imbalance between the following three factors:
¬ànumber of training samples n. if you have many training samples, you should learn
very well, even if the model is complex. conversely, if the model is complex but does
not have enough training samples, you will overfit it. the most serious problem in
regression is often insufficient training data.
¬àmodel order d. this refers to the complexity of the model. for example, if your model
uses a polynomial, drefers to the order of the polynomial. if your training set is too
small, you need to use a less complex model. the general rule of thumb is: ‚Äúless is
more‚Äù.
¬ànoise variance œÉ2. this refers to the variance of the error enyou add to the data.
the model we assumed in the previous numerical experiment is that
yn=g(xn) +en, n = 1, . . . , n.
where en‚àºgaussian(0 , œÉ2). if œÉincreases, it is inevitable that the fitting will be-
come more difficult. hence it would require more training samples, and perhaps a less
complex model would work better.
7.2.2 analysis of the linear case
let us spell out the details of these factors one by one. to make our discussion concrete, we
will use linear regression as a case study. the general analysis will be presented in the next
section.
notations
¬àground truth model . to start with, we assume that we have a population set d
containing infinitely many samples ( x, y) drawn according to some latent distributions.
the relationship between xandyis defined through an unknown target function
y=f(x) +e,
4207.2. overfitting
where e‚àºgaussian(0 , œÉ2) is the noise. for our analysis, we assume that f(¬∑) is linear,
so that
f(x) =xtŒ∏‚àó,
where Œ∏‚àó‚ààrdis the ground truth model parameter. notice that f(¬∑) is deterministic,
buteis random. therefore, any randomness we see in yis due to e.
¬àtraining and testing set . from d, we construct two datasets: the training data set
dtrainthat contains training samples {(x1, y1), . . . , (xn, yn)}and the testing dataset
dtestthat contains {(x1, y1), . . . , (xm, ym)}. both dtrainanddtestare subsets of d.
¬àpredictive model . we consider a predictive model gŒ∏(¬∑). for simplicity, we assume
thatgŒ∏(¬∑) is also linear:
gŒ∏(x) =xtŒ∏.
given the training dataset d={(x1, y1), . . . , (xn, yn)}, we construct a linear regres-
sion problem:
bŒ∏= argmin
Œ∏‚ààrd‚à•xŒ∏‚àíy‚à•2.
throughout our analysis, we assume that n‚â•dso that we have more training data
than the number of unknowns. we further assume that xtxis invertible, and so
there is a unique global minimizer
bŒ∏= (xtx)‚àí1xty.
¬àtraining error . given the estimated model parameter bŒ∏, we define the in-sample
prediction as
bytrain=xtrainbŒ∏,
where xtrain=xis the training data matrix. the in-sample prediction is the pre-
dicted value using the trained model for the training data. the corresponding error
with respect to the ground truth is called the training error :
etrain(bŒ∏) =ee1
n‚à•bytrain‚àíy‚à•2
,
where nis the number of training samples in the training dataset. note that the
expectation is taken with respect to the noise vector e, which follows the distribution
gaussian(0 , œÉ2i).
¬àtesting error . during testing, we construct a testing matrix xtest. this gives us the
estimated values bytest:
bytest=xtestbŒ∏.
the out-sample prediction is the predicted value using the trained model for the testing
data. the corresponding error with respect to the ground truth is called the testing
error:
etest(bŒ∏) =ee1
m‚à•bytest‚àíy‚à•2
,
where mis the number of testing samples in the testing dataset.
421chapter 7. regression
analysis of the training error
we first analyze the training error, which we defined as
etrain=ee1
n‚à•by‚àíy‚à•2
def= mse( by,y). (7.16)
for this particular choice of the training error, we call it the mean squared error (mse). it
measures the difference between byandy.
theorem 7.3. letŒ∏‚àó‚ààrdbe the ground truth linear model parameter, and x‚àà
rn√ódbe a matrix such that n‚â•dandxtxis invertible. assume that the data
follows the linear model y=xŒ∏‚àó+ewhere e‚àºgaussian (0, œÉ2i). consider the linear
regression problem bŒ∏=argmin
Œ∏‚ààrd‚à•xŒ∏‚àíy‚à•2, and the predicted value by=xbŒ∏. the
mean squared training error of this linear model is
etraindef=mse(by,y) =ee1
n‚à•by‚àíy‚à•2
=œÉ2
1‚àíd
n
. (7.17)
the proof below depends on some results from linear algebra that may be difficult for
first-time readers. we recommend you read the proof later.
proof . recall that the least squares linear regression solution is bŒ∏= (xtx)‚àí1xty. since
y=xŒ∏‚àó+e, we can substitute this into the predicted value byto show that
by=xbŒ∏=x(xtx)‚àí1xt
| {z }
=hy=x(xtx)‚àí1xt(xŒ∏‚àó+e) =xŒ∏‚àó+he.
therefore, substituting by=xŒ∏‚àó+heinto the mse,
mse(by,y)def=ee1
n‚à•by‚àíy‚à•2
=ee1
n‚à•xŒ∏‚àó+he‚àíxŒ∏‚àó‚àíe‚à•2
=ee1
n‚à•(h‚àíi)e‚à•2
.
at this point we need to use a tool from linear algebra. one useful identity3is that for any
v‚ààrn,
‚à•v‚à•2= tr(vvt).
3the reason for this identity is that
v=nx
n=1v2
n= trÔ£±
Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞v2
1v1v2¬∑¬∑¬∑ v1vn
v2v1 v2
2¬∑¬∑¬∑ v2vn
............
vnv1vnv2¬∑¬∑¬∑ v2
nÔ£π
Ô£∫Ô£∫Ô£∫Ô£ªÔ£º
Ô£¥Ô£¥Ô£¥Ô£Ω
Ô£¥Ô£¥Ô£¥Ô£æ= trn
vvto
.
4227.2. overfitting
using this identity, we have that
ee1
n‚à•(h‚àíi)e‚à•2
=1
nee
tr
(h‚àíi)eet(h‚àíi)t
=1
ntr
(h‚àíi)ee
eet
(h‚àíi)t
=œÉ2
ntr
(h‚àíi)(h‚àíi)t
,
where we used the fact that e[eet] =œÉ2i. the special structure of htells us that ht=h
andhth=h. thus, we have ( h‚àíi)t(h‚àíi) =i‚àíh. in addition, using the cyclic
property of trace tr( ab) = tr( ba), we have that
tr(h) = tr( x(xtx)‚àí1xt)
= tr(( xtx)‚àí1xtx) = tr( i) =d.
consequently,
œÉ2
ntr
(h‚àíi)(h‚àíi)t
=œÉ2
ntr
i‚àíh
=œÉ2
1‚àíd
n
.
this completes the proof.
‚ñ°
the end of the proof. please join us again.
practice exercise 1 . in the theorem above, we proved the mse of the prediction y.
in this example, we would like to prove the mse for the parameter . prove that
mse(bŒ∏,Œ∏‚àó)def=eebŒ∏‚àíŒ∏‚àó2
=œÉ2tr
(xtx)‚àí1
.
solution . let us start with the definition:
mse(bŒ∏,Œ∏‚àó) =ee(xtx)‚àí1xty‚àíŒ∏‚àó2
=ee(xtx)‚àí1xt(xŒ∏‚àó+e)‚àíŒ∏‚àó2
=eeŒ∏‚àó+ (xtx)‚àí1xte‚àíŒ∏‚àó2
=ee(xtx)‚àí1xte2
.
423chapter 7. regression
continuing the calculation,
ee(xtx)‚àí1xte2
=ee
tr
(xtx)‚àí1xte etx(xtx)‚àí1
= tr
(xtx)‚àí1xtee
eet
x(xtx)‚àí1
= tr
(xtx)‚àí1xt¬∑œÉ2i¬∑x(xtx)‚àí1
=œÉ2tr
(xtx)‚àí1xt¬∑x(xtx)‚àí1
=œÉ2tr
(xtx)‚àí1
.
analysis of the testing error
similarly to the training error, we can analyze the testing error. the testing error is defined
as
etest= mse( by,y‚Ä≤)def=ee,e‚Ä≤1
m‚à•by‚àíy‚Ä≤‚à•2
, (7.18)
whereby= [by1, . . . ,bym]tis a vector of mpredicted values and y‚Ä≤= [y‚Ä≤
1, . . . , y‚Ä≤
m]tis a vector
ofmtrue values in the testing data.4
we would like to derive something concrete. to make our analysis simple, we consider
a special case in which the testing set contains ( x1, y‚Ä≤
1), . . . , (xn, y‚Ä≤
n). that is, the inputs
x1, . . . , x nare identical for both training and testing (for example, suppose that you measure
the temperature on two different days but at the same time stamps.) in this case, we have
m=n, and we have xtest=xtrain=x. however, the noise added to the testing data is
still different from the noise added to the training data.
with these simplifications, we can derive the testing error as follows.
theorem 7.4. letŒ∏‚àó‚ààrdbe the ground truth linear model parameter, and x‚àà
rn√ódbe a matrix such that n‚â•dandxtxis invertible. assume that the training
data follows the linear model y=xŒ∏‚àó+e, where e‚àºgaussian (0, œÉ2i). consider
the linear regression problem bŒ∏= (xtx)‚àí1xty, and let by=xbŒ∏. letxtest=x
be the testing input data matrix, and define y‚Ä≤=xtestŒ∏‚àó+e‚Ä≤‚ààrn, with e‚Ä≤‚àº
gaussian (0, œÉ2i), be the testing output. then, the mean squared testing error of this
linear model is
etestdef=mse(by,y‚Ä≤) =ee,e‚Ä≤1
n‚à•by‚àíy‚Ä≤‚à•2
=œÉ2
1 +d
n
. (7.19)
in this definition, the expectation is taken with respect to a joint distribution of ( e,e‚Ä≤).
this is because, in testing, the trained model is based on yof which the randomness is e.
however, the testing data is based on y‚Ä≤, where the randomness comes from e‚Ä≤. we assume
thateande‚Ä≤are independent i.i.d. gaussian vectors.
4in practice, the number of testing samples mcan be much larger than the number of training samples n.
this probably does not agree with your experience, in which the testing dataset is often much smaller than
the training dataset. the reason for this paradox is that the practical testing data set is only a finite subset
of all the possible testing samples available. so the ‚Äútesting error‚Äù we compute in practice approximates the
true testing error. if you want to compute the true testing error, you need a very large testing dataset.
4247.2. overfitting
as with the previous proof, we recommend you study this proof later.
proof . the mse can be derived from the definition:
mse(by,y‚Ä≤) =ee,e‚Ä≤1
n‚à•by‚àíy‚Ä≤‚à•2
=1
nee,e‚Ä≤
‚à•xŒ∏‚àó+he‚àíxŒ∏‚àó‚àíe‚Ä≤‚à•2
=1
nee,e‚Ä≤
‚à•he‚àíe‚Ä≤‚à•2
.
since each noise term enande‚Ä≤
nis an i.i.d. copy of the same gaussian random variable, by
using the fact that
tr(h) = tr( x(xtx)‚àí1xt)
= tr(( xtx)‚àí1xtx) = tr( i) =d,
we have that
ee,e‚Ä≤h
‚à•he‚àíe‚Ä≤‚à•2i
=ee
‚à•he‚à•2
‚àíee,e‚Ä≤h
2ethte‚Ä≤i
| {z }
=0+ee‚Ä≤
‚à•e‚Ä≤‚à•2
=eeh
trn
heethtoi
+ee‚Ä≤
tr
e‚Ä≤e‚Ä≤t	
= trn
hee
eet
hto
+ tr{ee‚Ä≤
e‚Ä≤e‚Ä≤t
}
= trn
h¬∑œÉ2in√ón¬∑hto
+ tr
œÉ2in√ón	
=œÉ2trn
hhto
+ tr
œÉ2in√ón	
=œÉ2tr(id√ód) +œÉ2tr{in√ón}=œÉ2(d+n).
combining all the terms,
mse(by,y‚Ä≤) =ee,e‚Ä≤1
n‚à•by‚àíy‚Ä≤‚à•2
=œÉ2
1 +d
n
,
which completes the proof.
‚ñ°
the end of the proof.
7.2.3 interpreting the linear analysis results
let us summarize the two main theorems. they state that, for n‚â•d,
etraindef= mse( by,y) =ee1
n‚à•by‚àíy‚à•2
=œÉ2
1‚àíd
n
, (7.20)
etestdef= mse( by,y‚Ä≤) =ee,e‚Ä≤1
n‚à•by‚àíy‚Ä≤‚à•2
=œÉ2
1 +d
n
. (7.21)
this pair of equations tells us everything about the overfitting issue.
425chapter 7. regression
how do etrainandetestchange w.r.t. œÉ2?
¬àetrain‚ÜëasœÉ2‚Üë. thus noisier data are harder to fit.
¬àetest‚ÜëasœÉ2‚Üë. thus a noiser model is more difficult to generalize.
the reasons for these results should be clear from the following equations:
etrain=œÉ2
1‚àíd
n
,
etest=œÉ2
1 +d
n
.
asœÉ2increases, the training error etraingrows linearly w.r.t. œÉ2. since the training error
measures how good your model is compared with the training data, a larger etrainmeans it
is more difficult to fit. for the testing case, etestalso grows linearly w.r.t. œÉ2. this implies
that the model would be more difficult to generalize if the model were trained using noisier
data.
how do etrainandetestchange w.r.t. n?
¬àetrain‚Üëasn‚Üë. thus more training samples make fitting harder.
¬àetest‚Üìasn‚Üë. thus more training samples improve generalization.
the reason for this should also be clear from the following equations:
etrain=œÉ2
1‚àíd
n
,
etest=œÉ2
1 +d
n
.
asnincreases, the model sees more training samples. the goal of the model is to minimize
the error with all the training samples. thus the more training samples we have, the harder
it will be to make everyone happy, so the training error grows as ngrows. for testing, if the
model is trained with more samples it is more resilient to noise. hence the generalization
improves.
how do etrainandetestchange w.r.t. d?
¬àetrain‚Üìasd‚Üë. thus a more complex model makes fitting easier.
¬àetest‚Üëasd‚Üë. thus a more complex model makes generalization harder.
these results are perhaps less obvious than the others. the following equations tell us that
etrain=œÉ2
1‚àíd
n
,
etest=œÉ2
1+d
n
. (7.22)
4267.2. overfitting
for this linear regression model to work, dhas to be less than n; otherwise, the matrix
inversion ( xtx)‚àí1is invalid. however, as dgrows while nremains fixed, we ask the
linear regression to fit a larger and larger model while not providing any additional training
samples. equation (7.22) says that etrainwill drop as dincreases but etestwill increase as d
increases. therefore, a larger model will not generalize as well if nis fixed.
ifd > n , then the optimization
bŒ∏= argmin
Œ∏‚ààrd‚à•xŒ∏‚àíy‚à•2
will have many global minimizers (see figure 7.10 ), implying that the training error can go
to zero. our analysis of etrainandetestdoes not cover this case because our proofs require
(xtx)‚àí1to exist. however, we can still extrapolate what will happen. when the training
error is zero, it only means that we fit perfectly into the training data. since the testing
error grows as dgrows (though not in the particular form shown in equation (7.22)), we
should expect the testing error to become worse.
learning curve
the results we derived above can be summarized in the learning curve shown in figure 7.14 .
in this figure we consider a simple problem where
yn=Œ∏0+Œ∏1xn+en,
foren‚àºgaussian(0 ,1). therefore, according to our theoretical derivations, we have œÉ= 1
andd= 2. for every n, we compute the average training error etrainand the average testing
erroretest, and then mark them on the figure. these are our empirical training and testing
errors. on the same figure, we calculate the theoretical training and testing error according
to equation (7.22).
the matlab and python codes used to generate this learning curve are shown below.
nset = round(logspace(1,3,20));
e_train = zeros(1,length(nset));
e_test = zeros(1,length(nset));
a = [1.3, 2.5];
for j = 1:length(nset)
n = nset(j);
x = linspace(-1,1,n)‚Äô;
e_train_temp = zeros(1,1000);
e_test_temp = zeros(1,1000);
x = [ones(n,1), x(:)];
for i = 1:1000
y = a(1) + a(2)*x + randn(size(x));
y1 = a(1) + a(2)*x + randn(size(x));
theta = x\y(:);
yhat = theta(1) + theta(2)*x;
e_train_temp(i) = mean((yhat(:)-y(:)).^2);
e_test_temp(i) = mean((yhat(:)-y1(:)).^2);
end
e_train(j) = mean(e_train_temp);
427chapter 7. regression
e_test(j) = mean(e_test_temp);
end
semilogx(nset, e_train, ‚Äôkx‚Äô, ‚Äôlinewidth‚Äô, 2, ‚Äômarkersize‚Äô, 16); hold on;
semilogx(nset, e_test, ‚Äôro‚Äô, ‚Äôlinewidth‚Äô, 2, ‚Äômarkersize‚Äô, 8);
semilogx(nset, 1-2./nset, ‚Äôk‚Äô, ‚Äôlinewidth‚Äô, 4);
semilogx(nset, 1+2./nset, ‚Äôr‚Äô, ‚Äôlinewidth‚Äô, 4);
import numpy as np
import matplotlib.pyplot as plt
nset = np.logspace(1,3,20)
nset = nset.astype(int)
e_train = np.zeros(len(nset))
e_test = np.zeros(len(nset))
for j in range(len(nset)):
n = nset[j]
x = np.linspace(-1,1,n)
a = np.array([1, 2])
e_train_tmp = np.zeros(1000)
e_test_tmp = np.zeros(1000)
for i in range(1000):
y = a[0] + a[1]*x + np.random.randn(n)
x = np.column_stack((np.ones(n), x))
theta = np.linalg.lstsq(x, y, rcond=none)[0]
yhat = theta[0] + theta[1]*x
e_train_tmp[i] = np.mean((yhat-y)**2)
y1 = a[0] + a[1]*x + np.random.randn(n)
e_test_tmp[i] = np.mean((yhat-y1)**2)
e_train[j] = np.mean(e_train_tmp)
e_test[j] = np.mean(e_test_tmp)
plt.semilogx(nset, e_train, ‚Äôkx‚Äô)
plt.semilogx(nset, e_test, ‚Äôro‚Äô)
plt.semilogx(nset, (1-2/nset), linewidth=4, c=‚Äôk‚Äô)
plt.semilogx(nset, (1+2/nset), linewidth=4, c=‚Äôr‚Äô)
the training error curve and the testing error curve behave in opposite ways as n
increases. the training error etrain increases as nincreases, because when we have more
training samples it becomes harder for the model to fit all the data. by contrast, the testing
erroretestdecreases as nincreases, because when we have more training samples the model
becomes more robust to noise and unseen data. therefore, the testing error improves.
asngoes to infinity, both the training error and the testing error converge. this is
due to the law of large numbers, which says that the empirical training and testing errors
should converge to their respective expected values. if the training error and the testing error
converge to the same value, the training can generalize to testing. if they do not converge to
the same value, there is a mismatch between the training samples and the testing samples.
it is important to pay attention to the gap between the converged values. we often
assume that the training samples and the testing samples are drawn from the same distri-
bution, and therefore the training samples are good representatives of the testing samples.
4287.3. bias and variance trade-off
101102103
number of training samples, n0.80.850.90.9511.051.11.151.2errortraining error
testing error
figure 7.14: the learning curve is a pair of functions representing the training error and the testing
error. as nincreases we expect the training error to increase and the testing error to decrease. the two
functions will converge to the same value as ngoes to infinity. if they do not converge to the same
value, there is an intrinsic mismatch between the training samples and the testing samples, e.g., the
training samples are not representative enough for the dataset.
if the assumption is not true, there will be a gap between the converged training error and
the testing error. thus, what you claim in training cannot be transferred to the testing.
consequently, the learning curve provides you with a useful debugging tool to check how
well your training compares with your testing.
closing remark . in this section we have studied a very important concept in regression,
overfitting. we emphasize that overfitting is not only caused by the complexity of the model
but a combination of the three factors œÉ2,n, and d. we close this section by summarizing
the causes of overfitting:
what is the source of overfitting?
¬àoverfitting occurs because you have an imbalance between œÉ2,nandd.
¬àselecting the correct complexity for your model is the key to avoid overfitting.
7.3 bias and variance trade-off
our linear analysis has provided you with a rough understanding of what we experience in
overfitting. however, for general regression problems where the models are not necessarily
linear, we need to go deeper. the goal of this section is to explain the trade-off between
bias and variance. this analysis requires some patience as it involves many equations. we
recommend skipping this section on a first reading and then returning to it later.
429chapter 7. regression
if it is your first time reading it, we recommend you go through it slowly.
7.3.1 decomposing the testing error
notations
as we did at the beginning of section 7.2, we consider a ground truth model that relates
an input xand an output y:
y=f(x) +e,
where e‚àºgaussian(0 , œÉ2) is the noise. for example, if we use a linear model, then fcould
bef(x) =Œ∏tx, for some regression coefficients Œ∏.
during training , we pick a prediction model gŒ∏(¬∑) and try to predict the output when
given a training sample x:
by=gŒ∏(x).
for example, we may choose gŒ∏(x) =Œ∏tx, which is also a linear model. we may also choose
a linear model in another basis, e.g., gŒ∏(x) =Œ∏tœï(x) for some transformations œï(¬∑). in any
case, the goal of training is to minimize the training error:
bŒ∏= argmin
Œ∏1
nnx
n=1(gŒ∏(xn)‚àíyn)2,
where the sum is taken over the training samples dtrain={(x1, y1), . . . , (xn, yn)}. because
the model parameter bŒ∏is learned from the training dataset dtrain, the prediction model
depends on dtrain. to emphasize this dependency, we write
g(dtrain)= the model trained from
(x1, y1), . . . , (xn, yn)
.
during testing , we consider a testing dataset dtest={(x‚Ä≤
1, y‚Ä≤
1), . . . , (x‚Ä≤
m, y‚Ä≤
m)}. we put
these testing samples into the trained model to predict an output:
by‚Ä≤
m=g(dtrain)(x‚Ä≤
m), m = 1, . . . , m. (predicted value)
since the goal of regression is to make g(dtrain)as close to fas possible, it is natural to
expect by‚Ä≤
mto be close to y‚Ä≤
m.
testing error decomposition (noise-free)
so we can now compute the testing error ‚Äî the error that we ultimately care about. in the
noise-free condition, i.e., e= 0, the testing error is defined as
e(dtrain)
test =ex‚Ä≤h 
g(dtrain)(x‚Ä≤)‚àíf(x‚Ä≤)2i
(7.23)
‚âà1
mmx
m=1
g(dtraing )(x‚Ä≤
m)‚àíf(x‚Ä≤
m)2
.
there are several components in this equation. first, x‚Ä≤is a testing sample drawn from a
certain distribution. you can think of dtestas a finite subset drawn from this distribution.
4307.3. bias and variance trade-off
second, the error 
g(dtrain)(x‚Ä≤)‚àíf(x‚Ä≤)2measures the deviation between our predicted value
and the true value. note that this error term is specific to one testing sample x‚Ä≤. therefore,
we take expectation ex‚Ä≤to find the average of the error for the distribution of x‚Ä≤.
the testing error e(dtrain)
test is a function that is dependent on the training set dtrain,
because the model g(dtrain)is trained from dtrain. therefore, as we change the training
set, we will have a different model gand hence a different testing error. to eliminate the
randomness of the training set, we define the overall testing error as
etest=edtrain
e(dtrain)
test
=edtrain
ex‚Ä≤h 
g(dtrain)(x‚Ä≤)‚àíf(x‚Ä≤)2i
. (7.24)
note that this definition of the testing error is consistent with the special case in equa-
tion (7.18), in which the testing error involves a joint expectation over eande‚Ä≤. the ex-
pectation over eaccounts for the training samples, and the expectation over e‚Ä≤accounts for
the testing samples.
let us try to extract some meaning from the testing error. our method will be to
decompose the testing error into biasandvariance .
theorem 7.5. assume a noise-free condition. the testing error of a regression prob-
lem is given by
etest=ex‚Ä≤
(g(x‚Ä≤)‚àíf(x‚Ä≤))2
| {z }
=bias(x‚Ä≤)+edtrain[(g(dtrain)(x‚Ä≤)‚àíg(x‚Ä≤))2]| {z }
=var(x‚Ä≤)
, (7.25)
where g(x‚Ä≤)def=edtrain[g(dtrain)(x‚Ä≤)].
proof . to simplify our notation, we will drop the subscript ‚Äútrain‚Äù in dtrain when the
context is clear. we have that
etest=edh
ex‚Ä≤h
(g(d)(x‚Ä≤)‚àíf(x‚Ä≤))2ii
=ex‚Ä≤h
edh
(g(d)(x‚Ä≤)‚àíf(x‚Ä≤))2ii
.
continuing the calculation,
etest=ex‚Ä≤h
edh
(g(d)(x‚Ä≤)‚àíg(x‚Ä≤) +g(x‚Ä≤)‚àíf(x‚Ä≤))2ii
=ex‚Ä≤
edh
(g(d)(x‚Ä≤)‚àíg(x‚Ä≤))2i
+ 2edh
(g(d)(x‚Ä≤)‚àíg(x‚Ä≤))(g(x‚Ä≤)‚àíf(x‚Ä≤))i
+edh
(g(x‚Ä≤)‚àíf(x‚Ä≤))2i
.
since g(x‚Ä≤)def=ed[g(d)(x‚Ä≤)], it follows that
2edh
(g(d)(x‚Ä≤)‚àíg(x‚Ä≤))(g(x‚Ä≤)‚àíf(x‚Ä≤))i
= 0
431chapter 7. regression
because g(x‚Ä≤)‚àíf(x‚Ä≤) is independent of d, and
edh
(g(x‚Ä≤)‚àíf(x‚Ä≤))2i
= (g(x‚Ä≤)‚àíf(x‚Ä≤))2.
therefore,
etest=ex‚Ä≤
edh
(g(d)(x‚Ä≤)‚àíg(x‚Ä≤))2i
+h
(g(x‚Ä≤)‚àíf(x‚Ä≤))2i
.
thus, by defining two following terms we have proved the theorem.
bias(x‚Ä≤)def= (g(x‚Ä≤)‚àíf(x‚Ä≤))2,
var(x‚Ä≤)def=ed[(g(d)(x‚Ä≤)‚àíg(x‚Ä≤))2].
‚ñ°
let‚Äôs consider what this theorem implies. this result is a decomposition of the testing
error into bias andvariance . it is a universal result that applies to allregression models,
not only linear cases. to summarize the meanings of bias and variance:
what are bias and variance?
¬àbias = how far your average is from the truth.
¬àvariance = how much fluctuation you have around the average.
figure 7.15 gives a pictorial representation of bias and variance. in this figure, we
construct four scenarios of bias and variance. each cross represents the predictor g(dtrain),
with the true predictor fat the origin. figure 7.15 (a) shows the case with a low bias and
a low variance. all these predictors g(dtrain)are very close to the ground truth, and they
have small fluctuations around their average. figure 7.15 (b) shows the case of a high bias
and a low variance. it has a high bias because the entire group of g(dtrain)is shifted to the
corner. the bias, which is the distance from the truth to the average, is therefore large. the
variance remains small because the fluctuation around the average is small. figure 7.15 (c)
shows the case of a low bias but high variance. in this case, the fluctuation around the
average is large. figure 7.15 shows the case of high bias and high variance. we want to
avoid this case.
bias low bias high bias low bias high
var low var low var high var high
(a) (b) (c) (d)
figure 7.15: imagine that you are throwing a dart with a target at the center. the four subfigures show
the levels of bias and variance.
4327.3. bias and variance trade-off
testing error decomposition (noisy case)
let us consider a situation when there is noise. in the presence of noise, the training and
testing samples will follow the relationship
y=f(x) +e,
where e‚àºgaussian(0 , œÉ2). we assume that the noise is gaussian to make the proof easier.
we can consider other types of noise in theory, but the theoretical results will need to be
modified.
in the presence of noise, the testing error is
etest(x‚Ä≤)def=edtrain,e
g(dtrain)(x‚Ä≤)‚àíf(x‚Ä≤) +e2
=edtrain,e
g(dtrain)(x‚Ä≤)‚àíg(x‚Ä≤) +g(x‚Ä≤)‚àíf(x‚Ä≤) +e2
,
where we take the joint expectation over the training dataset dtrainand the error e. con-
tinuing the calculation, and using the fact that dtrainandeare independent (and e[e] = 0),
it follows that
etest(x‚Ä≤) =edtrain,e
g(dtrain)(x‚Ä≤)‚àíg(x‚Ä≤) +g(x‚Ä≤)‚àíf(x‚Ä≤) +e2
=edtrain,e
g(d)(x‚Ä≤)‚àíg(x‚Ä≤)2
+
g(x‚Ä≤)‚àíf(x‚Ä≤)2
+e2
=edtrain
g(dtrain)(x‚Ä≤)‚àíg(x‚Ä≤)2
| {z }
=var(x‚Ä≤)+
g(x‚Ä≤)‚àíf(x‚Ä≤)2
| {z }
=bias( x‚Ä≤)+eeh
e2i
|{z}
=noise.
taking the expectation of x‚Ä≤over the entire testing distribution gives us
etest=ex‚Ä≤[etest(x‚Ä≤)] =ex‚Ä≤[var(x‚Ä≤)]|{z}
var+ex‚Ä≤[bias(x‚Ä≤)]|{z}
bias+œÉ2.
the theorem below summarizes the results:
theorem 7.6. assume a noisy condition where y=f(x)+efor some i.i.d. gaussian
noise e‚àºgaussian (0, œÉ2). the testing error of a regression problem is given by
etest=ex‚Ä≤
(g(x‚Ä≤)‚àíf(x‚Ä≤))2
| {z }
=bias(x‚Ä≤)
+ex‚Ä≤
edtrain[(g(dtrain)(x‚Ä≤)‚àíg(x‚Ä≤))2]| {z }
=var(x‚Ä≤)
+œÉ2,(7.26)
where g(x‚Ä≤)def=edtrain[g(dtrain)(x‚Ä≤)].
7.3.2 analysis of the bias
let us examine the bias and variance in more detail. to discuss bias we must first understand
the quantity
g(x‚Ä≤)def=edtrain[g(dtrain)(x‚Ä≤)], (7.27)
433chapter 7. regression
which is known as the average predictor . the average predictor, as the equation suggests, is
the expectation of the predictor g(dtrain). remember that g(dtrain)is a predictor constructed
from a specific training set dtrain. if tomorrow our training set dtraincontains other data
(that come from the same underlying distribution), g(dtrain)will be different. the average
predictor gis the average across these random fluctuations of the dataset dtrain. here is an
example:
suppose we use a linear model with the ordinary polynomials as the bases. the data
points are generated according to
yn=d‚àí1x
p=0Œ∏pxp
n
|{z}
def=f(xn)=Œ∏txn+en. (7.28)
if we use a particular training set dtrainand run the regression, we will be able to obtain
one of the regression lines, as shown in figure 7.16 . let us call this line g(1). we repeat the
experiment by drawing another dataset, and call it g(2). we continue and eventually we will
find a set of regression lines g(1), g(2), . . . , g(k), where kdenotes the number of training sets
you are using to generate all the gray curves. the average predictor gis defined as
g(x‚Ä≤) =edtrain[g(dtrain)]‚âà1
kkx
k=1g(k)(x‚Ä≤).
thus if we take the average of all these gray curves we will obtain the average predictor,
which is the red curve shown in figure 7.16 .
-1 -0.5 0 0.5 1-10123
figure 7.16: we run linear regression many times for different training datasets. each one consists of
different random realizations of noise. the gray curves are the regression lines returned by each of the
training datasets. we then take the average of these gray curves to obtain the red curve, which is the
average predictor.
if you are curious about how this plot was generated, the matlab and python codes
are given below.
% matlab code to visualize the average predictor
n = 20;
4347.3. bias and variance trade-off
a = [5.7, 3.7, -3.6, -2.3, 0.05];
x = linspace(-1,1,n);
yhat = zeros(100,50);
for i=1:100
x = [x(:).^0, x(:).^1, x(:).^2, x(:).^3, x(:).^4];
y = x*a(:) + 0.5*randn(n,1);
theta = x\y(:);
t = linspace(-1, 1, 50);
yhat(i,:) = theta(1) + theta(2)*t(:) + theta(3)*t(:).^2 ...
+ theta(4)*t(:)^3 + theta(5)*t(:).^4;
end
figure;
plot(t, yhat, ‚Äôcolor‚Äô, [0.6 0.6 0.6]); hold on;
plot(t, mean(yhat), ‚Äôlinewidth‚Äô, 4, ‚Äôcolor‚Äô, [0.8 0 0]);
axis([-1 1 -2 2]);
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import eval_legendre
np.set_printoptions(precision=2, suppress=true)
n = 20
x = np.linspace(-1,1,n)
a = np.array([0.5, -2, -3, 4, 6])
yhat = np.zeros((50,100))
for i in range(100):
y = a[0] + a[1]*x + a[2]*x**2 + \
a[3]*x**3 + a[4]*x**4 + 0.5*np.random.randn(n)
x = np.column_stack((np.ones(n), x, x**2, x**3, x**4))
theta = np.linalg.lstsq(x, y, rcond=none)[0]
t = np.linspace(-1,1,50)
xhat = np.column_stack((np.ones(50), t, t**2, t**3, t**4))
yhat[:,i] = np.dot(xhat, theta)
plt.plot(t, yhat[:,i], c=‚Äôgray‚Äô)
plt.plot(t, np.mean(yhat, axis=1), c=‚Äôr‚Äô, linewidth=4)
we now show an analytic calculation to verify figure 7.16 .
example 7.4 . consider a linear model such that
y=xtŒ∏+e. (7.29)
what is the predictor g(dtrain)(x‚Ä≤)? what is the average predictor g(x‚Ä≤)?
solution . first, consider a training dataset dtrain ={(x1, y1), . . . , (xn, yn)}. we
assume that the xn‚Äôs are deterministic and fixed. therefore, the source of randomness
in the training set is caused by the noise e‚àºgaussian(0 , œÉ2) and hence by the noisy
435chapter 7. regression
observation y.
the training set gives us the equation y=xŒ∏+e, where xis the matrix
constructed from xn‚Äôs. the regression solution to this dataset is
bŒ∏= (xtx)‚àí1xty,
which should actually be bŒ∏(dtrain)because yis a dataset-dependent vector.
consequently,
g(dtrain)(x‚Ä≤) =bŒ∏tx‚Ä≤= (x‚Ä≤)t(xtx)‚àí1xty
= (x‚Ä≤)t(xtx)‚àí1xt(xŒ∏+e)
= (x‚Ä≤)tŒ∏+ (x‚Ä≤)t(xtx)‚àí1xte.
since the randomness of dtrainis caused by the noise, it follows that
g(x‚Ä≤) =edtrain[g(dtrain)(x‚Ä≤)] =ee[(x‚Ä≤)tŒ∏+ (x‚Ä≤)t(xtx)‚àí1xte]
= (x‚Ä≤)tŒ∏+ (x‚Ä≤)t(xtx)‚àí1xtee[e]
= (x‚Ä≤)tŒ∏+ 0 = f(x‚Ä≤).
so the average predictor will return the ground truth. however, note that not all
predictors will return the ground truth.
in the above example, we obtained an interesting result, namely that g(x‚Ä≤) =f(x‚Ä≤).
that is, the average predictor equals the true predictor. however, in general, g(x‚Ä≤) does
not necessarily equal f(x‚Ä≤). if this occurs, we have a deviation ( g(x‚Ä≤)‚àíf(x‚Ä≤))2>0. this
deviation is called the bias. bias is independent of the number of training samples because
we have taken the average of the predictors. therefore, bias is more of an intrinsic (or
systematic) error due to the choice of the model.
what is bias?
¬àbias is defined as bias = ex‚Ä≤[(g(x‚Ä≤)‚àíf(x‚Ä≤))2], where x‚Ä≤is a testing sample.
¬àit is the deviation from the average predictor to the true predictor.
¬àbias is not necessarily a bad thing. a good predictor can have some bias as long
as it helps to reduce the variance.
7.3.3 variance
the other quantity in the game is the variance . variance at a testing sample x‚Ä≤is defined
as
var(x‚Ä≤)def=edtrain[(g(dtrain)(x‚Ä≤)‚àíg(x‚Ä≤))2]. (7.30)
as the equation suggests, the variance measures the fluctuation between the predictor
g(dtrain)and the average predictor g.figure 7.17 illustrates the polynomial-fitting prob-
lem we discussed above. in this figure we consider two levels of variance by varying the
4367.3. bias and variance trade-off
noise strength of en. the figure shows that as the observation becomes noisier, the predictor
g(dtrain)will have a larger fluctuation for the average predictor.
-1 -0.5 0 0.5 1-2-1012
-1 -0.5 0 0.5 1-2-1012
(a) small variance (b) large variance
figure 7.17: variance measures the magnitude of fluctuation between the particular predictor g(dtrain)
and the average predictor g.
example 7.5 . continuing with example 7.4, we ask: what is the variance?
solution . we first determine the predictor and its average:
g(dtrain)= (xtx)‚àí1xty=Œ∏+ (xtx)‚àí1xte
g=e[g(dtrain)] =ee[Œ∏+ (xtx)‚àí1xte] =Œ∏,
so the prediction at a testing sample x‚Ä≤is
g(dtrain)(x‚Ä≤) = (x‚Ä≤)tŒ∏+ (x‚Ä≤)t(xtx)‚àí1xte
g(x‚Ä≤) = (x‚Ä≤)tŒ∏,
consequently, the variance is
edtrain
g(dtrain)(x‚Ä≤)‚àíg(x‚Ä≤)2
=ee
(x‚Ä≤)tŒ∏+ (x‚Ä≤)t(xtx)‚àí1xte‚àí(x‚Ä≤)tŒ∏2
=ee
(x‚Ä≤)t(xtx)‚àí1xte2
.
continuing the calculation,
edtrain
g(dtrain)(x‚Ä≤)‚àíg(x‚Ä≤)2
= (x‚Ä≤)t(xtx)‚àí1xtee[eet]x(xtx)‚àí1x‚Ä≤
= (x‚Ä≤)t(xtx)‚àí1xtœÉ2ix(xtx)‚àí1x‚Ä≤
=œÉ2(x‚Ä≤)t(xtx)‚àí1x‚Ä≤
=œÉ2trn
(xtx)‚àí1(x‚Ä≤)(x‚Ä≤)to
.
437chapter 7. regression
what will happen if we use more samples so that ngrows? as ngrows, the matrix xwill
have more rows. assuming that the magnitude of the entries remains unchanged, more rows
inxwill increase the magnitude of xtxbecause we are summing more terms. consider
a 2√ó2 ordinary polynomial system where
xtx=Ô£Æ
Ô£∞pn
n=1x2
npn
n=1xn
pn
n=1xn nÔ£π
Ô£ª.
asngrows, all the entries in the matrix grow. as a result, ( xtx)‚àí1will shrink in mag-
nitude and thus drive the variance œÉ2trn
(xtx)‚àí1(x‚Ä≤)(x‚Ä≤)to
to zero.
what is variance?
¬àvariance is the deviation between the predictor g(dtrain)and its average g.
¬àit can be reduced by using more training samples.
7.3.4 bias and variance on the learning curve
the decomposition of the testing error into bias and variance is portrayed visually by the
learning curve shown in figure 7.18 . this figure shows the testing error and the training
error as functions of the number of training samples. as nincreases, we observe that both
testing and training errors converge to the same value. at any fixed n, the testing error is
composed of bias and variance:
¬àthe bias is the distance from the ground to the steady-state level. this value is fixed
and is a constant w.r.t. n. in other words, regardless of how many training samples
you have, the bias is always there. it is the best outcome you can achieve.
¬àthe variance is the fluctuation from the steady-state level to the instantaneous state.
it drops as nincreases.
figure 7.18: the learning curve can be decomposed into the sum of the bias and the variance. the bias
is the testing error when n=‚àû. for finite n, the difference between the testing error and the bias is
the variance.
4387.3. bias and variance trade-off
figure 7.19 compares the learning curve of two models. the first case requires us to
fit the data using a simple model (marked in purple). the training error and the testing
error have small fluctuations around the steady-state because, for simple models, you need
only a small number of samples to make the model happy. the second case requires us to
fit the data using a complex model (marked in green). this set of curves has a much wider
fluctuation because it is harder to train and harder to generalize. however, when we have
enough training samples, the training error and the testing error will converge to a lower
steady-state value. therefore, you need to pay the price of using a complex model, but if
you do, you will enjoy a lower testing error.
101102103
number of training samples, n00.511.522.5errorsimple model - training error
simple model - testing error
complex model - training error
complex model - testing error
figure 7.19: the generalization capability of a model is summarized by the training and testing errors
of the model. if we use a simple model we will have an easier time with the training but the steady-state
testing error will be high. in contrast, if we use a complex model we need to have a sufficient number
of training samples to train the model well. however, when the complex model is well trained, the
steady-state error will be lower.
the implication of all this is that you should choose the model by considering the
number of data points. never buy an expensive toy when you do not have the money! if
you insist on using a complex model while you do not have enough training data, you will
suffer from a poor testing error even if you feel good about it.
closing remark . we close this section by revisiting the bias-variance trade-off:
etest=ex‚Ä≤
(g(x‚Ä≤)‚àíf(x‚Ä≤))2
| {z }
=bias( x‚Ä≤)
+ex‚Ä≤
edtrain[(g(dtrain)(x‚Ä≤)‚àíg(x‚Ä≤))2]| {z }
=var(x‚Ä≤)
+œÉ2. (7.31)
the relationship among the three terms is summarized below:
what is the trade-off offered by the bias-variance analysis?
¬àoverfitting improves if n‚Üë: variance drops as ngrows. bias is unchanged.
¬àoverfitting worsens if œÉ2‚Üë. if training noise grows, g(dtrain)will have more fluc-
tuations, so variance will grow. if testing noise grows, e2grows.
439chapter 7. regression
¬àoverfitting worsens if the target function fis too complicated to be approximated
byg.
end of the section. please join us again.
7.4 regularization
having discussed the source of the overfitting problem, we now discuss methods to allevi-
ate overfitting. the method we focus on here is regularization . regularization means that
instead of seeking the model parameters by minimizing the training loss alone, we add a
penalty term to force the parameters to‚Äúbehave better‚Äù. as a preview of the technique, we
change the original training loss
etrain(Œ∏) =nx
n=1
yn‚àíd‚àí1x
p=0Œ∏pœïp(xn)2
| {z }
data fidelity, (7.32)
which consists of only the data fidelity term, to a modified training loss
etrain(Œ∏) =nx
n=1
yn‚àíd‚àí1x
p=0Œ∏pœïp(xn)2
| {z }
f(Œ∏),data fidelity+ Œª¬∑d‚àí1x
p=0Œ∏2
p
|{z}
Œª¬∑r(Œ∏),regularization. (7.33)
putting this into the matrix form, we define the data fidelity term as
f(Œ∏) =‚à•xŒ∏‚àíy‚à•2. (7.34)
the newly added term r(Œ∏) is called the regularization function or the penalty function .
it can take a variety of forms, e.g.,
¬àridge regression: r(Œ∏) =pd‚àí1
p=0Œ∏2
p=‚à•Œ∏‚à•2.
¬àlasso regression: r(Œ∏) =pd‚àí1
p=0|Œ∏p|=‚à•Œ∏‚à•1.
in this section we aim to understand the role of the regularization functions by studying
these two examples of r(Œ∏).
7.4.1 ridge regularization
to explain the meaning of equation (7.33) we write it in terms of matrices and vectors:
minimize
Œ∏‚ààrd‚à•xŒ∏‚àíy‚à•2+Œª‚à•Œ∏‚à•2, (7.35)
where Œªis called the regularization parameter .it needs to be tuned by the user. we refer
to equation (7.35) as the ridge regression .5
5in signal processing and optimization, equation (7.35) is called the tikhonov regularization. we follow
the statistics community in calling it the ridge regression.
4407.4. regularization
how can the regularization function help to mitigate the overfitting problem? first
let‚Äôs find the solution to this problem.
practice exercise 1 . prove that the solution to equation (7.35) is
bŒ∏= (xtx+Œªi)‚àí1xty. (7.36)
solution . take the derivative with respect to Œ∏.athis yields
‚àáŒ∏
‚à•xŒ∏‚àíy‚à•2+Œª‚à•Œ∏‚à•2
= 2xt(xŒ∏‚àíy) + 2ŒªŒ∏= 0.
rearranging the terms gives
(xtx+Œªi)Œ∏=xty.
taking the inverse of the matrix on both sides yields the solution.
athe solution here requires some basic matrix calculus. you may refer to the university of water-
loo‚Äôs matrix cookbook https://www.math.uwaterloo.ca/ ~hwolkowi/matrixcookbook.pdf .
let us compare the ridge regression solution with the vanilla regression solutions:
bŒ∏vanilla = (xtx)‚àí1xty,
bŒ∏ridge(Œª) = (xtx+Œªi)‚àí1xty.
clearly, the only difference is the presence of the parameter Œª:
¬àifŒª‚Üí0, then bŒ∏ridge(0) =bŒ∏vanilla . this is because
etrain(Œ∏) =‚à•xŒ∏‚àíy‚à•2+Œª‚à•Œ∏‚à•2
|{z}
=0.
hence, when Œª‚Üí0, the regression problem goes back to the vanilla version, and so
does the solution.
¬àŒª‚Üí ‚àû , then bŒ∏ridge(‚àû) = 0. this happens because
etrain(Œ∏) =1
Œª‚à•xŒ∏‚àíy‚à•2
|{z}
=0+‚à•Œ∏‚à•2.
since we are now minimizing ‚à•Œ∏‚à•2, the solution will be Œ∏= 0 because zero is the
smallest value a squared function can achieve.
for any 0 < Œª < ‚àû, the net effect of ( xtx+Œªi) is the constant Œªadded to all the
eigenvalues of xtx. by taking the eigendecomposition of xtx,
[u,s] =eig(xtx),
we have that
xtx+Œªi=usut+Œªi
=usut+Œªuut=u(s+Œªi)ut.
441chapter 7. regression
therefore, if the eigenvalue matrix shas a zero eigenvalue it will be offset by Œª:
s=Ô£Æ
Ô£ØÔ£ØÔ£∞‚ô£
‚ô°
‚ô†
0Ô£π
Ô£∫Ô£∫Ô£ª‚àí‚Üí s+Œªi=Ô£Æ
Ô£ØÔ£ØÔ£∞‚ô£+Œª
‚ô°+Œª
‚ô†+Œª
ŒªÔ£π
Ô£∫Ô£∫Ô£ª
as a result, even if xtxis not invertible (or close to not invertible), the new matrix
xtx+Œªiis guaranteed to be invertible.
practice exercise 2. you may be wondering what happens if xtxhas a negative
eigenvalue so that when we add a positive Œª, the resulting matrix may have a zero
eigenvalue. prove that xtxwill never have a negative eigenvalue, and xtx+Œªi
always has positive eigenvalues.
solution . eigenvalues of a matrix aare nonnegative if and only if vtav‚â•0 for
anyv. thus we need to check whether vtxtxv‚â•0 for all v. however, this is easy:
vtxtxv=‚à•xv‚à•2,
which must be nonnegative for any v. matrices satisfying this property are called
positive semidefinite . therefore, xtxis positive semidefinite.
implementation
solving the ridge regression is easy. first, we observe that the regularization function r(Œ∏) =
‚à•Œ∏‚à•2is a quadratic function. therefore, it can be combined with the data fidelity term as
bŒ∏= argmin
Œ∏‚ààrd‚à•xŒ∏‚àíy‚à•2+Œª‚à•Œ∏‚à•2
= argmin
Œ∏‚ààrd‚à•xŒ∏‚àíy‚à•2+‚à•‚àö
ŒªiŒ∏‚àí0‚à•2
= argmin
Œ∏‚ààrdx‚àö
Œªi
Œ∏‚àíy
02
.
therefore, all we need to do is to concatenate the matrix xwith a d√ódidentity operator‚àö
Œªi, and concatenate ywith a d√ó1 all-zero vector.
in matlab and python, the implementation of the ridge regression is done by defining
a new matrix aand a new vector b, as shown below:
% matlab command for ridge regression
a = [x; sqrt(lambda)*eye(d)];
b = [y(:); zeros(d,1)];
theta = a\b;
% matlab command for ridge regression
a = np.vstack((x, np.sqrt(lambd)*np.eye(d)))
b = np.hstack((y, np.zeros(d)))
theta = np.linalg.lstsq(a, b, rcond=none)[0]
4427.4. regularization
example 7.6 . consider a dataset of n= 20 data points. these data points are
constructed from the model
yn= 0.5‚àí2xn‚àí3x2
n+ 4x3
n+ 6x4
n+en, n = 1, . . . , n,
where en‚àºgaussian(0 ,0.252) is the noise. fit the data using
(a) vanilla linear regression with a 4th-order polynomial.
(b) vanilla linear regression with a 20th-order polynomial.
(c) ridge regression with a 20th-order polynomial, by considering three choices of Œª:
Œª= 10‚àí6,Œª= 10‚àí3, and Œª= 10.
solution .
(a) we first fit the data using a 4th-order polynomial. this fitting is relatively
straightforward. in the matlab / python programs below, set d= 4 and
Œª= 0. the result is shown in figure 7.20 (a).
-1 -0.5 0 0.5 1-101234
data
fitted curve
-1 -0.5 0 0.5 1-101234
data
fitted curve
(a) vanilla, 4th-order polynomial (b) vanilla, 20th-order polynomial
figure 7.20: overfitting occurs when the model is too complex for the number of training samples.
when using a vanilla regression with a 20th-order polynomial, the curve overfits the data and
causes a catastrophic fitting error.
(b) suppose we use a 20th-order polynomial g(x) =p20
p=0Œ∏pxpto fit the data. we
plot the result in figure 7.20 (b). since the order of the polynomial is very high
relative to the number of training samples, it comes as no surprise that the fitting
is poor. this is overfitting, and we know the reason.
(c) next, we consider a ridge regression using three choices of Œª. the result is shown
infigure 7.21 . ifŒªis too small, we observe that some overfitting still occurs. if
Œªis too large, then the curve underfits the data. for an appropriately chosen Œª,
it can be seen that the fitting is reasonably good.
443chapter 7. regression
-1 -0.5 0 0.5 1-101234
data
fitted curve
-1 -0.5 0 0.5 1-101234
data
fitted curve
-1 -0.5 0 0.5 1-101234
data
fitted curve
(a) ridge, Œª= 10‚àí6(b) ridge, Œª= 10‚àí3(c) ridge, Œª= 10
figure 7.21: ridge regression addresses the overfitting problem by adding a regularization term
to the training loss. depending on the strength of the parameter Œª, the fitted curve can vary from
overfitting to underfitting.
the matlab and python codes used to generate the above plots are shown below.
% matlab code to demonstrate a ridge regression example
% generate data
n = 20;
x = linspace(-1,1,n);
a = [0.5, -2, -3, 4, 6];
y = a(1)+a(2)*x(:)+a(3)*x(:).^2+a(4)*x(:).^3+a(5)*x(:).^4+0.25*randn(n,1);
% ridge regression
lambda = 0.1;
d = 20;
x = zeros(n, d);
for p=0:d-1
x(:,p+1) = x(:).^p;
end
a = [x; sqrt(lambda)*eye(d)];
b = [y(:); zeros(d,1)];
theta = a\b;
% interpolate and display results
t = linspace(-1, 1, 500);
xhat = zeros(length(t), d);
for p=0:d-1
xhat(:,p+1) = t(:).^p;
end
yhat = xhat*theta;
plot(x,y, ‚Äôko‚Äô,‚Äôlinewidth‚Äô,2, ‚Äômarkersize‚Äô, 10); hold on;
plot(t,yhat,‚Äôlinewidth‚Äô,4,‚Äôcolor‚Äô,[0.2 0.2 0.9]);
# python code to demonstrate a ridge regression example
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import eval_legendre
np.set_printoptions(precision=2, suppress=true)
4447.4. regularization
n = 20
x = np.linspace(-1,1,n)
a = np.array([0.5, -2, -3, 4, 6])
y = a[0] + a[1]*x + a[2]*x**2 + \
a[3]*x**3 + a[4]*x**4 + 0.25*np.random.randn(n)
d = 20
x = np.zeros((n, d))
for p in range(d):
x[:,p] = x**p
lambd = 0.1
a = np.vstack((x, np.sqrt(lambd)*np.eye(d)))
b = np.hstack((y, np.zeros(d)))
theta = np.linalg.lstsq(a, b, rcond=none)[0]
t = np.linspace(-1, 1, 500)
xhat = np.zeros((500,d))
for p in range(d):
xhat[:,p] = t**p
yhat = np.dot(xhat, theta)
plt.plot(x,y,‚Äôo‚Äô,markersize=12)
plt.plot(t,yhat, linewidth=4)
plt.show()
why does ridge regression work?
¬àthe penalty term ‚à•Œ∏‚à•2in
bŒ∏ridge= argmin
Œ∏‚ààrd‚à•xŒ∏‚àíy‚à•2+Œª‚à•Œ∏‚à•2
does not allow solutions with very ‚à•Œ∏‚à•2.
¬àthe penalty term adds a positive offset to the eigenvalues of xtx.
¬àsince the denominator in ( xtx+Œªi)‚àí1xtybecomes larger than that of
(xtx)‚àí1xty, noise in yis less amplified.
choosing the parameter
how should we choose the parameter Œª? the honest answer is that there is no answer
because the optimal Œªcan only be found if we have access to the testing samples. if we do,
we can plot the mse (the testing error) with respect to Œª, as shown in figure 7.22 (a).
of course in reality we do not have access to the testing data. however, we can reserve
a small portion of the training samples and treat them as validation samples . then we
run the ridge regression for different choices of Œª. the Œªthat minimizes the error on these
validation samples is the one that you should deploy. if the training set is small, we can
445chapter 7. regression
10-1010-510010-310-210-1
10010500.050.10.150.20.250.3
(a) testing error vs Œª (b)f(bŒ∏Œª) vsr(bŒ∏Œª)
figure 7.22: (a) determining the optimal Œªrequires knowledge of the testing samples. in practice, we
can replace the testing samples with the validation samples, which are subsets of the training data. then
by plotting the validation error as a function of Œªwe can determine the optimal Œª. (b) the alternative
is to plot f(bŒ∏Œª)versus r(bŒ∏Œª). the optimal Œªcan be found by locating the elbow point.
shuffle the validation samples randomly and compute the average. this scheme is known as
cross-validation .
for some problems, there are ‚Äútactics‚Äù you may be able to employ for determining
the optimal Œª. the first approach is to ask yourself what would be the reasonable range
of‚à•Œ∏‚à•2or‚à•xŒ∏‚àíy‚à•2? are you expecting them to be large or small? approximately in
what order of magnitude? if you have some clues about this, then you can plot the function
f(bŒ∏Œª) =‚à•xbŒ∏Œª‚àíy‚à•2as a function of r(bŒ∏Œª) =‚à•bŒ∏Œª‚à•2, where bŒ∏Œªis a shorthand notation
forbŒ∏ridge(Œª), which is the estimated parameter using a specific value of Œª.figure 7.22 (b)
shows an example of such a plot. as you can see, by varying Œªwe have different values of
f(bŒ∏Œª) and r(bŒ∏Œª).
if you have some ideas about what ‚à•Œ∏‚à•2should be, say you want ‚à•Œ∏‚à•2‚â§œÑ, you can go
to the f(bŒ∏Œª) versus r(bŒ∏Œª) curve and find a point such that r(bŒ∏Œª)‚â§œÑ. on the other hand,
if you want ‚à•xŒ∏‚àíy‚à•2‚â§œµ, you can also go to the f(bŒ∏Œª) versus r(bŒ∏Œª) curve and find a
point such that ‚à•xŒ∏‚àíy‚à•2‚â§œµ. in either case, you have the freedom to shift the difficulty
of finding Œªto that of finding œÑorœµ. note that œÑandœµhave better physical interpretations.
the quantity œµtells us the upper bound of the prediction error, and œÑtells us the upper
bound of the parameter magnitude. if you have been working on your dataset long enough,
the historical data (and your experience) will help you determine these values.
another feasible option suggested in the literature is finding the anchor point of the
f(bŒ∏Œª) and r(bŒ∏Œª). the idea is that if the curve has a sharp elbow, the turning point would
indicate a rapid increase/decrease in f(bŒ∏Œª) (or r(bŒ∏Œª)).
how to determine Œª
¬àcross-validation: reserve a few training samples as validation samples. check
the prediction error w.r.t. these validation samples. the Œªthat minimizes the
validation error is the one you deploy.
¬à‚à•Œ∏‚à•2‚â§œÑ: plot the f(bŒ∏Œª) and r(bŒ∏Œª). then go along the r-axis to find the
4467.4. regularization
position where r(bŒ∏Œª)‚â§œÑ.
¬à‚à•xŒ∏‚àíy‚à•2‚â§œÑ: plot the f(bŒ∏Œª) and r(bŒ∏Œª). then go along the f-axis to find
the position where f(bŒ∏Œª)‚â§œµ.
¬àfind the elbow point of f(bŒ∏Œª) and r(bŒ∏Œª).
bias and variance trade-off for ridge regression
we now discuss the bias and variance trade-off of the ridge regression.
theorem 7.7. lety=xŒ∏+ebe the training data, where eis zero-mean and has a
covariance œÉ2i. consider the ridge regression
bŒ∏Œª=argmin
Œ∏‚ààrd‚à•xŒ∏‚àíy‚à•2+Œª‚à•Œ∏‚à•2. (7.37)
then the estimate has the properties that
bŒ∏Œª= (xtx+Œªi)‚àí1xtxŒ∏+ (xtx+Œªi)‚àí1xte,
e[bŒ∏Œª] = (xtx+Œªi)‚àí1xtxŒ∏=wŒªŒ∏,
cov[bŒ∏Œª] =œÉ2(xtx+Œªi)‚àí1xtx(xtx+Œªi)‚àí1,
mse(bŒ∏Œª,Œ∏) =œÉ2tr
wŒª(xtx)‚àí1wt
Œª
+Œ∏t(wŒª‚àíi)t(wŒª‚àíi)Œ∏,
where wŒª= (xtx+Œªi)‚àí1xtx.
proof . the proof of this theorem involves some tedious matrix operations that will be
omitted here. if you are interested in the proof you can consult van wieringen‚Äôs ‚Äúlecture
notes on ridge regression‚Äù, https://arxiv.org/pdf/1509.09169.pdf .
‚ñ°
the results of this theorem provide a way to assess the bias and variance. specifically,
from the mse we know that
mse(bŒ∏Œª,Œ∏) =eeh
‚à•bŒ∏Œª‚àíŒ∏‚à•2i
=‚à•ee[bŒ∏Œª]‚àíŒ∏‚à•2+ trn
cov[bŒ∏Œª]o
=Œ∏t(wŒª‚àíi)t(wŒª‚àíi)Œ∏| {z }
bias+œÉ2tr
wŒª(xtx)‚àí1wt
Œª
| {z }
variance.
the bias and variance are defined respectively as
bias(bŒ∏Œª,Œ∏) =Œ∏t(wŒª‚àíi)t(wŒª‚àíi)Œ∏,
var(bŒ∏Œª,Œ∏) =œÉ2tr
wŒª(xtx)‚àí1wt
Œª
.
we can then plot the bias and variance as a function of Œª. an example is shown in fig-
ure 7.23 .
447chapter 7. regression
10-410-310-210-110-1100101102103
mse
bias
variance
figure 7.23: the bias and variance of the ridge regression behave in opposite ways as Œªincreases. the
mse is the sum of bias and variance.
the result in figure 7.23 can be summarized in three points:
¬àbias‚ÜëasŒª‚Üë. this is because a large Œªpushes the solution towards Œ∏= 0. therefore,
the bias with respect to the ground truth Œ∏will increase.
¬àvariance ‚ÜìasŒª‚Üë. since variance is caused by noise, increasing Œªforces the solution
Œ∏to be small. hence, it becomes less sensitive to noise.
¬àmse reaches a minimum point somewhere in the middle. the mse is the sum of bias
and variance. therefore, it drops to the minimum and then rises again as Œªincreases.
with appropriate choice of Œª, we can show that the ridge regression can have a
lower mean squared error than the vanilla regression. the following result is due to c. m.
theobald:6
theorem 7.8. forŒª <2œÉ2‚à•Œ∏‚à•‚àí2,
mse
bŒ∏ridge(Œª),Œ∏
<mse
bŒ∏vanilla ,Œ∏
. (7.38)
this theorem says that as long as Œªis small enough, the ridge regression will have a lower
mse than the vanilla regression. thus ridge regression is almost always helpful. of course,
the optimal Œªis not provided by the theorem, which only tells us where to search for a
good Œª.
why does ridge regression reduce the testing error?
¬àthe regularization reduces the variance (see figure 7.23 when Œª >0)
¬àit pays the price of increasing the bias.
6theobald, c. m. (1974). generalizations of mean square error applied to ridge regression. journal of
the royal statistical society . series b (methodological), 36(1), 103-106.
4487.4. regularization
¬àusually, the drop in variance outweighs the increase in bias. so the overall mse
drops.
¬àbias is not always a bad thing.
7.4.2 lasso regularization
the ridge regression we discussed in the previous subsection is just one of the many possible
ways of doing regularization. one alternative is to replace ‚à•Œ∏‚à•2by‚à•Œ∏‚à•1, where
‚à•Œ∏‚à•1=d‚àí1x
p=0|Œ∏p|. (7.39)
this change from the sum-squares to sum-absolute-values has been main driving force in
data science, machine learning, and signal processing for at least the past two decades. the
optimization associated with ‚à•Œ∏‚à•1is
minimize
Œ∏‚ààrd‚à•xŒ∏‚àíy‚à•2+Œª‚à•Œ∏‚à•1, (7.40)
or
etrain(Œ∏) =nx
n=1
yn‚àíd‚àí1x
p=0Œ∏pœïp(xn)2
| {z }
f(Œ∏),data fidelity+ Œª¬∑d‚àí1x
p=0|Œ∏p|
|{z}
Œª¬∑r(Œ∏),regularization. (7.41)
seeking a sparse solution
to understand the choice of ‚à• ¬∑ ‚à• 1, we need to introduce the concept of sparsity .
definition 7.1. a vector Œ∏is called sparse if it has only a few non-zero elements.
as illustrated in figure 7.24 , a sparse Œ∏ensures that only a very few columns of the data
matrix xare active. this is an attractive property because, in some of the regression
problems, it is indeed possible to have just a few dominant factors. the lasso regression
says that if our problem possesses this sparse solution, then the ‚à• ¬∑ ‚à• 1can help us find the
sparse solution.
figure 7.24: a vector Œ∏is sparse if it only contains a few non-zero elements. if Œ∏is sparse, then the
observation yis determined by a few active components.
449chapter 7. regression
how can ‚à•Œ∏‚à•1promote sparsity? if we consider the sets
œâ1={Œ∏| ‚à•Œ∏‚à•1‚â§œÑ}={(Œ∏1, Œ∏2)| |Œ∏1|+|Œ∏2| ‚â§œÑ},
œâ2={Œ∏| ‚à•Œ∏‚à•2‚â§œÑ}={(Œ∏1, Œ∏2)|Œ∏2
1+Œ∏2
2‚â§œÑ},
we note that œâ 1has a diamond shape whereas œâ 2has a circular shape. since the data
fidelity term ‚à•xŒ∏‚àíy‚à•2is an ellipsoid, seeking the optimal value in the presence of the
regularization term can be viewed as moving the ellipsoid until it touches the set defined
by the regularization. as illustrated in figure 7.25 , since {Œ∏| ‚à•Œ∏‚à•2‚â§œÑ}is a circle, the
solution will be somewhere in the middle. on the other hand, since {Œ∏| ‚à•Œ∏‚à•1‚â§œÑ}is a
diamond, the solution will be one of the vertices. the difference between ‚Äúsomewhere in
the middle‚Äù and ‚Äúa vertex‚Äù is that the vertex is a sparse solution, since by the definition of
a vertex one coordinate must be zero and the other coordinate must be non-zero. we can
easily extrapolate this idea to the higher-dimensional spaces. in this case, we will see that
the solution for the ‚à• ¬∑ ‚à• 1problem has only a few non-zero entries.
figure 7.25: a vector Œ∏is sparse if it contains only a few non-zero elements. if Œ∏is sparse, then the
observation yis determined by a few active components.
the optimization formulated in equation (7.41) is known as the least absolute shrink-
age and selection operator (lasso). lasso problems are difficult, but over the past two
decades we have increased our understanding of the problem. the most significant break-
through is that we now have algorithms to solve the lasso problem efficiently. this is
important because, unlike the ridge regression, where we have a (very simple) closed-form
solution, the lasso problem can only be solved using iterative algorithms.
what is so special about lasso?
¬àlasso regularization promotes a sparse solution.
¬àif the underlying model has a sparse solution, e.g., you choose a 50th-order
polynomial, but the underlying model is a third-order polynomial, then there
should only be three non-zero regression coefficients in your 50th-order polyno-
mial. lasso will help in this case.
4507.4. regularization
¬àif the underlying model has a dense solution, then lasso is of limited value. a
ridge regression could be better.
¬àwhile ‚à•Œ∏‚à•1is not differentiable (at 0), there exist polynomial-time convex algo-
rithms to solve the problem, e.g., interior-point methods.
solving the lasso problem
today, there are many open-source packages to solve the lasso problem. they are mostly
developed in the convex optimization literature. one of the most user-friendly packages is
thecvxpackage developed by s. boyd and colleagues at stanford university.7once you
have downloaded and installed the package, solving the optimization can be done literally
by typing in the data fidelity term and the regularization term. an example is given below.
cvx_begin
variable theta(d)
minimize(sum_square(x*theta-y) + lambda*norm(theta,1))
cvx_end
as you can see, the program is extremely simple. you start by calling cvx_begin
and end it with cvx_end . inside the box we create a variable beta(d) , where ddenotes
the dimension of the vector theta . the main command is minimize . however, this line is
almost self-explanatory. as long as you follow the syntax given by the user guidelines, you
will be able to set it up properly.
in python, we can call the cvxpy library.
import cvxpy as cvx
theta = cvx.variable(d)
objective = cvx.minimize( cvx.sum_squares(x*theta-y) \
+ lambd*cvx.norm1(theta) )
prob = cvx.problem(objective)
prob.solve()
to see a concrete example, we use the crime rate data obtained from https://web.
stanford.edu/ ~hastie/statlearnsparsity/data.html . a snapshot of the data is shown
in the table below. in this dataset, the vector yis the crime rate, which is the last column
of the table. the feature/basis vectors are funding ,hs,not-hs ,college .
city crime rate funding hs no-hs college
1 478 40 74 11 31
2 494 32 72 11 43
3 643 57 71 18 16
4 341 31 71 11 25
..................
50 940 66 67 26 18
7the matlab version is here: http://cvxr.com/cvx/ . the python version is here: https://cvxopt.
org/. follow the instructions to install the package.
451chapter 7. regression
we consider two optimizations:
bŒ∏1(Œª) = argmin
Œ∏e1(Œ∏)def=‚à•xŒ∏‚àíy‚à•2+Œª‚à•Œ∏‚à•1,
bŒ∏2(Œª) = argmin
Œ∏e2(Œ∏)def=‚à•xŒ∏‚àíy‚à•2+Œª‚à•Œ∏‚à•2.
as we have discussed, the first optimization uses the ‚à•¬∑‚à•1regularized least squares, which is
the lasso problem. the second optimization is the standard ‚à•¬∑‚à•2regularized least squares.
since both solutions depend on the parameter Œª, we parameterize the solutions in terms of
Œª. note that the optimal ŒªforbŒ∏1is not necessarily the optimal ŒªforbŒ∏2.
one thing we would like to demonstrate in this example is visualizing the linear re-
gression coefficients bŒ∏1(Œª) andbŒ∏2(Œª) asŒªchanges. to solve the optimization, we use cvx
with the matlab and python implementation is shown below.
data = load(‚Äô./dataset/data_crime.txt‚Äô);
y = data(:,1); % the observed crime rate
x = data(:,3:end); % feature vectors
[n,d]= size(x);
lambdaset = logspace(-1,8,50);
theta_store = zeros(d,50);
for i=1:length(lambdaset)
lambda = lambdaset(i);
cvx_begin
variable theta(d)
minimize( sum_square(x*theta-y) + lambda*norm(theta,1) )
% minimize( sum_square(x*theta-y) + lambda*sum_square(theta) )
cvx_end
theta_store(:,i) = theta(:);
end
figure(1);
semilogx(lambdaset, theta_store, ‚Äôlinewidth‚Äô, 4);
legend(‚Äôfunding‚Äô,‚Äô% high‚Äô, ‚Äô% no high‚Äô, ‚Äô% college‚Äô, ...
‚Äô% graduate‚Äô, ‚Äôlocation‚Äô,‚Äônw‚Äô);
xlabel(‚Äôlambda‚Äô);
ylabel(‚Äôfeature attribute‚Äô);
import cvxpy as cvx
import numpy as np
import matplotlib.pyplot as plt
data = np.loadtxt("/content/data_crime.txt")
y = data[:,0]
x = data[:,2:7]
n,d = x.shape
lambd_set = np.logspace(-1,8,50)
4527.4. regularization
theta_store = np.zeros((d,50))
for i in range(50):
lambd = lambd_set[i]
theta = cvx.variable(d)
objective = cvx.minimize( cvx.sum_squares(x*theta-y) \
+ lambd*cvx.norm1(theta) )
# objective = cvx.minimize( cvx.sum_squares(x*theta-y) \
+ lambd*cvx.sum_squares(theta) )
prob = cvx.problem(objective)
prob.solve()
theta_store[:,i] = theta.value
for i in range(d):
plt.semilogx(lambd_set, theta_store[i,:])
10-2100102104106108
lambda-202468101214feature attributefunding
% high
% no high
% college
% graduate
10-2100102104106108
lambda-202468101214feature attributefunding
% high
% no high
% college
% graduate
(a) lasso (b) ridge
figure 7.26: ridge and lasso regression on the crime-rate dataset. (a) the lasso regression suggests
that there are only a few active components as we change Œª. (b) the ridge regression returns a set of
dense solutions for all choices of Œª.
figure 7.26 shows some interesting differences between the two regression models.
¬àtrajectory . for the ‚à• ¬∑ ‚à•2estimate bŒ∏2(Œª), the trajectory of the regression coefficients
is smooth. this is attributable to the fact that the training loss e2(Œ∏) is continuously
differentiable in Œ∏, and so the solution trajectory is smooth. by contrast, the ‚à• ¬∑ ‚à• 1
estimate bŒ∏1(Œª) has a more disruptive trajectory.
¬àactive members . for the lasso problem, bŒ∏1(Œª) switches the active member as Œª
changes. for example, the feature high-school is the first one being activated when
Œª‚Üì. this implies that if we limit ourselves to only onefeature, then high-school is
the feature we should select. the ridge regression does not have this feature-selection
property. how about when Œª= 106? in this case, the lasso has two active members:
funding and high-school . this suggests that if there are two contributing factors,
funding andhigh-school are the two. as Œª= 104, we see that in lasso, the green
curve goes to zero but then the red curve rises. this means a correlation between
453chapter 7. regression
high school andno high school , which should not be a surprise because they are
complementary to each other.
¬àmagnitude of solutions . the magnitude of the solutions does not necessarily convey
a clear conclusion because the feature vectors (e.g., high school ) and the observable
crime rate have different units.
¬àlimiting solutions . asŒª‚Üí0, both bŒ∏1(Œª) andbŒ∏2(Œª) reach the same solution, because
the training losses are identical when Œª= 0.
lasso for overfitting
does lasso help to mitigate the overfitting problem? not always, but it often does. in
figure 7.27 we consider fitting a dataset of n= 20 data points. the ground truth model
we use is
yn=l0(xn) + 0.5l1(xn) + 0.5l2(xn) + 1.5l3(xn) +l4(xn) +en,
where en‚àºgaussian(0 , œÉ2) for œÉ= 0.25. when fitting the data, we purposely choose a
20th-order legendre polynomial as the regression model. with only n= 20 data points, we
can be almost certain that there is overfitting.
the matlab and python codes for solving this lasso problem are shown below.
% matlab code to demonstrate overfitting and lasso
% generate data
n = 20;
x = linspace(-1,1,n)‚Äô;
a = [1, 0.5, 0.5, 1.5, 1];
y = a(1)*legendrep(0,x)+a(2)*legendrep(1,x)+a(3)*legendrep(2,x)+ ...
a(4)*legendrep(3,x)+a(5)*legendrep(4,x)+0.25*randn(n,1);
% solve lasso using cvx
d = 20;
x = zeros(n, d);
for p=0:d-1
x(:,p+1) = reshape(legendrep(p,x),n,1);
end
lambda = 2;
cvx_begin
variable theta(d)
minimize( sum_square( x*theta - y ) + lambda * norm(theta , 1) )
cvx_end
% plot results
t = linspace(-1, 1, 200);
xhat = zeros(length(t), d);
for p=0:d-1
xhat(:,p+1) = reshape(legendrep(p,t),200,1);
end
yhat = xhat*theta;
4547.4. regularization
plot(x,y, ‚Äôko‚Äô,‚Äôlinewidth‚Äô,2, ‚Äômarkersize‚Äô, 10); hold on;
plot(t,yhat,‚Äôlinewidth‚Äô,6,‚Äôcolor‚Äô,[0.2 0.5 0.2]);
# python code to demonstrate overfitting and lasso
import cvxpy as cvx
import numpy as np
import matplotlib.pyplot as plt
# setup the problem
n = 20
x = np.linspace(-1,1,n)
a = np.array([1, 0.5, 0.5, 1.5, 1])
y = a[0]*eval_legendre(0,x) + a[1]*eval_legendre(1,x) + \
a[2]*eval_legendre(2,x) + a[3]*eval_legendre(3,x) + \
a[4]*eval_legendre(4,x) + 0.25*np.random.randn(n)
# solve lasso using cvx
d = 20
lambd = 1
x = np.zeros((n, d))
for p in range(d):
x[:,p] = eval_legendre(p,x)
theta = cvx.variable(d)
objective = cvx.minimize( cvx.sum_squares(x*theta-y) \
+ lambd*cvx.norm1(theta) )
prob = cvx.problem(objective)
prob.solve()
thetahat = theta.value
# plot the curves
t = np.linspace(-1, 1, 500)
xhat = np.zeros((500,d))
for p in range(p):
xhat[:,p] = eval_legendre(p,t)
yhat = np.dot(xhat, thetahat)
plt.plot(x, y, ‚Äôo‚Äô)
plt.plot(t, yhat, linewidth=4)
let us compare the various regression results. figure 7.27 (b) shows the vanilla regres-
sion, which as you can see fits the n= 20 data points very well. however, no one would
believe that such a fitting curve can generalize to unseen data. figure 7.27 (c) shows the
ridge regression result. when performing the analysis, we sweep a range of Œªand pick the
value Œª= 0.5 so that the fitted curve is neither too ‚Äúwild‚Äù nor too ‚Äúflat‚Äù. we can see that
the fitting is improved. however, since the ridge regression only penalizes large-magnitude
coefficients, the fitting is still not ideal. figure 7.27 (d) shows the lasso regression result.
since the true model is a 4th-order polynomial and we use a 20th-order polynomial, the true
solution is sparse. therefore, lasso is helpful, and hence we can pick a sparse solution.
the significance of lasso is often not about the fitting of the data points but the
455chapter 7. regression
-1 -0.5 0 0.5 1-101234
data
fitted curve
-1 -0.5 0 0.5 1-101234
data
fitted curve
(a) ground truth model (b) vanilla regression
-1 -0.5 0 0.5 1-101234
data
fitted curve
-1 -0.5 0 0.5 1-101234
data
fitted curve
(c) ridge (d) lasso
figure 7.27: we fit a dataset of n= 20 data points. (a) the ground truth model that generates
the data. the model is a 4th-order ordinary polynomial. (b) vanilla regression result, without any
regularization. note that there is severe overfitting because the model complexity is too high. (c) ridge
regression result, by setting Œª= 0.5. (d) lasso regression result, by setting Œª= 2.
number of active coefficients. in figure 7.28 we show a comparison between the ground
truth coefficients, the vanilla regression coefficients, the ridge regression coefficients, and the
lasso regression coefficients. it is evident that the lasso solution contains a much smaller
number of non-zeros compared to the ridge regression. most of the high-order coefficients
are zero. by contrast, the vanilla regression coefficients are wild. the ridge regression is
better, but there are many non-zero high-order coefficients.
closing remark . in this section, we discussed two regularization techniques: ridge regres-
sion and lasso regression. both techniques are about adding a penalty term to the training
loss to constrain the regression coefficients. in the optimization literature, writings on ridge
and lasso regression are abundant, covering both algorithms and theoretical properties.
an example of a theoretical question addressed in the literature is: under what conditions
is lasso guaranteed to recover the correct support of the solution, i.e., locating the correct
positions of the non-zeros? problems like these are beyond the scope of this book.
4567.5. summary
-2-1012
5 10 15 20
-15-10-5051015
5 10 15 20
(a) ground truth model (b) vanilla regression
-2-1012
5 10 15 20
-2-1012
5 10 15 20
(c) ridge (d) lasso
figure 7.28: coefficients of the regression models. (a) the ground truth model, which is a 4th-order
polynomial. there are only 5 non-zero coefficients. (b) the vanilla regression coefficients. note that
the values are wild and large, although the curve fits the training data points very well. (c) the ridge
regression coefficients. while the overall magnitudes are significantly improved from the vanilla, some
high-order coefficients are still non-zero. (d) the lasso regression coefficients. there are very few
non-zeros, and the non-zeros match well with the ground truth.
7.5 summary
regression is one of the most widely used techniques in data science. the formulation of the
regression problem is as simple as setting up a system of linear equations:
minimize
Œ∏‚ààrd‚à•xŒ∏‚àíy‚à•2, (7.42)
which has a closed-form solution. the biggest problems in practice are outliers, lack of
training samples, and poor choice of the regression model.
¬àoutliers : we always recommend plotting the data whenever possible to check if there
are obvious outliers. there are also statistical tests in which you can evaluate the
validity of your samples. one simple way to debug outliers is to run the regression
457chapter 7. regression
and check the prediction error against each training sample. if you have an outlier,
and if your model is of reasonably low complexity, then a sample with an excessively
large prediction error is an outlier. for example, if most of the training samples are
within one standard deviation from your prediction but a few are substantially off,
you will know which ones are the outliers. robust linear regression is one technique for
countering outliers, but an experienced data scientist can often reject outliers before
running any regression algorithms. domain knowledge is of great value for this purpose.
¬àlack of training samples : as we have discussed in the overfitting section, it is ex-
tremely important to ensure that your model complexity is appropriate for the number
of training samples. if the training set is small, do not use a complex model. regu-
larization techniques are valuable tools to mitigate overfitting. however, choosing a
good regularization requires domain knowledge. for example, if you know that some
features are not important, you need to scale them properly so as not to over-influence
the regression solution.
¬àwrong model : we have mentioned several times that regression can always return you
a result because regression is an optimization problem. however, whether that result
is meaningful depends on how meaningful your regression problem is. for example, if
the noise is i.i.d. gaussian, a data fidelity term with ‚à• ¬∑ ‚à•2would be a good choice;
however, if the noise is i.i.d. poisson, ‚à• ¬∑ ‚à•2would become a very bad model. we need
a tighter connection with the statistics of the underlying data-generation model for
problems like these. this is the subject of our next chapter, on parameter estimation.
7.6 references
linear regression
treatment of standard linear regression is abundant. in the context of machine learning and
data science, the following references are useful.
7-1 gareth james, daniela witten, trevor hastie, and robert tibshirani, an introduction
to statistical learning with applications in r , springer 2013, chapter 3.
7-2 stephen boyd and lieven vandenberghe, convex optimization , cambridge university
press, 2004. chapter 6.
7-3 trevor hastie, robert tibshirani, and jerome friedman, the elements of statistical
learning , springer, 2001. chapter 3.
7-4 christopher bishop, pattern recognition and machine learning , springer 2006. chap-
ter 3.1.
7-5 yaser abu-mostafa, malik magdon-ismail and hsuan-tien lin, learning from data ,
aml book, 2012. chapter 3.2
overfitting and bias/variance
the theory of overfitting and the trade-off between bias and variance can be found in multiple
references. the following are basic treatments of the subject.
4587.7. problems
7-6 yaser abu-mostafa, malik magdon-ismail and hsuan-tien lin, learning from data ,
aml book, 2012. chapter 4.
7-7 christopher bishop, pattern recognition and machine learning , springer 2006. chap-
ter 3.2.
ridge and lasso regression
ridge and lasso regression are important tools in statistical learning today. the following
two textbooks cover some of the perspectives of the statistical community and the signal
processing community.
7-8 trevor hastie, robert tibshirani, and martin wainwright, statistical learning with
sparsity: the lasso and generalizations , crc press, 2015.
7-9 michael elad, sparse and redundant representations , springer, 2010. chapters 1
and 3.
7.7 problems
exercise 1.
(a) construct a dataset with n= 20 samples, following the model
yn=d‚àí1x
p=0Œ∏plp(xn) +en, (7.43)
where Œ∏0= 1, Œ∏1= 0.5,Œ∏2= 0.5,Œ∏3= 1.5,Œ∏4= 1, for ‚àí1< x < 1. here, lp(x) is the
legendre polynomial of the pth order. the n= 20 samples are random uniformly sam-
pled from the interval [ ‚àí1,1]. the noise samples enare i.i.d. gaussian with variance
œÉ2= 0.252. plot the dataset using the matlab or python command scatter .
(b) run the regression using the same model where d= 5, without any regularization.
plot the predicted curve and overlay with the training samples.
(c) repeat (b) by running the regression with d= 20. explain your observations.
(d) increase the number of training samples nton= 50, n= 500, and n= 5000, and
repeat (c). explain your observations.
(e) construct a testing dataset with m= 1000 testing samples. for each of the regression
models trained in (b)-(d), compute the testing error.
exercise 2.
consider a data generation model
xn=n‚àí1x
k=0cke‚àíj2œÄkn
n, n= 0, . . . , n ‚àí1.
459chapter 7. regression
(a) write the above equation in matrix-vector form
x=wc.
what are the vectors candx, and what is the matrix w?
(b) show that wis orthogonal, i.e.,, whw=i, where whis the conjugate transpose
ofw.
(c) using (b), derive the least squares regression solution.
exercise 3.
consider a simplified lasso regression problem:
bŒ∏= argmin
Œ∏‚ààrd‚à•y‚àíŒ∏‚à•2+Œª‚à•Œ∏‚à•1. (7.44)
show that the solution is given by
bŒ∏= sign( y)¬∑max (|y| ‚àíŒª,0), (7.45)
where ¬∑is the elementwise multiplication.
exercise 4.
a one-dimensional signal is corrupted by blur and noise:
yn=l‚àí1x
‚Ñì=0h‚Ñìxn‚àí‚Ñì+en.
(a) formulate the least squares regression problem in matrix-vector form y=hx+e.
findx,yandh.
(b) consider a regularization function
r(x) =nx
n=2(xn‚àíxn‚àí1)2.
show that this regularization is equivalent to r(x) =‚à•dx‚à•2for some d. find d.
(c) using the regularization in (b), derive the regularized least squares regression result:
minimize
x‚à•y‚àíhx‚à•2+Œª‚à•dx‚à•2.
exercise 5.
letœÉ(¬∑) be the sigmoid function
œÉ(a) =1
1 +ea.
we want to use œÉ(a) as a basis function.
4607.7. problems
(a) show that the tanh function and the sigmoid function are related by
tanh( a) = 2 œÉ(2a)‚àí1.
(b) show that a linear combination of sigmoid functions
yn=Œ∏0+d‚àí1x
p=1Œ∏pœÉxn‚àí¬µj
s
is equivalent to a linear combination of tanh functions
yn=Œ±0+d‚àí1x
p=1Œ±ptanhxn‚àí¬µj
2s
.
(c) find the relationship between Œ∏pandŒ±p.
exercise 6. (nhanes part 1)(data download)
the national health and nutrition examination survey (nhanes) is a program to assess
the health and nutritional status of adults and children in the united states8. the complete
survey result contains over 4,000 samples of health-related data of individuals who partici-
pated in the survey between 2011 and 2014. in the following exercises, we will focus on two
categories of the data for each individual: height (in mm) and body mass index (bmi). the
data is divided into two classes based on gender. table 1 contains snippets of the data.
index female bmi female stature mm
0 28.2 1563
1 22.2 1716
2 27.1 1484
3 28.1 1651index male bmi male stature mm
0 30 1679
1 25.6 1586
2 24.2 1773
3 27.4 1816
table 7.2: male and female data snippets
usecsv.reader to read the training data files for the two data classes.
important! before proceeding to the problems,
¬ànormalize the number in male_stature_mm andfemale_stature_mm by dividing them
by 1000, and
¬ànormalize that of male_bmi andfemale_bmi by dividing them by 10.
this will significantly reduce the numerical error.
consider a linear model:
gŒ∏=Œ∏tx, (7.46)
8https://www.cdc.gov/nchs/nhanes/index.htm
461chapter 7. regression
the regression problem we want to solve is
bŒ∏= argmin
Œ∏‚ààrdnx
n=1(yn‚àígŒ∏(xn))2,
where d={(xn, yn)}n
n=1is the training dataset. putting the equation into the matrix form,
we know that the optimization is equivalent to
bŒ∏= argmin
Œ∏‚ààrd‚à•y‚àíxŒ∏‚à•2
|{z}
etrain(Œ∏).
(a) derive the solution bŒ∏. state the conditions under which the solution is the unique
global minimum in terms of the rank of x. suggest two techniques that can be used
when xtxis not invertible.
(b) for the nhanes dataset, assign yn= +1 if the nth sample is a male and yn=‚àí1
if the nth sample is a female. implement your answer in (a) with python to solve the
problem. report your answer.
(c) repeat (b), but this time use cvxpy. report your answer, and compare with (b).
exercise 7. (nhanes part 2)(data download)
we want to do a classification based on the linear model we found in the previous exercise.
the classifier we will use is
predicted label = sign( gŒ∏(x)), (7.47)
where x‚ààrdis the a test sample. here, we label +1 for male and ‚àí1 for female. because
the dataset we consider in this exercise has only two columns, the linear model is
gŒ∏(x) =Œ∏0+Œ∏1x1+Œ∏2x2,
where x= [1, x1, x2]tis the input data and Œ∏= [Œ∏0, Œ∏1, Œ∏2]tis the parameter vector.
(a) first, we want to visualize the classifier.
(i) plot the training data points of the male and female classes. mark the male class
with blue circles and the female class with red dots.
(ii) plot the decision boundary gŒ∏(¬∑) and overlay it with the data plotted in (a).
hint: gŒ∏(¬∑) is a straight line in 2d. you can express x2in terms of x1and other
parameters.
(b) (this problem requires knowledge of the content of chapter 9). report the classifica-
tion accuracy. to do so, take testing data xand compute the prediction according to
equation (7.47).
(i) what is the type 1 error (false alarm) of classifying males? that is, what is the
percentage of testing samples that should be female but a male was predicted?
(ii) what is the type 2 error (miss) of classifying males? that is, what is the per-
centage of testing samples that should be male but a female was predicted?
4627.7. problems
(iii) what is the precision and recall for this classifier? for the definitions of precision
and recall, refer to chapter 9.5.4.
exercise 8. (nhanes part 3)(data download)
this exercise requires some background in optimization. please refer to reference [7.2, chap-
ter 9 and 10]. consider the following three optimization problems:
bŒ∏Œª= argmin
Œ∏‚ààrd‚à•xŒ∏‚àíy‚à•2
2+Œª‚à•Œ∏‚à•2
2, (7.48)
bŒ∏Œ±= argmin
Œ∏‚ààrd‚à•xŒ∏‚àíy‚à•2
2subject to ‚à•Œ∏‚à•2
2‚â§Œ±, (7.49)
bŒ∏œµ= argmin
Œ∏‚ààrd‚à•Œ∏‚à•2
2subject to ‚à•xŒ∏‚àíy‚à•2
2‚â§œµ. (7.50)
(a) set lambd = np.arange(0.1,10,0.1) . plot
¬à‚à•xbŒ∏Œª‚àíy‚à•2
2as a function of ‚à•bŒ∏Œª‚à•2
2.
¬à‚à•xbŒ∏Œª‚àíy‚à•2
2as a function of Œª.
¬à‚à•bŒ∏Œª‚à•2
2as a function of Œª.
(b) (i) write down the lagrangian for each of the three problems. note that the first
problem does not have any lagrange multiplier. for the second and third prob-
lems you may use the following notations:
¬àŒ≥Œ±= the lagrange multiplier of equation (7.49), and
¬àŒ≥œµ= the lagrange multiplier of equation (7.50).
(ii) state the first-order optimality conditions (the karush-kuhn-tucker or kkt
conditions) for each of the three problems. express your answers in terms of x,
Œ∏,y,Œª,Œ±,œµ, and the two lagrange multipliers Œ≥Œ±,Œ≥œµ.
(iii) fix Œª >0. we can solve equation (7.48) to obtain bŒ∏Œª. find Œ±and the lagrange
multiplier Œ≥Œ±in equation (7.49) such that bŒ∏Œªwould satisfy the kkt conditions
of equation (7.49).
(iv) fix Œª >0. we can solve equation (7.48) to obtain bŒ∏Œª. find œµand the lagrange
multiplier Œ≥œµin equation (7.50) such that bŒ∏Œªwould satisfy the kkt conditions
of equation (7.50).
(v) fix Œª >0. by using the Œ±andŒ≥Œ±you found in (iii), you can show that bŒ∏Œªwould
satisfy the kkt conditions of equation (7.49). is it enough to claim that bŒ∏Œªis
the solution of equation (7.49)? if yes, why? if no, what else do we need to show?
please elaborate through a proof, if needed.
exercise 9.
consider a training dataset dtrain={(x1, y1), . . . , (xn, yn)}and a weight w= [w1, . . . , w n]t.
find the regression solution to the following problem and discuss how you would choose the
weight.
bŒ∏= argmin
Œ∏‚ààrdnx
n=1wn 
yn‚àíxt
nŒ∏2. (7.51)
463chapter 7. regression
exercise 10.
consider a training dataset dtrain={(x1, y1), . . . , (xn, yn)}. suppose that the input data
xnis corrupted by i.i.d. gaussian noise en‚àºgaussian(0 , œÉ2id) so that the training set
becomes dtrain={(x1+e1, y1), . . . , (xn+en, yn)}. show that the (vanilla) least squares
linear regression by taking the expectation over en,
bŒ∏= argmin
Œ∏‚ààrdnx
n=1eenh 
yn‚àí(xn+en)tŒ∏2i
, (7.52)
is equivalent to a ridge regression.
464chapter 8
estimation
in this chapter, we discuss another set of important combat skills in data science, namely es-
timation . estimation has a close relationship with regression. regression primarily takes the
optimization route, while estimation takes the probabilistic route. as we will see, at a cer-
tain point the two will merge. that is, under some specific statistical conditions, estimation
processes will coincide with the regression.
estimation is summarized pictorially in figure 8.1 . imagine that we have some random
samples x1, . . . , x n. these samples are drawn from a distribution fx(x;Œ∏), where Œ∏is a
parameter that characterizes the distribution. the parameter Œ∏is not known to us. the
goal of estimation is to solve an inverse problem to recover the parameter based on the
observations x1, . . . , x n.
figure 8.1: estimation is an inverse problem of recovering the unknown parameters that were used by
the distribution. in this figure, the pdf of xusing a parameter Œ∏is denoted as fx(x;Œ∏). the forward
data-generation process takes the parameter Œ∏and creates the random samples x1, . . . , x n. estimation
takes these observed random samples and recovers the underlying model parameter Œ∏.
what is estimation?
estimation is an inverse problem with the goal of recovering the underlying pa-
rameter Œ∏of a distribution fx(x;Œ∏) based on the observed samples x1, . . . , x n.
465chapter 8. estimation
what are parameters?
before we discuss the methods of estimation, let us clarify the meaning of the parameter Œ∏.
all probability density functions (pdfs) have parameters. for example, a bernoulli random
variable is characterized by a parameter pthat defines the probability of getting a ‚Äúhead‚Äù. a
gaussian random variable is characterized by two parameters: the mean ¬µand variance œÉ2.
example 8.1 . (parameter of a bernoulli ) ifxnis a bernoulli random variable, then
the pmf has a parameter Œ∏:
pxn(xn;Œ∏) =Œ∏xn(1‚àíŒ∏)1‚àíxn.
remark . the pmf is expressed in this form because xnis either 1 or 0:
pxn(xn;Œ∏) =(
Œ∏1(1‚àíŒ∏)1‚àí1=Œ∏, if xn= 1,
Œ∏0(1‚àíŒ∏)1‚àí0= 1‚àíŒ∏, if xn= 0.
example 8.2 . (parameter of a gaussian ) ifxnis a gaussian random variable, the
pdf is
fxn(xn;Œ∏|{z}
=(¬µ,œÉ)) =1‚àö
2œÄœÉ2exp
‚àí(xn‚àí¬µ)2
2œÉ2
,
where Œ∏= [¬µ, œÉ] consists of both the mean and the variance. we can also designate
the parameter Œ∏to be the mean only. for example, if we know that œÉ= 1, then the
pdf is
fxn(xn;Œ∏|{z}
=¬µ) =1‚àö
2œÄexp
‚àí(xn‚àí¬µ)2
2
,
where Œ∏is the mean.
since all probability density functions have parameters, estimating them from the
observed random variables is a well-defined inverse problem. of course, there are better
estimates and there are worse estimates. let us look at the following example to develop
our intuitions about estimation.
figure 8.2 shows a dataset containing 1000 data points generated from a 2d gaussian
distribution with an unknown mean vector ¬µand an unknown covariance matrix œÉ. we
duplicate this dataset in the four subfigures. the estimation problem is to recover the
unknown mean vector ¬µand the covariance matrix œÉ. in the subfigures we propose four
candidates, each with a different mean vector and a different covariance matrix. we draw
the contour lines of the corresponding gaussians. it can be seen that some gaussians fit the
data better than others. the goal of this chapter is to develop a systematic way of finding
the best fit for the data.
plan for this chapter
the discussions in this chapter concern the three elementary distributions:
466-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345
-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345
-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345
-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345bad estimate bad estimate bad estimate good estimate
¬µ=
2
‚àí0.5
¬µ=
0
‚àí1.5
¬µ=
‚àí0.5
‚àí0.7
¬µ=
0
0
œÉ=0.25 0 .2
0.2 1
œÉ=1‚àí0.2
‚àí0.2 0 .1
œÉ=1 0
0 1
œÉ=0.25 0 .3
0.3 1
figure 8.2: an estimation problem. given a set of 1000 data points drawn from a gaussian distribution
with unknown mean ¬µand covariance œÉ, we propose several candidate gaussians and see which one
would be the best fit to the data. visually, we observe that the right-most gaussian has the best fit.
the goal of this chapter is to develop a systematic way of solving estimation problems of this type.
¬àlikelihood: fx|Œ∏(x|Œ∏), which is the conditional pdf of xgiven that the parameter
isŒ∏.
¬àprior: fŒ∏(Œ∏), which is the pdf of Œ∏.
¬àposterior: fŒ∏|x(Œ∏|x), which is the conditional pdf of Œ∏given the data x.
each of these density functions has its respective meaning, and consequently a set of different
estimation techniques. in section 8.1 we introduce the concept of maximum-likelihood (ml)
estimation. as the name suggests, the estimate is constructed by maximizing the likelihood
function. we will discuss a few examples of ml estimation and draw connections between
ml estimation and regression. in section 8.2 we will discuss several basic properties of an
ml estimate. specifically, we will introduce the ideas of unbiasedness, consistency, and the
invariance principle.
the second topic discussed in this chapter is the maximum-a-posteriori (map) esti-
mation, detailed in section 8.3. in map, the parameter Œ∏is a random variable. since Œ∏is a
random variable, it has its own probability density function fŒ∏(Œ∏), which we call the prior.
given the likelihood and the prior, we can define the posterior . the map estimation finds
the peak of the posterior distribution as a way to ‚Äúexplain‚Äù the data. several important
topics will be covered in section 8.3. for example, we will discuss the choice of the prior
via the concept of conjugate prior . we will also discuss how map is related to regularized
regressions such as the ridge and lasso regressions.
the third topic is the minimum mean-square estimation (mmse), outlined in sec-
tion 8.4. the mmse is a bayesian approach. an important result that will be demonstrated
is that the mmse estimate is the conditional expectation of the posterior distribution. in
other words, it is the mean of the posterior. an mmse estimate has an important difference
compared to a map estimate, namely that while an mmse estimate is the mean of the
posterior, a map estimate is the mode of the posterior. we discuss the formulation of the
estimation problem and ways of solving the problem. we also discuss how the mmse can
be performed for multidimensional gaussian distributions.
467chapter 8. estimation
8.1 maximum-likelihood estimation
maximum-likelihood (ml) estimation, as the name suggests, is an estimation method that
‚Äúmaximizes‚Äù the ‚Äúlikelihood‚Äù. therefore, to understand the ml estimation, we first need to
understand the meaning of likelihood, and why maximizing the likelihood would be useful.
8.1.1 likelihood function
consider a set of ndata points d={x1, x2, . . . , x n}. we want to describe these data points
using a probability distribution. what would be the most general way of defining such a
distribution?
since we have ndata points, and we do not know anything about them, the most gen-
eral way to define a distribution is as a high-dimensional probability density function (pdf)
fx(x). this is a pdf of a random vector x= [x1, . . . , x n]t. a particular realization of
this random vector is x= [x1, . . . , x n]t.
fx(x) is the most general description for the ndata points because fx(x) is the
joint pdf of all variables. it provides the complete statistical description of the vector x.
for example, we can compute the mean vector e[x], the covariance matrix cov( x), the
marginal distributions, the conditional distribution, the conditional expectations, etc. in
short, if we know fx(x), we know everything about x.
the joint pdf fx(x) is always parameterized by a certain parameter Œ∏. for example, if
we assume that xis drawn from a joint gaussian distribution, then fx(x) is parameterized
by the mean vector ¬µand the covariance matrix œÉ. so we say that the parameter Œ∏is
Œ∏= (¬µ,œÉ). to state the dependency on the parameter explicitly, we write
fx(x;Œ∏) = pdf of the random vector xwith a parameter Œ∏.
when you express the joint pdf as a function of xandŒ∏, you have two variables to
play with. the first variable is the observation x, which is given by the measured data. we
usually think about the probability density function fx(x) in terms of x, because the pdf
is evaluated at x=x. in estimation, however, xis something that you cannot control.
when your boss hands a dataset to you, xis already fixed. you can consider the probability
of getting this particular x, but you cannot change x.
the second variable stated in fx(x;Œ∏) is the parameter Œ∏. this parameter is what
we want to find out, and it is the subject of interest in an estimation problem. our goal is
to find the optimal Œ∏that can offer the ‚Äúbest explanation‚Äù to data x, in the sense that it
can maximize fx(x;Œ∏).
thelikelihood function is the pdf that shifts the emphasis to Œ∏:
definition 8.1. letx= [x1, . . . , x n]tbe a random vector drawn from a joint pdf
fx(x;Œ∏), and let x= [x1, . . . , x n]tbe the realizations. the likelihood function is a
4688.1. maximum-likelihood estimation
function of the parameter Œ∏given the realizations x:
l(Œ∏|x)def=fx(x;Œ∏). (8.1)
a word of caution: l(Œ∏|x) isnota conditional pdf because Œ∏is not a random variable.
the correct way to interpret l(Œ∏|x) is to view it as a function of Œ∏. this function changes
its shape according the observed data x. we will return to this point shortly.
independent observations
while fx(x) provides us with a complete picture of the random vector x, using fx(x) is
tedious. we need to describe how each xnis generated and describe how xnis related to
xmfor all pairs of nandm. if the vector xcontains nentries, then there are n2/2 pairs
of correlations we need to compute. when nis large, finding fx(x) would be very difficult
if not impossible.
in practice, fx(x) may sometimes be overkill. for example, if we measure the inter-
arrival time of a bus for several days, it is quite likely that the measurements will not be
correlated. in this case, instead of using the full fx(x), we can make assumptions about
the data points. the assumption we will make is that all the data points are independent
and that they are drawn from an identical distribution fx(x). the assumption that the
data points are independently and identically distributed (i.i.d.) significantly simplifies the
problem so that the joint pdf fxcan be written as a product of single pdfs fxn:
fx(x) =fx1,...,x n(x1, . . . , x n) =ny
n=1fxn(xn).
if you prefer a visualization, we can take a look at the covariance matrix, which goes
from a full covariance matrix to a diagonal matrix and then to an identity matrix:
Ô£Æ
Ô£ØÔ£∞var[x1] cov( x1, x2)¬∑¬∑¬∑ cov(x1, xn)
cov[x2, x1] var[ x2] ¬∑¬∑¬∑ cov(x2, xn)
............
cov(xn, x1) cov( xn, x2)¬∑¬∑¬∑ var[xn]Ô£π
Ô£∫Ô£ª=‚áí
independentÔ£Æ
Ô£ØÔ£∞var[x1] 0 ¬∑¬∑¬∑ 0
0 var[ x2]¬∑¬∑¬∑ 0
............
0 0 ¬∑¬∑¬∑ var[xn]Ô£π
Ô£∫Ô£ª
=‚áí
identicalÔ£Æ
Ô£ØÔ£∞œÉ20¬∑¬∑¬∑ 0
0œÉ2¬∑¬∑¬∑ 0
............
0 0 ¬∑¬∑¬∑ œÉ2Ô£π
Ô£∫Ô£ª.
the assumption of i.i.d. is strong. not all data can be modeled as i.i.d. (for example,
photons passing through a scattering medium have correlated statistics.) however, if the
i.i.d. assumption is valid, we can simplify the model significantly.
if the data points are i.i.d., then we can write the joint pdf as
fx(x;Œ∏) =ny
n=1fxn(xn;Œ∏).
this gives us a simplified form of the likelihood function, written as a product of the indi-
vidual pdfs.
469chapter 8. estimation
definition 8.2. given i.i.d. random variables x1, . . . , x nthat all have the same pdf
fxn(xn), the likelihood function is
l(Œ∏|x)def=ny
n=1fxn(xn;Œ∏). (8.2)
in computation we often take the log of the likelihood function. we call the resulting function
thelog-likelihood .
definition 8.3. given a set of i.i.d. random variables x1, . . . , x nwith pdf fxn(x; ;Œ∏),
thelog-likelihood is defined as
logl(Œ∏|x) = log fx(x;Œ∏) =nx
n=1logfxn(xn;Œ∏). (8.3)
example 8.3 . find the log-likelihood of a sequence of i.i.d. gaussian random variables
x1, . . . , x nwith mean ¬µand variance œÉ2.
solution . since the random variables x1, . . . , x nare i.i.d. gaussian, the pdf is
fx(x;¬µ, œÉ2) =ny
n=11‚àö
2œÄœÉ2e‚àí(xn‚àí¬µ)2
2œÉ2
. (8.4)
taking the log on both sides yields the log-likelihood function:
logl(¬µ, œÉ2|x) = log fx(x;¬µ, œÉ2)
= log(ny
n=11‚àö
2œÄœÉ2e‚àí(xn‚àí¬µ)2
2œÉ2)
=nx
n=1log1‚àö
2œÄœÉ2e‚àí(xn‚àí¬µ)2
2œÉ2
=nx
n=1
‚àí1
2log(2œÄœÉ2)‚àí(xn‚àí¬µ)2
2œÉ2
=‚àín
2log(2œÄœÉ2)‚àí1
2œÉ2nx
n=1(xn‚àí¬µ)2.
practice exercise 8.1 . find the log-likelihood of a sequence of i.i.d. bernoulli random
variables x1, . . . , x nwith parameter Œ∏.
4708.1. maximum-likelihood estimation
solution . ifx1, . . . , x nare i.i.d. bernoulli random variables, we have
fx(x;Œ∏) =ny
n=1
Œ∏xn(1‚àíŒ∏)1‚àíxn
.
taking the log on both sides of the equation yields the log-likelihood function:
logl(Œ∏|x) = log(ny
n=1
Œ∏xn(1‚àíŒ∏)1‚àíxn)
=nx
n=1log
Œ∏xn(1‚àíŒ∏)1‚àíxn
=nx
n=1xnlogŒ∏+ (1‚àíxn) log(1 ‚àíŒ∏)
= nx
n=1xn!
¬∑logŒ∏+ 
n‚àínx
n=1xn!
¬∑log(1‚àíŒ∏).
visualizing the likelihood function
the likelihood function l(Œ∏|x) is a function of Œ∏, but its value also depends on the under-
lying measurements x. it is extremely important to keep in mind the presence of both.
to help you visualize the effect of Œ∏andx, we consider a set of i.i.d. bernoulli random
variables. as we have just shown in the practice exercise, the likelihood function of these
i.i.d. random variables is
logl(Œ∏|x) = nx
n=1xn!
|{z}
s¬∑logŒ∏+ 
n‚àínx
n=1xn!
|{z }
n‚àís¬∑log(1‚àíŒ∏), (8.5)
where we define s=pn
n=1xnas the sum of the (binary) measurements.
to make the dependency on sandŒ∏explicit, we write l(Œ∏|x) as
logl(Œ∏|s) =slogŒ∏+ (n‚àís) log(1 ‚àíŒ∏), (8.6)
which emphasizes the role of sin defining the log-likelihood function. we plot the surface
ofl(Œ∏|s) as a function of sandŒ∏, assuming that n= 50. as shown on the left-hand side
offigure 8.3 , the surface l(Œ∏|s) has a saddle shape. along one direction the function goes
up, whereas along another direction the function goes down. in the middle of figure 8.3 ,
we show a bird‚Äôs-eye view of the surface, with the color-coding matched with the surface
plot. as you can see, when plotted as a function of Œ∏andx(in our case, we use a summary
statistic s=pn
n=1xn), the two-dimensional plot tells us how the log-likelihood function
changes when schanges. on the right-hand side of figure 8.3 , we show two particular
cross sections of the two-dimensional plot. one cross section is taken from s= 25 and the
other cross section is taken from s= 12. since the total number of heads in this numerical
experiment is assumed to be n= 50, the first cross section at s= 25 is obtained when
471chapter 8. estimation
figure 8.3: we plot the log-likelihood function as a function of s=pn
n=1xnandŒ∏. [left] we show
the surface plot of l(Œ∏|s) =slogŒ∏+ (n‚àís) log(1 ‚àíŒ∏). note that the surface has a saddle shape.
[middle] by taking a bird‚Äôs-eye view of the surface plot, we obtain a 2-dimensional contour plot of the
surface, where the color code matches the height of the log-likelihood function. [right] we take two
cross sections along s= 25 ands= 12 . observe how the shape changes.
half of the bernoulli measurements are ‚Äú1‚Äù, whereas the second cross section at s= 12 is
obtained when a quarter of the bernoulli measurements are ‚Äú1‚Äù.
the cross sections tell us the log-likelihood function log l(Œ∏|s) is a function defined
specifically for a given measurement x. as you can see from figure 8.3 , the log-likelihood
function changes when schanges. therefore, if our goal is to ‚Äúfind a Œ∏that maximizes the
log-likelihood function‚Äù, then for a different xwe will have a different answer. for example,
according to figure 8.3 , the maximum for log l(Œ∏|s= 25) occurs when Œ∏‚âà0.5, and the
maximum for log l(Œ∏|s= 12) occurs when Œ∏‚âà0.24. these are the maximum-likelihood
estimates for the respective measurements.
we use the following matlab code to generate the surface plot:
% matlab code to generate the surface plot
n = 50;
s = 1:n;
theta = linspace(0.1,0.9,100);
[s_grid, theta_grid] = meshgrid(s, theta);
l = s_grid.*log(theta_grid) + (n-s_grid).*log(1-theta_grid);
s = surf(s,theta,l);
s.linestyle = ‚Äô-‚Äô;
colormap jet
view(65,15)
for the bird‚Äôs-eye view plot, we replace surf with imagesc(s,theta,l) . for the cross
section plots, we call the commands plot(theta, l(:,12)) andplot(theta, l(:,25)) .
8.1.2 maximum-likelihood estimate
the likelihood is the pdf of xbut viewed as a function of Œ∏. the optimization problem
of maximizing l(Œ∏|x) is called the maximum-likelihood (ml) estimation:
4728.1. maximum-likelihood estimation
definition 8.4. letl(Œ∏)be the likelihood function of the parameter Œ∏given the
measurements x= [x1, . . . , x n]t. the maximum-likelihood estimate of the parameter
Œ∏is a parameter that maximizes the likelihood:
bŒ∏mldef=argmax
Œ∏l(Œ∏|x). (8.7)
example 8.4 . find the ml estimate for a set of i.i.d. bernoulli random variables
{x1, . . . , x n}with xn‚àºbernoulli( Œ∏) for n= 1, . . . , n .
solution . we know that the log-likelihood function of a set of i.i.d. bernoulli random
variables is given by
logl(Œ∏|x) = nx
n=1xn!
¬∑logŒ∏+ 
n‚àínx
n=1xn!
¬∑log(1‚àíŒ∏). (8.8)
thus, to find the ml estimate, we need to solve the optimization problem
bŒ∏ml= argmax
Œ∏( nx
n=1xn!
¬∑logŒ∏+ 
n‚àínx
n=1xn!
¬∑log(1‚àíŒ∏))
.
taking the derivative with respect to Œ∏and setting it to zero, we obtain
d
dŒ∏( nx
n=1xn!
¬∑logŒ∏+ 
n‚àínx
n=1xn!
¬∑log(1‚àíŒ∏))
= 0.
this gives us
pn
n=1xn
Œ∏‚àín‚àípn
n=1xn
1‚àíŒ∏= 0.
rearranging the terms yields
bŒ∏ml=1
nnx
n=1xn.
let‚Äôs do a sanity check to see if this result makes sense. the solution to this problem
says that bŒ∏mlis the empirical average of the measurements. assume that n= 50. let us
consider two particular scenarios as illustrated in figure 8.4 .
¬àscenario 1 :xis a vector of measurements such that sdef=pn
n=1xn= 25. since
n= 50, the formula tells us that bŒ∏ml=25
50= 0.5. this is the bestguess based on the
50 measurements where 25 are heads. if you look at figure 8.3 andfigure 8.4 , when
s= 25, we are looking at a particular cross section in the 2d plot. the likelihood
function we are inspecting is l(Œ∏|s= 25). for this likelihood function, the maximum
occurs at Œ∏= 0.5.
473chapter 8. estimation
figure 8.4: illustration of how the maximum-likelihood estimate of a set of i.i.d. bernoulli random
variables is determined. the subfigures above show two particular scenarios at s= 25 ands= 12 ,
assuming that n= 50 . when s= 25 , the likelihood function has a quadratic shape centered at
Œ∏= 0.5. this point is also the peak of the likelihood function when s= 25 . therefore, the ml estimate
isbŒ∏ml= 0.5. the second case is when s= 12 . the quadratic likelihood is shifted toward the left. the
ml estimate is bŒ∏ml= 0.24.
¬àscenario 2 :xis a vector of measurements such that sdef=pn
n=1xn= 12. the formula
tells us that bŒ∏ml=12
50= 0.24. this is again the bestguess based on the 50 measure-
ments where 12 are heads. referring to figure 8.3 andfigure 8.4 , we can see that
the likelihood function corresponds to another cross section l(Œ∏|s= 12) where the
maximum occurs at Œ∏= 0.24.
at this point, you may wonder why the shape of the likelihood function l(Œ∏|x) changes
so radically as xchanges? the answer can be found in figure 8.5 . imagine that we have
n= 50 measurements of which s= 40 give us heads. if these i.i.d. bernoulli random
variables have a parameter Œ∏= 0.5, it is quite unlikely that we will get 40 out of 50
measurements to be heads. (if it were Œ∏= 0.5, we should get more or less 25 out of 50
heads.) when s= 40, and without any additional information about the experiment, the
most logical guess is that the bernoulli random variables have a parameter Œ∏= 0.8. since
the measurement scan be as extreme as 0 out of 50 or 50 out of 50, the likelihood function
l(Œ∏|x) has to reflect these extreme cases. therefore, as we change x, we observe a big
change in the shape of the likelihood function.
as you can see from figure 8.5 ,s= 40 corresponds to the marked vertical cross
section. as we determine the maximum-likelihood estimate, we search among all the possi-
bilities, such as Œ∏= 0.2,Œ∏= 0.5,Œ∏= 0.8, etc. these possibilities correspond to the horizontal
lines we drew in the figure. among those horizontal lines, it is clear that the best estimate
occurs when Œ∏= 0.8, which is also the ml estimate.
4748.1. maximum-likelihood estimation
figure 8.5: suppose that we have a set of measurements such that s= 40 . to determine the ml
estimate, we look at the vertical cross section at s= 40 . among the different candidate parameters,
e.g.,Œ∏= 0.2,Œ∏= 0.5andŒ∏= 0.8, we pick the one that has the maximum response to the likelihood
function. for s= 40 , it is more likely that the underlying parameter is Œ∏= 0.8than Œ∏= 0.2orŒ∏= 0.5.
visualizing ml estimation as ngrows
maximum-likelihood estimation can also be understood directly from the pdf instead of
the likelihood function. to explain this perspective, let‚Äôs do a quick exercise.
practice exercise 8.2 . suppose that xnis a gaussian random variable. assume
thatœÉ= 1 is known but the mean Œ∏is unknown. find the ml estimate of the mean.
solution . the ml estimate bŒ∏mlis
bŒ∏ml= argmax
Œ∏logl(Œ∏|x)
= argmax
Œ∏log(ny
n=11‚àö
2œÄexp
‚àí(xn‚àíŒ∏)2
2)
= argmax
Œ∏‚àín
2log(2œÄ)‚àí1
2nx
n=1(xn‚àíŒ∏)2.
taking the derivative with respect to Œ∏, we obtain
d
dŒ∏(
‚àín
2log(2œÄ)‚àí1
2nx
n=1(xn‚àíŒ∏)2)
= 0.
this gives uspn
n=1(xn‚àíŒ∏) = 0. therefore, the ml estimate is
bŒ∏ml=1
nnx
n=1xn.
now we will draw the pdf and compare it with the measured data points. our focus
475chapter 8. estimation
is to analyze how the ml estimate changes as ngrows.
when n= 1.there is only one observation x1. the best gaussian that fits this sample
must be the one that is centered at x1. in fact, the optimization is1
bŒ∏ml= argmax
Œ∏logl(Œ∏|x1) = argmax
Œ∏log1‚àö
2œÄœÉ2exp
‚àí(x1‚àíŒ∏)2
2œÉ2
= argmax
Œ∏‚àí(x1‚àíŒ∏)2=x1.
therefore, the ml estimate is bŒ∏ml=x1.figure 8.6 illustrates this case. as we conduct the
ml estimation, we imagine that there are a few candidate pdfs. the ml estimation says
that among all these candidate pdfs we need to find one that can maximize the probability
of obtaining the observation x1. since we only have one observation, we have no choice but
to pick a gaussian centered at x1. certainly the sample x1=x1could be bad, and we may
find a wrong gaussian. however, with only one sample there is no way for us to make better
decisions.
-5 -4 -3 -2 -1 0 1 2 3 4 5
x00.050.10.150.20.250.30.350.40.450.5
data point
candidate pdf
estimated pdf
figure 8.6: n= 1. suppose that we are given one observed data point located around x=‚àí2.1. to
conduct the ml estimation we propose a few candidate pdfs, each being a gaussian with unit variance
but a different mean Œ∏. the ml estimate is a parameter Œ∏such that the corresponding pdf matches
the best with the observed data. in this example the best match happens when the estimated gaussian
pdf is centered at x1.
when n= 2.in this case we need to find a gaussian that fits both x1andx2. the
probability of simultaneously observing x1andx2is determined by the joint distribution.
by independence we then have
bŒ∏ml= argmax
Œ∏log(1‚àö
2œÄœÉ22
exp
‚àí(x1‚àíŒ∏)2+ (x2‚àíŒ∏)2)
2œÉ2)
= argmax
Œ∏
‚àí(x1‚àíŒ∏)2+ (x2‚àíŒ∏)2
2œÉ2
=x1+x2
2,
1we skip the step of checking whether the stationary point is a maximum or a minimum, which can be
done by evaluating the second-order derivative. in fact, since the function ‚àí(x1‚àíŒ∏)2is concave in Œ∏, a
stationary point must be a maximum.
4768.1. maximum-likelihood estimation
where the last step is obtained by taking the derivative:
d
dŒ∏
(x1‚àíŒ∏)2+ (x2‚àíŒ∏)2	
= 2(x1‚àíŒ∏) + 2( x2‚àíŒ∏).
equating this with zero yields the solution Œ∏=x1+x2
2. therefore, the best gaussian that
fits the observations is gaussian(x1+x2
2, œÉ2).
-5 -4 -3 -2 -1 0 1 2 3 4 5
x00.050.10.150.20.250.30.350.40.450.5
data point
candidate pdf
estimated pdf
figure 8.7: n= 2. suppose that we are given two observed data points located around x1=‚àí0.98
andx2=‚àí1.15. to conduct the ml estimation we propose a few candidate pdfs, each being a
gaussian with unit variance but a different mean Œ∏. the ml estimate is a parameter Œ∏such that the
corresponding pdf best matches the observed data. in this example the best match happens when the
estimated gaussian pdf is centered at (x1+x2)/2‚âà ‚àí1.07.
does this result make sense? when you have two data points x1andx2, the ml
estimation is trying to find a gaussian that can best fit both of these two data points.
your best bet here is bŒ∏ml= (x1+x2)/2, because there are no other choices. if you choose
bŒ∏ml=x1orbŒ∏ml=x2, it cannot be a good estimate because you are not using both data
points. as shown in figure 8.7 , for these two observed data points x1andx2, the pdf
marked in red (which is a gaussian centered at ( x1+x2)/2) is indeed the best fit.
when n= 10 andn= 100 .we can continue the above calculation for n= 10 and
n= 100. in this case the mle is
bŒ∏ml= argmax
Œ∏log(1‚àö
2œÄœÉ2n
exp
‚àí(x1‚àíŒ∏)2+¬∑¬∑¬∑+ (xn‚àíŒ∏)2
2œÉ2)
= argmax
Œ∏‚àínx
n=1(xn‚àíŒ∏)2
2œÉ2=1
nnx
n=1xn.
where the optimization is solved by taking the derivative:
d
dŒ∏nx
n=1(xn‚àíŒ∏)2=‚àí2nx
n=1(xn‚àíŒ∏)
equating this with zero yields the solution Œ∏=1
npn
n=1xn.
the result suggests that for an arbitrary number of training samples the ml estimate
is the sample average. these cases are illustrated in figure 8.8 . as you can see, the red
curves (the estimated pdf) are always trying to fit as many data points as possible.
the above experiment tells us something about the ml estimation:
477chapter 8. estimation
-5 -4 -3 -2 -1 0 1 2 3 4 500.050.10.150.20.250.30.350.40.450.5
data point
candidate pdf
estimated pdf
-5 -4 -3 -2 -1 0 1 2 3 4 500.050.10.150.20.250.30.350.40.450.5
data point
candidate pdf
estimated pdf
(c)n= 10 (d) n= 100
figure 8.8: when n= 10 andn= 100 , the ml estimation continues to evaluate the different
candidate pdfs. for a given set of data points, the ml estimation picks the best pdf to fit the data
points. in this gaussian example it was shown that the optimal parameter is bŒ∏ml= (1/n)pn
n=1xn,
which is the sample average.
how does ml estimation work, intuitively?
¬àthe likelihood function l(Œ∏|x) measures how ‚Äúlikely‚Äù it is that we will get xif
the underlying parameter is Œ∏.
¬àin the case of a gaussian with an unknown mean, you move around the gaussian
until you find a good fit.
8.1.3 application 1: social network analysis
ml estimation has extremely broad applicability. in this subsection and the next we discuss
two real examples. we start with an example in social network analysis.
in chapter 3, when we discussed the bernoulli random variables, we introduced the
erdÀù os-r¬¥ enyi graph ‚Äî one of the simplest models for social networks. the erdÀù os-r¬¥ enyi graph
is a single-membership network that assumes that all users belong to the same cluster. thus
the connectivity between users is specified by a single parameter, which is also the probability
of the bernoulli random variable.
in our discussions in chapter 3 we defined an adjacency matrix to represent a graph.
the adjacency matrix is a binary matrix, with the ( i, j)th entry indicating an edge connect-
ing nodes iandj. since the presence and absence of an edge is binary and random, we may
model each element of the adjacency matrix as a bernoulli random variable
xij‚àºbernoulli( p).
in other words, the edge xijlinking user iand user jin the network is either xij= 1 with
probability p, orxij= 0 with probability 1 ‚àíp. in terms of notation, we define the matrix
x‚ààrn√ónas the adjacency matrix, with the ( i, j)th element being xij.
a few examples of a single-membership erdÀù os-r¬¥ enyi graph are shown in figure 8.9 . as
the figure shows, the network connectivity increases as the bernoulli parameter pincreases.
this happens because pdefines the ‚Äúdensity‚Äù of the edges. if pis large, we have a greater
chance of getting xij= 1, and so there is a higher probability that an edge is present
between node iand node j. ifpis small, the probability is lower.
4788.1. maximum-likelihood estimation
-2 0 2-3-2-1012p = 0.3
  1
  2  3
  4
  5  6  7
  8  9  10
  11  12
  13
  14  15  16  17  18  19  20
  21  22  23  24
  25  26  27
  28  29
  30  31
  32  33
  34  35
  36
  37
  38  39
  40
-2 0 2-4-3-2-10123p = 0.5
  1
  2  3  4
  5  6  7
  8
  9
  10  11
  12  13  14  15
  16  17
  18   19  20
  21
  22
  23  24  25
  26  27  28
  29  30
  31  32  33
  34
  35
  36
  37  38
  39  40
-4 -2 0 2 4-4-3-2-10123p = 0.7
  1
  2
  3  4
  5  6
  7
  8  9  10
  11  12
  13  14
  15  16  17
  18
  19  20  21
  22  23
  24
  25  26  27  28
  29  30  31
  32  33
  34  35  36
  37  38  39
  40
-4 -2 0 2 4-4-2024p = 0.9
  1
  2  3
  4  5
  6  7
  8
  9  10
  11  12  13
  14  15
  16
  17  18
  19  20
  21  22
  23  24
  25
  26  27  28
  29
  30  31  32  33  34  35
  36
  37  38
  39  40
(a) graph representations of erdÀù os-r¬¥ enyi graphs at different p.
(b) adjacent matrices of the corresponding graphs.
figure 8.9: a single-membership erdÀù os-r¬¥ enyi graph is a graph structure in which the edge between
node iand node jis defined as a bernoulli random variable with parameter p. aspincreases, the graph
has a higher probability of having more edges. the adjacent matrices shown in the bottom row are the
mathematical representations of the graphs.
suppose that we are given onesnapshot of the network, i.e., one realization x‚ààrn√ón
of the adjacency matrix x‚ààrn√ón. the problem of recovering the latent parameter pcan
be formulated as an ml estimation.
example 8.5 . write down the log-likelihood function of the single-membership erdÀù os-
r¬¥ enyi graph ml estimation problem.
solution . based on the definition of the graph model, we know that
xij‚àºbernoulli( p).
therefore, the probability mass function of xijis
p[xij= 1] = p and p[xij= 0] = 1 ‚àíp.
this can be compactly expressed as
fx(x;p) =ny
i=1ny
j=1pxij(1‚àíp)1‚àíxij.
hence, the log-likelihood is
logl(p|x) =nx
i=1nx
j=1{xijlogp+ (1‚àíxij) log(1 ‚àíp)}.
479chapter 8. estimation
now that we have the log-likelihood function, we can proceed to estimate the param-
eterp. the solution to this is the ml estimate.
practice exercise 8.3 . solve the ml estimation problem:
bpml= argmax
plogl(p|x).
solution . using the log-likelihood we just derived, we have that
bpml=nx
i=1nx
j=1{xijlogp+ (1‚àíxij) log(1 ‚àíp)}.
taking the derivative and setting it to zero,
d
dplogl(p|x) =d
dpÔ£±
Ô£≤
Ô£≥nx
i=1nx
j=1{xijlogp+ (1‚àíxij) log(1 ‚àíp)}Ô£º
Ô£Ω
Ô£æ
=nx
i=1nx
j=1xij
p‚àí1‚àíxij
1‚àíp
= 0.
lets=pn
i=1pn
j=1xij. the equation above then becomes
s
p‚àín2‚àís
1‚àíp= 0.
rearranging the terms yields (1 ‚àíp)s=p(n2‚àís), which gives us
bpml=s
n2=1
n2nx
i=1nx
j=1xij. (8.9)
on computers, visualizing the graphs and computing the ml estimates are reasonably
straightforward. in matlab, you can call the command graph to build a graph from the
adjacency matrix a. this will allow you to plot the graph. the computation, however, is done
directly by the adjacency matrix. in the code below, you can see that we call rand to generate
the bernoulli random variables. the command triu extracts the upper triangular matrix
from the matrix a. this ensures that we do not pick the diagonals. the symmetrization of
a+a‚Äô ensures that the graph is indirectional, meaning that itojis the same as jtoi.
% matlab code to visualize a graph
n = 40; # number of nodes
p = 0.3 # probability
a = rand(n,n)<p;
a = triu(a,1);
a = a+a‚Äô; # adj matrix
g = graph(a); # graph
4808.1. maximum-likelihood estimation
plot(g); # drawing
p_ml = mean(a(:)); # ml estimate
in python, the computation is done similarly with the help of the networkx library.
the number of edges mis defined as m=pn2
2. this is because for a graph with nnodes, there
are at mostn2
2unique pairs of indirected edges. multiplying this number by the probability
pwill give us the number of edges m.
# python code to visualize a graph
import networkx as nx
import numpy as np
n = 40 # number of nodes
p = 0.3 # probability
m = np.round(((n ** 2)/2)*p) # number of edges
g = nx.gnm_random_graph(n,m) # graph
a = nx.adjacency_matrix(g) # adj matrix
nx.draw(g) # drawing
p_ml = np.mean(a) # ml estimate
as you can see in both the matlab and the python code, the ml estimate bpmlis de-
termined by taking the sample average. thus the ml estimate, according to our calculation,
isbpml=1
n2pn
i=1pn
j=1xij.
8.1.4 application 2: reconstructing images
being able to see in the dark is the holy grail of imaging. many advanced sensing technologies
have been developed over the past decade. in this example, we consider a single-photon image
sensor. this is a counting device that counts the number of photons arriving at the sensor.
physicists have shown that a poisson process can model the arrival of the photons. for
simplicity we assume a homogeneous pattern of npixels. the underlying intensity of the
homogeneous pattern is a constant Œª.
suppose that we have a sensor with npixels x1, . . . , x n. according to the poisson
statistics, the probability of observing a pixel value is determined by the poisson probability:
xn‚àºpoisson( Œª), n = 1, . . . , n,
or more explicitly,
p[xn=xn] =Œªxn
xn!e‚àíŒª,
where xnis the nth observed pixel value, and is an integer.
a single-photon image sensor is slightly more complicated in the sense that it does
not report xnbut instead reports a truncated version of xn. depending on the number of
incoming photons, the sensor reports
yn=(
1, x n‚â•1,
0, x n= 0.(8.10)
we call this type of sensors a one-bit single-photon image sensor (see figure 8.10 ). our
question is: if we are given the measurements x1, . . . , x n, can we estimate the underlying
parameter Œª?
481chapter 8. estimation
figure 8.10: a one-bit single-photon image sensor captures an image with binary bits: it reports a ‚Äú1‚Äù
when the number of photons exceeds certain threshold, and ‚Äú0‚Äù otherwise. the recovery problem here
is to estimate the underlying image from the measurements.
example 8.6 . derive the log-likelihood function of the estimation problem for the
single-photon image sensors.
solution . since ynis a binary random variable, its probability is completely specified
by the two states it takes:
p[yn= 0] = p[xn= 0] = e‚àíŒª
p[yn= 1] = p[xnÃ∏= 0] = 1 ‚àíe‚àíŒª.
thus, ynis a bernoulli random variable with probability 1 ‚àíe‚àíŒªof getting a value
of 1, and probability e‚àíŒªof getting a value of 0. by defining ynas a binary number
taking values of either 0 or 1, it follows that the log-likelihood is
logl(Œª|y) = logny
n=1 
1‚àíe‚àíŒªyn 
e‚àíŒª1‚àíyn
=nx
n=1
ynlog(1‚àíe‚àíŒª)‚àíŒª(1‚àíyn)
.
practice exercise 8.4 . solve the ml estimation problem
bŒªml= argmax
Œªlogl(Œª|y). (8.11)
solution . first, we define s=pn
n=1yn. this simplifies the log-likelihood function to
logl(Œª|y) =nx
n=1
ynlog(1‚àíe‚àíŒª)‚àíŒª(1‚àíyn)
=slog(1‚àíe‚àíŒª)‚àíŒª(n‚àís).
4828.1. maximum-likelihood estimation
the ml estimation is
bŒªml= argmax
Œªslog(1‚àíe‚àíŒª)‚àíŒª(n‚àís).
taking the derivative w.r.t. Œªyields
d
dŒª
slog(1‚àíe‚àíŒª)‚àíŒª(n‚àís)
=s
1‚àíe‚àíŒªe‚àíŒª‚àí(n‚àís).
moving around the terms, it follows that
s
1‚àíe‚àíŒªe‚àíŒª‚àí(n‚àís) = 0 = ‚áí Œª=‚àílog
1‚àís
n
.
therefore, the ml estimate is
bŒªml=‚àílog 
1‚àí1
nnx
n=1yn!
. (8.12)
for real images, you can extrapolate the idea from yntoyi,j,t, which denotes the ( i, j)th
pixel located at time t. defining yt‚ààrn√ónas the tth frame of the observed data, we can
usetframes to recover one image bŒªml‚ààrn√ón. it follows from the above derivation that
the ml estimate is
bŒªml=‚àílog 
1‚àí1
ttx
t=1yt!
. (8.13)
figure 8.11 shows a pair of input-output images of a 256 √ó256 image.
(a) observed data (1-frame) (b) ml estimate (using 100 frames)
figure 8.11: ml estimation for a single-photon image sensor problem. the observed data consists of
100 frames of binary measurements y1, . . . ,yt, where t= 100 . the ml estimate is constructed by
Œª=‚àílog(1‚àí1
tpt
t=1yt).
on a computer the ml estimation can be done in a few lines of matlab code. the
code in python requires more work, as it needs to read images using the opencv library.
483chapter 8. estimation
% matlab code to recover an image from binary measurements
lambda = im2double(imread(‚Äôcameraman.tif‚Äô));
t = 100; % 100 frames
x = poissrnd( repmat(lambda, [1,1,t]) ); % generate poisson r.v.
y = (x>=1); % binary truncation
lambdahat = -log(1-mean(y,3)); % ml estimation
figure(1); imshow(x(:,:,1));
figure(2); imshow(lambdahat);
# python code to recover an image from binary measurements
import cv2
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
lambd = cv2.imread(‚Äô./cameraman.tif‚Äô) # read image
lambd = cv2.cvtcolor(lambd, cv2.color_bgr2gray)/255 # gray scale
t = 100
lambdt = np.repeat(lambd[:, :, np.newaxis], t, axis=2) # repeat image
x = stats.poisson.rvs(lambdt) # poisson statistics
y = (x>=1).astype(float) # binary truncation
lambdhat = -np.log(1-np.mean(y,axis=2)) # ml estimation
plt.imshow(lambdhat,cmap=‚Äôgray‚Äô)
8.1.5 more examples of ml estimation
by now you should be familiar with the procedure for solving the ml estimation problem.
we summarize the two steps as follows.
how to solve an ml estimation problem
¬àwrite down the likelihood l(Œ∏|x).
¬àmaximize the likelihood by solving bŒ∏ml= argmax
Œ∏logl(Œ∏|x).
practice exercise 8.5 (gaussian ). suppose that we are given a set of i.i.d. gaus-
sian random variables x1, . . . , x n, where both the mean ¬µand the variance œÉ2are
unknown. let Œ∏= [¬µ, œÉ2]tbe the parameter. find the ml estimate of Œ∏.
solution . we first write down the likelihood. the likelihood of these i.i.d. gaussian
random variables is
l(Œ∏|x) =1‚àö
2œÄœÉ2n
exp(
‚àí1
2œÉ2nx
n=1(xn‚àí¬µ)2)
.
4848.1. maximum-likelihood estimation
to solve the ml estimation problem, we maximize the log-likelihood:
bŒ∏mldef= argmax
Œ∏l(Œ∏|x)
= argmax
¬µ,œÉ2(
‚àín
2log(2œÄœÉ2)‚àí1
2œÉ2nx
n=1(xn‚àí¬µ)2)
.
since we have two parameters, we need to take the derivatives for both.
d
d¬µ(
‚àín
2log(2œÄœÉ2)‚àí1
2œÉ2nx
n=1(xn‚àí¬µ)2)
= 0,
d
dœÉ2(
‚àín
2log(2œÄœÉ2)‚àí1
2œÉ2nx
n=1(xn‚àí¬µ)2)
= 0.
(note that the derivative of the second equation is taken w.r.t. to œÉ2and not œÉ.) this
pair of equations gives us
1
œÉ2nx
n=1(xn‚àí¬µ) = 0 ,and‚àín
2¬∑1
2œÄœÉ2¬∑(2œÄ) +1
2œÉ4nx
n=1(xn‚àí¬µ)2= 0.
rearranging the equations, we find that
b¬µml=1
nnx
n=1xn and bœÉ2
ml=1
nnx
n=1(xn‚àíb¬µml)2. (8.14)
practice exercise 8.6 . (poisson ) given a set of i.i.d. poisson random variables
x1, . . . , x nwith an unknown parameter Œª, find the ml estimate of Œª.
solution . for a poisson random variable, the likelihood function is
l(Œª|x) =ny
n=1Œªxn
xn!e‚àíŒª
. (8.15)
to solve the ml estimation problem, we note that
bŒªml= argmax
Œªl(Œª|x) = argmax
Œªlog(ny
n=1Œªxn
xn!e‚àíŒª)
= argmax
ŒªlogŒªp
nxn
q
nxn!e‚àínŒª
.
sinceq
nxn! is independent of Œª, its presence or absence will not affect the optimization
485chapter 8. estimation
problem. consequently we can drop the term. it follows that
bŒªml= argmax
Œªlogn
Œªp
nxne‚àínŒªo
= argmax
Œª x
nxn!
logŒª‚àínŒª.
taking the derivative and setting it to zero yields
d
dŒª( x
nxn!
logŒª‚àínŒª)
=p
nxn
Œª‚àín= 0.
rearranging the terms yields
bŒªml=1
nnx
n=1xn. (8.16)
the idea of ml estimation can also be extended to vector observations.
example 8.7 . (high-dimensional gaussian ) suppose that we are given a set of i.i.d.
d-dimensional gaussian random vectors x1, . . . ,xnsuch that
xn‚àºgaussian( ¬µ,œÉ).
we assume that œÉis fixed and known, but ¬µis unknown. find the ml estimate of ¬µ.
solution . the likelihood function is
l(¬µ|{xn}n
n=1) =ny
n=1fxn(xn;¬µ)
=ny
n=1(
1p
(2œÄ)d|œÉ|exp
‚àí1
2(xn‚àí¬µ)tœÇ‚àí1(xn‚àí¬µ))
= 
1p
(2œÄ)d|œÉ|!n
exp(
‚àí1
2nx
n=1(xn‚àí¬µ)tœÇ‚àí1(xn‚àí¬µ))
.
thus the log-likelihood function is
logl(¬µ|{xn}n
n=1) =n
2log|œÉ|+n
2log(2œÄ)d+nx
n=11
2(xn‚àí¬µ)tœÇ‚àí1(xn‚àí¬µ)
.
the ml estimate is found by maximizing this log-likelihood function:
b¬µml= argmax
¬µlogl(¬µ|{xn}n
n=1).
4868.1. maximum-likelihood estimation
taking the gradient of the function and setting it to zero, we have that
d
d¬µ(
n
2log|œÉ|+n
2log(2œÄ)d+nx
n=11
2(xn‚àí¬µ)tœÇ‚àí1(xn‚àí¬µ))
= 0.
the derivatives of the first two terms are zero because they do not depend on ¬µ). thus
we have that:
nx
n=1
œÉ‚àí1(xn‚àí¬µ)
= 0.
rearranging the terms yields the ml estimate b¬µml=1
npn
n=1xn.
example 8.8 . (high-dimensional gaussian ) assume the same problem setting as in
example 8.7, except that this time we assume that both the mean vector ¬µand the
covariance matrix œÉare unknown. find the ml estimate for Œ∏= (¬µ,œÉ).
solution . the log-likelihood follows from example 8.7:
logl(¬µ|{xn}n
n=1) =n
2log|œÉ|+n
2log(2œÄ)d+nx
n=11
2(xn‚àí¬µ)tœÇ‚àí1(xn‚àí¬µ)
.
finding the ml estimate requires taking the derivative with respect to both ¬µandœÇ:
d
d¬µ(
n
2log|œÉ|+n
2log(2œÄ)d+nx
n=11
2(xn‚àí¬µ)tœÇ‚àí1(xn‚àí¬µ))
= 0,
d
dœÇ(
n
2log|œÉ|+n
2log(2œÄ)d+nx
n=11
2(xn‚àí¬µ)tœÇ‚àí1(xn‚àí¬µ))
= 0.
after some tedious algebraic steps (see duda et al., pattern classification , problem
3.14), we have that
b¬µml=1
nnx
n=1xn, (8.17)
bœÉml=1
nnx
n=1(xn‚àíb¬µml)(xn‚àíb¬µml)t. (8.18)
8.1.6 regression versus ml estimation
ml estimation is closely related to regression. to understand the connection, we consider a
linear model that we studied in chapter 7. this model describes the relationship between
487chapter 8. estimation
the inputs x1, . . . ,xnand the observed outputs y1, . . . , y n, via the equation
yn=d‚àí1x
p=0Œ∏pœïp(xn) +en, n = 1, . . . , n. (8.19)
in this expression, œïp(¬∑) is a transformation that extracts the ‚Äúfeatures‚Äù of the input vector
xto produce a scalar. the coefficient Œ∏pdefines the relative weight of the feature œïp(xn) in
constructing the observed variable yn. the error endefines the modeling error between the
observation ynand the predictionpd‚àí1
p=0Œ∏pœïp(xn). we call this equation a linear model.
expressed in matrix form, the linear model is
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞y1
y2
...
ynÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
=y=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞œï0(x1)œï1(x1)¬∑¬∑¬∑ œïd‚àí1(x1)
œï0(x2)œï1(x2)¬∑¬∑¬∑ œïd‚àí1(x2)
... ¬∑¬∑¬∑......
œï0(xn)œï1(xn)¬∑¬∑¬∑œïd‚àí1(xn)Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
| {z }
=xÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£∞Œ∏0
Œ∏1
...
Œ∏d‚àí1Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
=Œ∏+Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞e1
e2
...
enÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
=e,
or more compactly as y=xŒ∏+e. rearranging the terms, it is easy to show that
nx
n=1e2
n=nx
n=1 
yn‚àíd‚àí1x
p=0Œ∏pœïp(xn)!2
=nx
n=1(yn‚àí[xŒ∏]n)2=‚à•y‚àíxŒ∏‚à•2.
now we make an assumption : that each noise enis an i.i.d. copy of a gaussian random
variable with zero mean and variance œÉ2. in other words, the error vector eis distributed
according to e‚àºgaussian( 0, œÉ2i). this assumption is not always true because there are
many situations in which the error is not gaussian. however, this assumption is necessary
for us to make the connection between ml estimation and regression.
with this assumption, we ask, given the observations y1, . . . , y n, what would be the
ml estimate of the unknown parameter Œ∏? we answer this question in two steps.
example 8.9 . find the likelihood function of Œ∏, given y= [y1, . . . , y n]t.
solution . the pdf of yis given by a gaussian:
fy(y;Œ∏) =ny
n=11‚àö
2œÄœÉ2exp
‚àí(yn‚àí[xŒ∏]n)2
2œÉ2
=1p
(2œÄœÉ2)nexp(
‚àí1
2œÉ2nx
n=1(yn‚àí[xŒ∏]n)2)
=1p
(2œÄœÉ2)nexp
‚àí1
2œÉ2‚à•y‚àíxŒ∏‚à•2
. (8.20)
4888.1. maximum-likelihood estimation
therefore, the log-likelihood function is
logl(Œ∏|y) = log(
1p
(2œÄœÉ2)nexp
‚àí1
2œÉ2‚à•y‚àíxŒ∏‚à•2)
=‚àín
2log(2œÄœÉ2)‚àí1
2œÉ2‚à•y‚àíxŒ∏‚à•2.
the next step is to solve the ml estimation by maximizing the log-likelihood.
example 8.10 . solve the ml estimation problem stated in example 8.9. assume that
xtxis invertible.
solution .
bŒ∏ml= argmax
Œ∏logl(Œ∏|y)
= argmax
Œ∏
‚àín
2log(2œÄœÉ2)‚àí1
2œÉ2‚à•y‚àíxŒ∏‚à•2
.
taking the derivative w.r.t. Œ∏yields
d
dŒ∏
‚àín
2log(2œÄœÉ2)‚àí1
2œÉ2‚à•y‚àíxŒ∏‚à•2
= 0.
sinced
dŒ∏Œ∏taŒ∏=a+at, it follows from the chain rule that
d
dŒ∏
‚àí1
2œÉ2‚à•y‚àíxŒ∏‚à•2
=d
dŒ∏
‚àí1
2œÉ2(y‚àíxŒ∏)t(y‚àíxŒ∏)
=1
œÉ2xt(xŒ∏‚àíy).
substituting this result into the equation,
1
œÉ2xt(xŒ∏‚àíy) = 0 .
rearranging terms we obtain xtxŒ∏=xty, of which the solution is
bŒ∏ml= (xtx)‚àí1xty. (8.21)
since the ml estimate in equation (8.21) is the same as the regression solution (see
chapter 7), we conclude that the regression problem of a linear model is equivalent to solving
an ml estimation problem.
the main difference between a linear regression problem and an ml estimation problem
is the underlying statistical model, as illustrated in figure 8.12 . in linear regression, you
do not care about the statistics of the noise term en. we choose ( ¬∑)2as the error because it
is differentiable and convenient. in ml estimation, we choose ( ¬∑)2as the error because the
noise is gaussian. if the noise is not gaussian, e.g., the noise follows a laplace distribution,
we need to choose | ¬∑ |as the error. therefore, you can always get a result by solving the
linear regression. however, this result will only become meaningful if you provide additional
489chapter 8. estimation
figure 8.12: ml estimation is equivalent to a linear regression when the underlying statistical model
for ml estimation is a gaussian. specifically, if the error term e=y‚àíxŒ∏is an independent gaussian
vector with zero mean and covariance matrix œÉ2i, then the resulting ml estimation is the same as linear
regression. if the underlying statistical model is not gaussian, then solving the regression is equivalent
to applying a gaussian ml estimation to a non-gaussian problem. this will still give us a result, but
that result will not maximize the likelihood, and thus it will not have any statistical guarantee.
information about the problem. for example, if you know that the noise is gaussian, then
the regression solution is also the ml solution. this is a statistical guarantee.
in practice, of course, we do not know whether the noise is gaussian or not. at this
point we have two courses of action: (i) use your prior knowledge/domain expertise to
determine whether a gaussian assumption makes sense, or (ii) select an alternative model
and see if the alternative model fits the data better. in practice, we should also question
whether maximizing the likelihood is what we want. we may have some knowledge and
therefore prefer the parameter Œ∏, e.g., we want a sparse solution so that Œ∏only contains a
few non-zeros. in that case, maximizing the likelihood without any constraint may not be
the solution we want.
ml estimation versus regression
¬àml estimation requires a statistical assumption, whereas regression does not.
¬àsuppose that you use a linear model yn=pd‚àí1
p=0Œ∏pœïp(xn) +enwhere en‚àº
gaussian(0 , œÉ2), for n= 1, . . . , n .
¬àthen the likelihood function in the ml estimation is
l(Œ∏|y) =1p
(2œÄœÉ2)nexp
‚àí1
2œÉ2‚à•y‚àíxŒ∏‚à•2
,
¬àthe ml estimate bŒ∏mlisbŒ∏ml= (xtx)‚àí1xty, which is exactly the same as
the regression solution. if the above statistical assumptions do not hold, then the
regression solution will not maximize the likelihood.
4908.2. properties of ml estimates
8.2 properties of ml estimates
ml estimation is a very special type of estimation. not all estimations are ml. if an estimate
is ml, are there any theoretical properties we can analyze? for example, will ml estimates
guarantee the recovery of the true parameter? if so, when will this happen? in this section
we investigate these theoretical questions so that you will acquire a better understanding of
the statistical nature of ml estimates.2
8.2.1 estimators
we know that an ml estimate is defined as
bŒ∏ml(x) = argmax
Œ∏l(Œ∏|x). (8.22)
we write bŒ∏ml(x) to emphasize that bŒ∏mlis a function of x. the dependency of bŒ∏ml(x) on
xshould not be a surprise. for example, if the ml estimate is the sample average, we have
that
bŒ∏ml(x1, . . . , x n) =1
nnx
n=1xn,
where x= [x1, . . . , x n]t.
however, in this setting we should always remember that x1, . . . , x nare realizations
of the i.i.d. random variables x1, . . . , x n. therefore, if we want to analzye the randomness
of the variables, it is more reasonable to write bŒ∏mlas a random variable bŒ∏ml. for example,
in the case of sample average, we have that
bŒ∏ml(x1, . . . , x n) =1
nnx
n=1xn. (8.23)
we call bŒ∏mlthe ml estimator of the true parameter Œ∏.
estimate versus estimator
¬àanestimate is anumber , e.g.,bŒ∏ml=1
nnx
n=1xn. it is the random realization of
a random variable.
¬àanestimator is arandom variable , e.g., bŒ∏ml=1
nnx
n=1xn. it takes a set of
random variables and generates another random variable.
2for notational simplicity, in this section we will focus on a scalar parameter Œ∏instead of a vector
parameter Œ∏.
491chapter 8. estimation
the ml estimators are one type of estimator, namely those that maximize the likeli-
hood functions. if we do not want to maximize the likelihood we can still define an estimator.
an estimator is any function that takes the data points x1, . . . , x nand maps them to a
number (or a vector of numbers). that is, an estimator is
bŒ∏(x1, . . . , x n).
we call bŒ∏ the estimator of the true parameter Œ∏.
example 8.11 . let x1, . . . , x nbe gaussian i.i.d. random variables with unknown
mean Œ∏and known variance œÉ2. construct two possible estimators.
solution . we define two estimators:
bŒ∏1(x1, . . . , x n) =1
nnx
n=1xn,
bŒ∏2(x1, . . . , x n) =x1,
in the first case, the estimator takes all the samples and constructs the sample average.
the second estimator takes all the samples and returns on the first element. both are
legitimate estimators. however, bŒ∏1is the ml estimator, whereas bŒ∏2is not.
8.2.2 unbiased estimators
while you can define estimators in any way you like, certain estimators are good and others
are bad. by ‚Äúgood‚Äù we mean that the estimator can provide you with the information about
the true parameter Œ∏; otherwise, why would you even construct such an estimator? however,
the difficulty here is that bŒ∏ is a random variable because it is constructed from x1, . . . , x n.
therefore, we need to define different metrics to quantify the usefulness of the estimators.
definition 8.5. an estimator bŒ∏isunbiased if
e[bŒ∏] = Œ∏. (8.24)
unbiasedness means that the average of the random variable bŒ∏ matches the true
parameter Œ∏. in other words, while we allow bŒ∏ to fluctuate, we expect the average to match
the true Œ∏. if this is not the case, using more measurements will not help us get closer to Œ∏.
example 8.12 . let x1, . . . , x nbe i.i.d. gaussian random variables with a unknown
mean Œ∏. it has been shown that the ml estimator is
bŒ∏ml=1
nnx
n=1xn. (8.25)
is the ml estimator bŒ∏mlunbiased?
4928.2. properties of ml estimates
solution : to check the unbiasedness, we look at the expectation:
e[bŒ∏ml] =1
nnx
n=1e[xn] =1
nnx
n=1Œ∏=Œ∏.
thus,bŒ∏ml=1
npn
n=1xnis an unbiased estimator of Œ∏.
example 8.13 . same as the example before, but this time we consider an estimator
bŒ∏ =x1+x2+ 5. (8.26)
is this estimator unbiased?
solution : in this case,
e[bŒ∏] =e[x1+x2+ 5] = e[x1] +e[x2] + 5 = 2 Œ∏+ 5Ã∏=Œ∏.
therefore, the estimator is biased.
example 8.14 . let x1, . . . , x nbe i.i.d. gaussian random variables with unknown
mean ¬µand unknown variance œÉ2. we have shown that the ml estimators are
b¬µml=1
nnx
n=1xn and bœÉ2
ml=1
nnx
n=1(xn‚àíb¬µml)2.
it is easy to show that e[b¬µml] =¬µ. how about bœÉ2
ml? is it an unbiased estimator?
solution : for simplicity we assume ¬µ= 0 so that e[x2
n] =e[(xn‚àí0)2] =œÉ2.
note that
e[bœÉ2
ml] =1
nnx
n=1
e[x2
n]‚àí2e[b¬µmlxn] +e[b¬µ2
ml]
=1
nnx
n=1Ô£±
Ô£≤
Ô£≥œÉ2‚àí2eÔ£Æ
Ô£∞1
nnx
j=1xjxnÔ£π
Ô£ª+eÔ£Æ
Ô£∞ 
1
nnx
n=1xn!2Ô£π
Ô£ªÔ£º
Ô£Ω
Ô£æ.
by independence, we observe that e[xjxn] =e[xj]e[xn] = 0, for any jÃ∏=n. there-
fore,
eÔ£Æ
Ô£∞1
nnx
j=1xjxnÔ£π
Ô£ª=1
ne
x1xn+¬∑¬∑¬∑+xnxn
=1
n(0 +¬∑¬∑¬∑+œÉ2+¬∑¬∑¬∑+ 0) =œÉ2
n.
493chapter 8. estimation
similarly, we have that
eÔ£Æ
Ô£∞ 
1
nnx
n=1xn!2Ô£π
Ô£ª=1
n2nx
n=1Ô£±
Ô£≤
Ô£≥e[x2
n] +x
jÃ∏=ne[xjxn]Ô£º
Ô£Ω
Ô£æ
=1
n2nx
n=1n
œÉ2+ 0o
=œÉ2
n.
combining everything, we arrive at the result:
e[bœÉ2
ml] =1
nnx
n=1Ô£±
Ô£≤
Ô£≥œÉ2‚àí2eÔ£Æ
Ô£∞1
nnx
j=1xjxnÔ£π
Ô£ª+eÔ£Æ
Ô£∞ 
1
nnx
n=1xn!2Ô£π
Ô£ªÔ£º
Ô£Ω
Ô£æ
=1
nnx
n=1
œÉ2‚àí2œÉ2
n+œÉ2
n
=n‚àí1
nœÉ2,
which is not equal to œÉ2. therefore, bœÉ2
mlis a biased estimator of œÉ2.
in the previous example, it is possible to construct an unbiased estimator for the
variance. to do so, we can use
bœÉ2
unbias =1
n‚àí1nx
n=1(xn‚àíb¬µml)2, (8.27)
so that e[bœÉ2
unbias ] =œÉ2. however, note that bœÉ2
unbias does not maximize the likelihood, so while
you can get unbiasedness, you cannot maximize the likelihood. if you want to maximize the
likelihood, you cannot get unbiasedness.
what is an unbiased estimator?
¬àan estimator bŒ∏ is unbiased if e[bŒ∏] = Œ∏.
¬àunbiased means that the statistical average of bŒ∏ is the true parameter Œ∏.
¬àifxn‚àºgaussian( Œ∏, œÉ2), then bŒ∏ = (1 /n)pn
n=1xnis unbiased, but bŒ∏ = x1is
biased.
8.2.3 consistent estimators
by definition, an estimator bŒ∏(x1, . . . , x n) is a function of nrandom variables x1, . . . , x n.
therefore, bŒ∏(x1, . . . , x n) changes as ngrows. in this subsection we analyze how bŒ∏ behaves
when nchanges. for notational simplicity we use the following notation:
bŒ∏n=bŒ∏(x1, . . . , x n). (8.28)
thus, as nincreases, we use more random variables in defining bŒ∏(x1, . . . , x n).
4948.2. properties of ml estimates
definition 8.6. an estimator bŒ∏nisconsistent ifbŒ∏np‚àí‚ÜíŒ∏, i.e.,
lim
n‚Üí‚àûpbŒ∏n‚àíŒ∏‚â•œµ
= 0. (8.29)
the definition here follows from our discussions of the law of large numbers in chapter 6.
the specific type of convergence is known as the convergence in probability . it says that
asngrows, the estimator bŒ∏ will be close enough to Œ∏so that the probability of getting a
large deviation will diminish, as illustrated in figure 8.13 .
-5 -4 -3 -2 -1 0 1 2 3 4 500.20.40.60.811.2
-5 -4 -3 -2 -1 0 1 2 3 4 500.20.40.60.811.2
(a)n= 1 (b) n= 2
-5 -4 -3 -2 -1 0 1 2 3 4 500.20.40.60.811.2
-5 -4 -3 -2 -1 0 1 2 3 4 500.20.40.60.811.2
(c)n= 4 (d) n= 8
figure 8.13: the four subfigures here illustrate the probability of error p
|bŒ∏n‚àíŒ∏| ‚â•œµ
, which is
represented by the areas shaded in blue. we assume that the estimator bŒ∏nis a gaussian random
variable following a distribution gaussian (0,œÉ2
n), where we set œÉ= 1. the threshold we use in this
figure is œµ= 1. asngrows, we see that the probability of error diminishes. if the probability of error
goes to zero, we say that the estimator is consistent .
the examples in figure 8.13 are typical situations for an estimator based on the
sample average. for example, if we assume that x1, . . . , x nare i.i.d. gaussian copies of
gaussian(0 , œÉ2), then the estimator
bŒ∏(x1, . . . , x n) =1
nnx
n=1xn
will follow a gaussian distribution gaussian(0 ,œÉ2
n). (please refer to chapter 6 for the deriva-
tion.) then, as ngrows, the pdf of bŒ∏nbecomes narrower and narrower. for a fixed œµ, it
follows that the probability of error will diminish to zero. in fact, we can prove that, for this
495chapter 8. estimation
example,
pbŒ∏n‚àíŒ∏‚â•œµ
=p
bŒ∏n‚àíŒ∏‚â•œµ
+p
bŒ∏n‚àíŒ∏‚â§ ‚àíœµ
=z‚àû
Œ∏+œµgaussian
zŒ∏,œÉ2
n
dz+zŒ∏‚àíœµ
‚àí‚àûgaussian
zŒ∏,œÉ2
n
dz
=z‚àû
Œ∏+œµ1p
2œÄœÉ2/ne‚àí(z‚àíŒ∏)2
2œÉ2/ndz+zŒ∏‚àíœµ
‚àí‚àû1p
2œÄœÉ2/ne‚àí(z‚àíŒ∏)2
2œÉ2/ndz
=z‚àû
œµ
œÉ/‚àö
n1‚àö
2œÄe‚àíz2
2dz+z‚àíœµ
œÉ/‚àö
n
‚àí‚àû1‚àö
2œÄe‚àíz2
2dz
= 1‚àíœÜœµ
œÉ/‚àö
n
+ œÜ‚àíœµ
œÉ/‚àö
n
= 2œÜ‚àíœµ
œÉ/‚àö
n
.
therefore, as n‚Üí ‚àû , it holds that‚àíœµ
œÉ/‚àö
n‚Üí ‚àí‚àû . hence,
lim
n‚Üí‚àûpbŒ∏n‚àíŒ∏‚â•œµ
= lim
n‚Üí‚àû2œÜ‚àíœµ
œÉ/‚àö
n
= 0.
this explains why in figure 8.13 the probability of error diminishes to zero as ngrows.
therefore, we say that bŒ∏nisconsistent .
in general, there are two ways to check whether an estimator is consistent:
¬àprove convergence in probability . this is based on the definition of a consistent
estimator. if we can prove that
lim
n‚Üí‚àûp
|bŒ∏n‚àíŒ∏| ‚â•œµ
= 0, (8.30)
then we say that the estimator is consistent.
¬àprove convergence in mean squared error :
lim
n‚Üí‚àûe[(bŒ∏n‚àíŒ∏)2] = 0. (8.31)
to see why convergence in the mean squared error is sufficient to guarantee consistency,
we recall chebyshev‚Äôs inequality in chapter 6, which says that
p
|bŒ∏n‚àíŒ∏| ‚â•œµ
‚â§e[(bŒ∏n‚àíŒ∏)2]
œµ2.
thus, if lim n‚Üí‚àûe[(bŒ∏n‚àíŒ∏)2] = 0, convergence in probability will also hold. how-
ever, since mean square convergence is stronger than convergence in probability, being
unable to show mean square convergence does not imply that an estimator is incon-
sistent.
be careful not to confuse a consistent estimator and an unbiased estimator. the two
are different concepts; one does not imply the other.
4968.2. properties of ml estimates
consistent versus unbiased
¬àconsistent = if you have enough samples, then the estimator bŒ∏ will converge to
the true parameter.
¬àunbiasedness does not imply consistency. for example (gaussian), if
bŒ∏ =x1,
thene[x1] =¬µ. butp[|bŒ∏‚àí¬µ|> œµ] does not converge to 0 as ngrows. so this
estimator is inconsistent. (see example 8.16 below.)
¬àconsistency does not imply unbiasedness. for example (gaussian), if
bŒ∏ =1
nnx
n=1(xn‚àí¬µ)2
is a biased estimate for variance, but it is consistent. (see example 8.17 below.)
example 8.15 . let x1, . . . , x nbe i.i.d. gaussian random variables with an unknown
mean ¬µand known variance œÉ2. we know that the ml estimator for the mean is
b¬µml= (1/n)pn
n=1xn. isb¬µmlconsistent?
solution . we have shown that the ml estimator is
b¬µml=1
nnx
n=1xn.
sincee[b¬µml] =¬µ, ande[(b¬µml‚àí¬µ)2] = var[ b¬µml] =œÉ2
n, it follows that
p
|b¬µml‚àí¬µ| ‚â•œµ
‚â§e[(b¬µml‚àí¬µ)2]
œµ2=œÉ2
nœµ2.
thus, when ngoes to infinity, the probability converges to zero, and hence the esti-
mator is consistent.
example 8.16 . let x1, . . . , x nbe i.i.d. gaussian random variables with an unknown
mean ¬µand known variance œÉ2. define an estimator b¬µ=x1. show that the estimator
is unbiased but inconsistent.
solution . we know that e[b¬µ] =e[x1] =¬µ. sob¬µis an unbiased estimator. however,
we can show that
e[(b¬µ‚àí¬µ)2] =e[(x1‚àí¬µ)2] =œÉ2.
since this variance e[(b¬µ‚àí¬µ)2] does not shrink as nincreases, it follows that no matter
497chapter 8. estimation
how many samples we use we cannot make e[(b¬µ‚àí¬µ)2] go to zero. to be more precise,
p
|b¬µ‚àí¬µ| ‚â•œµ
=p
|x1‚àí¬µ| ‚â•œµ
=p
x1‚â§¬µ‚àíœµ
+p
x1‚â•¬µ+œµ
=z¬µ‚àíœµ
‚àí‚àû1‚àö
2œÄœÉ2e‚àí(x‚àí¬µ)2
2œÉ2dx+z‚àû
¬µ+œµ1‚àö
2œÄœÉ2e‚àí(x‚àí¬µ)2
2œÉ2dx
= 2œÜ‚àíœµ
œÉ
,
which does not converge to zero as n‚Üí ‚àû . so the estimator is inconsistent.
example 8.17 . let x1, . . . , x nbe i.i.d. gaussian random variables with an unknown
mean ¬µand an unknown variance œÉ2. is the ml estimate of the variance, i.e., bœÉ2
ml,
consistent?
solution . we know that the ml estimator for the mean is
b¬µml=1
nnx
n=1xn,
and we have shown that it is an unbiased and consistent estimator of the mean. for
the variance,
bœÉ2
ml=1
nnx
n=1(xn‚àíb¬µml)2=1
nnx
n=1
x2
n‚àí2b¬µmlxn+b¬µ2
ml
=1
nnx
n=1x2
n‚àí2b¬µml¬∑1
nnx
n=1xn+b¬µ2
ml
=1
nnx
n=1x2
n‚àíb¬µ2
ml.
note that1
npn
n=1x2
nis the sample average of the second moment, and so by the
weak law of large numbers it should converge in probability to e[x2
n]. similarly, b¬µml
will converge in probability to ¬µ. therefore, we have
bœÉ2
ml=1
nnx
n=1x2
n‚àíb¬µ2
mlp‚àí‚Üí(œÉ2+¬µ2)‚àí¬µ2=œÉ2.
thus, we have shown that the ml estimator of the variance is biased but consistent.
4988.2. properties of ml estimates
the following discussions about the consistency of ml estimators can be skipped.
as we have said, there are many estimators. some estimators are consistent and some
are not. the ml estimators are special. it turns out that under certain regularity conditions
the ml estimators of i.i.d. observations are consistent.
without proving this result formally, we highlight a few steps to illustrate the idea.
suppose that we have a set of i.i.d. data points x1, . . . ,xndrawn from some distribution
f(x,|Œ∏true). to formulate the ml estimation, we consider the log-likelihood function (di-
vided by n):
1
nlogl(Œ∏|x) =1
nnx
n=1logf(xn;Œ∏). (8.32)
here, the variable Œ∏is unknown. we need to find it by maximizing the log-likelihood.
by the weak law of large numbers, we can show that the log-likelihood based on the
nsamples will converge in probability to
1
nnx
n=1logf(xn;Œ∏)
| {z }
gn(Œ∏)p‚àí‚Üíe[logf(x;Œ∏)]. (8.33)
the expectation can be evaluated by integrating over the true distribution:
e[logf(x;Œ∏)] =z
logf(x;Œ∏)¬∑f(x;Œ∏true)dx
| {z }
g(Œ∏).
where f(x;Œ∏true) denotes the true distribution of the samples xn‚Äôs. from these two results
we define two functions:
gn(Œ∏)def=1
nnx
n=1logf(xn;Œ∏),andg(Œ∏)def=z
logf(x;Œ∏)¬∑f(x;Œ∏true)dx,
and we know that gn(Œ∏)p‚àí‚Üíg(Œ∏).
we also know that bŒ∏mlis the ml estimator, and so
bŒ∏ml= argmax
Œ∏gn(Œ∏).
letŒ∏‚àóbe the maximizer of the limiting function, i.e.,
Œ∏‚àó= argmax
Œ∏g(Œ∏).
because gn(Œ∏)p‚Üíg(Œ∏), we can (loosely3) argue that bŒ∏mlp‚ÜíŒ∏‚àó. if we can show that
Œ∏‚àó=Œ∏true, then we have shown that bŒ∏mlp‚ÜíŒ∏true, implying that bŒ∏mlis consistent.
3to rigorously prove this statement we need some kind of regularity conditions on gnandg. a more
formal proof can be found in h. vincent poor, an introduction signal detection and estimation , springer,
1998, section iv.d.
499chapter 8. estimation
to show that Œ∏‚àó=Œ∏true, we note that
d
dŒ∏z
logf(x;Œ∏)¬∑f(x;Œ∏true)dx=zd
dŒ∏logf(x;Œ∏)¬∑f(x;Œ∏true)dx
=zf‚Ä≤(x;Œ∏)
f(x;Œ∏)¬∑f(x;Œ∏true)dx.
we ask whether this is equal to zero. putting Œ∏=Œ∏true, we have that
zf‚Ä≤(x;Œ∏true)
f(x;Œ∏true)¬∑f(x;Œ∏true)dx=z
f‚Ä≤(x;Œ∏true)dx.
however, this integral can be simplified to
z
f‚Ä≤(x;Œ∏true)dx=d
dŒ∏z
f(x;Œ∏)dx
|{z}
=1
Œ∏=Œ∏true= 0.
therefore, Œ∏trueis the maximizer for g(Œ∏), and so Œ∏true=Œ∏‚àó.
end of the discussion. please join us again.
8.2.4 invariance principle
another useful property satisfied by the ml estimate is the invariance principle . the in-
variance principle says that a monotonic transformation of the true parameter is preserved
for the ml estimates.
what is the invariance principle?
¬àthere is a monotonic function h.
¬àthere is an ml estimate bŒ∏mlforŒ∏.
¬àthe monotonic function hmaps the true parameter Œ∏7‚àí‚Üíh(Œ∏).
¬àthen the same function will map the ml estimate bŒ∏ml7‚àí‚Üíh(bŒ∏ml).
the formal statement of the invariance principle is given by the theorem below.
theorem 8.1. ifbŒ∏mlis the ml estimate of Œ∏, then for any one-to-one function h
ofŒ∏, the ml estimate of h(Œ∏)ish(bŒ∏ml).
proof . define the likelihood function l(Œ∏) (we have dropped xto simplify the notation).
then, for any monotonic function h, we have that
l(Œ∏) =l(h‚àí1(h(Œ∏))).
5008.2. properties of ml estimates
letbŒ∏mlbe the ml estimate:
bŒ∏ml= argmax
Œ∏l(Œ∏) = argmax
Œ∏l(h‚àí1(h(Œ∏))).
by the definition of ml, bŒ∏mlmust maximize the likelihood. therefore, l(h‚àí1(h(Œ∏))) is
maximized when h‚àí1(h(Œ∏)) =bŒ∏ml. this implies that h(Œ∏) =h(bŒ∏ml) because his monotonic.
since h(Œ∏) is the parameter we try to estimate, the equality h(Œ∏) =h(bŒ∏ml) implies that
h(bŒ∏ml) is the ml estimate of h(Œ∏). ‚ñ°
example 8.18 . consider the single-photon image sensor example we discussed in
section 8.1. we consider a set of i.i.d. bernoulli random variables with pmf
pxn(1) = 1 ‚àíe‚àíŒ∑and pxn(0) = e‚àíŒ∑. (8.34)
find the ml estimate through (a) direct calculation and (b) the invariance principle.
solution . (a) following the example in equation (8.12), the ml estimate of Œ∑is
bŒ∑ml= argmax
Œ∑ny
n=1(1‚àíe‚àíŒ∑)xn(e‚àíŒ∑)1‚àíxn=‚àílog 
1‚àí1
nnx
n=1xn!
.
(b) we can obtain the same result using the invariance principle. since xnis a
binary random variable, we assume that it is a bernoulli with parameter Œ∏. then the
ml estimate of Œ∏is
bŒ∏ml= argmax
Œ∏ny
n=1Œ∏xn(1‚àíŒ∏)1‚àíxn
=1
nnx
n=1xn.
the relationship between Œ∏andŒ∑is that Œ∏= 1‚àíe‚àíŒ∑, orŒ∑=‚àílog(1‚àíŒ∏). so we let
h(Œ∏) =‚àílog(1‚àíŒ∏). the invariance principle says that the ml estimate of h(Œ∏) is
bŒ∑mldef=dh(Œ∏)ml(i)=h(bŒ∏ml)
=‚àílog 
1‚àí1
nnx
n=1xn!
,
where (i) follows from the invariance principle.
the invariance principle can be very convenient, especially when the transformation his
complicated, so that a direct evaluation of the ml estimate is difficult.
the invariance principle is portrayed in figure 8.14 . we start with the bernoulli log-
likelihood
logl(Œ∏|s) =slogŒ∏+ (1‚àís) log(1 ‚àíŒ∏).
501chapter 8. estimation
figure 8.14: the invariance principle is a transformation of the ml estimate. in this example, we
consider a bernoulli log-likelihood function shown in the lowermost plot. for this log-likelihood, the ml
estimate is bŒ∏ml= 0.4. on the left-hand side we show another log-likelihood, derived for a truncated
poisson random variable. note that the ml estimate is bŒ∑ml= 0.5108. the invariance principle asserts
that, instead of computing these ml estimates directly, we can first derive the relationship between Œ∑
andŒ∏for any Œ∏. since we know that Œ∏= 1‚àíe‚àíŒ∑, it follows that Œ∑=‚àílog(1‚àíŒ∏). we define this
transformation as Œ∑=h(Œ∏) =‚àílog(1‚àíŒ∏). then the ml estimate is bŒ∑ml=h(bŒ∏ml) =h(0.4) = 0 .5108.
the invariance principle saves us the trouble of computing the maximization of the more truncated
poisson likelihood.
in this particular example we let s= 20, where sdenotes the sum of the n= 50 bernoulli
random variables. the other log-likelihood is the truncated poisson, which is given by
logl(Œ∑|s) =slog(1‚àíe‚àíŒ∑) + (1 ‚àís) log( e‚àíŒ∑).
the transformation between the two is the function Œ∑=h(Œ∏) =‚àílog(1‚àíŒ∏). putting
everything into the figure, we see that the ml estimate ( Œ∏= 0.4) is translated to Œ∑= 0.5108.
the invariance principle asserts that this calculation can be done by bŒ∑ml=h(bŒ∏ml) =
h(0.4) =‚àí0.5108.
8.3 maximum a posteriori estimation
in ml estimation, the parameter Œ∏is treated as a deterministic quantity. there are, however,
many situations where we have some prior knowledge about Œ∏. for example, we may not
know exactly the speed of a car, but we may know that the speed is roughly 65 mph
5028.3. maximum a posteriori estimation
with a standard deviation of 5 mph. how do we incorporate such prior knowledge into the
estimation problem?
in this section, we introduce the second estimation technique, known as the maximum
a posteriori (map) estimation. map estimation links the likelihood and the prior. the key
idea is to treat the parameter Œ∏as a random variable (vector) Œ∏with a pdf fŒ∏(Œ∏).
8.3.1 the trio of likelihood, prior, and posterior
to understand how the map estimation works, it is important first to understand the role
of the parameter Œ∏, which changes from a deterministic quantity to a random quantity.
recall the likelihood function we defined in the ml estimation; it is
l(Œ∏|x) =fx(x;Œ∏),
if we assume that we have a set of i.i.d. observations x= [x1, . . . , x n]t. by writing the pdf
ofxasfx(x;Œ∏), we emphasize that Œ∏is a deterministic but unknown parameter. there
is nothing random about Œ∏.
in map, we change the nature of Œ∏from deterministic to random. we replace Œ∏byŒ∏
and write
fx(x;Œ∏)becomes=‚áí fx|Œ∏(x|Œ∏). (8.35)
the difference between the left-hand side and the right-hand side is subtle but important.
on the left-hand side, fx(x;Œ∏) is the pdf of x. this pdf is parameterized by Œ∏. on the
right-hand side, fx|Œ∏(x|Œ∏) is a conditional pdf of xgiven Œ∏. the values they provide
are exactly the same. however, in fx|Œ∏(x|Œ∏),Œ∏is a realization of a random variable Œ∏.
because Œ∏is now a random variable (vector), we can define its pdf (yes, the pdf
ofŒ∏), and denote it by
fŒ∏(Œ∏), (8.36)
which is called the prior distribution. the prior distribution of Œ∏is unique in map estima-
tion. there is nothing called a prior in ml estimation.
multiplying fx|Œ∏(x|Œ∏) with the prior pdf fŒ∏(Œ∏), and using bayes‚Äô theorem, we
obtain the posterior distribution:
fŒ∏|x(Œ∏|x) =fx|Œ∏(x|Œ∏)fŒ∏(Œ∏)
fx(x). (8.37)
the posterior distribution is the pdf of Œ∏given the measurements x.
the likelihood, the prior, and the posterior can be confusing. let us clarify their mean-
ings.
¬àlikelihood fx|Œ∏(x|Œ∏): this is the conditional probability density of xgiven the pa-
rameter Œ∏. do not confuse the likelihood fx|Œ∏(x|Œ∏) defined in the map context
and the likelihood fx(x;|Œ∏) defined in the ml context. the former assumes that Œ∏
is random whereas the latter assumes that Œ∏is deterministic. they have the same
values.
¬àprior fŒ∏(Œ∏): this is the prior distribution of Œ∏. it does not come from the data x
but from our prior knowledge. for example, if we see a bike on the road, even before
we take any measurement we will have a rough idea of its speed. this is the prior
distribution.
503chapter 8. estimation
¬àposterior fŒ∏|x(Œ∏|x): this is the posterior density of Œ∏given that we have observed x.
do not confuse fŒ∏|x(Œ∏|x) andl(Œ∏|x). the posterior distribution fŒ∏|x(Œ∏|x) is a pdf
ofŒ∏given x=x. the likelihood l(Œ∏|x) is not a pdf. if you integrate fŒ∏|x(Œ∏|x)
with respect to Œ∏, you get 1, but if you integrate l(Œ∏|x) with respect to Œ∏, you do
not get 1.
what is the difference between ml and map?
likelihood ml fx(x;Œ∏) the parameter Œ∏is deterministic.
map fx|Œ∏(x|Œ∏) the parameter Œ∏is random.
prior ml there is no prior, because Œ∏is deterministic.
map fŒ∏(Œ∏) this is the pdf of Œ∏.
optimization ml find the peak of the likelihood fx(x;Œ∏).
map find the peak of the posterior fŒ∏|x(Œ∏|x).
maximum a posteriori (map) estimation is a form of bayesian estimation. bayesian
methods emphasize our prior knowledge or beliefs about the parameters. as we will see
shortly, the prior has something valuable to offer, especially when we have very few data
points.
8.3.2 understanding the priors
since the biggest difference between map and ml is the addition of the prior fŒ∏(Œ∏), we
need to take a closer look at what they mean. in figure 8.15 below, we show a set of six
different priors. we ask two questions: (1) what do they mean? (2) which one should we
use?
figure 8.15: this figure illustrates six different examples of the prior distribution fŒ∏(Œ∏), when the prior
is a 1d parameter Œ∏. the prior distribution fŒ∏(Œ∏)is the pdf of Œ∏. (a) fŒ∏(Œ∏) =Œ¥(Œ∏), which is a delta
function. (b) fŒ∏(Œ∏) =1
b‚àíafora‚â§Œ∏‚â§b. this is a uniform distribution. (c) this is also a uniform
distribution, but the spread is very wide. (d) fŒ∏(Œ∏) =gaussian (0, œÉ2), which is a zero-mean gaussian.
(e) the same gaussian, but with a different mean. (f) a gaussian with zero mean, but a large variance.
5048.3. maximum a posteriori estimation
what does the shape of a prior tell us?
it tells us your belief as to how the underlying parameter Œ∏should be distributed.
the meaning of this statement can be best understood from the examples shown in fig-
ure 8.15 :
¬àfigure 8.15 (a). this is a delta prior fŒ∏(Œ∏) =Œ¥(Œ∏) (or fŒ∏(Œ∏) =Œ¥(Œ∏‚àíŒ∏0)). if you use this
prior, you are absolutely sure that the parameter Œ∏ takes a specific value. there is no
uncertainty about your belief. since you are so confident about your prior knowledge,
you will ignore the likelihood that is constructed from the data. no one will use a
delta prior in practice.
¬àfigure 8.15 (b).fŒ∏(Œ∏) =1
b‚àíafora‚â§Œ∏‚â§b, and is zero otherwise. this is a bounded
uniform prior. you do not have any preference for the parameter Œ∏, but you do know
from your prior experience that a‚â§Œ∏‚â§b.
¬àfigure 8.15 (c). this prior is the same as (b) but is short and very wide. if you use
this prior, it means that you know nothing about the parameter. so you give up the
prior and let the likelihood dominate the map estimate.
¬àfigure 8.15 (d).fŒ∏(Œ∏) = gaussian(0 , œÉ2). you use this prior when you know something
about the parameter, e.g., that it is centered at certain location and you have some
uncertainty.
¬àfigure 8.15 (e). same as (d), but the parameter is centered at some other location.
¬àfigure 8.15 (f). same as (d), but you have less confidence about the parameter.
as you can see from these examples, the shape of the prior tells us how youwant Œ∏ to be
distributed. the choice you make will directly influence the map optimization, and hence
the map estimate.
since the prior is a subjective quantity in the map framework, you as the user have
the freedom to choose whatever you like. for instance, if you have conducted a similar
experiment before, you can use the results of the previous experiments as the current prior.
another strategy is to go with physics. for instance, we can argue that Œ∏should be sparse
so that it contains as few non-zeros as possible. in this case, a sparsity-driven prior, such
asfŒ∏(Œ∏) = exp {‚àí‚à•Œ∏‚à•1}, could be a choice. the third strategy is to choose a prior that is
computationally ‚Äúfriendlier‚Äù, e.g., in quadratic form so that the map is differentiable. one
such choice is the conjugate prior . we will discuss this later in section 8.3.6.
which prior should we choose?
¬àbased on your preference, e.g., you know from historical data that the parameter
should behave in certain ways.
¬àbased on physics, e.g., the parameter has a physical interpretation, so you need
to abide by the physical laws.
¬àchoose a prior that is computationally ‚Äúfriendlier‚Äù. this is the topic of the
conjugate prior , which is a prior that does not change the form of the posterior
distribution. (we will discuss this later in section 8.3.6.)
505chapter 8. estimation
8.3.3 map formulation and solution
our next task is to study how to formulate the map problem and how to solve it.
definition 8.7. letx= [x1, . . . , x n]tbe i.i.d. observations. let Œ∏be a random
parameter. the maximum-a-posteriori estimate of Œ∏is
bŒ∏map =argmax
Œ∏fŒ∏|x(Œ∏|x). (8.38)
philosophically speaking, ml and map have two different goals. ml considers a para-
metric model with a deterministic parameter. its goal is to find the parameter that maximizes
the likelihood for the data we have observed. map also considers a parametric model but
the parameter Œ∏is random. because Œ∏is random, we are finding one particular state Œ∏of
the parameter Œ∏that offers the best explanation conditioned on the data xwe observe.
in a sense, the two optimization problems are
bŒ∏ml= argmax
Œ∏fx|Œ∏(x|Œ∏),
bŒ∏map = argmax
Œ∏fŒ∏|x(Œ∏|x).
this pair of equations is interesting, as the pair tells us that the difference between the ml
estimation and the map estimation is the flipped order of xandŒ∏.
there are two reasons we care about the posterior. first, in map the posterior allows
us to incorporate the prior. ml does not allow a prior. a prior can be useful when the
number of samples is small. second, maximizing the posterior does have some physical
interpretations. map asks for the probability of Œ∏=Œ∏after observing ntraining samples
x=x. ml asks for the probability of observing x=xgiven a parameter Œ∏. both are
correct and legitimate criteria, but sometimes we might prefer one over the other.
to solve the map problem, we notice that
bŒ∏map = argmax
Œ∏fŒ∏|x(Œ∏|x)
= argmax
Œ∏fx|Œ∏(x|Œ∏)fŒ∏(Œ∏)
fx(x)
= argmax
Œ∏fx|Œ∏(x|Œ∏)fŒ∏(Œ∏), f x(x) does not contain Œ∏
= argmax
Œ∏logfx|Œ∏(x|Œ∏) + log fŒ∏(Œ∏).
therefore, what map adds is the prior log fŒ∏(Œ∏). if you use an uninformative prior, e.g., a
prior with extremely wide support, then the map estimation will return more or less the
same result as the ml estimation.
when does map = ml?
¬àthe relation ‚Äú=‚Äù does not make sense here, because Œ∏is random in map but
deterministic in ml.
¬àsolution of map optimization = solution of ml optimization, when fŒ∏(Œ∏) is
uniform over the parameter space.
5068.3. maximum a posteriori estimation
¬àin this case, fŒ∏(Œ∏) = constant and so it can be dropped from the optimization.
example 8.19 . let x1, . . . , x nbe i.i.d. random variables with a pdf fxn|Œ∏(xn|Œ∏)
for all n, and Œ∏ be a random parameter with pdf fŒ∏(Œ∏):
fxn|Œ∏(xn|Œ∏) =1‚àö
2œÄœÉ2exp
‚àí(xn‚àíŒ∏)2
2œÉ2
,
fŒ∏(Œ∏) =1p
2œÄœÉ2
0exp
‚àí(Œ∏‚àí¬µ0)2
2œÉ2
0
.
find the map estimate.
solution . the map estimate is
bŒ∏map = argmax
Œ∏"ny
n=11‚àö
2œÄœÉ2exp
‚àí(xn‚àíŒ∏)2
2œÉ2#
√ó"
1p
2œÄœÉ2
0exp
‚àí(Œ∏‚àí¬µ0)2
2œÉ2
0#
= argmax
Œ∏1‚àö
2œÄœÉ2n
√ó1p
2œÄœÉ2
0exp(
‚àínx
n=1(xn‚àíŒ∏)2
2œÉ2‚àí(Œ∏‚àí¬µ0)2
2œÉ2
0)
.
since the maximizer is not changed by any monotonic function, we apply logarithm
to the above equations. this yields
bŒ∏map = argmax
Œ∏
‚àín
2log 
2œÄœÉ2
‚àí1
2log(2œÄœÉ2
0)
‚àínx
n=1(xn‚àíŒ∏)2
2œÉ2‚àí(Œ∏‚àí¬µ0)2
2œÉ2
0
.
constants in the maximization do not matter. so by dropping the constant terms we
obtain
bŒ∏map = argmax
Œ∏(
‚àínx
n=1(xn‚àíŒ∏)2
2œÉ2‚àí(Œ∏‚àí¬µ0)2
2œÉ2
0)
. (8.39)
it now remains to solve the maximization. to this end we take the derivative w.r.t. Œ∏
and show that
d
dŒ∏(
‚àínx
n=1(xn‚àíŒ∏)2
2œÉ2‚àí(Œ∏‚àí¬µ0)2
2œÉ2
0)
= 0.
this yields
nx
n=1(xn‚àíŒ∏)
œÉ2‚àíŒ∏‚àí¬µ0
œÉ2
0= 0.
507chapter 8. estimation
rearranging the terms gives us the final result:
bŒ∏map =œÉ2
0
1
npn
n=1xn
+œÉ2
n¬µ0
œÉ2
0+œÉ2
n. (8.40)
practice exercise 8.7 . prove that if fŒ∏(Œ∏) =Œ¥(Œ∏‚àíŒ∏0), the map estimate is bŒ∏map =
Œ∏0.
solution . iffŒ∏(Œ∏) =Œ¥(Œ∏‚àíŒ∏0), then
bŒ∏map = argmax
Œ∏logfx|Œ∏(x|Œ∏) + log fŒ∏(Œ∏)
= argmax
Œ∏logfx|Œ∏(x|Œ∏) + log Œ¥(Œ∏‚àíŒ∏0)
=Ô£±
Ô£≤
Ô£≥argmax
Œ∏logfx|Œ∏(x|Œ∏)‚àí ‚àû, Œ∏Ã∏=Œ∏0.
argmax
Œ∏logfx|Œ∏(x|Œ∏) + 0, Œ∏=Œ∏0.
thus, if bŒ∏mapÃ∏=Œ∏0, the first case says that there is no solution, so we must go with
the second case bŒ∏map =Œ∏0. but if bŒ∏map =Œ∏0, there is no optimization because we
have already chosen bŒ∏map =Œ∏0. this proves the result.
8.3.4 analyzing the map solution
as we said earlier, map offers something that ml does not. to see this, we will use the
result of the gaussian random variables as an example and analyze the map solution as
we change the parameters nandœÉ0. recall that if x1, . . . , x nare i.i.d. gaussian random
variables with unknown mean Œ∏and known variance œÉ, the ml estimate is
bŒ∏ml=1
nnx
n=1xn.
assuming that the parameter Œ∏ is distributed according to a pdf gaussian( ¬µ0, œÉ2
0), we
have shown in the previous subsection that
bŒ∏map =œÉ2
0
1
npn
n=1xn
+œÉ2
n¬µ0
œÉ2
0+œÉ2
n=œÉ2
0bŒ∏ml+œÉ2
n¬µ0
œÉ2
0+œÉ2
n.
in what follows, we will take a look at the behavior of the map estimate bŒ∏map asn
andœÉ0change. the results of our discussion are summarized in figure 8.16 .
first, let‚Äôs look at the effect of n.
how does nchange bŒ∏map?
¬àasn‚Üí ‚àû , the map estimate bŒ∏map‚ÜíbŒ∏ml: if we have enough samples, we
trust the data.
5088.3. maximum a posteriori estimation
(a) effect of n (b) effect of œÉ0
figure 8.16: the map estimate bŒ∏mapswings between the ml estimate bŒ∏mland the prior ¬µ0. (a) when
nincreases, the likelihood is more reliable and so we lean towards the ml estimate. if nis small, we
should trust the prior more than the ml estimate. (b) when œÉ0decreases, we become more confident
about the prior and so we will use it. if œÉ0is large, we use more information from the ml estimate.
¬àasn‚Üí0, the map estimate bŒ∏map‚ÜíŒ∏0. if we do not have any samples, we
trust the prior.
these two results can be demonstrated by taking the limits. as n‚Üí ‚àû , the map estimate
converges to
lim
n‚Üí‚àûbŒ∏map = lim
n‚Üí‚àûœÉ2
0bŒ∏ml+œÉ2
n¬µ0
œÉ2
0+œÉ2
n=bŒ∏ml. (8.41)
this result is not surprising. when we have infinitely many samples, we will completely
rely on the data and make our estimate. thus, the map estimate is the same as the ml
estimate.
when n‚Üí0, the map estimate converges to
lim
n‚Üí0bŒ∏map = lim
n‚Üí0œÉ2
0bŒ∏ml+œÉ2
n¬µ0
œÉ2
0+œÉ2
n=¬µ0. (8.42)
this means that, when we do not have any samples, the map estimate bŒ∏map will completely
use the prior distribution, which has a mean ¬µ0.
the implication of this result is that map offers a natural swing between bŒ∏mlandbŒ∏0,
controlled by n. where does this ncome from? if we recall the derivation of the result, we
note that the naffects the likelihood term through the number of samples:
bŒ∏map = argmax
Œ∏
‚àínx
n=1(xn‚àíŒ∏)2
2œÉ2
|{z }
nterms here‚àí(Œ∏‚àí¬µ0)2
2œÉ2
0|{z}
1 term
.
thus, as nincreases, the influence of the data term grows, and so the result will gradually
shift towards bŒ∏ml.
figure 8.17 illustrates a numerical experiment in which we draw nrandom samples
x1, . . . , x naccording to a gaussian distribution gaussian( Œ∏, œÉ2), with œÉ= 1. we assume
that the prior distribution is gaussian( ¬µ0, œÉ2
0), with ¬µ0= 0 and œÉ0= 0.25. the ml estimate
of this problem is bŒ∏ml=1
npn
n=1xn, whereas the map estimate is given by equation (8.40).
509chapter 8. estimation
the figure shows the resulting pdfs. a helpful analogy is that the prior and the likelihood
are pulling a rope in two opposite directions. as ngrows, the force of the likelihood increases
and so the influence becomes stronger.
(a)n= 1 (b) n= 50
figure 8.17: the subfigures show the prior distribution fŒ∏(Œ∏)and the likelihood function fx|Œ∏(x|Œ∏),
given the observed data. (a) when n= 1, the estimated posterior distribution fŒ∏|x(Œ∏|x)is pulled
towards the prior. (b) when n= 50 , the posterior is pulled towards the ml estimate. the analogy for
the situation is that each data point is acting as a small force against the big force of the prior. as n
grows, the small forces of the data points accumulate and eventually dominate.
we next look at the effect of œÉ0.
how does œÉ0change bŒ∏map?
¬àasœÉ0‚Üí ‚àû , the map estimate bŒ∏map‚ÜíbŒ∏ml: if we have doubts about the prior,
we trust the data.
¬àasœÉ0‚Üí0, the map estimate bŒ∏map‚ÜíŒ∏0. if we are absolutely sure about the
prior, we ignore the data.
when œÉ0‚Üí ‚àû , the limit of bŒ∏map is
lim
œÉ0‚Üí‚àûbŒ∏map = lim
œÉ0‚Üí‚àûœÉ2
0bŒ∏ml+œÉ2
n¬µ0
œÉ2
0+œÉ2
n=bŒ∏ml. (8.43)
the reason why this happens is that œÉ0is the uncertainty level of the prior. if œÉ0is high,
we are not certain about the prior. in this case, map chooses to follow the ml estimate.
when œÉ0‚Üí0, the limit of bŒ∏map is
lim
œÉ0‚Üí0bŒ∏map = lim
œÉ0‚Üí0œÉ2
0bŒ∏ml+œÉ2
n¬µ0
œÉ2
0+œÉ2
n=¬µ0. (8.44)
note that when œÉ0‚Üí0, we are essentially saying that we are absolutely sure about the
prior. if we are so sure about the prior, there is no need to look at the data. in that case
the map estimate is ¬µ0.
5108.3. maximum a posteriori estimation
the way to understand the influence of œÉ0is to inspect the equation:
bŒ∏map = argmax
Œ∏
‚àínx
n=1(xn‚àíŒ∏)2
2œÉ2
|{z }
fixed w.r.t. œÉ0‚àí(Œ∏‚àí¬µ0)2
2œÉ2
0|{z}
changes with œÉ0
.
since œÉ0is purely a preference youdecide, you can control how much trust to put onto the
prior.
(a)œÉ0= 0.1 (b) œÉ0= 1
figure 8.18: the subfigures show the prior distribution fŒ∏(Œ∏)and the likelihood function fx|Œ∏(x|Œ∏),
given the observed data. (a) when œÉ0= 0.1, the estimated posterior distribution fŒ∏|x(Œ∏|x)is pulled
towards the prior. (b) when œÉ0= 1, the posterior is pulled towards the ml estimate. an analogy for
the situation is that the strength of the prior depends on the magnitude of œÉ0. ifœÉ0is small the prior
is strong, and so the influence is large. if œÉ0is large the prior is weak, and so the ml estimate will
dominate.
figure 8.18 illustrates a numerical experiment in which we compare œÉ0= 0.1 and
œÉ0= 1. if œÉ0is small, the prior distribution fŒ∏(Œ∏) becomes similar to a delta function. we
can interpret it as a very confident prior, so confident that we wish to align with the prior.
the situation can be imagined as a game of tug-of-war between a powerful bull and a horse,
which the bull will naturally win. if œÉ0is large the prior distribution will become flat. it
means that we are not very confident about the prior so that we will trust the data. in this
case the map estimate will shift towards the ml estimate.
8.3.5 analysis of the posterior distribution
when the likelihood is multiplied with the prior to form the posterior, what does the poste-
rior distribution look like? to answer this question we continue our gaussian example with
a fixed variance œÉand an unknown mean Œ∏. the posterior distribution is proportional to
fŒ∏|x(Œ∏|x) =fx|Œ∏(x|Œ∏)fŒ∏(Œ∏)
fx(x)‚àùfx|Œ∏(x|Œ∏)fŒ∏(Œ∏)
="ny
n=11‚àö
2œÄœÉ2exp
‚àí(xn‚àíŒ∏)2
2œÉ2#
¬∑"
1p
2œÄœÉ2
0exp
‚àí(Œ∏‚àí¬µ0)2
2œÉ2
0#
.(8.45)
511chapter 8. estimation
performing the multiplication and completing the squares,
nx
n=1(xn‚àíŒ∏)2
2œÉ2+(Œ∏‚àí¬µ0)2
2œÉ2
0=(Œ∏‚àíbŒ∏map)2
2œÉ2
map,
where
bŒ∏map =œÉ2
0bŒ∏ml+œÉ2
n¬µ0
œÉ2
0+œÉ2
n, and1
bœÉ2
map=1
œÉ2
0+n
œÉ2. (8.46)
in other words, the posterior distribution fŒ∏|x(Œ∏|x) is also a gaussian with
fŒ∏|x(Œ∏|x) = gaussian( bŒ∏map,bœÉ2
map).
iffx|Œ∏(x|Œ∏) =gaussian (x;Œ∏, œÉ), and fŒ∏(Œ∏) =gaussian (Œ∏;¬µ0, œÉ2
0), what is the
posterior fŒ∏|x(Œ∏|x)?
the posterior fŒ∏|x(Œ∏|x) is gaussian( bŒ∏map,bœÉ2
map), where
bŒ∏map =œÉ2
0bŒ∏ml+œÉ2
n¬µ0
œÉ2
0+œÉ2
n,and1
bœÉ2
map=1
œÉ2
0+n
œÉ2. (8.47)
the posterior tells us how nandœÉ0will influence the map estimate. as ngrows,
the posterior mean and variance becomes
lim
n‚Üí‚àûbŒ∏map =bŒ∏ml=Œ∏,and lim
n‚Üí‚àûbœÉmap = 0.
as a result, the posterior distribution fŒ∏|x(Œ∏|x) will converge to a delta function centered
at the ml estimate bŒ∏ml. therefore, as we try to solve the map problem by maximizing the
posterior, the map estimate has to improve because bœÉmap‚Üí0.
we can plot the posterior distribution gaussian( bŒ∏map,bœÉ2
map) as a function of the
number of samples n.figure 8.19 illustrates this example using the following configurations.
the likelihood is gaussian with ¬µ= 1, œÉ= 0.25. the prior is gaussian with ¬µ0= 0 and
œÉ= 0.25. we construct the gaussian according to gaussian( bŒ∏map,bœÉ2
map) by varying n.
the result shown in figure 8.19 confirms our prediction: as ngrows, the posterior becomes
more like a delta function whose mean is the true mean ¬µ. the posterior estimator bŒ∏map,
for each n, is the peak of the respective gaussian.
what is the pictorial interpretation of the map estimate?
¬àfor every n, map has a posterior distribution fŒ∏|x(Œ∏|x).
¬àasngrows, fŒ∏|x(Œ∏|x) converges to a delta function centered at bŒ∏ml.
¬àmap tries to find the peak of fŒ∏|x(Œ∏|x). for large n, it returns bŒ∏ml.
5128.3. maximum a posteriori estimation
-1 -0.5 0 0.5 1 1.502468
n = 0
n = 1
n = 2
n = 5
n = 8
n = 12
n = 20
figure 8.19: posterior distribution fŒ∏|x(Œ∏|x) =gaussian (bŒ∏map, œÉ2
map)asngrows. when nis small,
the posterior distribution is dominated by the prior. as nincreases, the posterior distribution changes
its mean and its variance.
8.3.6 conjugate prior
choosing the prior is an important topic in a map estimation. we have elaborated two
‚Äúengineering‚Äù solutions: use your prior experience or follow the physics. in this subsection,
we discuss the third option: to choose something computationally friendly. to explain what
we mean by ‚Äúcomputationally friendly‚Äù, let us consider the following example, thanks to
avinash kak.4
consider a bernoulli distribution with a pdf
fx|Œ∏(x|Œ∏) =ny
n=1Œ∏xn(1‚àíŒ∏)1‚àíxn. (8.48)
to compute the map estimate, we assume that we have a prior fŒ∏(Œ∏). therefore, the map
estimate is given by
bŒ∏map = argmax
Œ∏fx|Œ∏(x|Œ∏)fŒ∏(Œ∏)
= argmax
Œ∏"ny
n=1Œ∏xn(1‚àíŒ∏)1‚àíxn#
¬∑fŒ∏(Œ∏)
= argmax
Œ∏nx
n=1xnlogŒ∏+ (1‚àíxn) log(1 ‚àíŒ∏) + log fŒ∏(Œ∏).
let us consider three options for the prior. which one would you use?
¬àcandidate 1 :fŒ∏(Œ∏) =1‚àö
2œÄœÉ2expn
‚àí(Œ∏‚àí¬µ)2
2œÉ2o
, a gaussian prior. if you choose this
prior, the optimization problem will become
bŒ∏map = argmax
Œ∏nx
n=1
xnlogŒ∏+ (1‚àíxn) log(1 ‚àíŒ∏)
‚àí(Œ∏‚àí¬µ)2
2œÉ2.
4avinash kak ‚Äúml, map, and bayesian ‚Äî the holy trinity of parameter estimation and data pre-
diction‚Äù, https://engineering.purdue.edu/kak/tutorials/trinity.pdf
513chapter 8. estimation
we can still take the derivative and set it to zero. this gives
pn
n=1xn
Œ∏‚àín‚àípn
n=1xn
1‚àíŒ∏=Œ∏‚àí¬µ
œÉ2.
defining s=pn
n=1xnand moving the terms around, we have
(1‚àíŒ∏)œÉ2s‚àíŒ∏œÉ2(n‚àís) =Œ∏(1‚àíŒ∏)(Œ∏‚àí¬µ).
this is a cubic polynomial problem that has a closed-form solution and is also solvable
by a computer. but it‚Äôs also tedious, at least to lazy engineers like ourselves.
¬àcandidate 2 :fŒ∏(Œ∏) =Œª
2e‚àíŒª|Œ∏|, a laplace prior. in this case, the optimization problem
becomes
bŒ∏map = argmax
Œ∏nx
n=1
xnlogŒ∏+ (1‚àíxn) log(1 ‚àíŒ∏)
‚àíŒª|Œ∏|.
welcome to convex optimization! there is no closed-form solution. if you want to solve
this problem, you need to call a convex solver.
¬àcandidate 3 :fŒ∏(Œ∏) =1
cŒ∏Œ±‚àí1(1‚àíŒ∏)Œ≤‚àí1, a beta prior. this prior looks very compli-
cated, but let‚Äôs plug it into our optimization problem:
bŒ∏map = argmax
Œ∏nx
n=1
xnlogŒ∏
+ (1‚àíxn) log(1 ‚àíŒ∏)
+ (Œ±‚àí1) log Œ∏+ (Œ≤‚àí1) log(1 ‚àíŒ∏)
= argmax
Œ∏(s+Œ±‚àí1) log Œ∏+ (n‚àís+Œ≤‚àí1) log(1 ‚àíŒ∏),
where s=pn
n=1xn. taking the derivative and setting it to zero, we have
s+Œ±‚àí1
Œ∏=n‚àís+Œ≤‚àí1
1‚àíŒ∏.
rearranging the terms we obtain the final estimate:
bŒ∏map =s+Œ±‚àí1
n+Œ≤+Œ±‚àí2. (8.49)
there are a number of intuitions that we can draw from this beta prior, but most
importantly, we have obtained a very simple solution. that is because the posterior distri-
bution remains in the same form as the prior, after multiplying by the prior. specifically, if
we use the beta prior, the posterior distribution is
fŒ∏|x(Œ∏|x)‚àùfx|Œ∏(x|Œ∏)fŒ∏(Œ∏)
="ny
n=1Œ∏xn(1‚àíŒ∏)1‚àíxn#
¬∑1
cŒ∏Œ±‚àí1(1‚àíŒ∏)Œ≤‚àí1
=Œ∏s+Œ±‚àí1(1‚àíŒ∏)n‚àís+Œ≤‚àí1.
this is still in the form of Œ∏‚ãÜ‚àí1(1‚àíŒ∏)‚ñ†‚àí1, which is the same as the prior. when this
happens, we call the prior a conjugate prior. in this example, the beta prior is a conjugate
before the bernoulli likelihood.
5148.3. maximum a posteriori estimation
what is a conjugate prior?
¬àit is a prior such that when multiplied by the likelihood to form the posterior,
the posterior fŒ∏|x(Œ∏|x) takes the same form as the prior fŒ∏(Œ∏).
¬àevery likelihood has its conjugate prior.
¬àconjugate priors are not necessarily good priors. they are just computationally
friendly. some of them have good physical interpretations.
we can make a few interpretations of the beta prior, in the context of bernoulli likeli-
hood. first, the beta distribution takes the form
fŒ∏(Œ∏) =1
b(Œ±, Œ≤)Œ∏Œ±‚àí1(1‚àíŒ∏)Œ≤‚àí1, (8.50)
with b(Œ±, Œ≤) is the beta function5. the shape of the beta distribution is shown in fig-
ure 8.20 . for different choices of Œ±andŒ≤, the distribution has a peak located towards
either side of the interval [0 ,1]. for example, if Œ±is large but Œ≤is small, the distribution
fŒ∏(Œ∏) leans towards 1 (the yellow curve).
0 0.2 0.4 0.6 0.8 101234
 = 2,  = 8
 = 3,  = 7
 = 8,  = 2
figure 8.20: beta distribution fŒ∏(Œ∏)for various choices of Œ±andŒ≤. when (Œ±, Œ≤) = (2 ,8), the beta
distribution favors small Œ∏. when (Œ±, Œ≤) = (8 ,2), the beta distribution favors large Œ∏. by swinging
between the (Œ±, Œ≤)pairs, we obtain a prior that has a preference over Œ∏.
as a user, you have the freedom to pick fŒ∏(Œ∏). even if you are restricted to the beta
distribution, you still have plenty of degrees of freedom in choosing Œ±andŒ≤so that your
choice matches your belief. for example, if you know ahead of time that the bernoulli
experiment is biased towards 1 (e.g., the coin is more likely to come up heads), you can
choose a large Œ±and a small Œ≤. by contrast, if you believe that the coin is fair, you choose
Œ±=Œ≤. the parameters Œ±andŒ≤are known as the hyperparameters of the prior distribution.
hyperparameters are parameters for fŒ∏(Œ∏).
5the beta function is defined as b(Œ±, Œ≤) =Œ≥(Œ±)Œ≥(Œ≤)
Œ≥(Œ±+Œ≤), where Œ≥ is the gamma function. for integer n,
Œ≥(n) = (n‚àí1)!
515chapter 8. estimation
example 8.20 . (prior for gaussian mean) consider a gaussian likelihood for a fixed
variance œÉ2and unknown mean Œ∏:
fx|Œ∏(x|Œ∏) =1‚àö
2œÄœÉ2n
exp(
‚àínx
n=1(xn‚àíŒ∏)2
2œÉ2)
.
show that the conjugate prior is given by
fŒ∏(Œ∏) =1p
2œÄœÉ2
0exp
‚àí(Œ∏‚àí¬µ0)2
2œÉ2
0
. (8.51)
solution . we have shown this result previously. by some (tedious) completing squares,
we show that
fŒ∏|x(Œ∏|x) =1p
2œÄœÉ2
nexp
‚àí(Œ∏‚àí¬µn)2
2œÉ2
n
,
where
¬µn=œÉ2
nœÉ2
0+œÉ2¬µ0+nœÉ2
0
nœÉ2
0+œÉ2bŒ∏mland œÉ2
n=œÉ2œÉ2
0
œÉ2+nœÉ2
0.
since fŒ∏|x(Œ∏|x) is in the same form as fŒ∏(Œ∏), we know that fŒ∏(Œ∏) is a conjugate prior.
example 8.21 . (prior for gaussian variance ) consider a gaussian likelihood for a
mean ¬µand unknown variance œÉ2:
fx|œÉ(x|œÉ) =1‚àö
2œÄœÉ2n
exp(
‚àínx
n=1(xn‚àí¬µ)2
2œÉ2)
.
find the conjugate prior.
solution . we first define the precision Œ∏=1
œÉ2. the likelihood is
fx|Œ∏(x|Œ∏) =1‚àö
2œÄœÉ2n
exp(
‚àínx
n=1(xn‚àí¬µ)2
2œÉ2)
=1
(2œÄ)n/2Œ∏n/2exp(
‚àíŒ∏
2nx
n=1(xn‚àí¬µ)2)
.
we propose to choose the prior fŒ∏(Œ∏) as
fŒ∏(Œ∏) =1
Œ≥(a)baŒ∏a‚àí1exp{‚àíbŒ∏},
for some aandb. this fŒ∏(Œ∏) is called the gamma distribution gamma( Œ∏|a, b). we can
show that e[Œ∏] =a
band var[Œ∏] =a
b2. with some (tedious) completing squares, we
5168.3. maximum a posteriori estimation
show that the posterior is
fŒ∏|x(Œ∏|x)‚àùŒ∏(a0+n/2)‚àí1exp(
‚àí 
b0+1
2nx
n=1(xn‚àí¬µ)2!
Œ∏)
,
which is in the same form as the prior. so we know that our proposed fŒ∏(Œ∏) is a
conjugate prior.
the story of conjugate priors is endless because every likelihood has its conjugate prior.
table 8.1 summarizes a few commonly used conjugate priors, their likelihoods, and their
posteriors. the list can be expanded further to distributions with multiple parameters. for
example, if a gaussian has both unknown mean and variance, then there exists a conjugate
prior consisting of a gaussian multiplied by a gamma. conjugate priors also apply to multi-
dimensional distributions. for example, the prior for the mean vector of a high-dimensional
gaussian is another high-dimensional gaussian. the prior for the covariance matrix of a
high-dimensional gaussian is the wishart prior. the prior for both the mean vector and the
covariance matrix is the normal wishart.
table of conjugate priors
likelihood conjugate prior posterior
fx|Œ∏(x|Œ∏) fŒ∏(Œ∏) fŒ∏|x(Œ∏|x)
bernoulli( Œ∏) beta( Œ±, Œ≤) beta( Œ±+s, Œ≤+n‚àís)
poisson( Œ∏) gamma( Œ±, Œ≤) gamma
Œ±+s,Œ≤
1+n
exponential( Œ∏) gamma( Œ±, Œ≤) gamma
Œ±+n,Œ≤
1+Œ≤s
gaussian( Œ∏, œÉ2) gaussian( ¬µ0, œÉ2
0) gaussian
¬µ0/œÉ2
0+s/œÉ2
1/œÉ2
0+n/œÉ2
gaussian( ¬µ, Œ∏2) inv. gamma( Œ±, Œ≤) gamma
Œ±+n
2, Œ≤+1
2pn
n=1(xn‚àí¬µ)2
table 8.1: commonly used conjugate priors.
8.3.7 linking map with regression
ml and regression represent the statistics and the optimization aspects of the same problem.
with the parallel argument, map is linked to the regularized regression. the reason follows
immediately from the definition of map:
bŒ∏map = argmax
Œ∏logfx|Œ∏(x|Œ∏)|{z }
data fidelity+ log fŒ∏(Œ∏)|{z}
regularization.
517chapter 8. estimation
to make this more explicit, we consider following linear regression problem:
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞y1
y2
...
ynÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
=y=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞œï0(x1)œï1(x1)¬∑¬∑¬∑ œïd‚àí1(x1)
œï0(x2)œï1(x2)¬∑¬∑¬∑ œïd‚àí1(x2)
... ¬∑¬∑¬∑......
œï0(xn)œï1(xn)¬∑¬∑¬∑œïd‚àí1(xn)Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
| {z }
=xÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£∞Œ∏0
Œ∏1
...
Œ∏d‚àí1Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
=Œ∏+Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞e1
e2
...
enÔ£π
Ô£∫Ô£∫Ô£∫Ô£ª
|{z}
=e.
if we assume that e‚àºgaussian(0 , œÉ2i), the likelihood is defined as
fy|Œ∏(y|Œ∏) =1p
(2œÄœÉ2)nexp
‚àí1
2œÉ2‚à•y‚àíxŒ∏‚à•2
. (8.52)
in the ml setting, the ml estimate is the maximizer of the likelihood:
bŒ∏ml= argmax
Œ∏logfy|Œ∏(y|Œ∏)
= argmax
Œ∏‚àí1
2œÉ2‚à•y‚àíxŒ∏‚à•2.
for map, we add a prior term so that the optimization becomes
bŒ∏map = argmax
Œ∏logfy|Œ∏(y|Œ∏) + log fŒ∏(Œ∏)
= argmin
Œ∏1
2œÉ2‚à•y‚àíxŒ∏‚à•2‚àílogfŒ∏(Œ∏).
therefore, the regularization of the regression is exactly ‚àílogfŒ∏(Œ∏). we can perform reverse
engineering to find out the corresponding prior for our favorite choices of the regularization.
ridge regression . suppose that
fŒ∏(Œ∏) = exp
‚àí‚à•Œ∏‚à•2
2œÉ2
0
.
taking the negative log on both sides yields
‚àílogfŒ∏(Œ∏) =‚à•Œ∏‚à•2
2œÉ2
0.
putting this into the map estimate,
bŒ∏map = argmin
Œ∏1
2œÉ2‚à•y‚àíxŒ∏‚à•2+1
2œÉ2
0‚à•Œ∏‚à•2
= argmin
Œ∏‚à•y‚àíxŒ∏‚à•2+œÉ2
œÉ2
0|{z}
=Œª‚à•Œ∏‚à•2,
where Œªis the corresponding ridge regularization parameter. therefore, the ridge regression
is equivalent to a map estimation using a gaussian prior.
5188.3. maximum a posteriori estimation
how is map related to ridge regression?
¬àin map, define the prior as a gaussian:
fŒ∏(Œ∏) = exp
‚àí‚à•Œ∏‚à•2
2œÉ2
0
. (8.53)
¬àthe prior says that the solution Œ∏is naturally distributed according to a gaussian
with mean zero and variance œÉ2
0.
lasso regression . suppose that
fŒ∏(Œ∏) = exp
‚àí‚à•Œ∏‚à•1
Œ±
.
taking the negative log on both sides yields
‚àílogfŒ∏(Œ∏) =‚à•Œ∏‚à•1
Œ±.
putting this into the map estimate we can show that
bŒ∏map = argmin
Œ∏1
2œÉ2‚à•y‚àíxŒ∏‚à•2+1
Œ±‚à•Œ∏‚à•1
= argmin
Œ∏1
2‚à•y‚àíxŒ∏‚à•2+œÉ2
Œ±|{z}
=Œª‚à•Œ∏‚à•1.
to summarize:
how is map related to lasso regression?
¬àlasso is a map using the prior
fŒ∏(Œ∏) = exp
‚àí‚à•Œ∏‚à•1
Œ±
. (8.54)
at this point, you may be wondering what map buys us when regularized regression
can already do the job. the answer is about the interpretation. while regularized regression
can always return us a result, that is just a result. however, if you know that the parameter Œ∏
is distributed according to some distributions fŒ∏(Œ∏), map offers a statistical perspective of
the solution in the sense that it returns the peak of the posterior fŒ∏|x(Œ∏|x). for example, if
we know that the data is generated from a linear model with gaussian noise, and if we know
that the true regression coefficients are drawn from a gaussian, then the ridge regression is
guaranteed to be optimal in the posterior sense. similarly, if we know that there are outliers
and have some ideas about the outlier statistics, perhaps the lasso regression is a better
choice.
it is also important to note the different optimalities offered by map versus ml versus
regression. the optimality offered by regression is the training loss, which can always give
us a result even if the underlying statistics do not match the optimization formulation,
519chapter 8. estimation
e.g., there are outliers, and you use unregularized least-squares minimization. you can get a
result, but the outliers will heavily influence your solution. on the other hand, if you know
the data statistics and choose to follow the ml, then the ml solution is optimal in the sense
of optimizing the likelihood fx|Œ∏(x|Œ∏). if you further know the prior statistics, the map
solution will be optimal, but this time it is optimal w.r.t. to the posterior fŒ∏|x(Œ∏|x). since
each of these is optimizing for a different goal, they are only good for their chosen objectives.
for example, bŒ∏map can be a biased estimate if our goal is to maximize the likelihood. the
bŒ∏mlis optimal for the likelihood but can be a bad choice for the posterior. both bŒ∏map and
bŒ∏mlcan possibly achieve a reasonable mean-squared error, but their results may not make
sense (e.g., if Œ∏is an image then bŒ∏map may over-smooth the image whereas bŒ∏mlamplifies
noise). so it‚Äôs incorrect to think that bŒ∏map is superior to bŒ∏mlbecause it is more general.
here are some rules of thumb for map, ml, and regression:
when should i use regression, ml and map?
¬àregression : if you are lazy and you know nothing about the statistics, do the
regression with whatever regularization you prefer. it will give you a result. see
if it makes sense with your data.
¬àmap : if you know the statistics of the data, and if you have some preference for
the prior distribution, go with map. it will offer you the optimal solution w.r.t.
finding the peak of the posterior.
¬àml: if you are interested in some simple-form solution, and you want those nice
properties such as consistency and unbiasedness, then go with ml. it usually
possesses the ‚Äúfriendly‚Äù properties so that you can derive the performance limit.
8.4 minimum mean-square estimation
first-time readers are often tempted to think that the maximum-likelihood estimation or
the maximum a posteriori estimation are thebest methods to estimate parameters. in some
sense, this is true because both estimation procedures offer some form of optimal explanation
for the observed variables. however, as we said above, being optimal with respect to the
likelihood or the posterior only means optimal under the respective criteria. an ml estimate
is not necessarily optimal for the posterior, whereas a map estimate is not necessarily
optimal for the likelihood. therefore, as we proceed to the third commonly used estimation
strategy, we need to remind ourselves of the specific type of optimality we seek.
8.4.1 positioning the minimum mean-square estimation
mean-square error estimation, as it is termed, uses the mean-square error as the optimality
criterion. the corresponding estimation process is known as the minimum mean-square
estimation (mmse) . mmse is a bayesian approach, meaning that it uses the prior fŒ∏(Œ∏)
as well as the likelihood fx|Œ∏(x|Œ∏). as we will show shortly, the mmse estimate of a set
5208.4. minimum mean-square estimation
of i.i.d. observation x= [x1, . . . , x n]tis
bŒ∏mmse (x)(a)=eŒ∏|x[Œ∏|x=x] ( a) : we will discuss this.
=z
Œ∏¬∑fŒ∏|x(Œ∏|x)dŒ∏. (8.55)
you may find this equation very surprising, because it says that the mmse estimate is
themean of the posterior distribution fŒ∏|x(Œ∏|x). let‚Äôs compare this result with the ml
estimate and the map estimate:
bŒ∏ml= peak of fx|Œ∏(x|Œ∏),
bŒ∏map = peak of fŒ∏x|(Œ∏|x),
bŒ∏mmse = average of fŒ∏|x(Œ∏|x).
therefore, an mmse estimate is not by any means universally superior or inferior to a map
estimate or an ml estimate. it is just a different estimate with a different goal.
so how exactly are these estimates different? figure 8.21 illustrates a typical situation
of asymmetric distribution. here, we plot both the likelihood function fx|Œ∏(x|Œ∏) and the
posterior function fŒ∏x|(Œ∏|x).
figure 8.21: a typical example of an ml estimate, a map estimate and an mmse estimate.
as shown in the figure, the ml estimate is the peak of the likelihood, whereas the map
estimate is the peak of the posterior. the third estimate is the mmse estimate, which is
the average of the posterior distribution. it is easy to see that if the posterior distribution
is symmetric and has a single peak, the peak is always the mean. therefore, for single-peak
symmetric distributions, mmse and map estimates are identical.
what is so special about the mmse estimate?
¬àmmse is a bayesian estimation, so it requires a prior.
¬àan mmse estimate is the mean of the posterior distribution.
¬àmmse estimate = map estimate if the posterior distribution is symmetric and
has a single peak.
521chapter 8. estimation
8.4.2 mean squared error
the mmse is based on minimizing the mean squared error (mse). in this subsection we
discuss the mean squared error in the bayesian setting. in the deterministic setting, given
an estimate bŒ∏and a ground truth Œ∏, the mse is defined as
mse( Œ∏|{z}
ground truth,bŒ∏|{z}
estimate) = (Œ∏‚àíbŒ∏)2. (8.56)
in any estimation problem, the estimate bŒ∏is always a function of the observed variables.
thus, we have
bŒ∏(x) =g(x), where x= [x1, . . . , x n]t,
for some function g(¬∑). substituting this into the definition of mse, and recognizing that x
is drawn from a distribution fx(x), we take the expectation to define the mse as
mse( Œ∏,bŒ∏) = (Œ∏‚àíbŒ∏)2
‚áìreplace bŒ∏byg(x)
mse( Œ∏,bŒ∏) = (Œ∏‚àíg(x))2
‚áìtake expectation over x
mse( Œ∏,bŒ∏) =ex
(Œ∏‚àíg(x))2
.
thus we have arrived at the definition of mse. we call this the frequentist version, because
the parameter Œ∏is deterministic.
definition 8.8 (mean squared error, frequentist ).the mean squared error of an
estimate g(x)w.r.t. the true parameter Œ∏is
mse freq(Œ∏, g(¬∑)) =ex
(Œ∏‚àíg(x))2
. (8.57)
if the parameter Œ∏is high-dimensional, so is the estimate g(x), and the mse is
mse freq(Œ∏,g(¬∑)) =ex
‚à•Œ∏‚àíg(x)‚à•2
. (8.58)
note that in the above definition the mse is measured between the true parameter Œ∏and
the estimator g(¬∑). we use the function g(¬∑) here because we have taken the expectation
of all the possible inputs x. so we are not comparing Œ∏with a value g(x) but with the
function g(¬∑).
if we take a bayesian approach such as the map, then Œ∏itself is a random variable Œ∏.
to compute the mse, we then need to take the average across all the possible choices of
ground truth Œ∏. this leads to
mse( Œ∏,bŒ∏) =ex
(Œ∏‚àíg(x))2
‚áìreplace Œ∏by Œ∏
mse( Œ∏,bŒ∏) =ex
(Œ∏‚àíg(x))2
‚áìtake expectation over Œ∏
mse( Œ∏,bŒ∏) =ex,Œ∏
(Œ∏‚àíg(x))2
.
therefore, we have arrived at our definition of the mse, in the bayesian setting.
5228.4. minimum mean-square estimation
definition 8.9 (mean squared error, bayesian ).the mean squared error of an es-
timate g(x)w.r.t. the true parameter Œ∏is
mse bayes (Œ∏, g(¬∑)) =eŒ∏,x
(Œ∏‚àíg(x))2
. (8.59)
if the parameter Œ∏is high-dimensional, so is the estimate g(x), and the mse is
mse bayes (Œ∏,g(¬∑)) =eŒ∏,x
‚à•Œ∏‚àíg(x)‚à•2
. (8.60)
the difference between the bayesian mse and the frequentist mse is the expectation over Œ∏.
practically speaking, the frequentist mse is more of an evaluation metric than an objective
function for solving an inverse problem. the reason is that in an inverse problem, we never
have access to the true parameter Œ∏. (if we knew Œ∏, there would be no problem to solve.)
bayesian mse is more meaningful. it says that we do not know the true parameter Œ∏, but
we know its statistics. we are trying to find the best g(¬∑) that minimizes the error. our
solution will depend on the statistics of Œ∏ but not on the unknown true parameter Œ∏.
when we say minimum mean squared error estimation, we typically refer to the
bayesian mmse. in this case, the problem we solve is
g(¬∑) = argmin
g(¬∑)eŒ∏,x
(Œ∏‚àíg(x))2
. (8.61)
as you can see from definition 8.9, the goal of the bayesian mmse is to find a function
g:rn‚Üírsuch that the joint expectation eŒ∏,x
(Œ∏‚àíg(x))2
is minimized. in the case
where Œ∏is a vector, the problem becomes
g(¬∑) = argmin
g(¬∑)eŒ∏,x
‚à•Œ∏‚àíg(x)‚à•2
, (8.62)
where g(¬∑) :rn√ód‚ÜírdifŒ∏is ad-dimensional vector. the function gwill take a sequence
ofnobserved numbers and estimate the parameter Œ∏.
what is the bayesian mmse estimate?
the bayesian mmse estimate is obtained by minimizing the mse:
g(¬∑) = argmin
g(¬∑)eŒ∏,x
(Œ∏‚àíg(x))2
. (8.63)
8.4.3 mmse estimate = conditional expectation
theorem 8.2. thebayesian mmse estimate is
bŒ∏mmse =argmin
g(¬∑)eŒ∏,x
(Œ∏‚àíg(x))2
=eŒ∏|x[Œ∏|x=x]. (8.64)
523chapter 8. estimation
proof . first of all, we decompose the joint expectation:
eŒ∏,x
(Œ∏‚àíg(x))2
=z
eŒ∏|x
(Œ∏‚àíg(x))2|x=x
fx(x)dx.
since fx(x)‚â•0 for all x, andeŒ∏|x
(Œ∏‚àíg(x))2|x=x
‚â•0 because it is a square, it
follows that the integral is minimized when eŒ∏|x
(Œ∏‚àíg(x))2|x=x
is minimized.
the conditional expectation can be evaluated as
eŒ∏|x[(Œ∏‚àíg(x))2|x=x]
=eŒ∏|x
Œ∏2‚àí2Œ∏g(x) +g(x)2x=x
=eŒ∏|x
Œ∏2x=x
| {z }
def=v(x)‚àí2eŒ∏|x
Œ∏x=x
| {z }
def=u(x)g(x) +g(x)2
=v(x)‚àí2u(x)g(x) +g(x)2+u(x)2‚àíu(x)2
=v(x)‚àíu(x)2+ (u(x)‚àíg(x))2
‚â•v(x)‚àíu(x)2,‚àÄg(¬∑),
where the last inequality holds because no matter what g(¬∑) we choose, the square term
(u(x)‚àíg(x))2is non-negative. therefore, eŒ∏|x[(Œ∏‚àíg(x))2|x=x] is lower-bounded by
v(x)‚àíu(x)2, which is a bound that is independent of g(¬∑). if we can find a g(¬∑) such that
this lower bound can be met, the corresponding g(¬∑) is the minimizer.
to this end we only need to make eŒ∏|x[(Œ∏‚àíg(x))2|x=x] equal v(x)‚àíu(x)2,
but this is easy: the equality holds if and only if ( u(x)‚àíg(x))2= 0. in other words, if we
choose g(¬∑) such that g(x) =u(x), the corresponding g(¬∑) is the minimizer. this g(¬∑), by
substituting the definition of u(x), is
g(x) =eŒ∏|x
Œ∏x=x
. (8.65)
this completes the proof.
‚ñ°
what is the mmse estimate?
the mmse estimate is
bŒ∏mmse (x) =eŒ∏|x[Œ∏|x=x]. (8.66)
we emphasize that bŒ∏mmse (x) is a function of x, because for a different set of observations
xwe will have a different estimated value. since xis a random realization of the random
vector x, we can also define the mmse estimator as
bŒ∏mmse (x) =eŒ∏|x[Œ∏|x]. (8.67)
in this notation, we emphasize that the estimator bŒ∏mmse returns a random parameter. the
input to the estimator is the random vector x. because we are not looking at a particular
realization x=xbut the general x,bŒ∏mmse is a function of xand not x.
5248.4. minimum mean-square estimation
conditional expectation of what?
an mmse estimator is the conditional expectation of Œ∏ given x=x:
eŒ∏|x
Œ∏x=x
=z
Œ∏ fŒ∏|x(Œ∏|x)dŒ∏. (8.68)
this is the expectation using the posterior distribution fŒ∏|x(Œ∏|x). it should be compared
to the peak of the posterior, which returns us the map estimate. the posterior distribution
is constructed through bayes‚Äô theorem:
fŒ∏|x(Œ∏|x) =fx|Œ∏(x|Œ∏)fŒ∏(Œ∏)
fx(x). (8.69)
therefore, to evaluate the expectation of the condition distribution, we need to include the
normalization constant fx(x), which was omitted in map.
the discussion about the mean squared error and the vector estimates can be skipped if
this is your first time reading the book.
what is the mean squared error when using the mmse estimator?
¬àthe mean squared error conditioned on the observation is
mse(Œ∏ ,bŒ∏mmse (x))def=eŒ∏|x[(Œ∏‚àíbŒ∏mmse (x))2|x]
= var Œ∏|x[Œ∏|x],
which is the conditional variance.
¬àthe overall mean squared error, unconditioned, is
mse(Œ∏ ,bŒ∏mmse (¬∑)) =ex
varŒ∏|x[Œ∏|x]
= var Œ∏[Œ∏].
proof . let us prove these two statements. the resulting mse is obtained by substituting
bŒ∏mmse (x) =eŒ∏|x
Œ∏x
into the mse(Œ∏ ,bŒ∏mmse (x)). to this end, we have that
eŒ∏|x[(Œ∏‚àíbŒ∏mmse (x))2|x] =v(x)‚àíu(x)2
+ ( u(x)‚àíbŒ∏mmse (x))2
| {z }
=0,because bŒ∏mmse (x)=eŒ∏|x[Œ∏|x]=u(x).
the variables vanduare defined as
v(x) =eŒ∏|x
Œ∏2x
= 2nd moment of Œ∏ using fŒ∏|x(Œ∏|x),
u(x) =eŒ∏|x
Œ∏x
= 1st moment of Œ∏ using fŒ∏|x(Œ∏|x).
525chapter 8. estimation
since var[ z] =e[z2]‚àíe[z]2for any random variable z, it follows that
eŒ∏|x[(Œ∏‚àíbŒ∏mmse (x))2|x] =v(x)‚àíu(x)2
=eŒ∏|x
Œ∏2x
‚àí 
eŒ∏|x
Œ∏x2
= variance of Œ∏ using fŒ∏|x(Œ∏|x)
def= var Œ∏|x[Œ∏|x].
substituting this conditional variance into the mse definition,
mse(Œ∏ ,bŒ∏mmse (¬∑)) =z
eŒ∏|x[(Œ∏‚àíbŒ∏mmse (x))2|x=x]fx(x)dx
=z
varŒ∏|x[Œ∏|x=x]fx(x)dx
= var Œ∏[Œ∏].
‚ñ°
what happens if the parameter is a vector?
¬àthe mmse estimate is bŒ∏mmse (x) =eŒ∏|x[Œ∏|x=x].
¬àthe mse is
mse( Œ∏,bŒ∏mmse (¬∑)) = tr
exn
cov(Œ∏|x)o
. (8.70)
proof . the first statement, that the mmse estimate is
bŒ∏mmse (x) =eŒ∏|x[Œ∏|x=x],
is easy to understand since it just follows from the scalar case. the estimator is bŒ∏mmse (x) =
eŒ∏|x[Œ∏|x]. the corresponding mse is
mse( Œ∏,bŒ∏mmse (¬∑)) =eŒ∏,x[‚à•Œ∏‚àíbŒ∏mmse (x)‚à•2]
=ex
eŒ∏|x[‚à•Œ∏‚àíbŒ∏mmse (x)‚à•2|x]
,
where we have used the law of total expectation to decompose the joint expectation. using
the matrix identity below, we have that
ex
eŒ∏|x[‚à•Œ∏‚àíbŒ∏mmse (x)‚à•2|x]
=ex
eŒ∏|xh
trn
(Œ∏‚àíbŒ∏mmse (x))(Œ∏‚àíbŒ∏mmse (x))to
|xi
= tr
ex
eŒ∏|xh
(Œ∏‚àíbŒ∏mmse (x))(Œ∏‚àíbŒ∏mmse (x))t|xi
.
5268.4. minimum mean-square estimation
however, since the mmse estimator is the condition expectation of the posterior, it follows
that the inner expectation is the conditional covariance. therefore, we arrive at the second
statement:
mse( Œ∏,bŒ∏mmse (¬∑)) = tr
ex
eŒ∏|xh
(Œ∏‚àíbŒ∏mmse (x))(Œ∏‚àíbŒ∏mmse (x))t|xi
= tr
exn
cov(Œ∏|x)o
.
‚ñ°
to prove the two statements above, we need some tools from linear algebra. the two
specific matrix identities are given by the following lemma:
lemma 8.1. the following are matrix identities:
¬àfor any random vector Œ∏‚ààrd,
‚à•Œ∏‚à•2=tr(Œ∏tŒ∏) =tr(Œ∏Œ∏t).
¬àfor any random vector Œ∏‚ààrd,
eŒ∏[tr(Œ∏Œ∏t)] =tr(eŒ∏[Œ∏Œ∏t]).
the proof of these two results is straightforward. the first is due to the cyclic property of
the trace operator. the second statement is true because the trace is a linear operator that
sums the diagonal of a matrix.
the end of the discussion. please join us again.
example 8.22 . let
fx|Œ∏(x|Œ∏) =(
Œ∏e‚àíŒ∏x, x ‚â•0,
0, x < 0,and fŒ∏(Œ∏) =(
Œ±e‚àíŒ±Œ∏, Œ∏ ‚â•0,
0, Œ∏ < 0.
find the ml, map, and mmse estimates for a single observation x=x.
solution . we first find the posterior distribution:
fŒ∏|x(Œ∏|x) =fx|Œ∏(x|Œ∏)fŒ∏(Œ∏)
fx(x)
=Œ±Œ∏e‚àí(Œ±+x)Œ∏
r‚àû
0Œ±Œ∏e‚àí(Œ±+x)Œ∏dŒ∏
=Œ±Œ∏e‚àí(Œ±+x)Œ∏
Œ±
(Œ±+x)2
= (Œ±+x)2Œ∏e‚àí(Œ±+x)Œ∏.
527chapter 8. estimation
the mmse estimate is the conditional expectation of the posterior:
bŒ∏mmse (x) =eŒ∏|x[Œ∏|x=x]
=z‚àû
0Œ∏fŒ∏|x(Œ∏|x)dŒ∏
=z‚àû
0Œ∏(Œ±+x)2Œ∏e‚àí(Œ±+x)Œ∏dŒ∏
= (Œ±+x)z‚àû
0Œ∏2¬∑(Œ±+x)e‚àí(Œ±+x)Œ∏dŒ∏
| {z }
2nd moment of exponential distribution
= (Œ±+x)¬∑2
(Œ±+x)2=2
Œ±+x.
the map estimate is the peak of the posterior:
bŒ∏map(x) = argmax
Œ∏logfx|Œ∏(x|Œ∏) + log fŒ∏(Œ∏)
= argmax
Œ∏‚àíŒ∏x+ log Œ∏‚àíŒ±Œ∏+ log Œ±.
taking the derivative and setting it to zero yields ‚àíx+1
Œ∏‚àíŒ±= 0. this implies that
bŒ∏map(x) =1
Œ±+x.
finally, the ml estimate is
bŒ∏ml(x) = argmax
Œ∏logfx|Œ∏(x|Œ∏) =1
x.
practice exercise 8.8 . following the previous example, derive the estimates for
multiple observations x=x.
solution . the posterior is
fŒ∏|x(Œ∏|x) =fx|Œ∏(x|Œ∏)fŒ∏(Œ∏)
fx(x)
=(qn
n=1fx|Œ∏(xn|Œ∏))fŒ∏(Œ∏)
fx(x)
=Œ±Œ∏e‚àí(Œ±+pn
n=1xn)Œ∏
r‚àû
0Œ±Œ∏e‚àí(Œ±+pn
n=1xn)Œ∏dŒ∏
= 
Œ±+nx
n=1xn!2
Œ∏e‚àí(Œ±+pn
n=1xn)Œ∏.
5288.4. minimum mean-square estimation
therefore, we are only replacing xby the sumpn
n=1xnin the posterior. hence, the
estimates are:
bŒ∏mmse (x) =2
Œ±+pn
n=1xn,
bŒ∏map(x) =1
Œ±+pn
n=1xn,
bŒ∏ml(x) =1pn
n=1xn.
this example shows that as n‚Üí ‚àû , the ml estimate bŒ∏ml(x)‚Üí0. the reason is that the
likelihood is an exponential distribution. therefore, the peak is always at 0. the posterior is
an erlang distribution, and therefore the peak is offset by Œ±in the denominator. however,
asn‚Üí ‚àû the posterior distribution is dominated by the likelihood, so the peak is shifted
towards 0. finally, since the erlang distribution is asymmetric, the mean is different from
the peak. hence, the mmse estimate is different from the map estimate.
8.4.4 mmse estimator for multidimensional gaussian
the multidimensional gaussian has some very important uses in data science. accordingly,
we devote this subsection to the discussion of the mmse estimate of a gaussian. the main
result is stated as follows.
what is the mmse estimator for a multi-dimensional gaussian?
theorem 8.3. suppose Œ∏‚ààrdandx‚ààrnare jointly gaussian with a joint pdf
Œ∏
x
‚àºgaussian¬µŒ∏
¬µx
,œÉŒ∏Œ∏œÉŒ∏x
œÉxŒ∏œÉxx
.
the mmse estimator is
bŒ∏mmse (x) =¬µŒ∏+œÉŒ∏xœÇ‚àí1
xx(x‚àí¬µx). (8.71)
the proof of this result is not difficult but it is tedious. the flow of the argument is:
¬àstep 1: show that the posterior distribution fŒ∏|x(Œ∏|x) is a gaussian.
¬àstep 2: to do so we need to complete the squares for matrices.
¬àstep 3: once we have the fŒ∏|x(Œ∏|x), the posterior mean is the mmse estimator.
the proof below can be skipped if this is your first time reading the book.
529chapter 8. estimation
proof . the posterior pdf is
fŒ∏|x(Œ∏|x) =fŒ∏,x(Œ∏,x)
fx(x)
=1‚àö
(2œÄ)d+n|œÉ|exp(
‚àí1
2Œ∏‚àí¬µŒ∏
x‚àí¬µxtœÉŒ∏Œ∏œÉŒ∏x
œÉxŒ∏œÉxx‚àí1Œ∏‚àí¬µŒ∏
x‚àí¬µx)
1‚àö
(2œÄ)n|œÉxx|expn
‚àí1
2x‚àí¬µxtœÇ‚àí1
xxx‚àí¬µxo .
without loss of generality, we assume that ¬µx=¬µŒ∏= 0. then the posterior becomes
fŒ∏|x(Œ∏|x) =1p
(2œÄ)d|œÉ|/|œÉxx|
√óexp(
‚àí1
2Œ∏
xtœÉŒ∏Œ∏œÉŒ∏x
œÉxŒ∏œÉxx‚àí1Œ∏
x
+1
2xtœÇ‚àí1
xxx)
| {z }
h(Œ∏,x).
the tedious task here is to simplify h(Œ∏,x).
regardless of what the 2-by-2 matrix inverse is, the matrix will take the form
œÉŒ∏Œ∏œÉŒ∏x
œÉxŒ∏œÉxx‚àí1
=a b
c d
,
for some choices of matrices a,b,candd. therefore, the function h(Œ∏,x) can be written
as
h(Œ∏,x) =‚àí1
2n
Œ∏taŒ∏+Œ∏tbx+xtcŒ∏+xtdx‚àíxtœÇ‚àí1
xxxo
. (8.72)
our goal is to complete the square for h(Œ∏,x). to this end, we propose to write
h(Œ∏,x) =‚àí1
2n
(Œ∏‚àígx)ta(Œ∏‚àígx) +q(x)o
, (8.73)
for some matrix gand function q(¬∑) ofxonly. if we compare equation (8.72) and equa-
tion (8.73), we observe that gmust satisfy
g=‚àía‚àí1b.
therefore, if we can determine aandb, we will know g. if we know g, we have completed
the square for h(Œ∏,x). if we can complete the square for h(Œ∏,x), we can write
fŒ∏|x(Œ∏|x) =exp{‚àíq(x)/2}p
(2œÄ)d|œÉ|/|œÉxx|| {z }
constant in Œ∏√óexp
‚àí1
2(Œ∏‚àígx)ta(Œ∏‚àígx)
| {z }
a gaussian.
hence, the mmse estimate, which is the posterior mean e[Œ∏|x=x], is simply gx:
bŒ∏mmse (x) =e[Œ∏|x=x]
=gx
=‚àía‚àí1bx.
5308.4. minimum mean-square estimation
so it remains to determine aandbby solving the tedious matrix inversion problem. the
result is:6
a= (œÉŒ∏Œ∏‚àíœÉŒ∏xœÇ‚àí1
xxœÉxŒ∏)‚àí1,
b=‚àí(œÉŒ∏Œ∏‚àíœÉŒ∏xœÇ‚àí1
xxœÉxŒ∏)‚àí1œÉŒ∏xœÇ‚àí1
xx,
c= (œÉxx‚àíœÉxŒ∏œÇ‚àí1
Œ∏Œ∏œÉŒ∏x)‚àí1œÉxŒ∏œÇ‚àí1
Œ∏Œ∏,
d= (œÉxx‚àíœÉxŒ∏œÇ‚àí1
Œ∏Œ∏œÉŒ∏x)‚àí1.
therefore, plugging everything into the equation,
bŒ∏mmse (x) =‚àía‚àí1bx
=œÉŒ∏,xœÇ‚àí1
xxx.
for non-zero means, we can repeat the same arguments above and show that
bŒ∏mmse (x) =¬µŒ∏+œÉŒ∏,xœÇ‚àí1
xx(x‚àí¬µx).
‚ñ°
end of the proof. please join us again.
practice exercise 8.9 . suppose Œ∏‚ààrdandx‚ààrnare jointly gaussian with a
joint pdfŒ∏
x
‚àºgaussian¬µŒ∏
¬µx
,œÉŒ∏Œ∏œÉŒ∏x
œÉxŒ∏œÉxx
.
we know that the mmse estimator is
bŒ∏mmse (x) =¬µŒ∏+œÉŒ∏xœÇ‚àí1
xx(x‚àí¬µx). (8.74)
find the mean squared error when using the mmse estimator.
solution . conditioned on x=x, according to equation (8.70), the mmse is
mse( Œ∏,bŒ∏(x)) = tr {cov[Œ∏|x]}.
the conditional covariance cov[ Œ∏|x] is the covariance of the posterior distribution
fŒ∏|x(Œ∏|x), which is
tr{cov[Œ∏|x]}= tr{a}
= tr
(œÉŒ∏Œ∏‚àíœÉŒ∏xœÇ‚àí1
xxœÉxŒ∏)‚àí1	
.
6see matrix cookbook https://www.math.uwaterloo.ca/ ~hwolkowi/matrixcookbook.pdf section 9.1.5
on the schur complement.
531chapter 8. estimation
the overall mean squared error is
mse
Œ∏,bŒ∏(¬∑)
=ex
mse( Œ∏,bŒ∏(x))
=z
mse( Œ∏,bŒ∏(x))fx(x)dx
=z
tr{cov[Œ∏|x]}fx(x)dx
=z
tr
(œÉŒ∏Œ∏‚àíœÉŒ∏xœÇ‚àí1
xxœÉxŒ∏)‚àí1	
fx(x)dx
= tr
(œÉŒ∏Œ∏‚àíœÉŒ∏xœÇ‚àí1
xxœÉxŒ∏)‚àí1	z
fx(x)dx
= tr
(œÉŒ∏Œ∏‚àíœÉŒ∏xœÇ‚àí1
xxœÉxŒ∏)‚àí1	
.
for multidimensional gaussian, does mmse = map?
the answer is yes.
theorem 8.4. suppose Œ∏‚ààrdandx‚ààrnare jointly gaussian with a joint pdf
Œ∏
x
‚àºgaussian¬µŒ∏
¬µx
,œÉŒ∏Œ∏œÉŒ∏x
œÉxŒ∏œÉxx
.
the map estimate is
bŒ∏map(x) =¬µŒ∏+œÉŒ∏xœÇ‚àí1
xx(x‚àí¬µx). (8.75)
proof . the proof of this result is straightforward. if we return to the proof of the mmse
result, we note that
fŒ∏|x(Œ∏|x) =exp{‚àíq(x)/2}p
(2œÄ)d|œÉ|/|œÉxx|| {z }
constant in Œ∏√óexp
‚àí1
2(Œ∏‚àígx)ta(Œ∏‚àígx)
| {z }
a gaussian.
therefore, the maximizer of this posterior distribution, which is the map estimate, is
bŒ∏map(x) = argmax
Œ∏fŒ∏|x(Œ∏|x)
= argmax
Œ∏‚àí1
2(Œ∏‚àígx)ta(Œ∏‚àígx).
taking the derivative w.r.t. Œ∏and setting it zero, we have
bŒ∏map(x) =gx=œÉŒ∏,xœÇ‚àí1
xxx.
if the mean vectors are non-zero, we have bŒ∏map(x) =¬µŒ∏+œÉŒ∏xœÇ‚àí1
xx(x‚àí¬µx).
‚ñ°
5328.4. minimum mean-square estimation
8.4.5 linking mmse and neural networks
the blossoming of deep neural networks since 2010 has created a substantial impact on
modern data science. the basic idea of a neural network is to train a stack of matrices and
nonlinear functions (known as the network weights and the neuron activation functions,
respectively), among other innovative ideas, so that a certain training loss is minimized.
expressing this by equations, the goal of the learning is equivalent to solving the optimization
problem
bg(¬∑) = argmin
g(¬∑)ex,Œ∏
‚à•Œ∏‚àíg(x)‚à•2
, (8.76)
where x‚ààrmis the input data and Œ∏‚ààrdis the ground truth prediction. we want to
findg(¬∑) such that the error is minimized.
the error we choose here is the ‚Ñì2-norm error ‚à• ¬∑ ‚à•2. it is only one of many possi-
ble choices. you may recognize that this is exactly the same as the mmse optimization.
therefore, the neural network we are finding here is the mmse estimator. since the mmse
estimator is the conditional expectation of the posterior distribution, the neural network
approximates the mean of the posterior distribution.
often the struggle we have with deep neural networks is whether we can find the
optimal network parameters via optimization algorithms such as the stochastic gradient
descent algorithms. however, if we think about this problem more deeply, the equivalence
between the mmse estimator and the posterior mean tells us that the hard part is related
to the posterior distribution. in the high-dimensional landscape, it is close to impossible to
determine the posterior and its mean. if we add to these difficulties and the nonconvexity
of the function g, training a network is very challenging.
one misconception about neural networks is that if we can achieve a low training error,
and if the model can also achieve a low testing error, then the network is good. this is a false
sense of satisfaction. if a model can achieve very good training and testing errors, then the
model is only good with respect to the error you choose. for example, if we choose the ‚Ñì2-
norm error ‚à•¬∑‚à•2and if our model achieves good training and testing errors (in terms of ‚à•¬∑‚à•2),
we can conclude that the model does well with respect to ‚à• ¬∑ ‚à•2. the more serious problem
here, unfortunately, is that ‚à• ¬∑ ‚à•2is not necessarily a good metric of performance (for both
training and testing) because training with ‚à•¬∑‚à•2is equivalent to approximating the posterior
mean. there is absolutely no reason to believe that in the high-dimensional landscape, the
posterior mean is theoptimal. if we choose the posterior mode or the posterior median ,
we will also obtain a result. why are the modes and medians ‚Äúworse‚Äù than the mean ?
in practice, it has been observed that training deep neural networks for image-processing
tasks generally leads to over-smoothed images. this demonstrates how minimizing the mean
squared error ‚à• ¬∑ ‚à•2can be a fundamental mismatch with the problem.
is minimizing the mse the best option?
¬àno. minimizing the mse is equivalent to finding the mean of the posterior. there
is no reason why the mean is the ‚Äúbest‚Äù.
¬àyou can find the mode of the posterior, in which case you will get a map
estimator.
¬àyou can also find the median of the posterior, in which case you will get the
minimum absolute error estimator.
533chapter 8. estimation
¬àultimately, you need to define what is ‚Äúgood‚Äù and what is ‚Äúbad‚Äù.
¬àthe same principle applies to deep neural networks. especially in the regression
setting, why is ‚à• ¬∑ ‚à•2a good evaluation metric for testing (not just training)?
8.5 summary
in this chapter, we have discussed the basic principles of parameter estimation. the three
building blocks are:
¬àlikelihood fx|Œ∏(x|Œ∏): the pdf that we observe samples xconditioned on the un-
known parameter Œ∏. in the frequentist world, Œ∏is a deterministic quantity. in the
bayesian world, Œ∏is random and so it has a pdf.
¬àprior fŒ∏(Œ∏): the pdf of Œ∏. the prior fŒ∏(Œ∏) is used by all bayesian computation.
¬àposterior fŒ∏|x(Œ∏|x): the pdf that the underlying parameter is Œ∏=Œ∏given that we
have observed x=x.
the three building blocks give us several strategies to estimate the parameters:
¬àmaximum likelihood (ml) estimation: maximize fx|Œ∏(x|Œ∏).
¬àmaximum a posteriori (map) estimation: maximize fŒ∏|x(Œ∏|x).
¬àminimum mean-square estimation (mmse): minimize the mean squared error, which
is equivalent to finding the mean of fŒ∏|x(Œ∏|x).
as discussed in this chapter, no single estimation strategy is universally ‚Äúbetter‚Äù because
one needs to specify the optimality criterion. if the goal is to minimize the mean squared
error, then the mmse estimator is the optimal strategy. if the goal is to maximize the
likelihood without assuming any prior knowledge, the ml estimator would be the optimal
strategy. it may appear that if we knew the ground truth parameter Œ∏‚àówe could minimize
the distance between the estimated parameter Œ∏and the true value Œ∏‚àó. if the parameter
is a scalar, this will work. however, if the parameter is a vector, the noise of the distance
becomes an issue. for example, if one cares about the mean absolute error (mae), the
optimal estimator would be the median of the posterior distribution instead of the mean of
the posterior in the mmse case. therefore, it is the end user‚Äôs responsibility to specify the
optimality criterion.
whenever we consider parameter estimation, we tend to think that it is about estimat-
ing the model parameters, such as the mean of a gaussian pdf. while in many statistics
problems this is indeed the case, parameter estimation can be much broader if we link it
with regression. specifically, a regularized linear regression problem can be formulated as a
map estimation
Œ∏‚àó= argmax
Œ∏‚à•xŒ∏‚àíy‚à•2
|{z}
‚àílogfx|Œ∏(x|Œ∏)+Œªr(Œ∏)|{z}
‚àílogfŒ∏(Œ∏), (8.77)
for some regularization r(Œ∏), which is also the negative log of the prior. expressed in this
way, we recognize that the map estimation can be used to recover signals. for example, we
5348.6. references
can model xas a linear degradation process of certain imaging systems. then solving the
map estimation is equivalent to finding the best signal explaining the degraded observation
using the posterior as the criterion. there is rich literature dealing with solving map esti-
mation problems similar to these in subjects such as computational imaging, communication
systems, remote sensing, radar engineering, and recommendation systems, to name a few.
8.6 references
basic
8-1 dimitri p. bertsekas and john n. tsitsiklis, introduction to probability , athena sci-
entific, 2nd edition, 2008. chapter 8 and chapter 9.
8-2 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. chapter 6 and chapter 8.
8-3 athanasios papoulis and s. unnikrishna pillai, probability, random variables and
stochastic processes , mcgraw-hill, 4th edition, 2001. chapter 8.
8-4 henry stark and john w. woods, probability and random processes with applications
to signal processing , prentice hall, 3rd edition, 2002. chapter 5.
8-5 todd k. moon and wynn c. stirling, mathematical methods and algorithms for signal
processing , prentice-hall, 2000. chapter 12.
theoretical analysis
8-6 h. vincent poor, an introduction signal detection and estimation , springer, 1998.
8-7 steven m. kay, fundamentals of statistical signal processing: estimation theory ,
prentice-hall, 1993.
8-8 bernard c. levy, principles of signal detection and parameter estimation , springer,
2008.
8-9 athanasios papoulis and s. unnikrishna pillai, probability, random variables and
stochastic processes , mcgraw-hill, 2001. chapter 8.
8-10 larry wasserman, all of statistics: a concise course in statistical inference , springer,
2010.
8-11 erich l. lehmann, elements of large-sample theory , springer, 1999. chapter 7.
8-12 george casella and roger l. berger statistical inference , duxbury, 2002. chapter 7.
535chapter 8. estimation
machine-learning
8-13 christopher bishop, pattern recognition and machine learning , springer, 2006. chap-
ter 2 and chapter 3.
8-14 richard o. duda, peter e. hart and david g. stork, pattern classification , wiley
2001. chapter 3.
8.7 problems
exercise 1.
letx1, . . . , x nbe a sequence of i.i.d. bernoulli random variables with p[xn= 1] = Œ∏.
suppose that we have observed x1, . . . , x n.
(a) show that the pmf of xnispxn(xn|Œ∏) =Œ∏xn(1‚àíŒ∏)1‚àíxn. find the joint pmf
px1,...,x n(x1, . . . , x n).
(b) find the maximum likelihood estimate bŒ∏, i.e.,
bŒ∏ml= argmax
Œ∏logpx1,...,x n(x1, . . . , x n).
express your answer in terms of x1, . . . , x n.
(c) let Œ∏= 1/2. use chebyshev‚Äôs inequality to find an upper bound for p[|bŒ∏ml‚àíŒ∏|>0.1].
exercise 2.
letyn=Œ∏+wnbe the output of a noisy channel where the input is a scalar Œ∏and
wn‚àº n(0,1) is an i.i.d. gaussian noise. suppose that we have observed y1, . . . , y n.
(a) express the pdf of ynin terms of Œ∏andyn. find the joint pdf of y1, . . . , y n.
(b) find the maximum likelihood estimate bŒ∏ml. express your answer in terms of y1, . . . , y n.
(c) find e[bŒ∏ml].
exercise 3.
letx1, . . . , x nbe a sequence of i.i.d. gaussian random variables with unknown mean Œ∏1
and variance Œ∏2. suppose that we have observations x1, . . . , x n.
(a) express the pdf of xnin terms of xn,Œ∏1andŒ∏2. find the joint pdf of x1, . . . , x n.
(b) find the maximum likelihood estimates of Œ∏1andŒ∏2.
5368.7. problems
exercise 4.
in this problem we study a single-photon image sensor. first, recall that photons arrive
according to a poisson distribution, i.e., the probability of observing kphotons is
p[y=k] =Œªke‚àíŒª
k!,
where Œªis the (unknown) underlying photon arrival rate. when photons arrive at the single-
photon detector, the detector generates a binary response ‚Äú1‚Äù when one or more photons
are detected, and ‚Äú0‚Äù when no photon is detected.
(a) let bbe the random variable denoting the response of the single-photon detector.
that is,
b=(
1, y ‚â•1,
0, y = 0.
find the pmf of b.
(b) suppose we have obtained tindependent measurements with realizations b1=b1,
b2=b2, ...,bt=bt. show that the underlying photon arrival rate Œªcan be estimated
by
Œª=‚àílog 
1‚àípt
t=1bt
t!
.
(c) get a random image from the internet and turn it into a grayscale array with values
between 0 and 1. write a matlab or python program to synthetically generate a
sequence of t= 1000 binary images. then use the previous result to reconstruct the
grayscale image.
exercise 5.
consider a deterministic vector s‚ààrdand random vectors
fy|Œ∏(y|Œ∏) = gaussian( sŒ∏,œÉ),
fŒ∏(Œ∏) = gaussian( ¬µ, œÉ2).
(a) show that the posterior distribution is given by
fŒ∏|y(Œ∏|y) = gaussian( m, q2), (8.78)
where
d2=stœÇ‚àí1s,
m=
d2+1
œÉ2‚àí1
stœÇ‚àí1y+¬µ
œÉ2
,
q2=1
d2+1
œÉ2.
(b) show that the mmse estimate bŒ∏mmse (y) is given by
bŒ∏mmse (y) =œÉ2stœÇ‚àí1y+¬µ
œÉ2d2+ 1. (8.79)
537chapter 8. estimation
(c) show that the mse is given by
mse(Œ∏ ,bŒ∏mmse (y)) =1
d2+1
œÉ2. (8.80)
what happens when œÉ‚Üí0?
(d) give an interpretation of d2. what happens when d2‚Üí0 and when d2‚Üí ‚àû ?
exercise 6.
prove the following identity:
œÉŒ∏Œ∏œÉŒ∏x
œÉxŒ∏œÉxx‚àí1
=(œÉŒ∏Œ∏‚àíœÉŒ∏xœÇ‚àí1
xxœÉxŒ∏)‚àí1‚àí(œÉŒ∏Œ∏‚àíœÉŒ∏xœÇ‚àí1
xxœÉxŒ∏)‚àí1œÉŒ∏xœÇ‚àí1
xx
(œÉxx‚àíœÉxŒ∏œÇ‚àí1
Œ∏Œ∏œÉŒ∏x)‚àí1œÉxŒ∏œÇ‚àí1
Œ∏Œ∏ (œÉxx‚àíœÉxŒ∏œÇ‚àí1
Œ∏Œ∏œÉŒ∏x)‚àí1
.
hint: you can perform reverse engineering by checking whether the product of the left-hand
side and the right-hand side would give you the identity matrix.
exercise 7.
letx1,x2,x3andx4be four i.i.d. poisson random variables with mean Œ∏= 4. find the
mean and variance of the following estimators bŒ∏(x) for Œ∏and determine whether they are
biased or unbiased.
¬àbŒ∏(x) = (x1+x2)/2
¬àbŒ∏(x) = (x3+x4)/2
¬àbŒ∏(x) = (x1+ 2x2)/3
¬àbŒ∏(x) = (x1+x2+x3+x4)/4
exercise 8.
letx1, . . . , x nbe i.i.d. random variables with a uniform distribution of [0 , Œ∏]. consider the
following estimator:
bŒ∏(x) = max( x1, . . . , x n). (8.81)
(a) show that the pdf of bŒ∏ is fbŒ∏(Œ∏) =n[fx(x)]n‚àí1fx(x), where fxandfxare re-
spectively the pdf and cdf of xn.
(b) show that bŒ∏ is a biased estimator.
(c) find the variance of bŒ∏. is it a consistent estimator?
(d) find a constant cso that cbŒ∏ is unbiased.
exercise 9.
letx1, . . . , x nbe i.i.d. gaussian random variables with unknown mean Œ∏and known
variance œÉ= 1.
5388.7. problems
(a) show that the log-likelihood function is
logl(Œ∏|x) =‚àín
2log(2œÄ)‚àí1
2nx
n=1(xn‚àíŒ∏)2. (8.82)
(b) let x2=1
npn
n=1x2
nandx=1
npn
n=1xn. show that x2>(x)2if and only ifpn
n=1(xn‚àíŒ∏)2‚â•0 for all Œ∏.
(c) use python to plot the function log l(Œ∏|x), when x= 2 and x2= 1.
exercise 10.
letx1, . . . , x nbe i.i.d. uniform random variables over the interval [0 , Œ∏].
lett= max( x1, . . . , x n).
(a) consider the estimator h(x) =1
npn
n=1xn. ish(¬∑) an unbiased estimator?
(b) consider the estimator g(x) =1
npn
n=1xn. isg(¬∑) an unbiased estimator?
(c) show that
e[g(x)|t=t] =n+ 1
n
t.
(d) let bg(x) =e[g(x)|t] = n+1
n
t. show that
e[bg(x)2] =(n+ 1)2
n(n+ 2)
Œ∏2.
(e) show that
e[(bg(x)‚àíŒ∏)2] =1
n(n+ 2)
Œ∏2.
exercise 11.
the kullback-leibler divergence between two distributions p1(x) and p2(x) is defined as
kl(p1‚à•p2) =z
p1(x) logp1(x)
p2(x)dx. (8.83)
suppose we approximate p1using a distribution p2. let us choose p2= gaussian( ¬µ,œÉ).
show that ¬µandœÇ, which minimize the kl divergence, are such that
¬µ=ex‚àºp1(x)[x] and œÉ=ex‚àºp1(x)[(x‚àí¬µ)(x‚àí¬µ)t].
exercise 12.
(a) recall that the trace operator is defined as tr[ a] =pd
i=1[a]i,i. prove the matrix
identity
xtax= tr[axxt], (8.84)
where a‚ààrd√ód.
539chapter 8. estimation
(b) show that the likelihood function
p(d |œÉ) =ny
n=11
(2œÄ)d/2|œÉ|1/2expn
‚àí1
2(xn‚àí¬µ)tœÇ‚àí1(xn‚àí¬µ)o
(8.85)
can be written as
p(d|œÉ) =1
(2œÄ)nd/2|œÉ‚àí1|n/2exp(
‚àí1
2tr"
œÉ‚àí1nx
n=1(xn‚àí¬µ)(xn‚àí¬µ)t#)
.(8.86)
(c) let a=œÉ‚àí1bœÉml, and Œª1, ..., Œª dbe the eigenvalues of a. show that the result from
part (b) leads to
p(d|œÉ) =1
(2œÄ)nd/2|bœÉml|n/2 dy
i=1Œªi!n/2
exp(
‚àín
2dx
i=1Œªi)
. (8.87)
hint: for matrix awith eigenvalues Œª1, ..., Œª d, tr[a] =pd
i=1Œªi.
(d) find Œª1, . . . , Œª dsuch that equation (8.87) is maximized.
(e) with the choice of Œªigiven in (d), derive the ml estimate bœÉml.
(f) what would be the alternative way of finding bœÉml? you do not need to prove it. just
briefly describe the idea.
(g)bœÉmlis abiased estimate of the covariance matrix because e[bœÉml]Ã∏=œÉ. can you
suggest an unbiased estimate bœÉunbias such that e[bœÉunbias ] =œÉ? you don‚Äôt need to
prove it. just state the result.
540chapter 9
confidence and hypothesis
in chapters 7 and 8 we learned about regression and estimation, which allow us to determine
the underlying parameters of our statistical models. after obtaining the estimates, we would
like to quantify the accuracy of the estimates and draw statistical conclusions. additionally,
we would like to understand the confidence of these estimates along with their statistical
significance . this chapter presents a few principles that involve analyzing the confidence of
the estimates and conducting hypothesis testing. there are two main questions that we will
address:
¬àhow good is our estimate? this is a fundamental question about the estimator bŒ∏, a
random variable with a pdf, a mean, and a variance.1the estimator we construct
today may be different from the estimator we construct tomorrow due to variations in
the observed data. therefore, the quality of the estimator depends on the randomness
and the number of samples used to construct it. to measure the quality of the estimator
we need to introduce an important concept known as the confidence .
¬àis there statistical significance? suppose that we ran a campaign and observed that
there is a change in the statistics. on what basis do we claim that the change is
statistically significant? how should the cutoff be determined? if we claim that a
result is statistically significant but there is no significance in reality, how much error
will we suffer? these questions are the subjects of hypothesis testing .
these two principal questions are critical for modern data science. if they are not properly
answered, our statistical conclusions could potentially be flawed. a toy example:
imagine that you are developing a covid-19 vaccine. you tested the vaccine on three
patients, and all of them show positive responses to the vaccine. you felt excited because
your vaccine has a 100% success rate. you submit your vaccine application to fda. within
1 second your application is rejected. why? the answer is obvious. you only have three
testing samples. how reliable can these three samples be?
while you are laughing at this toy example, it raises deep statistical questions. first,
why are three samples not enough? well, it is because the variance of the estimator can
potentially be huge. more samples are better because if the estimator is the sample average of
the individual responses, the estimator will behave like a gaussian according to the central
1not all random variables have a well-defined pdf, mean, and variance. e.g., a cauchy variable does
not have a mean.
541chapter 9. confidence and hypothesis
limit theorem. the variance of this gaussian will diminish as we have more samples.
therefore, if we want to control the variance of the estimator, we need more samples. second,
even if we have many samples, how confident is this estimator with respect to the unknown
population parameter? note that the population parameter is unknown, and so we cannot
measure things such as the mean squared error. we need a tool to report confidence. third,
for simple estimators such as the sample average, we can approximate it by a gaussian .
however, if the estimator is more complicated, e.g., the sample median, how do we estimate
the variance and the confidence? fourth, suppose that we have expanded the vaccine test
to, say, 951 patients, and we have obtained some statistics. to what extent can we declare
that the vaccine is effective? we need a decision rule that turns the statistics into a binary
decision. finally, even if we declare that the vaccine is effective with a confidence of 95%,
what about the remaining 5%? what if we want to push the confidence to 99%? what is
the trade-off?
as you can see, these questions are the recurring themes of all data science problems.
no matter if you are developing a medical diagnostic system, a computer vision algorithm,
a speech recognition system, a recommendation system, a search engine, stock forecast,
fraud detection, or robotics controls, you need to answer these questions. this chapter will
introduce useful concepts related to data analysis in the form of five basic principles :
1.confidence interval (section 9.1). a confidence interval is a random interval that
includes the true parameter. we will discuss how a confidence interval is constructed
and the correct way to interpret the confidence interval.
2.bootstrapping (section 9.2). when constructing the confidence interval, we need the
variance of the estimator. however, since we do not know the true distribution, we
need an alternative way to estimate the variance. bootstrapping is designed for this
purpose.
3.hypothesis testing (section 9.3). many statistical tasks require a binary decision at
the end, e.g., there is a disease versus there is no disease. hypothesis testing is a
principle for making a systematic decision with statistical guarantees.
4.neyman-pearson decision (section 9.4). the simple hypothesis testing procedure has
many limitations that can only be resolved if we understand a more general framework.
we will study such a framework, called the neyman-pearson decision rule.
5.roc and pr curves (section 9.5). no decision rule is perfect. there is always a
trade-off between how much we can detect and how much we will miss. the receiver
operating characteristic (roc) curve and the precision-recall (pr) curve can give us
more insight into this trade-off. we will establish the equivalence between the roc
and the pr curve and correct any misconceptions about them.
after reading this chapter, we hope that you will be able to apply these principles
to your favorite data analysis problems correctly. with these principles, you can tell your
customers or bosses the statistical significance of your conclusions. you will also be able to
help your friends understand the many misconceptions that they may find on the internet.
5429.1. confidence interval
9.1 confidence interval
the first topic we discuss in this chapter is the confidence interval . at a high level, the
confidence interval tells us the quality of our estimator with respect to the number of sam-
ples. we begin this section by reviewing the randomness of an estimator. then we develop
the concept of the confidence interval. we discuss several methods for constructing and
interpreting these confidence intervals.
9.1.1 the randomness of an estimator
imagine that we have a dataset x={x1, . . . , x n}, where we assume that xnare i.i.d.
copies drawn from a distribution fx(x;Œ∏). we want to construct an estimator bŒ∏ of Œ∏from
the dataset x. for example, if fxis a gaussian distribution with an unknown mean Œ∏, we
would like to estimate Œ∏using the sample average bŒ∏. in statistics, an estimator bŒ∏ is also
known as a statistic , which is constructed from the samples. in this book we use the terms
‚Äúestimator‚Äù and ‚Äústatistic‚Äù interchangeably. written as equations, an estimator is a function
of the samples:
bŒ∏|{z}
estimator=g(x1, . . . , x n)|{z }
function of x,
where gis a function that takes the samples x1, . . . , x nand returns a random variable bŒ∏.
for example, the sample average
bŒ∏ =1
nnx
n=1xn
|{z}
g(x1,...,x n)
is an estimator because it is computed by summing the samples x1, . . . , x nand dividing it
byn.
what is an estimator?
¬àan estimator bŒ∏ is a function of the samples x1, . . . , x n:
bŒ∏ =g(x1, . . . , x n). (9.1)
¬àbŒ∏ is a random variable. it has a pdf, cdf, mean, variance, etc.
by construction, bŒ∏ is a random variable because it is a function of the random samples.
therefore, bŒ∏ has its own pdf, cdf, mean, variance, etc. since bŒ∏ is a random variable,
we should report both the estimator‚Äôs value and the estimator‚Äôs confidence when reporting
its performance. the confidence measures the quality of bŒ∏ when compared to the true
parameter Œ∏. it provides a measure of the reliability of the estimator bŒ∏. ifbŒ∏ fluctuates a
great deal we may not be confident of our estimates. let‚Äôs consider the following example.
543chapter 9. confidence and hypothesis
example 9.1 . a class of 1000 students took a test. the distribution of the score is
roughly a gaussian with mean 50 and standard deviation 20. a teaching assistant
was too lazy to calculate the true population mean. instead, he sampled a subset of 5
scores listed as follows:
student id 1 2 3 4 5
scores 11 97 1 78 82
he calculated the average, which is 53.8. this is a very good estimate of the class
average (which is 50). what is wrong with his procedure?
solution . he was just lucky. it quite possible that if he sampled another 5 scores, he
would get something very different. for example, if he looks at the 11 to 15 student
scores, he could get:
student id 11 12 13 14 15
scores 44 29 19 27 15
in this case the average is 26.8.
both 53.8 and 26.8 are legitimate estimates, but they are the random realizations
of a random variable bŒ∏. this bŒ∏ has a pdf, cdf, mean, variance, etc. it may be
misleading to simply report the estimated value from a particular instant, so the
confidence of the estimator must be specified.
distributions of bŒ∏. we next discuss the distribution of bŒ∏.figure 9.1 illustrates several
key ideas. suppose that the population distribution fx(x) is a mixture of two gaussians.
letŒ∏be the mean of this distribution (somewhere between the two peak locations). we
sample n= 50 data points x1, . . . , x nfrom this distribution. however, the 50 data points
we sample today could differ from the 50 data points we sample tomorrow. if we compute
the sample average from each of these finite-sample distributions, we will obtain a set of
sample averages bŒ∏. notably, we have a setofbŒ∏ because today we have one bŒ∏ and tomorrow
we have another bŒ∏. by plotting the histogram of the sample averages bŒ∏, we will have a
distribution.
the histogram of bŒ∏ depends on several factors. according to central limit theorem,
the shape of fbŒ∏(Œ∏) is a gaussian because bŒ∏ is the average of ni.i.d. random variables.
ifbŒ∏ is not the average of i.i.d. random variables, the shape is not necessarily a gaussian.
this results in additional complications, so we will discuss some tools for dealing with this
problem. the spread of the sample distribution is mainly driven by the number of samples
we have in each subdataset. as you can imagine, the more samples we have in a subdataset
the more accurate the distribution. thus you will have a more accurate sample average. the
fluctuation of the sample average will also be smaller.
before we continue, let‚Äôs summarize the randomness of bŒ∏:
what is the randomness of bŒ∏?
¬àbŒ∏ is generated from a finite-sample dataset. each time we draw a finite-sample
dataset, we introduce randomness.
5449.1. confidence interval
figure 9.1: pictorial illustration of the randomness of the estimator bŒ∏. given a population, our datasets
are usually a subset of the population. computing the sample average from these finite-sample distribu-
tions introduces the randomness to bŒ∏. if we plot the histogram of the sample averages, we will obtain
a distribution. the mean of this distribution is the population mean, but there is a nontrivial amount of
fluctuation. the purpose of the concept of confidence interval is to quantify this fluctuation.
¬àifbŒ∏ is the sample average, the pdf is (roughly) a gaussian. if bŒ∏ is not a sample
average, the pdf is not necessarily a gaussian.
¬àthe spread of the fluctuation depends on the number of samples in each sub-
dataset.
9.1.2 understanding confidence intervals
the confidence interval is a probabilistic statement about bŒ∏. instead of studying bŒ∏ as a
point , we construct an interval
i=h
bŒ∏‚àíœµ,bŒ∏ +œµi
, (9.2)
for some œµto be determined. note that this interval is a random interval : if we have a
different realization of bŒ∏, we will have a different i. we call itheconfidence interval for
the estimator bŒ∏.
given this random interval, we ask: what is the probability that iincludes Œ∏? that
means that we want to evaluate the probability
p[Œ∏‚àà i] =ph
bŒ∏‚àíœµ‚â§Œ∏‚â§bŒ∏ +œµi
.
we emphasize that the randomness in this probability is caused by bŒ∏, not Œ∏. this is because
the interval ichanges when we conduct a different experiment to obtain a different bŒ∏. the
545chapter 9. confidence and hypothesis
situation is similar to that illustrated on the left-hand side of figure 9.2 . the confidence
interval ichanges but the true parameter Œ∏is fixed.
figure 9.2: confidence interval is the random interval i= [bŒ∏‚àíœµ,bŒ∏ +œµ], not the deterministic interval
[Œ∏‚àíœµ, Œ∏+œµ]. the random interval in the former case does not require any knowledge about the true
parameter Œ∏, whereas the latter requires Œ∏. by claiming a 95% confidence interval, we say that there
is 95% chance that the random interval will include the true parameter. so if you have 100 random
realizations of the confidence intervals, then 95 on average will include the true parameter.
confidence intervals can be confusing. often the confusion arises because of the fol-
lowing identity:
ph
bŒ∏‚àíœµ‚â§Œ∏‚â§bŒ∏ +œµi
=ph
‚àíœµ‚â§Œ∏‚àíbŒ∏‚â§œµi
=ph
‚àíœµ‚àíŒ∏‚â§ ‚àíbŒ∏‚â§œµ‚àíŒ∏i
=ph
Œ∏‚àíœµ‚â§bŒ∏‚â§Œ∏+œµi
. (9.3)
although the values of the two probabilities are the same, the two events are interpreted
differently. the right-hand side of figure 9.2 illustrates p[Œ∏‚àíœµ‚â§bŒ∏‚â§Œ∏+œµ]. the interval
[Œ∏‚àíœµ, Œ∏+œµ] is fixed. what is the probability that the estimator bŒ∏ lies within this deterministic
interval? to find this probability, we need to know the true parameter Œ∏, which is not
available. by contrast, the other probability p[bŒ∏‚àíœµ‚â§Œ∏‚â§bŒ∏ +œµ] does not require any
knowledge about the true parameter Œ∏. what is the probability that the true parameter is
included inside the random interval? if the probability is high, we say that there is a good
chance that our confidence interval will contain the true parameter. this is observed in the
left-hand side of figure 9.2 .
in practice we often set p[bŒ∏‚àíœµ‚â§Œ∏‚â§bŒ∏ +œµ] to be greater than a certain confidence
level, say 95%, and then we determine œµ. once we have determined œµ, we can claim that
5469.1. confidence interval
with 95% probability the interval [ bŒ∏‚àíœµ,bŒ∏ +œµ] will include the unknown parameter Œ∏. we
do not need to know Œ∏at any point in this process.
to make this more general, we define 1 ‚àíŒ±as the confidence level for some parame-
terŒ±. for example, if we would like to have a 95% confidence level, we set Œ±= 0.05. then
the probability inequality
ph
bŒ∏‚àíœµ‚â§Œ∏‚â§bŒ∏ +œµi
‚â•1‚àíŒ± (9.4)
tells us that there is at least a 95% chance that the random interval i= [bŒ∏‚àíœµ,bŒ∏ +œµ] will
include the true parameter Œ∏. in this case we say that iis a ‚Äú 95% confidence interval ‚Äù.
what is a 95% confidence interval?
¬àit is a random interval [ bŒ∏‚àíœµ,bŒ∏ +œµ] such that there is 95% probability for it to
include the true parameter Œ∏.
¬àit is not the deterministic interval [ Œ∏‚àíœµ, Œ∏+œµ], because we never know Œ∏.
let‚Äôs consider the following two examples to clarify any misconceptions.
example 9.2 . after analyzing the life expectancy of people in the united states, it
was concluded that the 95% confidence interval is (77.8, 79.1) years old. is the following
claim valid?
about 95% of the people in the united states have a life expectancy between 77.8
years old and 79.1 years old.
solution . no. the confidence interval tells us that with 95% probability the random
interval (77 .8,79.1) will include the true average. we emphasize that (77 .8,79.1) is
random because it is constructed from a small set of data points. if we survey another
set of people we will have another interval.
since we do not know the true average, we do not know the percentage of people
whose life expectancy is between 77.8 years old and 79.1 years old. it could be that the
true average is 80 years old, which is out of the range. it could also be that the true
average is 77.9 years old, which is within the range, but only 10% of the population
may have life expectancy in (77 .8,79.1).
example 9.3 . after studying the sat scores of 1000 high school students, it was
concluded that the 95% confidence interval is (1134, 1250) points. is the following
claim valid?
there is a 95% probability that the average sat score in the population is in the
range 1134 and 1250.
solution . yes, but it can be made clearer. the average sat score in the population
remains unknown. it is a constant and it is deterministic, so there is no probability
associated with it. a better way to say this is: ‚Äúthere is 95% probability that the
547chapter 9. confidence and hypothesis
random interval 1134 and 1250 will include the average sat score.‚Äù we emphasize
that the 95% probability is about the random interval, not the unknown parameter.
9.1.3 constructing a confidence interval
let‚Äôs consider an example. suppose that we have a set of i.i.d. observations x1, . . . , x n
that are gaussians with an unknown mean Œ∏and a known variance œÉ2. we consider the
maximum-likelihood estimator, which is the sample average:
bŒ∏ =1
nnx
n=1xn.
our goal is to construct a confidence interval.
figure 9.3: conceptual illustration of how to construct a confidence interval. starting with the pop-
ulation, we draw random subsets. each random subset gives us an estimator, and correspondingly an
interval.
before we consider the equations, let‚Äôs look at a graph illustrating what we want to
achieve. figure 9.3 shows a population distribution, which is a gaussian in this example.
we draw nsamples from the gaussian to construct a random subset. based on this random
subset we construct the estimator bŒ∏. since this estimator is based on the particular random
subset we have, we can follow the same approach by drawing another random subset. to
differentiate the estimators constructed by the different random subsets, let‚Äôs call the esti-
mators bŒ∏(1)andbŒ∏(2), respectively. for each estimator we construct an interval [ bŒ∏‚àíœµ,bŒ∏+œµ]
to obtain two different intervals:
i1= [bŒ∏(1)‚àíœµ,bŒ∏(1)+œµ] and i2= [bŒ∏(2)‚àíœµ,bŒ∏(2)+œµ].
5489.1. confidence interval
if we can determine œµ, we have found the confidence interval.
we can determine the confidence interval by observing the histogram of bŒ∏, which in
our case is the histogram of the sample average, since the histogram of bŒ∏ is well-defined,
especially if we are looking at the sample average. the histogram of the sample average is a
gaussian because the average of ni.i.d. gaussian random variables is gaussian. therefore,
the width of this gaussian is determined by the answer to this question:
for what œµcan we cover 95% of the histogram of bŒ∏?
to find the answer, we set up the following probability inequality:
pÔ£Æ
Ô£∞|bŒ∏‚àíe[bŒ∏]|q
var[bŒ∏]‚â§œµÔ£π
Ô£ª‚â•1‚àíŒ±.
this probability says that we want to find an œµsuch that the majority of bŒ∏ is living close
to its mean. the level 1 ‚àíŒ±is our confidence level, which is typically 95%. equivalently, we
letŒ±= 0.05.
in the above equation, we can define the quotient as
bzdef=bŒ∏‚àíe[bŒ∏]q
var[bŒ∏].
we know that bzis a zero-mean unit-variance gaussian because it is the standardized vari-
able. [note: not all normalized variables are gaussian, but if bŒ∏ is a gaussian the normalized
variable will remain a gaussian.] thus, the probability inequality we are looking at is
ph
|bz| ‚â§œµi
|{z}
two tails of a standard gaussian‚â• 1‚àíŒ±.
the pdf of bzis shown in figure 9.4 . as you can see, to achieve 95% confidence we need
to pick an appropriate œµsuch that the shaded area is less than 5%.
-3 -2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 300.050.10.150.20.250.30.350.4
figure 9.4: pdf of the random variable bz= (bŒ∏‚àíe[bŒ∏])/q
var[bŒ∏]. the shaded area denotes the
Œ±= 0.05confidence level.
549chapter 9. confidence and hypothesis
sincep[bz‚â§œµ] is the cdf of a gaussian, it follows that
p[|bz| ‚â§œµ] =p[‚àíœµ‚â§bz‚â§œµ]
=p[bz‚â§œµ]‚àíp[bz‚â§ ‚àíœµ]
= œÜ ( œµ)‚àíœÜ (‚àíœµ).
using the symmetry of the gaussian, it follows that œÜ ( ‚àíœµ) = 1‚àíœÜ (œµ) and hence
p[|bz| ‚â§œµ] = 2œÜ ( œµ)‚àí1.
equating this result with the probability inequality p[|bz| ‚â§œµ]‚â•1‚àíŒ±, we have that
œµ‚â•œÜ‚àí1
1‚àíŒ±
2
.
the remainder of this problem is solvable on a computer. on matlab, we can call
icdf to compute the inverse cdf of a standard gaussian. on python, the command is
stats.norm.ppf . the commands are as shown below.
% matlab code to compute the width of the confidence interval
alpha = 0.05;
mu = 0; sigma = 1; % standard gaussian
epsilon = icdf(‚Äônorm‚Äô,1-alpha/2,mu,sigma)
# python code to compute the width of the confidence interval
import scipy.stats as stats
alph = 0.05;
mu = 0; sigma = 1; # standard gaussian
epsilon = stats.norm.ppf(1-alph/2, mu, sigma)
print(epsilon)
if everything is done properly, we see that for a 95% confidence level ( Œ±= 0.05) the corre-
sponding œµisœµ= 1.96.
after determining œµ, it remains to determine e[bŒ∏] and var[ bŒ∏] in order to complete the
probability inequality. to this end, we note that
e[bŒ∏] =e"
1
nnx
n=1xn#
=Œ∏,
var[bŒ∏] = var"
1
nnx
n=1xn#
=œÉ2
n,
if we assume that the population distribution is gaussian( Œ∏, œÉ2), where Œ∏is unknown but œÉ
is known. substituting these into the probability inequality, we have that
pÔ£Æ
Ô£∞|bŒ∏‚àíe[bŒ∏]|q
var[bŒ∏]‚â§œµÔ£π
Ô£ª=p
bŒ∏‚àíœµœÉ‚àö
n‚â§Œ∏‚â§bŒ∏ +œµœÉ‚àö
n
=p
bŒ∏‚àí1.96œÉ‚àö
n‚â§Œ∏‚â§bŒ∏ + 1 .96œÉ‚àö
n
,
5509.1. confidence interval
where we let œµ= 1.96 for a 95% confidence level. therefore, the 95% confidence interval is

bŒ∏‚àí1.96œÉ‚àö
n,bŒ∏ + 1 .96œÉ‚àö
n
. (9.5)
as you can see, we do not need to know the value of Œ∏at any point of the derivation because
the confidence interval in equation (9.5) does not involve Œ∏. this is an important difference
with the other probability p[Œ∏‚àíœµ‚â§bŒ∏‚â§Œ∏+œµ], which requires Œ∏.
how to construct a confidence interval
¬àcompute the estimator bŒ∏.
¬àdetermine the width of the confidence interval œµby inspecting the confidence
level 1 ‚àíŒ±. ifbŒ∏ is gaussian, then œµ= œÜ‚àí1(1‚àíŒ±
2).
¬àifbŒ∏ is not a gaussian, replace the gaussian cdf by the cdf of bŒ∏.
¬àthe confidence interval is [ bŒ∏‚àíœµ,bŒ∏ +œµ].
9.1.4 properties of the confidence interval
some important properties of the confidence interval are listed below.
¬àprobability of bŒ∏is the same as probability of bz. first, the two random variables bŒ∏
andbzhave a one-to-one correspondence. we proved the following in chapter 6:
ifbŒ∏‚àºgaussian( Œ∏,œÉ2
n), then
bzdef=bŒ∏‚àíŒ∏
œÉ/‚àö
n‚àºgaussian(0 ,1). (9.6)
for example, if bŒ∏‚àºgaussian( Œ∏,œÉ2
n) with n= 1, Œ∏= 1 and œÉ= 2, then a 95%
confidence level is
0.95‚âàp[‚àí1.96‚â§bz‚â§1.96], (bzis within 1.96 std from bz‚Äôs mean)
=p[‚àí1.96‚â§bŒ∏‚àíŒ∏
œÉ/‚àö
n‚â§1.96]
=p
Œ∏‚àí1.96œÉ‚àö
n‚â§bŒ∏‚â§Œ∏+ 1.96œÉ‚àö
n
=p[‚àí2.92‚â§bŒ∏‚â§4.92]. (bŒ∏ is within 1.96 std from bŒ∏‚Äôs mean)
note that while the range for bzis different from the range for bŒ∏, they both return the
same probability. the only difference is that bŒ∏ is constructed before the normalization
andbzis constructed after the normalization.
551chapter 9. confidence and hypothesis
¬àstandard error . in this estimation problem we know that bŒ∏ is the sample average. we
assume that the mean Œ∏is unknown but the variance var[ bŒ∏] is known. the standard
deviation of bŒ∏ is called the standard error :
se=q
var[bŒ∏] =œÉ‚àö
n. (9.7)
¬àcritical value . the value 1 .96 in our example is often known as the critical value . it
is defined as
zŒ±= œÜ‚àí1
1‚àíŒ±
2
. (9.8)
thezŒ±value gives us a multiplier applied to the standard error that will result in a
value within the confidence interval. this is because, by the definition of the confidence
interval, the interval is

bŒ∏‚àí1.96œÉ‚àö
n,bŒ∏ + 1 .96œÉ‚àö
n
=h
bŒ∏‚àízŒ±se,bŒ∏ +zŒ±sei
¬àmargin of error . the margin of error is defined as
margin of error = zŒ±œÉ‚àö
n. (9.9)
the margin of error is also the width of the confidence interval. as the name implies,
the margin of error tells us how much error the confidence interval includes when
predicting the population parameter.
practice exercise 9.1 . suppose that the number of photos a facebook user uploads
per day is a random variable with œÉ= 2. in a set of 341 users, the sample average is
2.9. find the 90% confidence interval of the population mean.
solution . we set Œ±= 0.1. the zŒ±-value is
zŒ±= œÜ‚àí1
1‚àíŒ±
2
= 1.6449.
the 90% confidence interval is then

bŒ∏‚àí1.642‚àö
341,bŒ∏ + 1 .642‚àö
341
= [2.72,3.08].
therefore, with 90% probability, the interval [2 .72,3.08] includes the population mean.
example 9.4 . professional cyber-athletes have a standard deviation of œÉ= 73 .4
actions per minute. if we want to estimate the average actions per minute of the
population, how many samples are needed to obtain a margin of error <20 at 90%
confidence?
5529.1. confidence interval
solution . with a 90% confidence level, the zŒ±-value is
zŒ±= œÜ‚àí1
1‚àíŒ±
2
= œÜ‚àí1(0.95) = 1 .645.
the margin of error is 20. so we have
zŒ±œÉ‚àö
n= 20.
moving around the terms gives us
n‚â•
zŒ±œÉ
202
= 36.45.
therefore, we need at least n= 37 samples to ensure a margin of error of <20 at a
90% confidence level.
figure 9.5: relationships between the standard error se, the zŒ±value, and the margin of error. the
confidence level Œ±is the area under the curve for the tails of each pdf.
the concepts of standard error se, thezŒ±value, and the margin of error are summarized
infigure 9.5 . the left-hand side is the pdf of bz. it is the normalized random variable,
which is also the standard gaussian. the right-hand side is the pdf of bŒ∏, the unnormalized
random variable. the zŒ±value is located in the bz-space. it defines the range of bzin the
pdf within which we are confident about the true parameter. the corresponding value
in the bŒ∏-space is the margin of error . this is found by multiplying zŒ±with the standard
deviation of bŒ∏, known as the standard error . correspondingly, in the bz-space the standard
deviation is the unity.
two further points about the confidence interval should be mentioned:
¬ànumber of samples n. the confidence interval is a function of n. as we increase the
number of samples, the distribution of the estimator bŒ∏ becomes narrower. specifically,
ifbŒ∏ follows a gaussian distribution
bŒ∏‚àºgaussian
Œ∏,œÉ2
n
,
thenbŒ∏p‚ÜíŒ∏asn‚Üí ‚àû .figure 9.6 illustrates a few examples of bŒ∏ as ngrows. in the
limit when n‚Üí ‚àû , we observe that the interval becomes

bŒ∏‚àí1.96œÉ‚àö
n,bŒ∏ + 1 .96œÉ‚àö
n
‚àí‚Üíh
bŒ∏,bŒ∏i
=bŒ∏.
553chapter 9. confidence and hypothesis
in this case, the statement Œ∏‚ààh
bŒ∏‚àí1.96œÉ‚àö
n,bŒ∏ + 1 .96œÉ‚àö
ni
becomes Œ∏=bŒ∏. that
means the estimator bŒ∏ returns the correct true parameter Œ∏. of course, it is possible
thate[bŒ∏]Ã∏=Œ∏, i.e., the estimator is biased. in that case, having more samples will
approach another estimate that is not Œ∏.
-1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 100.511.522.533.54
n = 10
n = 25
n = 100
figure 9.6: the pdf of bŒ∏as the number of samples ngrows. here, we assume that xnare i.i.d.
gaussian random variables with mean Œ∏= 0and variance œÉ2= 1.
¬àdistribution of bz. when defining the confidence interval we constructed an interme-
diate variable
bz=bŒ∏‚àíŒ∏
œÉ/‚àö
n.
since xn‚Äôs are i.i.d. gaussian, it follows that zis also gaussian. this gives us a way
to calculate the probability using the standard gaussian table. what happens when
xn‚Äôs are notgaussian? the good news is that even if xn‚Äôs are not gaussian, for
sufficiently large n, the random variable bŒ∏ is more or less gaussian, because of the
central limit theorem. therefore, even if xn‚Äôs are not gaussian we can still use the
gaussian probability table to construct Œ±andœµ.
9.1.5 student‚Äôs t-distribution
in the discussions above, we estimate the population mean Œ∏using the estimator bŒ∏. the
assumption was that the variance œÉ2was known a priori and hence is fixed. in practice,
however, there are many situations where œÉ2is not known. thus we not only need to use
the mean estimator bŒ∏ but also the variance estimator bs, which can be defined as
bs2def=1
n‚àí1nx
n=1(xn‚àíbŒ∏)2,
where bŒ∏ is the estimator of the mean. what is the confidence interval for bŒ∏?
for a confidence interval to be valid, we expect it to take the form of
i="
bŒ∏‚àízŒ±bs‚àö
n,bŒ∏ +zŒ±bs‚àö
n#
,
5549.1. confidence interval
which is essentially the confidence interval we have just derived but with œÉreplaced by bs.
however, there is a problem with this. when we derive the confidence interval assuming a
known œÉ, the zŒ±value is determined by checking the standard gaussian
bz=bŒ∏‚àíŒ∏
œÉ/‚àö
n,
which gives us zŒ±= œÜ‚àí1(1‚àíŒ±/2). the whole derivation is based on the fact that bzis a
standard gaussian. now that we have replaced œÉbybs, the new random variable
tdef=bŒ∏‚àíŒ∏
bs/‚àö
n(9.10)
isnota standard gaussian.
it turns out that the distribution of tis student‚Äôs t-distribution with n‚àí1 degrees
of freedom. the pdf of student‚Äôs t-distribution is given as follows.
definition 9.1. ifxis a random variable following student‚Äôs t-distribution ofŒΩ
degrees of freedom, then the pdf of xis
fx(x) =Œ≥ ŒΩ+1
2
‚àöŒΩœÄŒ≥ ŒΩ
2
1 +x2
ŒΩ‚àíŒΩ+1
2
. (9.11)
we may compare student‚Äôs t-distribution with the gaussian distribution. figure 9.7 shows
the standard gaussian and several tdistributions with ŒΩ=n‚àí1 degrees of freedom. note
that student‚Äôs t-distribution has a similar shape to the gaussian but it has a heavier tail.
-5 -4 -3 -2 -1 0 1 2 3 4 500.050.10.150.20.250.30.350.4
gaussian(0,1)
t-dist, n = 11
t-dist, n = 3
t-dist, n = 2
figure 9.7: the pdf of student‚Äôs t-distribution with ŒΩ=n‚àí1degrees of freedom.
since t=bŒ∏‚àíŒ∏
bs/‚àö
nis at-random variable, to determine the zŒ±value we can follow the
same procedure by considering the cdf of t. let the cdf of the student‚Äôs t-distribution
with ŒΩdegrees of freedom be
œàŒΩ(z) = cdf of xatz.
555chapter 9. confidence and hypothesis
if we want p[|t| ‚â§zŒ±] = 1‚àíŒ±, it follows that
zŒ±= œà‚àí1
ŒΩ
1‚àíŒ±
2
. (9.12)
therefore, the new confidence interval, assuming an unknown bs, is
i="
bŒ∏‚àízŒ±bs‚àö
n,bŒ∏ +zŒ±bs‚àö
n#
,
with zŒ±defined in equation (9.12), using ŒΩ=n‚àí1.
practice exercise 9.2 . a survey asked n= 14 people for their rating of a movie. as-
sume that the mean estimator is bŒ∏ and the variance estimator is bs. find the confidence
interval.
solution . if we use student‚Äôs t-distribution, it follows that
zŒ±= œà‚àí1
13
1‚àíŒ±
2
= 2.16,
where the degrees of freedom are ŒΩ= 14‚àí1 = 13. thus the confidence interval is
i="
bŒ∏‚àí2.16bs‚àö
n,bŒ∏ + 2 .16bs‚àö
n#
.
the matlab and python codes to report the zŒ±value of a student‚Äôs t-distribution
are shown below. they are both called through the inverse cdf function. in matlab it
isicdf, and in python it is stats.t.ppf .
% matlab code to compute the z_alpha value of t distribution
alpha = 0.05;
nu = 13;
z = icdf(‚Äônorm‚Äô,1-alpha/2,nu)
# python code to compute the z_alpha value of t distribution
import scipy.stats as stats
alph = 0.05
nu = 13
z = stats.t.ppf(1-alph/2, nu)
print(z)
example 9.5 . a class of 10 students took a midterm exam. their scores are given in
the following table.
student 1 2 3 4 5 6 7 8 9 10
score 72 69 75 58 67 70 60 71 59 65
find the 95% confidence interval.
5569.1. confidence interval
solution . the mean and standard deviation of the datasets are respectively bŒ∏ = 66 .6
andbs= 5.61. the critical zŒ±value is determined by student‚Äôs t-distribution:
zŒ±= œà‚àí1
9
1‚àíŒ±
2
= 2.26.
the confidence interval is
"
bŒ∏‚àízŒ±bs‚àö
n,bŒ∏ +zŒ±bs‚àö
n#
= [62 .59,70.61].
therefore, with 95% probability, the interval [62 .59,70.61] will include the true popu-
lation mean.
remark 1 . make sure you understand the meaning of ‚Äúpopulation mean‚Äù in this
example. since we have ten students, isn‚Äôt the population mean just the average of the
ten scores? this is incorrect. in statistics, we assume that these ten students are the
realizations of some underlying (unknown) random variable xwith some pdf fx(x).
the population mean Œ∏is therefore the expectation e[x], where the expectation is
taken w.r.t. fx. the sample average bŒ∏, which is the average of the ten numbers, is an
estimator of the population mean Œ∏.
remark 2 . you may be wondering why we are using student‚Äôs t-distribution here
when we do not even know the pdf of x. the answer is that it is an approximation.
when xis gaussian, the sample average bŒ∏ is a student‚Äôs t-distribution, assuming
that the variance is approximated by the sample variance bs. this result is attributed
to the original paper of william gosset, who developed student‚Äôs t-distribution.
the above example can be solved computationally. an implementation through python
is given below, and the matlab implementation is straightforward if you translate it from
the python.
# python code to generate a confidence interval
import numpy as np
import scipy.stats as stats
x = np.array([72, 69, 75, 58, 67, 70, 60, 71, 59, 65])
theta_hat = np.mean(x) # sample mean
s_hat = np.std(x) # sample standard deviation
nu = x.size-1 # degrees of freedom
alpha = 0.05 # confidence level
z = stats.t.ppf(1-alph/2, nu)
ci_l = theta_hat-z*s_hat/np.sqrt(n)
ci_u = theta_hat+z*s_hat/np.sqrt(n)
print(ci_l, ci_u)
what is student‚Äôs t-distribution?
¬àit was developed by william gosset in 1908. when he published the paper he
used the pseudonym student.
557chapter 9. confidence and hypothesis
¬àwe use student‚Äôs t-distribution to model the estimator bŒ∏‚Äôs pdf when the vari-
ance œÉ2is replaced by the sample variance bs2.
¬àstudent‚Äôs t-distribution has a heavier tail than a gaussian.
9.1.6 comparing student‚Äôs t-distribution and gaussian
we now discuss an important theoretical result regarding the relationship between a stu-
dent‚Äôs t-distribution and gaussian distribution. the main result is that the standard gaus-
sian is a limiting distribution of the tdistribution as the degrees of freedom ŒΩ‚Üí ‚àû .
theorem 9.1. asŒΩ‚Üí ‚àû , the student‚Äôs t-distribution approaches the standard gaus-
sian distribution:
lim
ŒΩ‚Üí‚àû(
Œ≥ ŒΩ+1
2
‚àöŒΩœÄŒ≥ ŒΩ
2
1 +y2
ŒΩ‚àíŒΩ+1
2)
=1‚àö
2œÄe‚àít2
2. (9.13)
the proof of the theorem requires stirling‚Äôs approximation, which is not essential for this
book. feel free to skip it if needed.
proof . there are two results we need to use:
¬àstirling‚Äôs approximation:2Œ≥(z)‚âàq
2œÄ
z z
ez.
¬àexponential approximation: (1 +x
k)‚àík‚Üíe‚àíx, ask‚Üí ‚àû .
we have that
Œ≥ ŒΩ+1
2
‚àöŒΩœÄŒ≥ ŒΩ
2‚âàq
2œÄ
ŒΩ+1
2 ŒΩ+1
2eŒΩ+1
2
‚àöŒΩœÄq
2œÄ
ŒΩ
2 ŒΩ
2eŒΩ
2
=1‚àöŒΩœÄrŒΩ
ŒΩ+ 11‚àöeŒΩ+ 1
ŒΩŒΩ
2‚àöŒΩ+ 1‚àöŒΩ
=1‚àöŒΩœÄ‚àöŒΩ‚àö
2eŒΩ+ 1
ŒΩŒΩ
2
=1‚àö
2œÄe
1 +1
ŒΩŒΩ
2
.
putting a limit of ŒΩ‚Üí ‚àû , we have that
lim
ŒΩ‚Üí‚àû1‚àö
2œÄe
1 +1
ŒΩŒΩ
2
=1‚àö
2œÄee1
2=1‚àö
2œÄ.
2k. g. binmore, mathematical analysis: a straightforward approach . cambridge university press, 1977.
section 17.7.2.
5589.2. bootstrapping
the other limit follows from the fact that
lim
ŒΩ‚Üí‚àû
1 +t2
ŒΩ‚àíŒΩ+1
2
=e‚àít2
2.
combining the two limits proves the theorem. ‚ñ°
end of the proof. please join us again.
this theorem has several implications:
¬àwhen nis large, s2‚ÜíœÉ2. the gaussian approximation kicks in, and so student‚Äôs
t-distribution is more or less the same as the gaussian.
¬àstudent‚Äôs t-distribution is better for small n, usually n‚â§30. if n‚â•30, using the
gaussian approximation suffices.
¬àifxis gaussian, student‚Äôs t-distribution is an excellent model. if xis not gaussian,
student‚Äôs t-distribution will have some issues unless nincreases.
9.2 bootstrapping
when estimating the confidence interval, we focus exclusively on the sample average bŒ∏ =
(1/n)pn
n=1xn. there are, however, many estimators that are not sample averages. for
example, we might be interested in an estimator that estimates the sample median: bŒ∏ =
median {x1, . . . , x n}. in such cases, the gaussian-based analysis or the student‚Äôs t-based
analysis we just derived would not work.
stepping back a little further, it is important to understand the hierarchy of estimation.
figure 9.8 illustrates a rough breakdown of the various techniques. on the left-hand side
of the tree, we have three point estimation methods: mle, map, and mmse. they are
so-called point estimation methods because they are reporting a point ‚Äî a single value.
this stands in contrast to the right-hand side of the tree, in which we report the confidence
interval . note that point estimates and confidence intervals do not conflict with each other.
the point estimates are used for the actual engineering solution and the confidence intervals
are used to report the confidence about the point estimates. under the branch of confidence
intervals we discussed sample average. however, if we want to study an estimator that is
not the sample average, we need the technique known as the bootstrapping ‚Äî a method
for estimating the confidence interval. notably, it does not give you a better point estimate.
as we have frequently emphasized, since bŒ∏ is a random variable , it has its own pdf,
cdf, mean, variance, etc. the confidence interval introduced in the previous section pro-
vides one way to quantify the randomness of bŒ∏. throughout the derivation of the confidence
interval we need to estimate the variance var( bŒ∏). for simple problems such as the sample
average, analyzing var( bŒ∏) is not difficult. however, if bŒ∏ is a more complicated statistic, e.g.,
themedian , analyzing var( bŒ∏) may not be as straightforward. bootstrapping is a technique
that is suitable for this purpose.
559chapter 9. confidence and hypothesis
figure 9.8: hierarchy of estimation. bootstrapping belongs to the category of confidence interval. it is
used to report the confidence intervals for estimators that are not the sample averages.
why is it difficult to provide a confidence interval for estimators such as the median?
a couple of difficulties arise:
¬àmany estimators do not have a simple expression for the variance. for simple esti-
mators such as the sample average bŒ∏ = (1 /n)pn
n=1xn, the variance is œÉ2/n. if the
estimator is the median bŒ∏ = median {x1, . . . , x n}, the variance of bŒ∏ will depend on
the underlying distribution of the xn‚Äôs. if the estimator is something beyond the sam-
ple median, the variance of bŒ∏ can be even more complicated to determine. therefore,
techniques such as central limit theorem do not apply here.
¬àwe typically have only oneset of data points. we cannot re-collect more i.i.d. samples
to estimate the variance of the estimator. therefore, our only option is to squeeze the
information from the data we have been given.
when do we use bootstrapping?
¬àbootstrapping is a technique to estimate the confidence interval.
¬àwe use bootstrapping when the estimator does not have a simple expression for
the variance.
¬àbootstrapping allows us to estimate the variance without re-collecting more data.
¬àbootstrapping does not improve your point estimates.
9.2.1 a brute force approach
before we discuss the idea of bootstrapping, we need to elaborate on the difficulty of esti-
mating the variance using repeated measurements. suppose that we somehow have access to
the population distribution. let us denote the cdf of this population distribution by fx,
5609.2. bootstrapping
and the pdf by fx. by having access to the population distribution we can synthetically
generate as many samples xn‚Äôs as we want. this is certainly hypothetical, but let‚Äôs assume
that it is possible for now.
if we have full access to the population distribution, then we are able to draw k
replicate datasets x1, . . . ,xkfrom fx:
x(1)={x(1)
1, . . . , x(1)
n} ‚àºfx,
x(2)={x(2)
1, . . . , x(2)
n} ‚àºfx, (9.14)
...
x(k)={x(k)
1, . . . , x(k)
n} ‚àºfx.
each dataset x(k)contains ndata points, and by virtue of i.i.d. all the samples have the
same underlying distribution fx.
for each dataset we construct an estimator bŒ∏ = g(¬∑) for some function g(¬∑). the
estimator takes the data points of the dataset xand returns a value. since we have k
datasets, correspondingly we will have kestimators:
bŒ∏(1)=g(x(1)) =g(x(1)
1, . . . , x(1)
n),
bŒ∏(2)=g(x(2)) =g(x(2)
1, . . . , x(2)
n), (9.15)
...
bŒ∏(k)=g(x(k)) =g(x(k)
1, . . . , x(k)
n).
note that these estimators g(¬∑) can be anything. it can be the sample average or it can be
the sample median. there is no restriction.
since we are interested in constructing the confidence interval for bŒ∏, we need to analyze
the mean and variance of bŒ∏. the true mean and the estimated mean of bŒ∏ are
e[bŒ∏] = true mean of bŒ∏, (9.16)
m(bŒ∏) = estimated mean based on bŒ∏(1), . . . ,bŒ∏(k)
def=1
kkx
k=1bŒ∏(k)=1
kkx
k=1g(x(k)), (9.17)
respectively. similarly, the true variance and the estimated variance of bŒ∏ are
var[bŒ∏] = true variance of bŒ∏, (9.18)
v(bŒ∏) = estimated variance based on bŒ∏(1), . . . ,bŒ∏(k)
def=1
kkx
k=1
bŒ∏(k)‚àím(bŒ∏)2
=1
kkx
k=1
g(x(k))‚àím(bŒ∏)2
. (9.19)
561chapter 9. confidence and hypothesis
these two equations should be familiar: since bŒ∏ is a random variable, and {bŒ∏(k)}are i.i.d.
copies of bŒ∏, we can compute the average of bŒ∏(1), . . . ,bŒ∏(k)and the corresponding variance.
as the number of repeated trials kapproaches ‚àû, the estimated variance v(bŒ∏) will converge
to var( bŒ∏) according to the law of large numbers.
we can summarize the procedure we have just outlined. to produce an estimate of the
variance, we run the algorithm below.
algorithm 1: brute force method to generate an estimated variance
¬àassume: we have access to fx.
¬àstep 1: generate datasets x(1), . . . ,x(k)from fx.
¬àstep 2: compute m(bŒ∏) and v(bŒ∏) based on the samples.
¬àoutput: the estimated variance is v(bŒ∏).
the problem, however, is that we only have onedataset x(1). we do not have access to
x(2), . . . ,x(k), and we do not have access to fx. therefore, we are not able to approxi-
mate the variance using the above brute force simulation. bootstrapping is a computational
technique to mimic the above simulation process by using the available data in x(1).
9.2.2 bootstrapping
the idea of bootstrapping is illustrated in figure 9.9 . imagine that we have a population
cdf fxand pdf fx. the dataset we have in hand, x, is a collection of the random realiza-
tions of the random variable x. this dataset xcontains ndata points x={x1, . . . , x n}.
figure 9.9: a conceptual illustration of bootstrapping. given the observed dataset x, we synthetically
construct kbootstrapped datasets (colored in yellow) by sampling with replacement from x. we
then compute the estimators, e.g., computing the median, for every bootstrapped dataset. finally, we
construct the estimator‚Äôs histogram (in blue) to compute the bootstrapped mean and variance.
in bootstrapping, we synthesize kbootstrapped datasets y(1), . . . ,y(k), where each
bootstrapped dataset y(k)consists of nsamples redrawn from x. essentially, we draw with
5629.2. bootstrapping
replacement nsamples from the observed dataset x:
y(1)={y(1)
1, . . . , y(1)
n}=nrandom samples from x,
...
y(k)={y(k)
1, . . . , y(k)
n}=nrandom samples from x.
afterward, we construct our estimator bŒ∏ according to our desired function g(¬∑). for example,
ifg(¬∑) = median, we have
bŒ∏(1)
boot=g(y(1)) = median( y(1)),
...
bŒ∏(k)
boot=g(y(k)) = median( y(k)).
then, we define the bootstrapped mean and the bootstrapped variance as
mboot(bŒ∏) =1
kkx
k=1bŒ∏(k)
boot, (9.20)
vboot(bŒ∏) =1
kkx
k=1
bŒ∏(k)
boot‚àímboot(bŒ∏)2
. (9.21)
the procedure we have just outlined can be summarized as follows.
algorithm 2: bootstrapping to generate an estimated variance
¬àassume: we do not have access to fx, but we have one dataset x.
¬àstep 1: generate datasets y(1), . . . ,y(k)fromx, by sampling with replacement from
x.
¬àstep 2: compute mboot(bŒ∏) and vboot(bŒ∏) based on the samples.
¬àoutput: the bootstrapped variance is vboot(bŒ∏).
the only difference between this algorithm and the previous one is that we are not synthe-
sizing data from the population but rather from the observed dataset x.
what makes bootstrapping work? the basic principle of bootstrapping is based on
three approximations:
varf(bŒ∏)(a)‚âàvfull(bŒ∏)(b)‚âà
varbf(bŒ∏)(c)‚âàvboot(bŒ∏)
in this set of equations, the ultimate quantity we want to know is var f(bŒ∏), which is the
variance of bŒ∏ under f. (by ‚Äúunder f‚Äù we mean that the variance was found by integrating
with respect to the distribution fx.) however, since we do not have access to f, we have
563chapter 9. confidence and hypothesis
to approximate var f(bŒ∏) by vfull(bŒ∏).vfull(bŒ∏) is the sample variance computed from the
khypothetical datasets x(1), . . . ,x(k). we call it ‚Äúfull‚Äù because we can generate as many
hypothetical datasets as we want. it is marked as the approximation ( a) above.
in the bootstrapping world, we approximate the underlying distribution fby some
other distribution bf. for example, if fis the cdf of a gaussian distribution, we can
choose bfto be the finite-sample staircase function approximating f. in our case, we use the
observed dataset xto serve as a proxy bftof. this is the second approximation, marked by
(b). normally, if you have a reasonably large x, it is safe to assume that this finite-sample
dataset xhas a cdf bfthat is close to the true cdf f.
the third approximation is to find a numerical estimate var bf(bŒ∏) via the simulation
procedure we have just outlined. this is essentially the same line of argument for ( a) but
now applied to the bootstrapping world. we mark this approximation by ( c). its goal is to
approximate var bf(bŒ∏) via vboot(bŒ∏).
the three approximations have their respective influence on the accuracy of the boot-
strapped variance:
how does bootstrapping work?
¬àit is based on three approximations:
¬à(a): a hypothetical approximation. the best we can do is that we have access
tof. it is practically impossible to achieve, but it gives us intuition.
¬à(b): approximate fbybf, where bfis the empirical cdf of the observed data.
this is usually the source of error. the approximation error reduces when you
use more samples to approximate f.
¬à(c): approximate the theoretical bootstrapped variance by a finite approxima-
tion. this approximation error is usually small because you can generate as many
bootstrapped datasets as you want.
one ‚Äúmysterious‚Äù property of bootstrapping is the sampling with replacement scheme
used to synthesize the bootstrapped samples. the typical questions are:
¬à(1)why does sampling from the observed dataset xlead to meaningful boot-
strapped datasets y(1), . . . ,y(k)? to answer this question we consider the following
toy example. suppose we have a dataset xcontaining n= 20 samples, as shown
below.
x = [0 0 0 0 0 0 1 1 1 1 2 2 2 2 2 2 2 2 2 2]
this dataset is generated from a random variable xwith a pdf bfhaving three states:
0 (30%), 1 (20%), 2 (50%). as we draw samples from x, the percentage of the states
will determine the likelihood of one state being drawn. for example, if we randomly
pick a sample ynfromx, we have a 30% chance of having ynto be 0, 20% chance
of having it to be 1, and 50% chance of having it to be 2. therefore, the pdf of yn
(the randomly drawn sample from x) will be 0 (30%), 1 (20%), 2 (50%), the same
as the original pdf. if you think about this problem more deeply, by ‚Äúsampling with
replacement‚Äù we essentially assign each xnwith an equal probability of 1 /n. if one
of the states is more popular, the individual probabilities will add to form a higher
probability mass.
5649.2. bootstrapping
¬à(2)why can‚Äôt we do sampling without replacement, aka permutation ? we need to
understand that sampling without replacement is the same as permuting the data in x.
by permuting the data in x, the simple probability assignments such as p[x= 0] =6
20,
p[x= 1] =4
20andp[x= 2] =10
20will be destroyed. moreover, permuting the data
does not change the mean and variance of the data because we are only shuffling the
order. as far as constructing the confidence interval is concerned, shuffling the order
is not useful.
on computers it is easy to generate the bootstrapped dataset, along with their mean
and variance. in matlab the key step is to call a forloop. inside the forloop, we draw
nrandom indices randi from 1 to nand pick the samples. the estimator thetahat is then
constructed by calling your target estimator function g(¬∑). in this example the estimator is
the median. after the forloop, we compute the mean and variance of bŒ∏. these are the
bootstrapped mean and variance, respectively.
% matlab code to estimate a bootstrapped variance
x = [72, 69, 75, 58, 67, 70, 60, 71, 59, 65];
n = size(x,2);
k = 1000;
thetahat = zeros(1,k);
for i=1:k % repeat k times
idx = randi(n,[1, n]); % sampling w/ replacement
y = x(idx);
thetahat(i) = median(y); % estimator
end
m = mean(thetahat) % bootstrapped mean
v = var(thetahat) % bootstrapped variance
the python commands are similar. we call np.random.randint to generate random
integers and we pick samples according to y = x[idx] . after generating the bootstrapped
dataset, we compute the bootstrap estimators thetahat .
# python code to estimate a bootstrapped variance
import numpy as np
x = np.array([72, 69, 75, 58, 67, 70, 60, 71, 59, 65])
n = x.size
k = 1000
thetahat = np.zeros(k)
for i in range(k):
idx = np.random.randint(n, size=n)
y = x[idx]
thetahat[i] = np.median(y)
m = np.mean(thetahat)
v = np.var(thetahat)
after we have constructed the bootstrapped variance, we can define the bootstrapped
standard error as
bseboot=q
vboot(bŒ∏). (9.22)
565chapter 9. confidence and hypothesis
accordingly we define the bootstrapped confidence interval as
i=bŒ∏‚àízŒ±bseboot,bŒ∏ +zŒ±bseboot
, (9.23)
where zŒ±is the critical value of the gaussian.
the validity of the confidence intervals constructed by bootstrapping is subject to
the validity of zŒ±. ifbŒ∏ is roughly a gaussian, the bootstrapped confidence interval will be
reasonably good. if bŒ∏ is not gaussian, there are advanced methods to replace zŒ±with better
estimates. this topic is beyond the scope of this book; we refer interested readers to larry
wasserman, all of statistics , springer 2003, chapter 8.
9.3 hypothesis testing
imagine that you are a vaccine company developing covid-19 vaccines. you gave the
vaccine to 934 patients, and 928 patients have developed antigens. how confident can you
be that your vaccine is effective? questions like this are becoming more common nowadays
in situations in which we need to make statistically informed choices between yes and no.
the subject of this section is hypothesis testing ‚Äî a principled statistical procedure used
to evaluate statements that should be accepted or rejected.
9.3.1 what is a hypothesis?
a hypothesis is a statement that requires testing by observation to determine whether it is
true or false. a few examples:
¬àthe coin is unbiased.
¬àstudents entering the graduate program have gpa ‚â•3.
¬àmore people like orange juice than lemonade.
¬àalgorithm a performs better than algorithm b.
as you can see from these examples, a hypothesis is something we can test based on the
data. therefore, being ‚Äúcorrect‚Äù or ‚Äúwrong‚Äù depends on the statistics we have and the cutoff
threshold. accepting or rejecting a hypothesis does not mean that the statement is correct
or wrong, since the truth is unknown. if we accept a hypothesis, we have made a better
decision solely based on the statistical evidence. it is possible that tomorrow when you have
collected more data we may reject a previously accepted hypothesis.
the procedure for testing whether a hypothesis should be accepted or rejected is known
ashypothesis testing . in hypothesis testing, we often have two opposite hypotheses:
¬àh0: null hypothesis. it is the ‚Äústatus quo‚Äù, or the current status.
¬àh1: alternative hypothesis. it is the alternative to the null hypothesis.
to better understand hypothesis testing, consider a courthouse. by default, any person
being prosecuted is assumed to be innocent. the police need to show sufficient evidence in
order to prove the person guilty. the null hypothesis is the default assumption. hypothesis
testing asks whether we have strong enough evidence to reject the null hypothesis. if our
evidence is not strong enough, we must assume that the null hypothesis is possibly true.
5669.3. hypothesis testing
example 9.6 . suggest a null hypothesis and an alternative hypothesis regarding
whether a coin is unbiased.
solution : let Œ∏be the probability of getting a head.
¬àh0:Œ∏= 0.5, and h1:Œ∏ >0.5. this is a one-sided alternative.
¬àh0:Œ∏= 0.5, and h1:Œ∏ <0.5. this is another one-sided alternative.
¬àh0:Œ∏= 0.5, and h1:Œ∏Ã∏= 0.5. this is a two-sided alternative.
practice exercise 9.3 . suggest a null and an alternative hypothesis regarding whether
more than 62% of people in the united states use microsoft windows.
solution : let Œ∏be the proportion of people using microsoft windows in united states.
¬àh0:Œ∏‚â•0.62, and h1:Œ∏ <0.62. this is a one-sided alternative.
practice exercise 9.4 . suggest a null and an alternative hypothesis regarding whether
self-checkout at walmart is faster than using a cashier.
solution : let Œ∏be the proportion of people that check out faster with self-checkout..
¬àh0:Œ∏‚â•0.5, and h1:Œ∏ <0.5. this is a one-sided alternative.
9.3.2 critical-value test
in hypothesis testing, there are two major approaches: the critical-value test , and the
p-value test . the two tests are more or less equivalent. if you reject the null hypothesis using
the critical-value test, you will reject the hypothesis using the p-value. in this subsection,
we will discuss the critical-value test. let us consider a toy problem:
suppose that we have a 4-sided die and our goal is to test whether the die is unbiased.
to do so, we define the null and the alternative hypotheses as
¬àh0:Œ∏= 0.25, which is our default belief.
¬àh1:Œ∏ >0.25, which is a one-sided alternative.
there is no particular reason for considering the one-sided alternative other than the fact
that the calculation is slightly easier. you are welcome to consider the two-sided alternative.
we must obtain data prior to conducting any hypothesis testing. let‚Äôs assume that we
have thrown the die n= 1000 times. we find that ‚Äú3‚Äù appears 290 times (we could just as
well have chosen 1, 2, or 4). we let x1, . . . , x 1000be the n= 1000 binary random variables
representing whether we have obtained a ‚Äú3‚Äù or not. if the true probability is Œ∏= 0.25, then
we will have p[xn= 3] = Œ∏= 0.25 and p[xnÃ∏= 3] = 1 ‚àíŒ∏= 0.75. we know that we cannot
access the true probability, so we can only construct an estimator of the probability:
bŒ∏ =1
nnx
n=1xn.
567chapter 9. confidence and hypothesis
in this experiment, we can show that bŒ∏ = 290 /1000 = 0 .29.
to make our problem slightly easier, we pretend that we know the variance var[ xn].
in practice, we certainly do not know var[ xn], and so we need to estimate the variance. if
we knew the variance, it should be var[ xn] =Œ∏(1‚àíŒ∏) = 0 .25(1‚àí0.25) = 0 .1875, because
xnis a bernoulli random variable with a mean Œ∏.
the question asked by hypothesis testing is: how far is ‚Äú bŒ∏ = 0 .29‚Äù from ‚Äú Œ∏= 0.25‚Äù?
if the statistic generated by our data, bŒ∏ = 0 .29, is ‚Äúfar‚Äù from the hypothesized Œ∏= 0.25,
then we need to reject h0because h0says that Œ∏= 0.25. however, if there is no strong
evidence that Œ∏ >0.25, we will need to assume that h0may possibly be true. so the key
question is what is meant by ‚Äúfar‚Äù.
for many problems like this one, it is possible to analyze the pdf of bŒ∏. since bŒ∏ is the
sample average of a sequence of bernoulli random variables, it follows that bŒ∏ is a binomial
(with a scaling constant 1 /n). ifnis large enough, e.g., n‚â•30, the central limit theorem
tells us that bŒ∏ is also very close to a gaussian. therefore, we can more or less claim that
bŒ∏‚àºgaussian
Œ∏,œÉ2
n
.
with a simple translation and scaling, we can normalize bŒ∏ to obtain bz:
bz=bŒ∏‚àíŒ∏
œÉ/‚àö
n‚àºgaussian (0 ,1).
figure 9.10 illustrates the range of values for this problem. there are two axes: the bŒ∏-
axis (which is the estimator) and the bz-axis (which is the normalized variable). the values
corresponding to each axis are shown in the figure. for example. bŒ∏ = 0 .29 is equivalent
tobz= 2.92, and bŒ∏ = 0 .25 is equivalent to bz= 0, etc. therefore, when we ask how far
‚ÄúbŒ∏ = 0 .29‚Äù is from ‚Äú Œ∏= 0.25‚Äù, we can map this question from the bŒ∏-axis to the bz-axis,
and ask the relative position of bzfrom the origin.
figure 9.10: the mapping between bŒ∏andbz. to decide whether we want to reject or keep h0, the
critical-value approach compares bzrelative to the critical value zŒ±.
on a computer, obtaining these values is quite straightforward. using matlab, find-
ingbzcan be done by calling the following commands. the python code is analogous.
5689.3. hypothesis testing
% matlab command to estimate the z_hat value.
theta_hat = 0.29; % your estimate
theta = 0.25; % your hypothesis
sigma = sqrt(theta*(1-theta)); % known standard deviation
n = 1000; % number of samples
z_hat = (theta_hat - theta)/(sigma/sqrt(n));
# python command to estimate the z_hat value
import numpy as np
theta_hat = 0.29 # your estimate
theta = 0.25 # your hypothesis
n = 1000 # number of samples
sigma = np.sqrt(theta*(1-theta)) # known standard deviation
z_hat = (theta_hat - theta)/(sigma / np.sqrt(n))
print(z_hat)
one essential element of hypothesis testing is the cutoff threshold, which is defined
through the critical level Œ±. it is the area under the curve of the pdf of bz. typically,
Œ±is chosen to be a small value, such as Œ±= 0.05 (corresponding to a 5% margin). the
corresponding cutoff is known as the critical value . it is defined as
zŒ±= cutoff location where area under the curve is Œ±.
ifbzis gaussian(0,1) and if we are looking at the right-hand tail, it follows that
zŒ±= œÜ‚àí1(1‚àíŒ±). (9.24)
in our example, we find that z0.05= 1.65, which is marked in figure 9.10 .
on computers, determining the critical value zŒ±is straightforward. in matlab the
command is icdf, and in python the command is stats.norm.ppf .
% matlab code to compute the critical value
alpha = 0.05;
z_alpha = icdf(‚Äônorm‚Äô, 1-alpha, 0, 1);
# python code to compute the critical value
import scipy.stats as stats
alpha = 0.05
z_alpha = stats.norm.ppf(1-alpha, 0, 1)
do we have enough evidence to reject h0in this example? of course! the estimated
valuebŒ∏ = 0 .29 is equivalent to bz= 2.92, which is much too far from the cutoff zŒ±= 1.65.
in other words, we conclude that at a 5% critical level we have strong evidence to believe
that the die is biased. therefore, we need to reject h0.
this conclusion makes a lot of sense if you think about it carefully. the estimator
bŒ∏ = 0 .29 is obtained from n= 1000 independent experiments. if we were only conducting
n= 20 experiments, it might be consistent with the null hypothesis to have bŒ∏ = 0 .29.
569chapter 9. confidence and hypothesis
however, if we have n= 1000 experiments, having bŒ∏ = 0 .29 does not seem likely when
there is no systematic bias. if there is no systematic bias, the estimator bŒ∏ should slightly
jitter around bŒ∏ = 0 .25, but it is quite unlikely to vary wildly to bŒ∏ = 0 .29. thus, based on
the available statistics, we decide to reject the null hypothesis.
the decision based on comparing the critical value is known as the critical-value test .
the idea (for testing a right-hand tail of a gaussian random variable) is summarized in
three steps:
how to conduct a critical-value test
¬àset a critical value zŒ±. compute bz= (bŒ∏‚àíŒ∏)/(œÉ/‚àö
n).
¬àifbz‚â•zŒ±, then reject h0.
¬àifbz < z Œ±, then keep h0.
if you are testing a left-hand tail , you can switch the order of the inequalities.
the critical-value test belongs to a larger family of testing procedures based on deci-
sion theory. to give you a preview of the general theory of hypothesis testing, we define a
decision rule , a function that maps a realization of the estimator to a binary decision space.
in our problem the estimator is bz(or equivalently bŒ∏). we denote its realization by bz. the
binary decision space is {h0, h1}, corresponding to whether we want to claim h0orh1.
claiming h0is equivalent to keeping h0, and claiming h1is equivalent to rejecting h0.
for the critical-value test, the decision rule Œ¥(¬∑) :r‚Üí {0,1}is given by the equation (for
testing a right-hand tail):
Œ¥(bz) =(
1,ifbz‚â•zŒ±, (claim h1),
0,ifbz < z Œ±, (claim h0).(9.25)
example 9.7 . it was found that only 35% of the children in a kindergarten eat
broccoli. the teachers conducted a campaign to get more kids to eat broccoli, after
which it was found that 390 kids out of 1009 kids reported that they had eaten broccoli.
has the campaign successfully increased the number of kids eating broccoli? assume
that the standard deviation is known.
solution . we setup the null and the alternative hypothesis.
h0:Œ∏= 0.35, h 1:Œ∏ >0.35.
we construct an estimator bŒ∏ = (1 /n)pn
n=1xn, where xnis bernoulli with proba-
bility Œ∏. based on Œ∏,œÉ2=Œ∏(1‚àíŒ∏) = 0 .227. (again, in practice we do not know the
true variance, but in this problem we pretend that we know it.)
by the central limit theorem, bŒ∏ is roughly a gaussian. we compute the test
statistics bŒ∏ =390
1009= 0.387. standardization gives bz=bŒ∏‚àíŒ∏
œÉ/‚àö
n= 2.432. at a 5%
critical level, we have that zŒ±= 1.65. so bz= 2.432>1.65 = zŒ±, and hence we need
to reject the null hypothesis. even if we choose a 1% critical level so that zŒ±= 2.32,
our estimator bz= 2.432>2.32 = zŒ±will still reject the null hypothesis.
5709.3. hypothesis testing
a graphical illustration of this problem is shown in figure 9.11 . it can be seen
thatbŒ∏ = 0 .387 is actually quite far away from the cutoff 1.65. thus, we need to reject
the null hypothesis.
figure 9.11: example of a critical-value test. in this example, the test statistic bŒ∏ = 0 .387is
equivalent to bz= 2.432, which is significantly larger than the cutoff zŒ±= 1.65. therefore, we
have strong evidence to reject the null hypothesis, because the probability of obtaining bŒ∏ = 0 .387
is very low if h0is true.
9.3.3 p-value test
an alternative to the critical-value test is the p-value test. instead of looking at the cutoff
value zŒ±, we inspect the probability of obtaining our observation if h0is true. to understand
how the p-value test works, we consider another toy problem.
suppose that we have two hypotheses about flipping a coin:
¬àh0:Œ∏= 0.9, which is our default belief.
¬àh1:Œ∏ <0.9, which is a one-sided alternative.
it was found that with n= 150 coin flips, the coin landed on heads 128 times. thus the
estimator is bŒ∏ =128
150= 0.853. then, by following our previous procedures, we have that
bz=bŒ∏‚àíŒ∏
œÉ/‚àö
n=0.853‚àí0.9q
0.9(1‚àí0.9)
150=‚àí1.92.
at this point we can follow the previous subsection by computing the critical value zŒ±
and make the decision. however, let‚Äôs take a different route. we want to know what is the
probability under the curve if we integrate the pdf of bzfrom‚àí‚àûto‚àí1.92. this is easy.
sincebzis gaussian(0 ,1), it follows from the cdf of a gaussian that
p[bz‚â§ ‚àí1.92]|{z}
p-value= 0.0274.
referring to figure 9.12 , the value 0.0274 is the pink area under the curve, which is the
pdf of bz. since the area under the curve is less than the critical level Œ±(say 5%), we reject
the null hypothesis.
on computers, computing the p-value is done using the cdf commands.
571chapter 9. confidence and hypothesis
figure 9.12: thep-value test asks us to look at the probability of bz‚â§bz. if this probability (the p-value)
is less than the critical level Œ±, we have significant evidence to reject the null hypothesis.
% matlab code to compute the p-value
p = cdf(‚Äônorm‚Äô, -1.92, 0, 1);
# python code to compute the p-value
import scipy.stats as stats
p = stats.norm.cdf(-1.92,0,1)
in this example, the probability p[bz‚â§ ‚àí1.92] is known as the p-value . it is the
probability of bz‚â§z, under the distribution mandated by the null hypothesis, where z
is the (normalized) estimated value based on data. using our example, zis‚àí1.92. by
‚Äúdistribution mandated by the null hypothesis‚Äù we mean that the pdf of bzis the pdf that
the null hypothesis wants. in the above example the pdf is gaussian(0 ,1), corresponding
to gaussian( Œ∏, œÉ/‚àö
n) forbŒ∏.
more formally, the p-value for a left-hand tail test is defined as
p-value( bz) =p[bz‚â§bz],
where bzis the random realization of bzestimated from the data. the decision rule based
on the p-value is (for the left-hand tail):
Œ¥(bz) =(
1,p[bz‚â§bz]< Œ± (claim h1),
0,p[bz‚â§bz]‚â•Œ± (claim h0).(9.26)
if the alternative hypothesis is right-handed, then the probability becomes p[bz‚â•bz] instead.
relationship between critical-value and p-value tests . there is a one-to-one corre-
spondence between the p-value and the critical value. in the p-value test, if bzis gaussian,
it follows that
p-value = p[bz‚â§bz] = œÜ(bz),
5729.3. hypothesis testing
where œÜ is cdf of the standard gaussian. taking the inverse, the corresponding bzis
bz= œÜ‚àí1(p-value) .
in practice, we do not need to take any inverse of the p-value to obtain bzbecause it is
directly available from the data.
to test the p-value, we compare it with the critical level Œ±by checking
p-value < Œ±.
taking the inverse of both sides, it follows that the decision rule is equivalent to
œÜ‚àí1(p-value)|{z}
bz<œÜ‚àí1(Œ±)|{z}
zŒ±,
where the quantity on the right-hand side is the critical value zŒ±. therefore, if the test
statistic fails in the p-value test it will also fail in the critical-value test, and vice versa.
what is the difference between the critical-value test and p-value test?
¬àcritical-value test: compare w.r.t. critical value, which is the cutoff on the z-
axis.
¬àp-value test: compare w.r.t. Œ±, which is the probability.
¬àboth will give you the same statistical conclusion. so it does not matter which
one you use.
example 9.8 . we flip a coin for n= 150 times and find that 128 are heads. consider
two hypotheses
¬àh0:Œ∏= 0.9, which is our default belief.
¬àh1:Œ∏Ã∏= 0.9, which is a two-sided alternative.
for a critical level of Œ±= 0.05, shall we keep or reject h0?
solution . we know that bŒ∏ = 128 /150 = 0 .853. the normalized statistic is
bz=bŒ∏‚àíŒ∏
œÉ/‚àö
n=0.853‚àí0.9q
0.9(1‚àí0.9)
150=‚àí1.92.
to compute the p-value, we observe that the two-sided test means that we consider
the two tails. thus, we have
p-value = p[|bz|>1.92]
= 2√óp[bz >1.92]
= 2√ó0.0274 = 0 .055.
573chapter 9. confidence and hypothesis
for a critical level of Œ±= 0.05, the p-value is larger. this means that the probability
of obtaining |z|>1.92 is not extreme enough. therefore, we do not have sufficient
evidence to reject the null hypothesis.
if we take the critical-value test, we will reach the same conclusion. the critical
value for Œ±= 0.05 is determined by taking the inverse cdf at 1 ‚àí0.025, giving
zŒ±= œÜ‚àí1
1‚àíŒ±
2
= 1.96.
sincebz= 1.92 has not passed this threshold, we conclude that there is not enough
evidence to reject the null hypothesis.
figure 9.13: example of a two-sided test using the p-value and the zŒ±-value.
9.3.4 z-test and t-test
the critical-value test and the p-value tests are generic tools for hypothesis testing. in this
subsection we introduce the z-test and the t-test. it is important to understand that the
z-test and the t-test refer to the distributional assumptions we make about the variance.
they define the distribution we use to conduct the test but not the tools. in fact, both the
z-test and the t-test can be implemented using the critical-value test or the p-value test.
figure 9.14 illustrates the hierarchy of the tests.
figure 9.14: when conducting a hypothesis testing of the sample average, we may or may not know
the variance. if we know the variance, we use the gaussian distribution to conduct either a p-value test
or a critical-value test. if we do not know the variance, we use student‚Äôs t-distribution.
the difference between the gaussian distribution and the tdistribution is mainly
5749.3. hypothesis testing
attributable to the knowledge about the population variance. if the variance is known,
the distribution of the estimator (which in our case is the sample average) is gaussian. if
the variance is estimated from the sample, the distribution of the estimator will follow a
student‚Äôs t-distribution.
to introduce the z-test and the t-test we consider the following two examples. the
first example is a z-test.
example 9.9 (z-test). suppose we have a gaussian random variable with unknown
mean Œ∏and a known variance œÉ= 11.6. we draw n= 25 samples and construct an
estimator bŒ∏ = 80 .94. we propose two hypotheses:
¬àh0:Œ∏= 85, which is our default belief.
¬àh1:Œ∏ <85, which is a one-sided alternative.
for a critical level of Œ±= 0.05, shall we keep or reject the null hypothesis?
solution . the test statistic is
bz=bŒ∏‚àíŒ∏
œÉ/‚àö
n=‚àí1.75.
since the individual samples are assumed to follow a gaussian, the sample average bŒ∏
is also a gaussian. hence, bzis distributed according to gaussian(0 ,1).
figure 9.15: a one-sided z-test using the p-value and the zŒ±-value.
for a critical level of 0 .05, a one-sided critical value is
zŒ±= œÜ‚àí1(1‚àíŒ±) =‚àí1.645.
sincebz=‚àí1.75, which is more extreme than the critical value, we conclude that we
need to reject h0.
if we use the p-value test, we have that the p-value is
p[bz‚â§ ‚àí1.75] = œÜ( ‚àí1.75) = 0 .0401.
575chapter 9. confidence and hypothesis
since the p-value is smaller than the critical level Œ±= 0.05, it implies that bz=‚àí1.75
is more extreme. hence, we reject h0.
the following example is a t-test. in a t-test we do not know the population variance
but only know the sample variance bs. thus the test statistic we use is a trandom variable.
example 9.10 (t-test). suppose we have a gaussian random variable with unknown
mean Œ∏and an unknown variance œÉ. we draw n= 100 samples and construct an
estimator bŒ∏ = 130 .1, with a sample variance bs= 21.21. we propose two hypotheses:
¬àh0:Œ∏= 120, which is our default belief.
¬àh1:Œ∏Ã∏= 120, which is a two-sided alternative.
for a critical level of Œ±= 0.05, shall we keep or reject the null hypothesis?
solution . the test statistic is
bt=bŒ∏‚àíŒ∏
bs/‚àö
n= 4.762.
note that while the sample average bŒ∏ is a gaussian, the test statistic btis distributed
according to a tdistribution with n‚àí1 degrees of freedom. for a critical level of
0.05, a two-sided critical value is
tŒ±= œà‚àí1
99
1‚àíŒ±
2
= 1.984.
sincebt= 4.762, which is more extreme than the critical value, we conclude that we
need to reject h0.
if we use the p-value test, we have that the p-value is
p[|bt| ‚â•4.762] = 2 √óp[bt‚â•4.762] = 3 .28√ó10‚àí6.
since the p-value is (much) smaller than the critical level Œ±= 0.05, it implies that
|bt| ‚â•4.762 is quite extreme. hence, we reject h0.
figure 9.16: a two-sided t-test using the p-value and the zŒ±-value.
for this example, the matlab and python commands to compute tŒ±and the p-value are
5769.4. neyman-pearson test
% matlab code to compute critical-value and p-value
t_alpha = icdf(‚Äôt‚Äô, 1-0.025, 99);
p = 1-cdf(‚Äôt‚Äô, 4.762, 99);
# python code to compute critical value and p-value
import scipy.stats as stats
t_alpha = stats.t.ppf(1-0.025,99)
p = 1-stats.t.cdf(4.762,99)
what are the z-test and the t-test?
¬àboth are hypothesis testings for the sample averages.
¬àz-test: assume known variance. hence, use the gaussian distribution.
¬àt-test: assume unknown variance. hence, use the student‚Äôs t-distribution.
remark . we are exclusively analyzing the sample average in this section. there are other
types of estimators we can analyze. for example, we can discuss the difference between the
two means, the ratio of two random variables, etc. if you need tools for these more advanced
problems, please refer to the reference section at the end of this chapter.
9.4 neyman-pearson test
the hypothesis testing procedures we discussed in the previous section are elementary in
the sense that we have not discussed much theory. this section aims to fill the gap so that
you can understand hypothesis testing from a broader perspective. this generalization will
also help to bridge statistics to other disciplines such as classification in machine learning
and detection in signal processing. we call this theoretical analysis the neyman-pearson
framework .
9.4.1 null and alternative distributions
when we discussed hypothesis testing in the previous section, we focused exclusively on the
null hypothesis h0. regardless of whether we are studying the z-test or the t-test, using
the critical value or the p-value, all the distributions are associated with the distribution
under h0.
what do we mean by ‚Äúdistribution under h0‚Äù? using bŒ∏ as an example, the pdf of
bŒ∏ is assumed to be gaussian( Œ∏, œÉ2/n). this gaussian, centered at Œ∏, is the distribution
assumed under h0. as we decide whether to keep or reject h0, we look at the critical value
and the p-value of the test statistic under gaussian( Œ∏, œÉ2/n).
importantly, the analysis of hypothesis testing is not just about h0‚Äî it is also about
the alternative hypothesis h1, which uses a different pdf. for example, h1could use
577chapter 9. confidence and hypothesis
gaussian( Œ∏‚Ä≤, œÉ2/n) forŒ∏‚Ä≤> Œ∏. therefore, for the same testing statistic bŒ∏, we can check how
close it is to h1.
to capture both distributions, we define
f0(y) =fy(y|h0),
f1(y) =fy(y|h1).
the first pdf defines the distribution when the true model is h0. the second pdf is the
distribution when the true model is h1.
example 9.11 . consider an estimator y‚àºgaussian( Œ∏, œÉ2/n). define two hypotheses
h0:Œ∏= 120 and h1:Œ∏ >120. the two pdfs are then
f0(y) =fy(y|h0) = gaussian(120 , œÉ2/n),
f1(y) =fy(y|h1) = gaussian( Œ∏‚Ä≤, œÉ2/n), Œ∏‚Ä≤>120.
a graph of the two distributions is shown in figure 9.17 . in this figure we plot the
pdf under the null hypothesis and the pdf under an alternative hypothesis. the decision
is based on the null, where we marked the critical value.
figure 9.17: the pdf of the estimator under hypotheses h0andh1. the yellow region defines the
rejection zone rŒ±. if the estimator has a realization y=ythat falls into the rejection zone rŒ±, we
need to reject h0.
students are frequently confused about the exact equation of the pdf under h1. if
the alternative hypothesis is defined as Œ∏ >120, shall we define the pdf as a gaussian
centered at 130 or 151.4? they are both valid alternative hypotheses. the answer is that
we are going to express all equations based on Œ∏‚Ä≤. for example, if we want to analyze the
prediction error (this term will be explained later), the prediction error will be a function
ofŒ∏‚Ä≤. ifŒ∏‚Ä≤is close to Œ∏, we will expect a larger prediction error. however, if Œ∏‚Ä≤is far away
from Œ∏, the prediction error may be small.
whenever we discuss hypothesis testing, a decision rule is always implied. a decision
rule is a mapping Œ¥(¬∑) from sample space yof the test statistic y(orbŒ∏ if you prefer) to the
5789.4. neyman-pearson test
binary space of {0,1}:
Œ¥(y) =(
1, ify‚ààrŒ±,(we will reject h0),
0, ifyÃ∏‚ààrŒ±,(we will keep h0).(9.27)
here rŒ±is the rejection zone . for example, in a one-sided testing at a critical level Œ±, the
rejection zone is rŒ±={y‚â•œÜ‚àí1(1‚àíŒ±)}. therefore, as long as y‚â•œÜ‚àí1(1‚àíŒ±), we will
reject the null hypothesis. otherwise, we will keep the null hypothesis. a rejection zone can
be one-sided, two-sided, or even more complicated.
example 9.12 . consider h0:Œ∏= 0.35 and h1:Œ∏ >0.35. it was found that the
sample average over 1009 samples is bŒ∏ = 0 .387, with œÉ2= 0.227. the normalized test
statistic is bz=‚àö
n(bŒ∏‚àíŒ∏)/œÉ= 2.432. at a 5% critical level, define the decision rule
based on the critical-value approach.
solution . ifŒ±= 0.05, it follows that zŒ±= œÜ‚àí1(1‚àí0.05) = 1 .65. therefore, the
decision rule is
Œ¥(bz) =(
1, ifbz‚â•1.65,(we will reject h0),
0, ifbz <1.65,(we will keep h0),
where bzis the realization of bz. in this particular problem, we have bz= 2.432. thus,
according to the decision rule, we need to reject h0.
a decision rule is something youcreate. you do not need to follow the critical-value
or the p-value procedure ‚Äî you can create your own decision rule. for example, you can
say ‚Äúreject h0when|y|>0.000001‚Äù. there is nothing wrong with this decision rule except
that you will almost always reject the null hypothesis (so it is a bad decision rule). see
figure 9.18 for a graph of a similar example. if you follow the critical-value or the p-value
procedures, it turns out that the resulting decision rule is equivalent to some form of optimal
decision rule. this concept is the neyman-pearson framework, which we will explain shortly.
9.4.2 type 1 and type 2 errors
since hypothesis testing is about applying a decision rule to the test statistics, and since
no decision rule is perfect, it is natural to ask about the error expected from a particular
decision rule. in this subsection we define the decision error. however, the terminology varies
from discipline to discipline. we will explain the decision error first through the statistics
perspective and then through the signal processing perspective.
two tables of the cases that can be generated by a binary decision-making process are
shown in figure 9.19 . the columns of the tables are the true statements, i.e., whether the
test statistic has a population distribution under h0orh1. the rows of the tables are the
statements predicted by the decision rule, i.e., whether we should declare the statistics are
from h0orh1. each combination of the truth and prediction has a label:
¬àtrue positive: the truth is h1, and you declare h1.
¬àtrue negative: the truth is h0, and you declare h0.
¬àfalse positive: the truth is h0, and you declare h1.
579chapter 9. confidence and hypothesis
figure 9.18: two possible decision rules Œ¥1(y)andŒ¥2(y). in this example, Œ¥1(y)is designed according
to the critical-value approach at Œ±= 0.025, whereas Œ¥2(y)is arbitrarily designed. both are valid decision
rules, although Œ¥2should not be used because it tends to reject the null hypothesis more often than
desired.
¬àfalse negative: the truth is h1, and you declare h0.
different communities have different ways of labeling these quantities. in the statistics com-
munity the false negative rate (i.e., the number of false negative cases divided by the total
number of cases) is called the type 2 error , and the false positive rate is called the type 1
error. the true positive rate is called the power of the decision rule.
in the engineering community (e.g., radar engineering and signal processing) the ob-
jective is to detect whether a target (e.g., a missile or an enemy aircraft) is present. in this
context, the false positive rate is known as the probability of false alarm , since personnel
will be alerted when no target is present. the false negative rate is known as the probability
ofmiss because you miss a target. if the truth is h1and the prediction is also h1, we call
this the probability of detection .
figure 9.19: terminologies used in labeling the prediction error. the terms ‚Äútype 1 error‚Äù and ‚Äútype
2 error‚Äù are commonly used by the statistics community, whereas the terms ‚Äúfalse alarm‚Äù, ‚Äúmiss‚Äù and
‚Äúdetection‚Äù are more often used in the engineering community.
the diagram in figure 9.20 will help to clarify these definitions. given two hypotheses
h0andh1, there exists the corresponding distributions f0(y) and f1(y), which are the pdfs
5809.4. neyman-pearson test
of the test statistics y(orbŒ∏ if you prefer). supposing that our decision rule is to declare
h1when y‚â•Œ∑for some Œ∑, for example, Œ∑= 1.65 for a 5% critical level, there are two areas
under the curve that we need to consider.
¬àtype 1 / false alarm . the blue region under the curve represents the probability of
declaring h1(i.e., we choose to reject the null) while the truth is actually h0(i.e., we
should have not rejected the null). mathematically, this probability is
pf=p[y‚â•Œ∑|h0] =z
y‚â•Œ∑f0(y)dy. (9.28)
¬àtype 2 / miss . the pink region under the curve represents the probability of declaring
h0(i.e., we choose to keep the null) while the truth is actually h1(i.e., we should
have rejected the null). mathematically, this probability is
pm=p[y < Œ∑ |h1] =z
y<Œ∑f1(y)dy. (9.29)
figure 9.20: definition of type 1 and type 2 errors.
thepower of the decision rule is also known as the detection. it is defined as
pd=p[y‚â•Œ∑|h1]. (9.30)
a plot illustrating the power of the decision rule is shown in figure 9.21 . since pdis the
conditional probability of y‚â•Œ∑given h1, it is the complement of pm, and so we have the
identity
pd= 1‚àípm.
some communities refer to the above quantities in terms of the counts instead of the
probabilities . the difference is that the probabilities are normalized to [0 ,1] whereas the
counts are just the raw integers obtained from running an experiment. we prefer to use the
probabilities because they are the theoretical values . if you tell us the distributions f0and
f1, we can report the probabilities. the counts, by contrast, are just another form of sample
statistics . the number of counts today may be different from the number of counts tomorrow
581chapter 9. confidence and hypothesis
figure 9.21: the power of the decision rule is the area under the curve of f1, integrated for yinside
the rejection zone.
because they are obtained from the experiments. the difference between probabilities and
counts is analogous to the difference between pmfs and histograms.
since the probability of errors changes as the decision rule changes, it is necessary to
define pf,pdandpmas functions of Œ¥. in addition, hypothesis testing is not limited to one-
sided tests. we can define the rejection zone as rŒ±={y|reject h0using a critical level Œ±}.
the probabilities pfandpmare defined as
pf(Œ¥) =z
Œ¥(y)f0(y)dy=z
y‚ààrŒ±f0(y)dy, (9.31)
pm(Œ¥) =z
Œ¥(y)f1(y)dy=z
yÃ∏‚ààrŒ±f1(y)dy. (9.32)
using the property that pd= 1‚àípm, we have that
pd(Œ¥) = 1‚àípm(Œ¥) =z
y‚ààrŒ±f1(y)dy. (9.33)
note that the rejection zone does not need to depend on Œ±. you can arbitrarily define the
rejection zone, and the probabilities pf,pm, and pdcan still be defined.
example 9.13 . find pf(Œ¥1) and pf(Œ¥2) for the decision rule in figure 9.18 .
solution . since f0is a gaussian with zero mean and unit variance, it follows that
pf(Œ¥1) =z‚àû
1.961‚àö
2œÄe‚àíy2
2dy= 1‚àíœÜ(1.92) = 0 .025,
pf(Œ¥2) =z‚àû
0.51‚àö
2œÄe‚àíy2
2dy= 1‚àíœÜ(0.5) = 0 .3085.
9.4.3 neyman-pearson decision
at this point you have probably observed something about the critical-value test and the
p-value test. among the four types of decision combinations, we are looking at the false
5829.4. neyman-pearson test
positive rate, or the probability of false alarm pf(Œ¥). the critical-value test requires us to
findŒ¥such that pf(Œ¥) is equal to Œ±. that is, if you tell us the critical level Œ±(e.g., Œ±= 0.05),
we will find a decision rule (by telling you the cutoff) such that the false alarm rate is Œ±.
consider an example:
example 9.14 . let Œ±= 0.05. assume that f0is a gaussian with zero-mean and
unit-variance. let us do a one-sided test for h0:Œ∏= 0 versus h1:Œ∏ >0. find Œ¥such
thatpf(Œ¥) =Œ±.
solution . let the decision rule Œ¥be
Œ¥(y) =(
1, y‚â•Œ∑,
0, y < Œ∑.
our goal is to find Œ∑. the probability of false alarm is
pf(Œ¥) =z‚àû
Œ∑1‚àö
2œÄe‚àíy2
2dy= 1‚àíœÜ(Œ∑).
equating this to Œ±, it follows that 1 ‚àíœÜ(Œ∑) =Œ±implies Œ∑= œÜ‚àí1(1‚àíŒ±) = 1 .65. so the
decision rule becomes
Œ¥(y) =(
1, y ‚â•1.65,
0, y < 1.65.
if you apply this decision rule, you are guaranteed that the false alarm rate is Œ±= 0.05.
but why should we aim for pf(Œ¥)equal to Œ±? isn‚Äôt a lower false alarm rate better?
indeed, we would not mind having a lower false alarm, so we are happy to have any Œ¥
that satisfies pf(Œ¥)‚â§Œ±. however, changing the equality to an inequality means that we
now have a set of Œ¥instead of a unique Œ¥. more important, we need to pay attention to
the trade-off between pf(Œ¥) and pd(Œ¥). the smaller the pf(Œ¥) a decision rule Œ¥provides,
the smaller the pd(Œ¥) you can achieve. this is immediately apparent from figure 9.20 and
figure 9.21 . (if you move the cutoff to the right, the gray area and the blue area will both
shrink.) therefore, the desired optimization should be formulated as: from all the decision
rules Œ¥that have a false alarm rate of no larger than Œ±, we pick the one that maximizes the
detection rate. the resulting decision rule is known as the neyman-pearson decision rule .
definition 9.2. theneyman-pearson decision rule is defined as the solution to the
optimization
Œ¥‚àó=argmax
Œ¥pd(Œ¥),
subject to pf(Œ¥)‚â§Œ±. (9.34)
figure 9.22 illustrates two decision rules Œ¥‚àó(y) and Œ¥(y). the first decision rule Œ¥‚àó(y) is
obtained according to the critical-value approach, with Œ±= 0.025. as we will prove shortly,
this is also the optimal neyman-pearson decision rule for a one-sided hypothesis testing at
Œ±= 0.025. the second decision rule Œ¥(y) has a harsher cutoff, meaning that you need an
extreme test statistic to reject the null hypothesis. clearly, the p-value obtained by Œ¥(y) is
583chapter 9. confidence and hypothesis
less than Œ±= 0.025. thus, Œ¥(y) is a valid decision rule according to the neyman-pearson
formulation. however, Œ¥(y) is not optimal because the detection rate is not maximized.
figure 9.22: two decision rules Œ¥(y)andŒ¥‚àó(y). assume that Œ±= 0.025. then Œ¥(y)is one of the many
feasible choices in the neyman-pearson optimization, but Œ¥‚àó(y)is the optimal solution.
because of the complementary behavior of pfandpd, it follows that pdis maximized
when pfhits the upper bound. if we want to maximize the detection rate we need to stretch
the false alarm rate as much as possible. as a result, the neyman-pearson solution occurs
when pf(Œ¥) =Œ±, i.e., when the equality is met.
the neyman-pearson framework is a general framework for all distributions f0andf1,
as opposed to the critical-value and p-value examples, which are either gaussian or student‚Äôs
t-distribution. the solution to the neyman-pearson optimization is a decision rule known
as the likelihood ratio test . the likelihood ratio is defined as follows.
definition 9.3. thelikelihood ratio for two distributions f1(y)andf0(y)is
l(y) =f1(y)
f0(y). (9.35)
it turns out that the solution to the neyman-pearson optimization takes the form of the
likelihood ratio.
theorem 9.2. the solution to the neyman-pearson optimization is a decision rule
that checks the likelihood ratio
Œ¥‚àó(y) =(
1, l (y)‚â•Œ∑,
0, l (y)< Œ∑,(9.36)
for some decision boundary Œ∑which is a function of the critical level Œ±.
5849.4. neyman-pearson test
what is so special about neyman-pearson decision rule?
¬àit is the optimal decision. its optimality is defined w.r.t. maximizing the detection
rate while keeping a reasonable false alarm rate:
Œ¥‚àó= argmax
Œ¥pd(Œ¥),
subject to pf(Œ¥)‚â§Œ±.
¬àif your goal is to maximize the detection rate while maintaining the false alarm
rate, you cannot do better than neyman-pearson.
¬àits solution is the likelihood ratio test:
Œ¥‚àó(y) =(
1, l (y)‚â•Œ∑,
0, l (y)< Œ∑,
where l(y) =f1(y)/f0(y) is the likelihood ratio.
¬àthe critical-value test and the p-value test are special cases of the neyman-
pearson test.
deriving the solution to the neyman-pearson optimization can be skipped if this is your
first time reading the book.
proof . given Œ±, choose Œ¥‚àósuch that the false alarm rate is maximized: pf(Œ¥‚àó) =Œ±. then,
by substituting the definition of Œ¥‚àóinto the false alarm rate,
Œ±=pf(Œ¥‚àó) =z‚àû
‚àí‚àûŒ¥‚àó(y)f0(y)dy
=z
l(y)‚â•Œ∑1¬∑f0(y)dy+z
l(y)<Œ∑0¬∑f0(y)dy. (9.37)
now, consider another decision rule Œ¥that is not optimal but is feasible. that means that
Œ¥satisfies pf(Œ¥)‚â§Œ±. therefore,
Œ±‚â•pf(Œ¥) =z‚àû
‚àí‚àûŒ¥(y)f0(y)dy
=z
l(y)‚â•Œ∑Œ¥(y)¬∑f0(y)dy+z
l(y)<Œ∑Œ¥(y)¬∑f0(y)dy. (9.38)
our goal is to show that pd(Œ¥‚àó)‚â•pd(Œ¥), because by proving this result we can claim that
Œ¥‚àómaximizes the detection rate.
by combining equation (9.37) and equation (9.38), we have
0‚â§pf(Œ¥‚àó)‚àípf(Œ¥)
=z
l(x)‚â•Œ∑(1‚àíŒ¥(y))f0(y)dy‚àíz
l(y)<Œ∑Œ¥(y)f0(y)dy. (9.39)
585chapter 9. confidence and hypothesis
define l(y) =f1(y)
f0(y). then l(y)‚â•Œ∑if and only if f1(y)‚â•Œ∑f0(y). so,
pd(Œ¥‚àó)‚àípd(Œ¥) =z
l(y)‚â•Œ∑(1‚àíŒ¥(y))f1(y)dy‚àíz
l(y)<Œ∑Œ¥(y)f1(y)dy
=z
l(y)‚â•Œ∑(1‚àíŒ¥(y))Œ∑f0(y)dy‚àíz
l(y)<Œ∑Œ¥(y)Œ∑f0(y)dy
=Œ∑"z
l(y)‚â•Œ∑(1‚àíŒ¥(y))f0(y)dy‚àíz
l(y)<Œ∑Œ¥(y)f0(y)dy#
‚â•0,
where the last inequality holds because of equation (9.39). therefore, we conclude that Œ¥‚àó
maximizes pd. ‚ñ°
end of the proof. please join us again.
at this point, you may object that the likelihood ratio test (i.e., the neyman-pearson
decision rule) is very different from the hypothesis testing examples we have seen in the
previous chapter because now we need to handle the likelihood ratio l(y). rest assured
that they are the same, as illustrated by the following example.
example 9.15 . consider two hypotheses: h0:y‚àºgaussian(0 , œÉ2), and h1:y‚àº
gaussian( ¬µ, œÉ2), with ¬µ > 0. construct the neyman-pearson decision rule (i.e., the
likelihood ratio test).
solution . let us first define the likelihood functions. it is clear from the description
that
f0(y) =1‚àö
2œÄœÉ2exp
‚àíy2
2œÉ2
and f1(y) =1‚àö
2œÄœÉ2exp
‚àí(y‚àí¬µ)2
2œÉ2
.
therefore, the likelihood ratio is
l(y) =f1(y)
f0(y)= exp
‚àí1
2œÉ2(¬µ2‚àí2¬µy)
.
the likelihood ratio test states that the decision rule is
Œ¥‚àó(y) =(
1, l (y)‚â•Œ∑,
0, l (y)< Œ∑.
so it remains to simplify the condition l(y)‚ãõŒ∑. to this end, we observe that
l(y)‚â•Œ∑ ‚áê‚áí ‚àí1
2œÉ2(¬µ2‚àí2¬µy)‚â•logŒ∑
‚áê‚áí y‚â•¬µ
2‚àíœÉ2
¬µlogŒ∑
|{z}
def=œÑ.
5869.4. neyman-pearson test
therefore, instead of determining Œ∑, we just need to define œÑbecause the decision rules
based on Œ∑andœÑare equivalent.
to determine œÑ, neyman-pearson states that pf(Œ¥)‚â§Œ±(and at the optimal point
the equality has to hold). substituting this criterion into the decision rule,
Œ±=pf(Œ¥) =z
l(y)‚â•Œ∑f0(y)dy
=z
y‚â•œÑf0(y)dy
=z
y‚â•œÑ1‚àö
2œÄœÉ2e‚àíy2
2œÉ2dy
= 1‚àíœÜœÑ
œÉ
.
taking the inverse of the cdf, we obtain œÑ:
œÑ=œÉœÜ‚àí1(1‚àíŒ±).
putting everything together, the final decision rule is
Œ¥‚àó(y) =(
1, y ‚â•œÉœÜ‚àí1(1‚àíŒ±),
0, y < œÉ œÜ‚àí1(1‚àíŒ±).
so if Œ±= 0.05 we will reject h0when y‚â•1.65œÉ. we can also replace œÉbyœÉ/‚àö
nif
the estimator is constructed from multiple measurements.
the above example tells us that even though the likelihood ratio test may appear
complicated at first glance, the decision is the same as the good old hypothesis testing rules
we have derived. the flexibility we have gained with the likelihood ratio test is the variety
of distributions we can handle. instead of restricting ourselves to gaussians or student‚Äôs
t-distribution (which exclusively focuses on the sample averages), the likelihood ratio test
allows us to consider any distributions. the exact decision rule could be less obvious, but
the method is generalizable to a broad range of problems.
practice exercise 9.5 . in a telephone system, the waiting time is defined as the
inter-arrival time between two consecutive calls. however, it is known that sometimes
the waiting time can be mistakenly recorded as the time between three consecutive
calls (i.e., by skipping the second one). since the interarrival time of an independent
poisson process is either an exponential random variable or an erlang random variable,
depending on how many occurrences we are counting, we define the hypotheses
f0(y) =(
e‚àíy, y ‚â•0,
0, y < 0,and f1(y) =(
ye‚àíy, y ‚â•0,
0, y < 0.
suppose we are given one measurement y=y. find the neyman-pearson decision
rule for Œ±= 0.05.
587chapter 9. confidence and hypothesis
solution . the likelihood ratio is
l(y) =f1(y)
f0(y)=ye‚àíy
e‚àíy=y, y ‚â•0.
substituting this into the decision rule, we have
Œ¥‚àó(y) =(
1, l (y)‚â•Œ∑‚áê‚áíy‚â•Œ∑,
0, l (y)< Œ∑‚áê‚áíy < Œ∑.
it remains to determine Œ∑. inspecting pf(Œ¥), we have that
Œ±=pf(Œ¥‚àó) =z
l(y)‚â•Œ∑f0(y)dy
=z
y‚â•Œ∑e‚àíydy=e‚àíŒ∑.
setting e‚àíŒ∑=Œ±, we have that Œ±=‚àílogŒ±. hence, the decision rule is
Œ¥‚àó(y) =(
1, l (y)‚â•Œ∑‚áê‚áíy‚â• ‚àílogŒ±,
0, l (y)< Œ∑‚áê‚áíy <‚àílogŒ±.
forŒ±= 0.05, we reject the null hypothesis when y‚â•2.9957. figure 9.23 illustrates
the hypothesis testing rule.
figure 9.23: neyman-pearson decision rule at Œ±= 0.05.
remark . this example is instructive in that we have only one measurement y=y.
if we have repeated measurements and take the average, then the central limit the-
orem will kick in. in that case, we can resort to our favorite gaussian distribution
or student‚Äôs t-distribution instead of dealing with the exponential and the erlang
distributions. however, the example demonstrates the usefulness of neyman-pearson,
especially when the distributions are complicated.
5889.5. roc and precision-recall curve
9.5 roc and precision-recall curve
being a binary decision rule, the hypothesis testing procedure shares many similarities with
a two-class classification algorithm.3given a testing statistic or a testing sample, both
the hypothesis testing and a classification algorithm will report yes or no. therefore,
any performance evaluation metric developed for hypothesis testing is equally applicable to
classification and vice versa.
the topic we study in this section is the receiver operating characteristic (roc) curve
and the precision-recall (pr) curve. the roc curve and the pr curve are arguably the
most popular metrics in modern machine learning, in particular for classification, detection,
and segmentation tasks in computer vision. there are many unresolved questions about
these two curves and there are many debates about how to use them. our goal is not to add
another voice to the debate; rather, we would like to fill in the gap between the hypothesis
testing theory (particularly the neyman-pearson framework) and these two sets of curves.
we will establish the equivalence between the two curves and leave the open-ended debates
to you.
9.5.1 receiver operating characteristic (roc)
our approach to understanding the roc curve and the pr curve is based on the neyman-
pearson framework. under this framework, we know that the optimal decision rule w.r.t to
the neyman-pearson criterion is the solution to the optimization
Œ¥‚àó(Œ±) = argmax
Œ¥pd(Œ¥)
subject to pf(Œ¥)‚â§Œ±.
as a result of this optimization, the decision rule Œ¥‚àówill achieve a certain false alarm rate
pf(Œ¥‚àó) and detection rate pd(Œ¥‚àó). clearly, the decision rule Œ¥‚àóchanges as we change the
critical level Œ±. accordingly we write Œ¥‚àóasŒ¥‚àó(Œ±) to reflect this dependency.
what this observation implies is that as we sweep through the range of Œ±‚Äôs, we construct
different decision rules, each one with a different pfandpd. if we denote the decision rules
byŒ¥1, Œ¥2, . . . , Œ¥ m, we have mpairs of false alarm rate pfand detection rate pd:
¬àdecision rule Œ¥1: false alarm rate pf(Œ¥1) and detection rate pd(Œ¥1).
¬àdecision rule Œ¥2: false alarm rate pf(Œ¥2) and detection rate pd(Œ¥2).
¬à...
¬àdecision rule Œ¥m: false alarm rate pf(Œ¥m) and detection rate pd(Œ¥m).
3in a classification algorithm, the goal is to look at the testing sample yand compute certain thresholding
criteria. for example, a typical decision rule of a classification algorithm is Œ¥(y) =(
1,wtœï(y)‚â•œÑ
0,wtœï(y)< œÑ. here,
you can think of the vector was the regression coefficient, and œï(¬∑) is some kind of feature transform. the
equation says that class 1 will be reported if the inner product is larger than a threshold œÑ, and class 0
will be reported otherwise. therefore, a binary classification, when written in this form, is the same as a
hypothesis testing procedure.
589chapter 9. confidence and hypothesis
if we plot pd(Œ¥) on the y-axis as a function of pf(Œ¥) on the x-axis, we obtain a curve shown
infigure 9.24 (see the example below for the problem setting). the black curve shown on
the right is known as the receiver operating characteristic (roc) curve.
figure 9.24: an example of an roc curve, where we consider two hypotheses: h0:y‚àºgaussian (0,2),
andh1:y‚àºgaussian (3,2). we construct the neyman-pearson decision rule for a range of critical
levels Œ±. for each Œ±we compute the theoretical pf(Œ±)andpd(Œ±), shown on the left-hand side of the
figure. the pair of (pd, pf)is then plotted as the right-hand side curve by sweeping the Œ±‚Äôs.
the setup of the figure follows the example below.
example 9.16 . we consider two hypotheses: h0:y‚àºgaussian(0 ,2), and h1:y‚àº
gaussian(3 ,2). derive the neyman-pearson decision rule and plot the roc curve.
solution . we construct a neyman-pearson decision rule:
Œ¥‚àó(y) =(
1, y ‚â•œÉœÜ‚àí1(1‚àíŒ±),
0, y < œÉ œÜ‚àí1(1‚àíŒ±).
where œÑis a tunable threshold. for example, if Œ±= 0.05, then œÉœÜ‚àí1(1‚àí0.05) = 3 .2897,
and if Œ±= 0.1, then œÉœÜ‚àí1(1‚àí0.1) = 2 .5631. therefore, the false alarm rate and the
detection rate are functions of the critical level Œ±.
for this particular example, we have the false alarm rate and detection rate in
closed form, as functions of Œ±:
pf(Œ±) =z‚àû
œÉœÜ‚àí1(1‚àíŒ±)1‚àö
2œÄœÉ2e‚àíy2
2œÉ2dy
= 1‚àíœÜœÉœÜ‚àí1(1‚àíŒ±)
œÉ
=Œ±,
5909.5. roc and precision-recall curve
pd(Œ±) =z‚àû
œÉœÜ‚àí1(1‚àíŒ±)1‚àö
2œÄœÉ2e‚àí(y‚àí¬µ)2
2œÉ2dy
= 1‚àíœÜ
œÜ‚àí1(1‚àíŒ±)‚àí¬µ
œÉ
.
these give us the two curves on the left-hand side of figure 9.24 .
what is an roc curve?
¬àit is a plot showing pdon the y-axis and pfon the x-axis.
¬àpd= detection rate (also known as the power of the test).
¬àpf= false alarm rate (also known as the type 1 error of the test).
the roc curve tells us the behavior of the decision rule as we change the threshold Œ±.
a graphical illustration is shown in figure 9.25 . there are a few key observations we need
to pay attention to:
figure 9.25: interpreting the roc curve.
¬àthe roc curve must go through (0 ,0). this happens when you always keep the
null hypothesis or always declare class 0, no matter what observations. if you always
keep h0, certainly you will not make any false positive (or false alarm), because you
will never say h0is wrong. therefore, the detection rate (or the power of the test) is
also 0. this is a useless decision rule for both classification and hypothesis testing.
¬àthe roc curve must go through (1 ,1). this happens when you always reject the null
hypothesis, no matter what observations we have. if you always reject h0, you will
always say that ‚Äúthere is a target‚Äù. as far as detection is concerned, you are perfect
591chapter 9. confidence and hypothesis
because you have not missed any targets. however, the false positive rate is also high
because you will falsely declare a target when there is nothing. therefore, this is also
a useless decision rule.
¬àthe roc curve tells us the operating point of the decision rule as we change the thresh-
old. a threshold is a universal concept for both hypothesis testing and classification.
in hypothesis testing, we have the critical level Œ±, say 0.05 or 0.1. in classification, we
also have a threshold for judging whether a sample should be classified as class 1 or
class 0. often in classification, the intermediate estimates are probabilities or distances
to decision boundaries. these real numbers need to be binarized to generate a binary
decision. the roc curve tells us that if you pick a threshold, your decision rule will
have a certain pfandpdas predicted by the curve. if you want to tolerate a higher
pf, you can move along the curve to find your operating point.
¬àthe ideal operating point on a roc curve is when pf= 0 and pd= 1. however, this
is a hypothetical situation that does not happen in any real decision rule.
9.5.2 comparing roc curves
because of how the roc curves are constructed, every binary decision rule has its own roc
curve. typically, when one tries to compare classification algorithms, the area under the
curve (auc ) occupied by the roc curve is compared. a decision rule having a larger auc
is often a ‚Äúbetter‚Äù decision rule.
to illustrate the idea of comparing estimators, we consider a trivial decision rule based
on a blind guess.
example 9.17 . (a blind guess decision) consider a decision rule that we reject h0
with probability Œ±and keep h0with probability 1 ‚àíŒ±. we call this a blind guess, since
the decision rule ignores observation y. mathematically, this trivial decision rule is
Œ¥(y) =(
1, with probability Œ±,
0, with probability 1 ‚àíŒ±.
find pf,pd, and auc.
solution . for this decision rule we compute its false positive rate (or false alarm rate)
and its true positive rate (or detection rate). however, since Œ¥(y) is now random, we
need to take the expectation over the two random states that Œ¥(y) can take. this gives
us
pf(Œ±) =ez
Œ¥(y)f0(y)dy
=z
1¬∑f0(y)dyp[Œ¥(y) = 1] +z
0¬∑f0(y)dyp[Œ¥(y) = 0]
=Œ±z
f0(y)dy=Œ±.
5929.5. roc and precision-recall curve
similarly, the detection rate is
pd(Œ±) =ez
Œ¥(y)f1(y)dy
=Œ±z
f1(y)dy=Œ±.
if we plot pdas a function of pf, we notice that the function is a straight line going
from (0 ,0) to (1 ,1). this decision rule is useless. comparing this with the neyman-
pearson decision rule, it is clear that neyman-pearson has a larger auc. the auc
for this trivial decision rule is the area of the triangle, which is 0.5.
figure 9.26: the roc curve of the blind guess decision rule is a straight line. the auc is 0.5.
if you set Œ±= 0.5, then the decision rule becomes
Œ¥(y) =(
1, with probability1
2,
0, with probability1
2.
this is equivalent to flipping a fair coin with probability 1 /2 of declaring h0and 1 /2
declaring h1. its operating point is the yellow circle.
computing the auc can be done by calling special library functions. however, to
spell out the details we demonstrate something more elementary. the program below is a
piece of matlab code plotting two roc curves corresponding to two different decision
rules. the first decision rule is the trivial decision rule, where we have just shown that
pf(Œ±) =pd(Œ±) =Œ±. the second decision rule is the neyman-pearson decision rule, for
which we showed in figure 9.24 that pf(Œ±) =Œ±andpd(Œ±) = 1 ‚àíœÜ 
œÜ‚àí1(1‚àíŒ±)‚àí¬µ
œÉ
.
using the matlab code below, we can plot the two roc curves shown in figure 9.26 .
% matlab code to plot roc curve
sigma = 2; mu = 3;
alphaset = linspace(0,1,1000);
pf1 = zeros(1,1000); pd1 = zeros(1,1000);
pf2 = zeros(1,1000); pd2 = zeros(1,1000);
for i=1:1000
alpha = alphaset(i);
593chapter 9. confidence and hypothesis
pf1(i) = alpha;
pd1(i) = alpha;
pf2(i) = alpha;
pd2(i) = 1-normcdf(norminv(1-alpha)-mu/sigma);
end
figure;
plot(pf1, pd1,‚Äôlinewidth‚Äô, 4, ‚Äôcolor‚Äô, [0.8, 0, 0]); hold on;
plot(pf2, pd2,‚Äôlinewidth‚Äô, 4, ‚Äôcolor‚Äô, [0, 0, 0]); hold off;
to compute the auc we perform a numerical integration:
auc =z
pd(Œ±)¬∑dpf(Œ±)‚âàx
ipd(Œ±i)¬∑‚àÜpf(Œ±i)
=x
ipd(Œ±i)¬∑
pf(Œ±i)‚àípf(Œ±i‚àí1)
,
where Œ±iis the ith critical level we use to plot the roc curve. (we assume that the Œ±‚Äôs are
sorted in ascending order.) in matlab, the commands are
auc1 = sum(pd1.*[0 diff(pf1)])
auc2 = sum(pd2.*[0 diff(pf2)])
the auc of the two decision rules computed by matlab are 0.8561 and 0.5005, respec-
tively. the small slack of 0.0005 is caused by the numerical approximation at the tail, which
can be ignored as long as you are consistent for all the roc curves.
the commands for python are analogous to the commands for matlab.
# python code to plot roc curve
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
sigma = 2; mu = 3;
alphaset = np.linspace(0,1,1000)
pf1 = np.zeros(1000); pd1 = np.zeros(1000)
pf2 = np.zeros(1000); pd2 = np.zeros(1000)
for i in range(1000):
alpha = alphaset[i]
pf1[i] = alpha
pd1[i] = alpha
pf2[i] = alpha
pd2[i] = 1-stats.norm.cdf(stats.norm.ppf(1-alpha)-mu/sigma)
plt.plot(pf1,pd1)
plt.plot(pf2,pd2)
to compute the auc, the python code is (continuing from the previous code):
5949.5. roc and precision-recall curve
auc1 = np.sum(pd1 * np.append(0, np.diff(pf1)))
auc2 = np.sum(pd2 * np.append(0, np.diff(pf2)))
it is possible to get a decision rule that is worse than a blind guess. the following
example illustrates a trivial setup.
practice exercise 9.6 . (flipped neyman-pearson). consider two hypotheses
h0= gaussian(0 , œÉ2),
h1= gaussian( ¬µ, œÉ2), ¬µ > 0.
letŒ±be the critical level. the neyman-pearson decision rule is
Œ¥‚àó(y) =(
1, y ‚â•œÉœÜ‚àí1(1‚àíŒ±),
0, y < œÉ œÜ‚àí1(1‚àíŒ±).
now, consider a flipped neyman-pearson decision rule
Œ¥+(y) =(
1, y < œÉ œÜ‚àí1(1‚àíŒ±),
0, y ‚â•œÉœÜ‚àí1(1‚àíŒ±).
find pf,pd, and auc for the new decision rule Œ¥+.
solution . since we flip the rejection zone, the probability of false alarm is
pf(Œ±) =z
Œ¥+(y)f0(y)dy
=zœÉœÜ‚àí1(1‚àíŒ±)
‚àí‚àû1‚àö
2œÄœÉ2e‚àíy2
2œÉ2dy
= œÜœÉœÜ‚àí1(1‚àíŒ±)
œÉ
= 1‚àíŒ±.
similarly, the probability of detection is
pd(Œ±) =z
Œ¥+(y)f1(y)dy
=zœÉœÜ‚àí1(1‚àíŒ±)
‚àí‚àû1‚àö
2œÄœÉ2e‚àí(y‚àí¬µ)2
2œÉ2dy
= œÜœÉœÜ‚àí1(1‚àíŒ±)‚àí¬µ
œÉ
= œÜ
œÜ‚àí1(1‚àíŒ±)‚àí¬µ
œÉ
.
595chapter 9. confidence and hypothesis
if you plot pdas a function of pf, you will obtain a curve shown in figure 9.27 .
the auc for this flipped decision rule is 0.1439, whereas that for neyman-pearson is
0.8561. the two numbers are complements of each other, meaning that their sum is
unity.
figure 9.27: the roc curve of a flipped neyman-pearson decision rule.
what if we arbitrarily construct a decision rule that is neither neyman-pearson nor
the blind guess? the following example demonstrates one possible choice.
practice exercise 9.7 . consider two hypotheses
h0= gaussian(0 , œÉ2),
h1= gaussian( ¬µ, œÉ2), ¬µ > 0.
letŒ±be the critical level. consider the following decision rule:
Œ¥‚ô£(y) =(
1,|y| ‚â•œÉœÜ‚àí1(1‚àíŒ±),
0,|y|< œÉœÜ‚àí1(1‚àíŒ±).
find pf,pd, and auc for the new decision rule Œ¥‚ô£.
solution . the probability of false alarm is
pf(Œ±) =z
Œ¥‚ô£(y)f0(y)dy
= 1‚àízœÉœÜ‚àí1(1‚àíŒ±)
‚àíœÉœÜ‚àí1(1‚àíŒ±)1‚àö
2œÄœÉ2e‚àíy2
2œÉ2dy
= 1‚àíœÜ 
œÜ‚àí1(1‚àíŒ±)
+ œÜ 
‚àíœÜ‚àí1(1‚àíŒ±)
= 2Œ±.
5969.5. roc and precision-recall curve
similarly, the probability of detection is
pd(Œ±) =z
Œ¥‚ô£(y)f1(y)dy
= 1‚àízœÉœÜ‚àí1(1‚àíŒ±)
‚àíœÉœÜ‚àí1(1‚àíŒ±)1‚àö
2œÄœÉ2e‚àí(y‚àí¬µ)2
2œÉ2dy
= 1‚àíœÜœÉœÜ‚àí1(1‚àíŒ±)‚àí¬µ
œÉ
+ œÜ‚àíœÉœÜ‚àí1(1‚àíŒ±)‚àí¬µ
œÉ
= 1‚àíœÜ
œÜ‚àí1(1‚àíŒ±)‚àí¬µ
œÉ
+ œÜ
‚àíœÜ‚àí1(1‚àíŒ±)‚àí¬µ
œÉ
.
if you plot pdas a function of pf, you will obtain a curve shown in figure 9.28 .
the auc for this proposed decision rule is 0.7534, whereas that of neyman-pearson
is 0.8561. therefore, the neyman-pearson decision rule is better.
figure 9.28: the roc curve of a proposed decision rule.
the matlab code we used to generate figure 9.28 is shown below. note that we
need to separate the calculations of the two curves, because the proposed curve can only
take 0 < Œ± < 0.5. the python code is implemented analogously.
% matlab code to generate the roc curve.
sigma = 2; mu = 3;
pf1 = zeros(1,1000); pd1 = zeros(1,1000);
pf2 = zeros(1,1000); pd2 = zeros(1,1000);
alphaset = linspace(0,0.5,1000);
for i=1:1000
alpha = alphaset(i);
pf1(i) = 2*alpha;
pd1(i) = 1-(normcdf(norminv(1-alpha)-mu/sigma)-...
normcdf(-norminv(1-alpha)-mu/sigma));
end
alphaset = linspace(0,1,1000);
597chapter 9. confidence and hypothesis
for i=1:1000
alpha = alphaset(i);
pf2(i) = alpha;
pd2(i) = 1-normcdf(norminv(1-alpha)-mu/sigma);
end
figure;
plot(pf1, pd1,‚Äôlinewidth‚Äô, 4, ‚Äôcolor‚Äô, [0.8, 0, 0]); hold on;
plot(pf2, pd2,‚Äôlinewidth‚Äô, 4, ‚Äôcolor‚Äô, [0, 0, 0]); hold off;
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
sigma = 2; mu = 3;
pf1 = np.zeros(1000); pd1 = np.zeros(1000)
pf2 = np.zeros(1000); pd2 = np.zeros(1000)
alphaset = np.linspace(0,0.5,1000)
for i in range(1000):
alpha = alphaset[i]
pf1[i] = 2*alpha
pd1[i] = 1-(stats.norm.cdf(stats.norm.ppf(1-alpha)-mu/sigma) \
-stats.norm.cdf(-stats.norm.ppf(1-alpha)-mu/sigma))
alphaset = np.linspace(0,1,1000)
for i in range(1000):
alpha = alphaset[i]
pf2[i] = alpha
pd2[i] = 1-stats.norm.cdf(stats.norm.ppf(1-alpha)-mu/sigma)
plt.plot(pf1, pd1)
plt.plot(pf2, pd2)
9.5.3 the roc curve in practice
if the neyman-pearson decision rule is the optimal rule, why don‚Äôt we always use it? the
problem is that in practice we may not have access to the distributions. for example, if we
classify images, how do we know that the data follows a gaussian distribution or a mixture
of distributions? consequently, the roc curves we discussed in the subsections above are
thetheoretical roc curves. in practice, we plot the empirical roc curves.
plotting an empirical roc curve for a binary classification method (and hypothesis
testing) is intuitive. the ingredients we need are a set of scores and a set of labels .
the scores are the probability values determining the likelihood of a sample belonging to
one class. generally speaking, for empirical data this requires looking at the training data,
building a model, and computing the likelihood. we will not go into the details of how a
binary classifier is built. instead, we assume that you have already built a binary classifier
and have obtained the scores. our goal is to show you how to plot the roc curve.
5989.5. roc and precision-recall curve
the following matlab code uses a dataset fisheriris . the code builds a binary
classifier and returns the scores.
% matlab code to train a classification algorithm.
% do not worry if you cannot understand this code.
% it is not the focus on this book.
load fisheriris
pred = meas(51:end,1:2);
resp = (1:100)‚Äô>50;
mdl = fitglm(pred,resp,‚Äôdistribution‚Äô,‚Äôbinomial‚Äô,‚Äôlink‚Äô,‚Äôlogit‚Äô);
scores = mdl.fitted.probability;
labels = [ones(1,50), zeros(1,50)];
save(‚Äôch9_roc_example_data‚Äô,‚Äôscores‚Äô,‚Äôlabels‚Äô);
to give you an idea of how the scores of the classifier look, we plot the histogram of
the scores in figure 9.29 . as you can see, there is no clear division between the two classes.
no matter what threshold œÑwe use, some cases will be misclassified.
figure 9.29: the distribution of probability scores obtained from a binary classifier for the dataset
fisheriris . the green vertical lines represent the threshold for turning the scores into binary decisions.
any score greater than œÑwill be classified as class 1, and any score that is less than œÑwill be classified
as class 0. these predicted labels would then be compared to the true labels to plot the roc curve.
recall that the roc curve is a function of pdversus pf. using terminology from
statistics, pdis the true positive rate and pfis the false positive rate. by sweeping a range
of decision thresholds (over the scores), we can compute the corresponding pf‚Äôs and pd‚Äôs.
on a computer this can be done by setting up two columns of labels: the true label labels
and the predicted labels prediction . for any threshold œÑ, we binarize the scores to turn
them into a decision vector. then we count the number of true positives, true negatives,
false positives, and false negatives. the total of these numbers will give us pfandpd.
in matlab, the above description can be easily implemented by sweeping through
the range of œÑ.
% matlab code to generate an empirical roc curve
load ch9_roc_example_data
599chapter 9. confidence and hypothesis
tau = linspace(0,1,1000);
for i=1:1000
idx = (scores <= tau(i));
predict = zeros(1,100);
predict(idx) = 1;
true_positive = 0; true_negative = 0;
false_positive = 0; false_negative = 0;
for j=1:100
if (predict(j)==1) && (labels(j)==1)
true_positive = true_positive + 1; end
if (predict(j)==1) && (labels(j)==0)
false_positive = false_positive + 1; end
if (predict(j)==0) && (labels(j)==1)
false_negative = false_negative + 1; end
if (predict(j)==0) && (labels(j)==0)
true_negative = true_negative + 1; end
end
pf(i) = false_positive/50;
pd(i) = true_positive/50;
end
plot(pf, pd, ‚Äôlinewidth‚Äô, 4, ‚Äôcolor‚Äô, [0, 0, 0]);
the python codes of this problem are similar. we give them here for completeness.
# python code to generate an empirical roc curve
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
scores = np.loadtxt(‚Äôch9_roc_example_data.txt‚Äô)
labels = np.append(np.ones(50), np.zeros(50))
tau = np.linspace(0,1,1000)
pf = np.zeros(1000)
pd = np.zeros(1000)
for i in range(1000):
idx = scores<= tau[i]
predict = np.zeros(100)
predict[idx] = 1
true_positive = 0; true_negative = 0
false_positive = 0; false_negative = 0
for j in range(100):
if (predict[j]==1) and (labels[j]==1): true_positive += 1
if (predict[j]==1) and (labels[j]==0): false_positive += 1
if (predict[j]==0) and (labels[j]==1): false_negative += 1
if (predict[j]==0) and (labels[j]==0): true_negative += 1
pf[i] = false_positive/50
pd[i] = true_positive/50
plt.plot(pf, pd)
6009.5. roc and precision-recall curve
the empirical roc curve for this problem is shown in figure 9.30 . each point on the
curve is a coordinate ( pf, pd), evaluated at a particular threshold œÑ. mathematically, the
decision rule we used was
Œ¥(y) =(
1, score (y)‚â•œÑ,
0, score (y)< œÑ.
for every œÑ, we have a false alarm rate and a detection rate. since this is an empirical
dataset with only 100 samples, there are many occasions where pfdoes not change but
pdincreases, or pdstays constant but pfincreases. for this particular example, we can
compute the auc, which is 0.7948.
figure 9.30: the empirical roc curve for the dataset fisheriris , using a classifier based on the
logistic regression.
note that the empirical roc is rough. it does not have the smooth concave shape of
the theoretical roc curve. one can prove that if the decision rule is neyman-pearson, i.e.,
if we conduct a likelihood ratio test, then the resulting roc curve is concave. otherwise,
you can still obtain an empirical roc curve for real datasets and classifiers. however, the
shape is not necessarily concave.
9.5.4 the precision-recall (pr) curve
in modern data science, an alternative performance metric to the roc curve is the precision-
recall (pr) curve. the precision and recall are defined as follows.
definition 9.4. let tp = true positive, fp = false positive, fn = false negative.
theprecision is defined as
precision =tp
tp+fp=pd
pd+pf, (9.40)
and the recall is defined as
recall =tp
tp+fn=pd
pd+pm=pd. (9.41)
601chapter 9. confidence and hypothesis
in this definition, tp, fp, and fn are the numbers of samples that are classified as true
positive, false positive, and false negative, respectively. however, both precision and recall are
defined as ratios of numbers. the ratios can be equivalently defined through the rates. using
our terminology, this gives us the definitions in terms of pd,pfandpm. since pd= 1‚àípm,
it also holds that the recall is pd.
let us take a moment to consider the meanings of precision and recall. precision is
defined as
precision =tp
tp + fp=# true positives
total # positives you claim. (9.42)
the numerator of the precision is the number of true positive samples and the denominator
is the total number of positives that you claim. this includes the true positives and the
false positives. therefore, precision measures how trustworthy your claim is. there are two
scenarios to consider:
¬àhigh precision : this means that among all the positives you claim, many of them are
the true positives. therefore, whatever you claim is trustworthy. one possibility for
obtaining a high precision is that the critical level Œ±of the neyman-pearson decision
rule approaches 1. in other words, you are very accepting of the null hypotheses. thus,
whenever you reject, it will be a reliable reject.
¬àlow precision : this means that you are overclaiming the positives, and so there are
many false positives. thus, even though you claim many positives, not all are trust-
worthy. one reason why low precision occurs is that you are too eager to reject the
null. thus you tend to overkill the unnecessary cases.
a similar analysis can be applied to the recall. the recall is defined as
recall =tp
tp + fn=# true positives
total # positives in the distribution. (9.43)
the difference between the recall and the precision is the denominator. for recall, the
denominator is the total number of positives in the distribution . we are not interested
in knowing what you have claimed but in knowing how many of them are there in the
distribution. if you examine the definition using pd, you can see that recall is the probability
of detection ‚Äî how successfully you can detect a target. a high recall and a low recall can
occur in two situations:
¬àhigh recall : this means that you are very good at detecting the target or rejecting
the null appropriately. a high recall can happen when the critical level Œ±is low so that
you never miss a target. however, if the critical level Œ±is low, you will suffer from a
low precision.
¬àlow recall : this means that you are too accepting of the null hypotheses, and so you
never claim that there is a target. as a result the number of successful detections is
low. however, having a low recall can buy you high precision because you do not reject
the null unless it has extreme evidence (hence there is no false alarm.)
as you can see from the discussions above, the precision-recall has a trade-off, just as
the roc curve does. since the pr curve and roc curve are derived from pfandpd, there
is a one-to-one correspondence. this can be proved by rearranging the terms in the previous
theorem.
6029.5. roc and precision-recall curve
theorem 9.3. thefalse alarm rate pfand the detection rate pdcan be expressed
in terms of the precision and recall as
pf=recall(1 ‚àíprecision)
precision, (9.44)
pd= recall .
this result implies that whenever we have an roc curve we can convert it to a pr curve.
moreover, whenever we have a pr curve we can convert it to an roc curve. therefore,
there is no additional information one can squeeze out by converting the curves. what we
can claim, at most, is that the two curves offer different ways of interpreting the decision
rule.
to illustrate the equivalence between an roc curve and a pr curve, we plot two
different decision rules in figure 9.31 . any point on the roc curve will have a corresponding
point on the pr curve, and vice versa.
figure 9.31: there is a one-to-one correspondence between the roc curve and the pr curve.
the matlab and python codes for generating the pr curve are straightforward.
assuming that we have run the code used to generate figure 9.28 , we plot the pr curve as
follows (this will give us figure 9.31 ).
% matlab code to generate a pr curve
precision1 = pd1./(pd1+pf1);
precision2 = pd2./(pd2+pf2);
recall1 = pd1;
recall2 = pd2;
plot(recall1, precision1, ‚Äôlinewidth‚Äô, 4); hold on;
plot(recall2, precision2, ‚Äôlinewidth‚Äô, 4); hold off;
603chapter 9. confidence and hypothesis
practice exercise 9.8 . suppose that the decision rule is a blind guess:
Œ¥(y) =(
1, with probability Œ±,
0, with probability 1 ‚àíŒ±,
plot the roc curve and the pr curve.
solution : as we have shown earlier, pf(Œ±) and pd(Œ±) for this decision rule are pf(Œ±) =
Œ±andpd(Œ±) =Œ±. therefore,
precision =pd
pd+pf=Œ±
Œ±+Œ±=1
2,and recall = pd=Œ±.
thus the pr curve is a straight line with a level of 0.5.
figure 9.32: the pr curve of a blind-guess decision rule is a straight line.
practice exercise 9.9 . convert the roc curve in figure 9.30 to a pr curve.
solution : the conversion is done by first computing pfandpd. defining the precision
and recall in terms of pfandpd, we plot the pr curves below.
figure 9.33: the pr curve of a real dataset.
6049.6. summary
as you can see from the figure, the pr curve behaves very differently from the
roc curve. it is sometimes argued that the two curves can be interpreted differently,
even though they describe the same decision rule for the same dataset.
9.6 summary
in this chapter, we have discussed five principles for quantifying the confidence of an esti-
mator and making statistical decisions. to summarize the chapter, we clarify a few common
misconceptions about these topics.
¬àconfidence interval . students frequently become confused about the meaning of a
confidence interval. it is not the interval that 95% of the samples will fall inside. it
is also not the interval within which the estimator has a 95% chance to show up.
a confidence interval is a random interval that has a 95% chance of including the
population parameter. a better way to think about a confidence interval is to think of
it as an alternative to a point estimate. a point estimate only gives a point, whereas
a confidence interval extends the point to an interval. all the randomness of the point
estimate is also there in the confidence interval. however, if the confidence interval is
narrow, there is a good chance for the point estimate to be accurate.
¬àbootstrapping . the most common misconception about bootstrapping is that it can
create something from nothing. another misconception is that bootstrapping can make
your estimates better. both beliefs are wrong. bootstrapping is a technique for esti-
mating the estimator‚Äôs variance, and consequently it provides a confidence interval.
bootstrapping does not improve the point estimate, no matter how many bootstrap-
ping samples you synthesize. bootstrapping works because the sampling with the re-
placement step is equivalent to drawing samples from the empirical distribution. the
whole process relies on the proximity between the empirical distribution and the true
population. if you do not have enough samples and the empirical distribution does not
approximate the population, bootstrapping will not work. therefore, bootstrapping
does not create something from nothing; it uses whatever you have and tells you how
reliable the estimate is.
¬àhypothesis testing . students are often overwhelmed at first by the great number of
tests one can use for hypothesis testing, e.g., p-value, critical value, z-test, t-test, œá2
test, f-test, etc. our advice is to forget about them and remember that hypothesis
testing is a court trial. your job is to decide whether you have enough evidence to
declare that the defendant is guilty. to reach a guilty verdict, you need to make sure
that the test statistic is unlikely to happen. therefore, the best practice is to draw
the distributions of the test statistic and ask yourself how likely is it that the test
statistic has such a value. when you draw the pictures of the distributions, you will
know whether you should use a gaussian z, a student‚Äôs t, aœá2, af-statistic, etc.
when you examine the likelihood of the test statistic, you will know whether you want
to use the p-value or the critical value. if you follow this principle, you will never be
confused by the oceans of tests you find in the textbooks.
605chapter 9. confidence and hypothesis
¬àneyman-pearson . beginners often find neyman-pearson abstract and do not under-
stand why it is useful. in this chapter, however, we have explained why we need to
understand neyman-pearson. it is a very general framework for many kinds of hy-
pothesis testing problems. all it says is that if we want to maximize the detection rate
while maintaining the false alarm rate, then the optimal testing procedure boils down
to the critical-value test and the p-value test. this gives us a certificate that our usual
hypothesis testing is optimal according to the neyman-pearson framework.
¬àroc and pr curves . on the internet nowadays there is a huge quantity of articles,
blogs, and tutorials about howto plot the roc curve and the pr curve. often these
curves are explained through programming examples such as python, r, or matlab.
our advice for studying the roc curve and the pr curve is to go back to the neyman-
pearson framework. these two curves do not come out of the blue. the roc curve is
the natural figure explaining the objective and the constraint in the neyman-pearson
framework. by changing the coordinates, we obtain the pr curve. therefore, the two
curves are the same in terms of the amount of information, but they offer different
interpretations.
9.7 reference
confidence interval
9-1 dimitri p. bertsekas and john n. tsitsiklis, introduction to probability , athena sci-
entific, 2nd edition, 2008. chapter 9.1.
9-2 michael j evans and jeffrey s. rosenthal, probability and statistics , w. h. freeman,
2nd edition, 2009. chapter 6.3.
9-3 robert v. hogg, joseph w. mckean, and allen t. craig, introduction to mathematical
statistics , pearson, 7th edition, 2013. chapter 4.2.
9-4 larry wasserman, all of statistics , springer 2003. chapter 6.
9-5 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. chapter 8.4.
bootstrapping
9-6 trevor hastie, robert tibshirani, and jerome friedman, elements of statistical learn-
ing, springer, 2nd edition. chapter 8.2.
9-7 larry wasserman, all of statistics , springer 2003. chapter 8.
9-8 michael j evans and jeffrey s. rosenthal, probability and statistics , w. h. freeman,
2nd edition, 2009. chapter 6.4.
9-9 robert v. hogg, joseph w. mckean, and allen t. craig, introduction to mathematical
statistics , pearson, 7th edition, 2013. chapter 4.9.
6069.8. problems
hypothesis testing
9-10 robert v. hogg, joseph w. mckean, and allen t. craig, introduction to mathematical
statistics , pearson, 7th edition, 2013. chapter 4.5.
9-11 athanasios papoulis and s. unnikrishna pillai, probability, random variables and
stochastic processes , mcgraw-hill, 4th edition, 2001. chapter 8.
9-12 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , prentice hall, 3rd edition, 2008. chapter 8.5.
9-13 dimitri p. bertsekas and john n. tsitsiklis, introduction to probability , athena sci-
entific, 2nd edition, 2008. chapter 9.
9-14 michael j evans and jeffrey s. rosenthal, probability and statistics , w. h. freeman,
2nd edition, 2009. chapter 6.3.
9-15 larry wasserman, all of statistics , springer 2003. chapter 10.
9-16 laura simon, introduction to mathematical statistics , penn state university stat
415 textbook, online materials. accessed 12/2020. https://online.stat.psu.edu/
stat415/
neyman-pearson and roc curves
9-17 robert v. hogg, joseph w. mckean, and allen t. craig, introduction to mathematical
statistics , pearson, 7th edition, 2013. chapter 8.
9-18 h. vincent poor, an introduction signal detection and estimation , springer, 1998.
9-19 bernard c. levy, principles of signal detection and parameter estimation , springer,
2008.
9-20 steven m. kay, fundamentals of statistical signal processing: estimation theory ,
prentice-hall, 1993.
9-21 steven m. kay, fundamentals of statistical signal processing: detection theory , prentice-
hall, 1998.
9.8 problems
exercise 1.
consider i.i.d. gaussian random variables x1, . . . , x nwith an unknown mean Œ∏and a known
variance œÉ2= 1. suppose n= 30. find the confidence level 1 ‚àíŒ±for the confidence intervals
of the mean bŒ∏:
(a)i= [bŒ∏‚àí2.14œÉ‚àö
n,bŒ∏ +2.14œÉ‚àö
n]
(b)i= [bŒ∏‚àí1.85œÉ‚àö
n,bŒ∏ +1.85œÉ‚àö
n]
607chapter 9. confidence and hypothesis
exercise 2.
suppose that we have conducted an experiment with n= 100 samples. a 95% confidence
interval of the mean was 0 .45‚â§¬µ‚â§0.82.
(a) would a 99% confidence interval calculated from the sample data be wider or narrower?
(b) is it correct to interpret the confidence interval as saying that there is a 95% chance
that¬µis between 0.49 and 0.82? you may answer yes, no, or partially correct. explain.
(c) is it correct to say that if we conduct the experiment 1000 times, there will be 950
confidence intervals that will contain ¬µ? you may answer yes, no, or partially correct.
explain.
exercise 3.
suppose that we have conducted an experiment. we know that œÉ= 25. we obtained n= 20
samples and found that the sample mean is bŒ∏ = 1014.
(a) construct a 95% two-sided confidence interval of bŒ∏.
(b) construct a 95% one-sided confidence interval (the lower tail) of bŒ∏.
exercise 4.
letx1, . . . , x nbe i.i.d. gaussian with xn‚àºgaussian(0 ,1). let yn=exn, and suppose
we have n= 100 samples. we want to compute a 95% confidence interval for skewness.
(a) randomly subsample the dataset with b= 30 samples. repeat the exercise 5 times.
plot the resulting histograms using matlab or python.
(b) repeat (a) for m= 500 times and compute the 95% bootstrapped confidence interval
of the skewness.
(c) try using a larger b= 70 and a smaller b= 10. report the 95% bootstrapped
confidence interval of the skewness.
exercise 5.
letx1, . . . , x nbe i.i.d. uniform with xn‚àºuniform(0 , Œ∏). let bŒ∏ = max {x1, . . . , x n}.
generate a dataset of n= 50 with Œ∏= 1.
(a) find the distribution of the estimator bŒ∏.
(b) show that p[bŒ∏ =Œ∏] = 1‚àí(1‚àí(1/n))n. thus, as n‚Üí ‚àû , we have p[bŒ∏ =Œ∏] = 0.
(c) use python or matlab to generate the histogram of bŒ∏ from bootstrapping. how
does the bootstrapped histogram look as ngrows? why?
exercise 6.
letxbe a gaussian random variable with unknown mean and unknown variance. it was
found that with n= 15,
nx
n=1xn= 250 ,nx
n=1x2
n= 10000 .
6089.8. problems
find a 95% confidence interval of the mean of x.
exercise 7.
letbŒ∏ be the sample mean of a dataset containing nsamples. it is known that the samples
are drawn from gaussian( Œ∏,32). find nsuch that
p[bŒ∏‚àí1‚â§Œ∏‚â§bŒ∏ + 1] = 0 .95.
exercise 8.
which of the following statements are valid hypothesis testing problems?
(a)h0:¬µ= 25 and h1:¬µÃ∏= 25.
(b)h0:œÉ >10 and h1:œÉ= 10.
(c)h0:x= 50 and h1:xÃ∏= 50.
(d)h0:p-value = 0.1, h1:p-value = 0.5.
exercise 9.
it is claimed that the mean is Œ∏= 12 with a standard deviation 0.5. consider h0:Œ∏= 12
andh1:Œ∏ <12. ten samples are obtained, and it is found that bŒ∏ = 13 .5. with a 95%
confidence level, should we accept or reject the null hypothesis?
exercise 10.
consider a hypothesis testing problem: h0:Œ∏= 175 versus an alternative hypothesis h1:
Œ∏ >175. assume n= 10 and œÉ= 20.
(a) find the type 1 error if the critical region is bŒ∏>185.
(b) find the type 2 error if the true mean is 195.
exercise 11.
consider h0:Œ∏= 30000 versus an alternative hypothesis h1:Œ∏ >30000. suppose n= 16,
and let œÉ= 1500.
(a) if we want Œ±= 0.01, what is zŒ±?
(b) what is the type 2 error when Œ∏= 31000?
exercise 12.
letwn‚àºgaussian(0 , œÉ2), and consider two hypotheses:
h0: xn=Œ∏0+wn, n = 1, . . . , n,
h1: xn=Œ∏1+wn, n = 1, . . . , n.
letx= (1/n)pn
n=1xn.
609chapter 9. confidence and hypothesis
(a) show that the likelihood of observing x1, . . . , x ngiven h0is
fx(x|h0) =1
(2œÄœÉ2)n/2exp(
‚àí1
2œÉ2nx
n=1(xn‚àíŒ∏0)2)
.
(b) find the likelihood fx(x|h1) of observing x1, . . . , x ngiven h1.
(c) the likelihood ratio test states that
fx(x|h1)
fx(x|h0)‚â∑h1
h0œÑ.
show that the likelihood ratio test is given by
x‚â∑h1
h0Œ∏0+Œ∏1
2+œÉ2logœÑ
n(Œ∏1‚àíŒ∏0).
610chapter 10
random processes
in modern data science, many problems involve time. the stock market changes every
minute; a speech signal changes every millisecond; a car changes its steering angle constantly;
the examples are endless. a common theme among all these examples is randomness. we
do not know whether a stock will go up or down tomorrow, although we may be able to
make some predictions based on previous observations. we do not know the next word of a
sentence, but we can guess based on the context. random processes are tools that can be
applied to these situations. we treat a random process as an infinitely long vector of random
variables where the correlations between the individual variables define the statistical prop-
erties of the process. if we can determine these correlations, we will be able to summarize
the past and predict the future.
the objective of this chapter is to introduce the basic concepts of random processes .
given the breadth of the subject, we can only cover the most elementary results, but they
are sufficient for many engineering and data science problems. however, there are complex
situations for which these elementary results will be insufficient. the references at the end
of this chapter contain more in-depth discussions of random processes.
plan of this chapter
we begin by outlining the definition of random processes and ways to characterize their
randomness in section 10.1. in section 10.2 we discuss the mean function, the autocorrelation
function, and the autocovariance function of a random process. in section 10.3 we look at
a special subclass of random processes known as the wide-sense stationary processes. wide-
sense stationary processes allow us to use tools in the fourier domain to make statistical
statements. based on wide-sense stationary processes, we discuss power spectral density in
section 10.4. with this concept, we can ask what will happen to the random process when we
pass it through a linear transformation. in section 10.5 we discuss such interactions between
the random process and a linear time-invariant system. finally, we discuss a practical usage
of random processes in the subject of optimal linear filters in section 10.6.
611chapter 10. random processes
10.1 basic concepts
10.1.1 everything you need to know about a random process
here is the single most important thing you need to remember about random processes:
what is a random process?
a random process is a function indexed by a random key .
that‚Äôs it. now you may be wondering what exactly a ‚Äúfunction indexed by a random key ‚Äù
means. to help you see the picture, we consider two examples.
example 10.1 . we consider a set of straight lines. we define two random variables a
andbthat are uniformly distributed in a certain range. we then define a function:
f(t) =at+b, ‚àí2‚â§t‚â§2. (10.1)
clearly, f(t) is a function of time t. but since aandbare random, f(t) is also random.
the randomness is caused by aandb. to emphasize this dependency, we write f(t) as
f(t, Œæ) =a(Œæ)t+b(Œæ),‚àí2‚â§t‚â§2,
where Œæ‚ààœâ denotes the random index of the constants ( a, b) and œâ is the sample
space of Œæ. therefore, by picking a different pair of constants ( a(Œæ), b(Œæ)), we will have
a different function f(t, Œæ), which in our case is a straight line of different slope and
y-intercept.
-2 -1 0 1 2
t-1-0.500.51f(t)
figure 10.1: the set of straight lines f(x) =ax+bwhere a, b‚ààr.
as a special case of the example, suppose that the sample space contains only
two pairs of constants: ( a, b) = (1 .2,0.6) and ( a, b) = (‚àí0.75,1.8). the probability of
61210.1. basic concepts
getting either pair is1
2. then the function f(t, Œæ) will take two forms:
f(t, Œæ) =(
1.2t+ 0.6, with probability1
2,
‚àí0.75t+ 1.8, with probability1
2.
every time you pick a sample you pick one of the two functions, either f(t, Œæ1) or
f(t, Œæ2). so we say that f(t, Œæ) is a random process because it is a function f(t) indexed
by a random key Œæ.
example 10.2 . this example studies the function
f(t) = cos( œâ0t+ Œ∏),‚àí1‚â§t‚â§1,
where Œ∏ is a random phase distributed uniformly over the range [0 ,2œÄ]. depending on
the randomness of Œ∏, the function f(t) will take a different phase offset. to emphasize
this dependency, we write
f(t, Œæ) = cos( œâ0t+ Œ∏(Œæ)),‚àí1‚â§t‚â§1. (10.2)
-1 -0.5 0 0.5 1
t-2-1012f(t)
figure 10.2: the set of phase-shifted cosines f(t) = cos( œâ0t+Œ∏)where Œ∏‚àà[0,2œÄ].
again, Œædenotes the index of the random variable Œ∏. since Œ∏ is drawn uniformly
from the interval [0 ,2œÄ], the following functions are two possible realizations:
f(t, Œæ1) = cos
œâ0t+3œÄ
4
,‚àí1‚â§t‚â§1,
f(t, Œæ2) = cos
œâ0t‚àí7œÄ
3
,‚àí1‚â§t‚â§1.
just as with the previous example, f(t) is a function indexed by a random key Œæ.
these two examples should give you a feeling for what to expect from a random process.
a random process is quite similar to a random variable because they are both contained
in a certain sample space. for (discrete) random variables, the sample space is a collection
of outcomes {Œæ1, Œæ2, . . . , Œæ n}. the random variable x:f ‚Üíris a mapping that maps
Œæntox(Œæn), where x(Œæn) is a number. for random processes, the sample space is also
613chapter 10. random processes
{Œæ1, Œæ2, . . . , Œæ n}. however, the mapping xdoes not map Œænto anumber x(Œæn) but to a
function x(t, Œæn). a function has the time index t, which is absent in the number. therefore,
for the same Œæn,x(t1, Œæn) can take one value and x(t2, Œæn) can take another value.
figure 10.3: the sample space of a random process x(t, Œæ)contains many functions. therefore, each
random realization is afunction .
figure 10.3 shows the sample space of a random process. each outcome in the sample
space is a function. the probability of getting a function is specified by the probability
mass or the probability density of the associated random key Œæ. if you put your hand into
the sample space, the sample you pick will be a function that will change with time and is
indexed by the random key. from our discussions of joint random variables in chapter 5,
you can think of the function as a vector. when you pull a sample from the sample space,
you pull the entire vector and not just an element.
10.1.2 statistical and temporal perspectives
since a random process is a function indexed by a random key, it is a two-dimensional object.
it is a function both of time tand of the random key Œæ. that‚Äôs why we use the notation
x(t, Œæ) to denote a random process. these two axes play different roles, as illustrated in
figure 10.4 .
temporal perspective : let us fix the random key at Œæ=Œæ0. this gives us a function
x(t, Œæ0). since Œæis already fixed at Œæ0, we are looking at a particular realization drawn
from the sample space. this realization is expressed as a function x(t, Œæ0), which is just
a deterministic function that evolves over time. there is no randomness associated with
it. this is analogous to a random variable. while xitself is a random variable, by fixing
the random key Œæ=Œæ0,x(Œæ0) is just a real number. for random processes, x(t, Œæ0) now
becomes a function.
since x(t, Œæ0) is a function that evolves over time, we view it along the horizontal axis.
for example, we can study the sequence
x(t1, Œæ0), x(t2, Œæ0), . . . , x (tk, Œæ0),
where t1, . . . , t kare the time indices of the function. this sequence is deterministic and is
just a sequence of numbers, although the numbers evolve as tchanges.
statistical perspective : the other perspective, which could be slightly more abstract,
is the statistical perspective. let us fix the time at t=t0. the random key Œæcan take any
61410.1. basic concepts
(a) temporal perspective (b) statistical perspective
figure 10.4: temporal and statistical perspectives of a random process. for the temporal perspective
(which we call the horizontal perspective), we fix the random key Œæand look at the function in time.
for the statistical perspective (which we call the vertical perspective), we fix the time and look at the
function at different random keys .
state defined in the sample space. so if the sample space contains {Œæ1, . . . , Œæ n}, the sequence
{x(t0, Œæ1), . . . , x (t0, Œæn)}is a sequence of random variables, because the Œæ‚Äôs can go from
one state to another state.
a good way to visualize the statistical perspective is the vertical perspective in which
we write the sequence as a vertical column of random variables:
x(t0, Œæ1)
x(t0, Œæ2)
...
x(t0, Œæn)
that is, if you fix the time at t=t0, you are getting a sequence of random variables. the
probability of getting a particular value x(t0) depends on which random state you land on.
why do we bother to differentiate the temporal perspective and the statistical per-
spective? the reason is that the operations associated with the two are different, even if
sometimes they give you the same result. for example, if we take the temporal average of
the random process, we get a number:
x(Œæ) =1
tzt
0x(t, Œæ)dt. (10.3)
we call this the ‚Äútemporal average‚Äù because we have integrated the function over time. the
resulting value will not change with time. however, x(Œæ) depends on the random key you
provide. if you pick a different random realization, x(Œæ) will take a different value. so the
temporal average is a random variable .
on the other hand, if we take the statistical average of the random process, we get
e[x(t)] =z
œâx(t, Œæ)p(Œæ)dŒæ, (10.4)
615chapter 10. random processes
where p(Œæ) is the pdf of the random key Œæ. we call this the statistical average because we
have taken the expectation over all possible random keys. the resulting object e[x(t)] is
deterministic but a function of time.
no matter how you look at the temporal average or the statistical average, they are
different with the following exception: that x(Œæ) = const and e[x(t)] = const, for example,
x(Œæ) =e[x(t)] = 0. this happens only for some special (and useful) random processes
known as ergodic processes that allow us to approximate the statistical average using the
temporal average, with some guarantees derived from the law of large numbers. we will
return to this point later.
example 10.3 . let a‚àºuniform[0 ,1]. define x(t, Œæ) =a(Œæ) cos(2 œÄt).
in this example, the magnitude a(Œæ) is a random variable depending on the
random key Œæ. for example if we draw Œæ1, perhaps we will get a value a(Œæ1) = 0 .5.
then x(t, Œæ1) = 0 .5 cos(2 œÄt). to take another example, if we draw Œæ2, we may get
a(Œæ2) = 1. then x(t, Œæ2) = 1 cos(2 œÄt).figure 10.5 shows a few random realizations
of the cosines. we can look at x(t, Œæ) from the statistical and the temporal views.
-2 -1 0 1 2-1-0.500.51
x1(t)
x2(t)
x3(t)
x4(t)
x5(t)
figure 10.5: five different realizations of the random process x(t) =acos(2 œÄt).
¬àstatistical view : fix t(for example t= 10). in this case, we have
x(t, Œæ) =a(Œæ) cos(2 œÄ(10)) = a(Œæ) cos(20 œÄ),
which is a random variable because cos(20 œÄ) is a constant. the randomness of
xcomes from the fact that a(Œæ)‚àºuniform[0 ,1].
¬àtemporal view : fix Œæ(for example a(Œæ) = 0 .7). in this case, we have
x(t, Œæ) = 0 .7 cos(2 œÄt),
which is a deterministic function of t.
example 10.4 . let abe a discrete random variable with a pmf
p(a= +1) =1
2andp(a=‚àí1) =1
2.
61610.1. basic concepts
we define the function x[n, Œæ] =a(Œæ)(‚àí1)n. in this example, acan only take two
states. if a= +1, then x[n, Œæ] = (‚àí1)n. ifa=‚àí1, then x[n, Œæ] = (‚àí1)n+1.
-1.5-1-0.500.511.5
0 1 2 3 4 5x1(n)
-1.5-1-0.500.511.5
0 1 2 3 4 5x2(n)
figure 10.6: realizations of the random process x[n] =a(‚àí1)n.
the graphical illustration of this example is shown in figure 10.6 . again, we can
look at x[n, Œæ] from two views.
¬àstatistical view : fix n, say n= 10. then,
x(Œæ) =(
(‚àí1)10= 1, with prob 1 /2,
(‚àí1)11=‚àí1, with prob 1 /2,
which is a bernoulli random variable.
¬àtemporal view : fix Œæ. then,
x[n] =(
(‚àí1)n, ifa= +1 ,
(‚àí1)n+1, ifa=‚àí1,
which is a time series.
in this example, we see that the sample space of x(n, Œæ) consists of only two functions
with probabilities
p(x[n] = (‚àí1)n) =1
2,
p(x[n] = (‚àí1)n+1) =1
2,
therefore, if there is a sequence outside the sample space, e.g.,
p 
x[n] =1 1 1 ‚àí1 1 ‚àí1¬∑¬∑¬∑
= 0
then the probability of obtaining that sequence is 0.
what do we mean by statistical average and temporal average?
¬àstatistical average: take the expectation of x(t, Œæ) over Œæ. this is the vertical
617chapter 10. random processes
average.
¬àtemporal average: take the expectation of x(t, Œæ) over t. this is the horizontal
average.
¬àin general, statistical average Ã∏= temporal average.
10.2 mean and correlation functions
given a random variable, we often want to know the expectation and variance, and often
we also want to know the expectation and variance for the random processes. nevertheless,
we need to consider the time axis. in this section, we discuss the mean function and the
autocorrelation function .
10.2.1 mean function
definition 10.1. themean function ¬µx(t)of a random process x(t)is
¬µx(t) =e[x(t)]. (10.5)
let‚Äôs consider the ‚Äúexpectation‚Äù of x(t). recall that a random process is actually x(t, Œæ)
where Œæis the random key. therefore, the expectation is taken with respect to Œæ, or to state
it more explicitly,
¬µx(t) =e[x(t)] =z
œâx(t, Œæ)p(Œæ)dŒæ,
where p(Œæ) is the pdf of the random key. this is an abstract definition, but it is not difficult
to understand if you follow the example below.
example 10.5 . let a‚àºuniform[0 ,1], and let x(t) =acos(2 œÄt). find ¬µx(t).
solution . the solution to this problem is actually very simple:
¬µx(t) =e[x(t)] =e[acos(2 œÄt)]
= cos(2 œÄt)e[a] =1
2cos(2 œÄt).
so the answer is ¬µx(t) =1
2cos(2 œÄt).
we can link the equations to the definition more explicitly. to do so, we rewrite
x(t) as
x(t, Œæ) =a(Œæ) cos(2 œÄt).
61810.2. mean and correlation functions
then we take the expectation over a:
¬µx(t) =z
œâx(t, a)pa(a)da=z1
0acos(2 œÄt)¬∑1da
= cos(2 œÄt)a2
21
0=1
2cos(2 œÄt).
-2 -1.5 -1 -0.5 0 0.5 1 1.5 2-1-0.500.51
figure 10.7: the mean function of x(t) =acos(2 œÄt).
an illustration is provided in figure 10.7 , in which we observe many random
realizations of the random process x(t, Œæ). on top of these, we also see the mean
function. the way to visualize the mean function is to use the statistical perspective .
that is, fix a time tand look at all the possible values that the function can take. for
example, if we fix t=t0, then we will have a set of realizations of one random variable:

0.71 cos(2 œÄt0),0.58 cos(2 œÄt0), . . . , 0.93 cos(2 œÄt0)
‚Üítake expectation
therefore, when we take the expectation, it is that of the underlying random variable.
if we move to another timestamp t=t1, we will have a different expectation because
cos(2 œÄt0) now becomes cos(2 œÄt1).
the matlab/python codes used to generate figure 10.7 are shown below. you can
also replace the line 0.5*cos(2*pi*t) by the mean function mean(x) (in matlab).
% matlab code for example 10.5
x = zeros(1000,20);
t = linspace(-2,2,1000);
for i=1:20
x(:,i) = rand(1)*cos(2*pi*t);
end
plot(t, x, ‚Äôlinewidth‚Äô, 2, ‚Äôcolor‚Äô, [0.8 0.8 0.8]); hold on;
plot(t, 0.5*cos(2*pi*t), ‚Äôlinewidth‚Äô, 4, ‚Äôcolor‚Äô, [0.6 0 0]);
# python code for example 10.5
x = np.zeros((1000,20))
619chapter 10. random processes
t = np.linspace(-2,2,1000)
for i in range(20):
x[:,i] = np.random.rand(1)*np.cos(2*np.pi*t)
plt.plot(t,x,color=‚Äôgray‚Äô)
plt.plot(t,0.5*np.cos(2*np.pi*t),color=‚Äôred‚Äô)
plt.show()
example 10.6 . let Œ∏ ‚àºuniform[ ‚àíœÄ, œÄ], and let x(t) = cos( œât+ Œ∏). find ¬µx(t).
solution .
¬µx(t) =e[cos(œât+ Œ∏)] =zœÄ
‚àíœÄcos(œât+Œ∏)¬∑1
2œÄdŒ∏= 0.
again, as in the previous example, we can try to map this simple calculation with the
definition. write x(t) as
x(t, Œæ) = cos( œât+ Œ∏(Œæ)).
then the expectation is
¬µx(t) =z
œâcos(œât+Œ∏)pŒ∏(Œ∏)dŒ∏
=zœÄ
‚àíœÄcos(œât+Œ∏)¬∑1
2œÄdŒ∏= 0.
-2 -1.5 -1 -0.5 0 0.5 1 1.5 2-1-0.500.51
figure 10.8: the mean function of x(t) = cos( œât+ Œ∏) .
figure 10.8 illustrates the random realizations for x(t) = cos( œât+ Œ∏) and the
mean function. the zero mean should not be a surprise because if we take the statistical
average (the vertical average) across all the possible values at any time instant, the
positive and negative values of the realizations will make the mean zero.
we should emphasize that the statistical average is not the same as the temporal
average, even if they give you the same value. why do we say that? if we calculate
thetemporal average of the function cos( œât+Œ∏0) for a specific value Œ∏ = Œ∏0, then we
62010.2. mean and correlation functions
have
x=1
tzt
0cos(œât+Œ∏0)dt= 0,
assuming that tis a multiple of the cosine period. this implies that the temporal
average is zero, which is the same as the statistical average. this gives us an example
in which the statistical average and the temporal average have the same value, although
we know they are two completely different things.
the matlab/python codes used to generate figure 10.8 are shown below.
% matlab code for example 10.6
x = zeros(1000,20);
t = linspace(-2,2,1000);
for i=1:20
x(:,i) = cos(2*pi*t+2*pi*rand(1));
end
plot(t, x, ‚Äôlinewidth‚Äô, 2, ‚Äôcolor‚Äô, [0.8 0.8 0.8]); hold on;
plot(t, 0*cos(2*pi*t), ‚Äôlinewidth‚Äô, 4, ‚Äôcolor‚Äô, [0.6 0 0]);
# python code for example 10.6
x = np.zeros((1000,20))
t = np.linspace(-2,2,1000)
for i in range(20):
theta = 2*np.pi*(np.random.rand(1))
x[:,i] = np.cos(2*np.pi*t+theta)
plt.plot(t,x,color=‚Äôgray‚Äô)
plt.plot(t,np.zeros((1000,1)),color=‚Äôred‚Äô)
plt.show()
example 10.7 . let us consider a discrete-time random process. let x[n] =sn, where
s‚àºuniform[0 ,1]. find ¬µx[n].
¬µx[n] =e[sn] =z1
0snds=1
n+ 1.
in this example the randomness goes with the constant s. thus, if we write x[n] as
x[n, Œæ] = [s(Œæ)]n,
the expectation is
e[x[n]] =z
œâsnps(s)ds=z1
0sn¬∑1ds=1
n+ 1.
the graphical illustration is provided in figure 10.9 .
621chapter 10. random processes
-0.200.20.40.60.811.2
0 5 10 15 20
figure 10.9: the mean function of x[n] =sn, where s‚àºuniform [0,1].
the matlab code used to generate figure 10.9 is shown below. we skip the python
implementation because it is straightforward.
% matlab code for example 10.7
t = 0:20;
for i=1:20
x(:,i) = rand(1).^t;
end
stem(t, x, ‚Äôlinewidth‚Äô, 2, ‚Äôcolor‚Äô, [0.8 0.8 0.8]); hold on;
stem(t, 1./(t+1), ‚Äôlinewidth‚Äô, 2, ‚Äômarkersize‚Äô, 8);
10.2.2 autocorrelation function
in random processes, the notions of ‚Äúvariance‚Äù and ‚Äúcovariance‚Äù are trickier than for random
variables. let us first define the concept of an autocorrelation function .
definition 10.2. theautocorrelation function of a random process x(t)is
rx(t1, t2) =e[x(t1)x(t2)]. (10.6)
rx(t1, t2) is not difficult to calculate ‚Äî just integrate x(t1)x(t2) using the appropriate
pdfs.
example 10.8 . let a‚àºuniform[0 ,1],x(t) =acos(2 œÄt). find rx(t1, t2).
solution .
rx(t1, t2) =e[acos(2 œÄt1)acos(2 œÄt2)]
=e[a2] cos(2 œÄt1) cos(2 œÄt2) =1
3cos(2 œÄt1) cos(2 œÄt2).
62210.2. mean and correlation functions
example 10.9 . let Œ∏ ‚àºuniform[ ‚àíœÄ, œÄ],x(t) = cos( œât+ Œ∏). find rx(t1, t2).
solution .
rx(t1, t2) =e[cos(œât1+ Œ∏) cos( œât2+ Œ∏)]
=1
2œÄzœÄ
‚àíœÄcos(œât1+Œ∏) cos( œât2+Œ∏)dŒ∏
(a)=1
2œÄzœÄ
‚àíœÄ1
2
cos(œâ(t1+t2) + 2Œ∏) + cos( œâ(t1‚àít2))
dŒ∏
=1
2cos
œâ(t1‚àít2)
,
where in (a) we applied the trigonometric formula:
cosacosb=1
2[cos(a+b) + cos( a‚àíb)],
as you can see, the calculations are not difficult. the tricky thing is the interpretation
ofrx(t1, t2).
how do we understand the meaning of e[x(t1)x(t2)]?
e[x(t1)x(t2)] is analogous to the correlation e[xy] between two random variables
xandy.
the autocorrelation function e[x(t1)x(t2)] is analogous to the correlation e[xy] in rela-
tion to a pair of random variables. in our discussions of e[xy], we mentioned that e[xy]
could be regarded as the inner product of two vectors, and so it is a measure of the closeness
between xandy. now, if we substitute xandywith x(t1) and x(t2) respectively, then
we are effectively asking about the closeness between x(t1) and x(t2). so, in a nutshell, the
autocorrelation function tells us the correlation between the function at two different time
stamps.
what do we mean by the correlation between two timestamps? remember that x(t1)
andx(t2) are two random variables. consider the following example.
example 10.10 . letx(t) =acos(2 œÄt), where a‚àºuniform[0 ,1]. find e[x(0)x(0.5)].
solution . ifx(t) =acos(2 œÄt), then
x(0) = acos(0) = a,
x(0.5) = acos(œÄ) =‚àía.
when you have two random variables, you consider their correlations. using this ex-
623chapter 10. random processes
ample, we have that
e[x(0)x(0.5)] =‚àíe[a¬∑a]
=‚àíe[a2] =‚àí1
3.
a picture will reveal what is happening. figure 10.10 presents the realizations of the
random process x(t) =acos(2 œÄt). if we consider x(0) and x(0.5), each of them is a
random variable, and thus we can ask about their pdfs. it is obvious from the illustration
that the random variable x(0) has a pdf that is a uniform distribution from 0 to 1,
whereas the random variable x(0.5) has a pdf that is a uniform distribution from ‚àí1 to 0.
mathematically, the pdfs are
fx(0)(x) =(
1,0‚â§x‚â§1,
0,otherwiseand fx(0.5)(x) =(
1,‚àí1‚â§x‚â§0,
0,otherwise .
since x(0) and x(0.5) have their own pdfs, we can calculate their correlation. this will
give us e[x(0)x(0.5)] which after some calculations is e[x(0)x(0.5)] =‚àí1
3.
-2 -1.5 -1 -0.5 0 0.5 1 1.5 2-1-0.500.51
figure 10.10: the autocorrelation between x(0)andx(0.5)should be regarded as the correlation
between two random variables. each random variable has its own pdf.
we can now consider the autocorrelation for any t1andt2. when you are evaluating
the autocorrelation function, you are not just evaluating at t= 0 and t= 0.5, you are
also evaluating the correlation for all pairs of t1andt2. now you want to know what the
correlation is between t= 0 and t= 0.5,t= 2 and t= 3.1, etc. of course, there are
infinitely many pairs of time instants. the point of the autocorrelation function is to tell
you the correlation of allthe pairs. in other words, if we tell you rx(t1, t2), you will be able
to plug in a value of t1and a value of t2and tell us the correlation at ( t1, t2). how is this
possible? to find out, let‚Äôs consider the following example.
example 10.11 . let a‚àºuniform[0 ,1],x(t) =acos(2 œÄt). find rx(0,0.5), and draw
rx(t1, t2).
62410.2. mean and correlation functions
solution . from the previous example, we know that
rx(t1, t2) =1
3cos(2 œÄt1) cos(2 œÄt2).
therefore, rx(0,0.5) =1
3cos(2 œÄ0) cos(2 œÄ0.5) =‚àí1
3, which is the same as if we had
computed it from the first principle.
the autocorrelation function tells you how one point of a time series is correlated
with another point of the time series. if rx(t1, t2) gives a high value, then it means the
random variables at t1andt2have a strong correlation. to understand this, suppose
we let t1= 0, and let us vary t2. then
rx(0, t2) =1
3cos(2 œÄ0) cos(2 œÄt2) =1
3cos(2 œÄt2).
this is a periodic function that cycles through itself whenever t2is an integer. as
we recall from figure 10.10 , ift2= 0.5, the random variable x(t2) will take only
the negative values, but otherwise it is correlated with x(0). on the other hand, if
t2= 0.25, then figure 10.10 says that the random variable x(t2) is a constant 0, and
so the correlation with x(0) is zero.
clearly, rx(t1, t2) is a 2-dimensional function of t1andt2. you need to tell rx
which of the two time instants you want to compare, and then rxwill tell you the
correlation. so no matter what happens, you must specify two time instants. because
rx(t1, t2) is a 2-dimensional function, we can visualize it by calculating all the possible
values it takes. for example, if rx(t1, t2) =1
3cos(2 œÄt1) cos(2 œÄt2), we can plot rxas
a function of t1andt2.figure 10.11 shows the plot.
-2 -1.5 -1 -0.5 0 0.5 1 1.5 2-1-0.500.51
-1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1
t1-1
-0.75
-0.5
-0.25
0
0.25
0.5
0.75
1t2
figure 10.11: the autocorrelation function rx(t1, t2) =1
3cos(2 œÄt1) cos(2 œÄt2).
the matlab/python code for figure 10.11 is shown below.
% matlab code for example 10.11
t = linspace(-1,1,1000);
r = (1/3)*cos(2*pi*t(:)).*cos(2*pi*t);
imagesc(t,t,r);
# python code for example 10.11
import numpy as np
625chapter 10. random processes
import matplotlib.pyplot as plt
t = np.linspace(-1,1,1000)
r = (1/3)*np.outer(np.cos(2*np.pi*t), np.cos(2*np.pi*t))
plt.imshow(r, extent=[-1, 1, -1, 1])
plt.show()
to understand the 2d function shown on the right hand side of figure 10.11 , we can
take a closer look by drawing figure 10.12 . for any two time instants t1andt2, we have
two random variables x(t1) and x(t2). the joint expectation e[x(t1)x(t2)] will return us
some value, and this is a point in the 2d plot rx(t1, t2). the value tells us the correlation
between x(t1) and x(t2). in the example in which t1= 0 and t2= 0.5, the correlation is
‚àí1
3. interestingly, if we pick another pair of time instants t1=‚àí0.5 and t2= 0, the joint
expectation is e[x(‚àí0.5)x(0)] = ‚àí1
3, which is the same value. however, this ‚àí1
3is located
at a different valley than e[x(0)x(0.5)] in the 2d plot.
figure 10.12: to understand the autocorrelation function, pick two time instants t1andt2, and then
evaluate the joint expectation e[x(t1)x(t2)].
the above example shows a periodic autocorrelation function. the fact that it is peri-
odic is coincidental because the random process x(t) is a periodic function. in general, an
arbitrary random process can have an arbitrary autocorrelation function that is not periodic.
there are, of course, various properties of the autocorrelation functions and special types
of autocorrelation functions. we will study one of them, called the wide-sense stationary
processes , later.
example 10.12 . let Œ∏ ‚àºuniform[ ‚àíœÄ, œÄ],x(t) = cos( œât+ Œ∏). draw the autocorrela-
tion function rx(t1, t2).
solution . from the previous example we know that
rx(t1, t2) =1
2cos
œâ(t1‚àít2)
.
figure 10.13 shows the realizations, and the mean and autocorrelation functions.
62610.2. mean and correlation functions
note that the autocorrelation function has a structure: every row is a shifted
version of the previous row. we call this a toeplitz structure. an autocorrelation with
a toeplitz structure is specified once we know any of the rows. a toeplitz structure also
implies that the autocorrelation function does not depend on the pair ( t1, t2) but only
on the difference t1‚àít2. in other words, rx(0,1) is the same as rx(11.6,12.6), and so
knowing rx(0,1) is enough to know all rx(t0, t0+t). not all random processes have
a toeplitz autocorrelation function. random processes with a toeplitz autocorrelation
function are ‚Äúnice‚Äù processes that we will study in detail later.
-2 -1.5 -1 -0.5 0 0.5 1 1.5 2-1-0.500.51
-1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1
t1-1
-0.75
-0.5
-0.25
0
0.25
0.5
0.75
1t2
figure 10.13: the autocorrelation function rx(t1, t2) =1
2cos
œâ(t1‚àít2)
.
the matlab code used to generate figure 10.13 is shown below.
% matlab code for example 10.12
t = linspace(-1,1,1000);
r = toeplitz(0.5*cos(2*pi*t(:)));
imagesc(t,t,r);
grid on;
xticks(-1:0.25:1);
yticks(-1:0.25:1);
practice exercise 10.1 . let Œ∏ ‚àºuniform[0 ,2œÄ],x(t) = cos( œât+ Œ∏). find the pdf
ofx(0).
solution . let z=x(0) = cos Œ∏. then the cdf of zis
fz(z) =p[z‚â§z]
=p[cos Œ∏ ‚â§z]
=p[cos‚àí1z‚â§Œ∏‚â§2œÄ‚àícos‚àí1z]
= 1‚àícos‚àí1z
œÄ.
627chapter 10. random processes
then by the fundamental theorem of calculus,
fz(z) =1
œÄ‚àö
1‚àíz2.
a similar concept to the autocorrelation function is the autocovariance function. the
idea is to remove the mean before computing the correlation. this is analogous to the
covariance cov( x, y) =e[(x‚àí¬µx)(y‚àí¬µy)] as opposed to the correlation e[xy] in the
random variable case.
definition 10.3. theautocovariance function of a random process x(t)is
cx(t1, t2) =e[(x(t1)‚àí¬µx(t1)) (x(t2)‚àí¬µx(t2))]. (10.7)
as one might expect, the autocovariance function is closely related to the autocorrelation
function.
theorem 10.1.
cx(t1, t2) =rx(t1, t2)‚àí¬µx(t1)¬µx(t2). (10.8)
proof . plugging in the definition, we have that
cx(t1, t2) =e[(x(t1)‚àí¬µx(t1)) (x(t2)‚àí¬µx(t2))]
=e[x(t1)x(t2)‚àíx(t1)¬µx(t2)‚àíx(t2)¬µx(t1) +¬µx(t1)¬µx(t2)]
=rx(t1, t2)‚àí¬µx(t1)¬µx(t2)‚àí¬µx(t1)¬µx(t2) +¬µx(t1)¬µx(t2)
=rx(t1, t2)‚àí¬µx(t1)¬µx(t2).
‚ñ°
practice exercise 10.2 . suppose x(t) =acos(2 œÄt) for a‚àºuniform[0 ,1]. find
cx(t1, t2).
solution .
cx(t1, t2) =rx(t1, t2)‚àí¬µx(t1)¬µx(t2)
=1
3cos(2 œÄt1) cos(2 œÄt2)‚àí1
2cos(2 œÄt1)¬∑1
2cos(2 œÄt2)
=1
12cos(2 œÄt1) cos(2 œÄt2).
practice exercise 10.3 . suppose x(t) = cos( œât+ Œ∏) for Œ∏ ‚àºuniform[ ‚àíœÄ, œÄ]. find
cx(t1, t2).
62810.2. mean and correlation functions
solution .
cx(t1, t2) =rx(t1, t2)‚àí¬µx(t1)¬µx(t2)
=1
2cos
œâ(t1‚àít2)
‚àí0¬∑0
=1
2cos
œâ(t1‚àít2)
.
in some problems we are interested in modeling the correlation between two random
processes x(t) and y(t). this gives us the cross-correlation and the cross-covariance func-
tions.
definition 10.4. thecross-correlation function ofx(t)andy(t)is
rx,y(t1, t2) =e[x(t1)y(t2)]. (10.9)
definition 10.5. thecross-covariance function ofx(t)andy(t)is
cx,y(t1, t2) =e[(x(t1)‚àí¬µx(t1)) (y(t2)‚àí¬µy(t2))]. (10.10)
remark . if¬µx(t1) =¬µy(t2) = 0, then cx,y(t1, t2) =rx,y(t1, t2) =e[x(t1)y(t2)].
10.2.3 independent processes
how do we establish independence for two random processes? we know that for two random
variables to be independent, the joint pdf can be written as a product of two pdfs:
fx,y(x, y) =fx(x)fy(y). (10.11)
if we extrapolate this idea to random processes, a natural formulation would be
fx(t),y(t)(x, y) =fx(t)(x)fy(t)(y). (10.12)
but this definition has a problem because x(t) and y(t) are functions. it is not enough to
just look at one time index, say t=t0. the way to think about this situation is to consider
a pair of random vectors xandy. when you say xandyare independent, you require
fx,y(x,y) =fx(x)fy(y). the pdf fx(x) itself is a joint distribution, i.e., fx(x) =
fx1,...,x n(x1, . . . , x n). therefore, for random processes, we need something similar.
definition 10.6. two random processes x(t)andy(t)areindependent if for any
t1, . . . , t n,
fx(t1),...,x (tn),y(t1),...,y (tn)(x1, . . . , x n, y1, . . . , y n)
=fx(t1),...,x (tn)(x1, . . . , x n)√ófy(t1),...,y (tn)(y1, . . . , y n).
629chapter 10. random processes
this definition is reminiscent of fx,y(x,y) =fx(x)fy(y). the requirement here is that
the factorization holds for anyn, including very small nand very large n, because x(t)
andy(t) are infinitely long.
independence means that the behavior of one process will not influence the behavior
of the other process. we define uncorrelated as follows.
definition 10.7. two random processes are x(t)andy(t)uncorrelated if
e[x(t1)y(t2)] =e[x(t1)]e[y(t2)], (10.13)
independence implies uncorrelation, as we can see from the following. if x(t) and y(t) are
independent, it follows that
e[x(t1)y(t2)] =z
x(t1, Œæ)y(t2, Œ∂)fx,y(Œæ, Œ∂)dŒæ dŒ∂
=z
x(t1, Œæ)y(t2, Œ∂)fx(Œæ)fy(Œ∂)dŒæ dŒ∂, independence
=z
x(t1, Œæ)fx(Œæ)dŒæz
y(t2, Œ∂)fy(Œ∂)dŒ∂=e[x(t1)]e[y(t2)].
if two random processes are uncorrelated, they are not necessarily independent.
independent x and y‚áí
‚áçuncorrelated x and y
example 10.13 . let y(t) =x(t) +n(t), where x(t) and n(t) are independent.
then
rx,y(t1, t2) =e[x(t1)y(t2)] =e[x(t1) (x(t2) +n(t2))]
=rx(t1, t2) +¬µx(t1)¬µn(t2).
10.3 wide-sense stationary processes
as we have seen in the previous sections, some random processes have a ‚Äúnice‚Äù autocor-
relation function, in the sense that the 2d function rx(t1, t2) has a toeplitz structure.
random processes with this property are known as wide-sense stationary (wss) processes.
wss processes belong to a very small subset in the entire universe of random processes,
but they are practically the most useful ones. before we discuss how to use them, we first
present a formal definition of a wss process.1
1many textbooks introduce strictly stationary processes before discussing a wide-sense stationary process.
we skip the former because, throughout our book, we only use wss processes. readers interested in strictly
stationary processes can consult the references listed at the end of this chapter.
63010.3. wide-sense stationary processes
10.3.1 definition of a wss process
definition 10.8. a random process x(t)iswide-sense stationary if:
1.¬µx(t) =constant ,for all t, and
2.rx(t1, t2) =rx(t1‚àít2)for all t1, t2.
there are two criteria that define a wss process. the first criterion is that the mean is a
constant. that is, the mean function does not change with time. the second criterion is that
the autocorrelation function only depends on the difference t1‚àít2and not on the absolute
starting point. for example, rx(0.1,1.1) needs to be the same as rx(6.3,7.3), because the
intervals are both 1.
how can these two criteria be mapped to the toeplitz structure we discussed in the
previous examples? figure 10.14 shows the autocorrelation function rx(t1, t2), which is a
2d function. we take three cross sections corresponding to t2=‚àí0.13,t2= 0 and t2= 0.3.
as you can see from the figure, each rx(t1, t2) is a shifted version of another one. to obtain
any value rx(t1, t2) on the function, there is no need to probe to the 2d map; you only
need to probe to the red curve and locate the position marked as t1‚àít2, and you will be
able to obtain the value rx(t1, t2).
-1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1
t1-1
-0.75
-0.5
-0.25
0
0.25
0.5
0.75
1t2
-1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1
t1-1-0.500.51
t2 = -0.13
t2 = 0
t2 = 0.3
figure 10.14: cross sections of the autocorrelation function rx(t1, t2) =1
2cos
œâ(t1‚àít2)
.
not all random processes have a toeplitz autocorrelation function. for example, the
random process x(t) =acos(2 œÄt) is not a wss process, because the autocorrelation func-
tion is
rx(t1, t2) =1
3cos(2 œÄt1) cos(2 œÄt2),
which cannot be written as the difference t1‚àít2.
remark 1 . wss processes can also be defined using the autocovariance function instead
of the autocorrelation function, because if a process is wss, then the mean function is a
constant. if the mean function is a constant, then cx(t1, t2) =rx(t1, t2)‚àí¬µ2. so any
geometric structure that rxpossesses will be translated to cx, as the constant ¬µ2will not
631chapter 10. random processes
influence the geometry. therefore, it is equally valid to say that a wss process has
cx(t1, t2) =cx(t1‚àít2).
remark 2 . because a wss is completely characterized by the difference t1‚àít2, there is
no need to keep track of the absolute indices t1andt2. we can rewrite the autocorrelation
function as
rx(œÑ) =e[x(t+œÑ)x(t)]. (10.14)
there is nothing new in this equation: it only says that instead of writing rx(t+œÑ, t), we
can write rx(œÑ) because the time index tplays no role in terms of rx. thus from now on,
for any wss processes we will write the autocorrelation function as rx(œÑ).
10.3.2 properties of rx(œÑ)
when x(t) is wss, rx(œÑ) has several important properties.
corollary 10.1. rx(0) = average power of x(t).
proof . since
rx(0) =e[x(t+ 0)x(t)] =e[x(t)2],
and since e[x(t)2] is the average power, rx(0) is the average power of x(t). ‚ñ°
corollary 10.2. rx(œÑ)is symmetric. that is, rx(œÑ) =rx(‚àíœÑ).
proof . note that rx(œÑ) =e[x(t+œÑ)x(t)]. by switching the order of multiplication in the
expectation, we have
e[x(t+œÑ)x(t)] =e[x(t)x(t+œÑ)] =rx(‚àíœÑ).
‚ñ°
corollary 10.3.
p(|x(t+œÑ)‚àíx(œÑ)|> œµ)‚â§2(rx(0)‚àírx(œÑ))
œµ2.
this result says that if rx(œÑ) is slowly decaying from rx(0), the probability of having a
large deviation |x(t+œÑ)‚àíx(œÑ)|is small.
proof .
p(|x(t+œÑ)‚àíx(œÑ)|> œµ)‚â§e[(x(t+œÑ)‚àíx(œÑ))2]/œµ2
=
e[x(t+œÑ)2]‚àí2e[x(t+œÑ)x(t)] +e[x(t)2]
/œµ2
=
2e[x(t)2]‚àí2e[x(t+œÑ)x(t)]
/œµ2
= 2
rx(0)‚àírx(œÑ)
/œµ2.
‚ñ°
63210.3. wide-sense stationary processes
corollary 10.4. |rx(œÑ)| ‚â§rx(0), for all œÑ.
proof . by cauchy‚Äôs inequality e[xy]2‚â§e[x2]e[y2], we can show that
rx(œÑ)2=e[x(t)x(t+œÑ)]2
‚â§e[x(t)2]e[x(t+œÑ)2]
=e[x(t)2]2=rx(0)2.
‚ñ°
10.3.3 physical interpretation of rx(œÑ)
how should we understand the autocorrelation function rx(œÑ) for wss processes? cer-
tainly, by definition, rx(œÑ) =e[x(t+œÑ)x(t)] means that we can analyze rx(œÑ) from the
statistical perspective. but in this section we want to take a slightly different approach by
answering the question from a computational perspective.
consider the following function:
brx(œÑ)def=1
2tzt
‚àítx(t+œÑ)x(t)dt. (10.15)
this function is the temporal average ofx(t+œÑ)x(t), as opposed to the statistical average.
why do we want to consider this temporal average? we first show the main result, that
e[brx(œÑ)] =rx(œÑ).
lemma 10.1. letbrx(œÑ)def=1
2trt
‚àítx(t+œÑ)x(t)dt. then
eh
brx(œÑ)i
=rx(œÑ). (10.16)
proof .
eh
brx(œÑ)i
=1
2tzt
‚àíte[x(t+œÑ)x(t)]dt
=1
2tzt
‚àítrx(œÑ)dt=rx(œÑ)1
2tzt
‚àítdt=rx(œÑ).
‚ñ°
this lemma implies that if the signal x(t) is long enough, we can approximate rx(œÑ)
bybrx(œÑ). the approximation is asymptotically consistent, in the sense that e[brx(œÑ)] =
rx(œÑ). now, the more interesting question is the interpretation of brx(œÑ). what is it?
how should we understand brx(œÑ)?
brx(œÑ) is the ‚Äúunflipped convolution‚Äù, or correlation , ofx(œÑ) and x(t+œÑ).
633chapter 10. random processes
correlation is analogous to convolution. for convolution , the definition is
y(œÑ) =zt
‚àítx(t‚àíœÑ)x(t)dt, (10.17)
whereas for correlation , the definition is
y(œÑ) =zt
‚àítx(t+œÑ)x(t)dt. (10.18)
clearly, brx(œÑ) is the latter. a graphical illustration of the difference between convolution
and correlation is provided in figure 10.15 . the only difference between the two is that the
correlation does not flip the function, whereas the convolution does flip the function.
-0.100.10.20.30.40.50.6
-10 -5 0 5 10
-0.100.10.20.30.40.50.6
-10 -5 0 5 10
(a) convolution (b) correlation
figure 10.15: the difference between convolution and correlation. in convolution, the function x(t)is
flipped before we compute the result. for correlation, the function is not flipped.
the temporal correlation is easy to visualize. starting with the function x(t+œÑ), if you
make œÑlarger or smaller, then effectively you are shifting x(t) left or right. the integrationrt
‚àítx(t+œÑ)x(t)dtcalculates the energy accumulated. if the integral is large, there is a
strong correlation between x(t) and x(t+œÑ). otherwise the correlation is small. here is
an extreme example:
example 10.14 . consider a random process x(t) such that for every t,x(t) is an
i.i.d. gaussian random variable with zero mean and unit variance. then
rx(œÑ) =e[x(t+œÑ)x(t)] =(
e[x2(t)], œÑ = 0,
e[x(t+œÑ)]e[x(t)], œÑ Ã∏= 0.
using the fact that x(t) is i.i.d. gaussian for all t, we can show that e[x2(t)] = 1 for
anyt, ande[x(t+œÑ)]e[x(t)] = 0. therefore, we have
rx(œÑ) =(
1, œÑ = 0,
0. œÑ Ã∏= 0.
63410.3. wide-sense stationary processes
the equation says that since the random process is i.i.d. gaussian, shifting and in-
tegrating will give maximum correlation at the origin. as soon as the shift is not at
the origin, the correlation is zero. this makes sense because the samples are just i.i.d.
gaussian. one pixel offset is enough to destroy any correlation.
now let‚Äôs calculate the temporal correlation. we know that
brx(œÑ) =1
2tzt
‚àítx(t+œÑ)x(t)dœÑ.
this equation says that we shift x(t) to the left and right and then integrate. if œÑ
is not zero, the product x(t+œÑ)x(t) will sometimes be positive and sometimes be
negative. after integrating the entire period, we cancel out most of the terms. let‚Äôs
plot the functions and see if all these steps make sense. in figure 10.16 (a), we show
two random realizations of the random process x(t). they are just i.i.d. gaussian
samples.
infigure 10.16 (b) we plot the temporal autocorrelation function brx(œÑ). since
brx(œÑ) itself is a random process, it has different realizations. we plot two random
realizations, which are computed based on shifting and integrating x(t). in the same
plot, we also show the statistical expectation rx(œÑ). as we can see from the plot,
the temporal correlation and the statistical correlation match reasonably well except
for the fluctuation in brx(œÑ), which is expected because it is computed from a finite
number of samples.
0 200 400 600 800 1000-4-3-2-101234
0 500 1000 1500 2000-0.200.20.40.60.811.2
correlation of sample 1
correlation of sample 2
auto-correlation function
(a)x(t) (b) brx(œÑ)
figure 10.16: (a) a random process x(t)with two different realizations. (b) as we calculate the
temporal correlation of each of the two realizations, we obtain a noisy function that is nearly an
impulse. if we take the average of many of these realizations, we obtain a pure delta function.
on a computer, the commands to do the autocorrelation function are xcorr in mat-
lab and np.correlate in python. below are the codes used to generate figure 10.16 .
% matlab code to demonstrate autocorrelation
n = 1000; % number of sample paths
t = 1000; % number of time stamps
x = 1*randn(n,t);
xc = zeros(n,2*t-1);
for i=1:n
635chapter 10. random processes
xc(i,:) = xcorr(x(i,:))/t;
end
plot(xc(1,:),‚Äôb:‚Äô, ‚Äôlinewidth‚Äô, 2); hold on;
plot(xc(2,:),‚Äôk:‚Äô, ‚Äôlinewidth‚Äô, 2);
# python code to demonstrate autocorrelation
n = 1000
t = 1000
x = np.random.randn(n,t)
xc= np.zeros((n,2*t-1))
for i in range(n):
xc[i,:] = np.correlate(x[i,:],x[i,:],mode=‚Äôfull‚Äô)/t
plt.plot(xc[0,:],‚Äôb:‚Äô)
plt.plot(xc[1,:],‚Äôk:‚Äô)
plt.show()
under what conditions will brx(œÑ)‚Üírx(œÑ) ast‚Üí ‚àû ? the answer to this question
is provided by an important theorem called mean-square ergodic theorem , which can be
thought of as the random process version of the weak law of large numbers. we leave the
discussion of the mean ergodic theorem to the appendix.
everything you need to know about a wss process
¬àthe mean of a wss process is a constant (does not need to be zero)
¬àthe correlation function only depends on the difference, so rx(t1, t2) is toeplitz.
¬àyou can write rx(t1, t2) asrx(œÑ), where œÑ=t1‚àít2.
¬àrx(œÑ) tells you how much correlation you have with someone located at a time
instant œÑfrom you.
10.4 power spectral density
beginning with this section we are going to focus on wss processes. by wss, we mean that
the autocorrelation function rx(t1, t2) has a toeplitz structure. putting it in other words,
we assume rx(t1, t2) can be simplified to rx(œÑ), where œÑ=t1‚àít2. we call this property
time invariance .
10.4.1 basic concepts
assuming that rx(œÑ) is square integrable, i.e.,r‚àû
‚àí‚àûrx(œÑ)2dœÑ <‚àû, we can now define the
fourier transform of rx(œÑ) which is called the power spectral density .
63610.4. power spectral density
theorem 10.2 (einstein-wiener-khinchin theorem ).the power spectral density
sx(œâ)of a wss process is
sx(œâ) =z‚àû
‚àí‚àûrx(œÑ)e‚àíjœâœÑdœÑ=f(rx(œÑ)),
assuming thatr‚àû
‚àí‚àûrx(œÑ)2dœÑ <‚àûso that the fourier transform of rx(œÑ)exists.
practice exercise 10.4 . let rx(œÑ) =e‚àí2Œ±|œÑ|. find sx(œâ).
solution . using the fourier transform table,
sx(œâ) =f {rx(œÑ)}=4Œ±
4Œ±2+œâ2.
figure 10.17 shows the autocorrelation function and the power spectral density.
-2 -1 0 1 200.20.40.60.81
rx()
-10 -5 0 5 1000.20.40.60.81
sx()
figure 10.17: example for rx(œÑ) =e‚àí2Œ±|œÑ|, with Œ±= 1.
why is theorem 10.2 a theorem rather than a definition ? this is because power spectral
density has its definition. there is no way that you can get any ‚Äúpower‚Äù information merely
by looking at the fourier transform of rx(œÑ). we will discuss the origin of the power spectral
density later, but for now, we only need to know that sx(œâ) is the fourier transform of
rx(œÑ).
remark . the power spectral density is defined for wss processes. if the process is not
wss, then rxwill be a 2d function instead of a 1d function of œÑ, so we cannot take the
fourier transform in œÑ. we will discuss this in detail shortly.
practice exercise 10.5 . let x(t) =acos(œâ0t+Œ∏),Œ∏‚àºuniform[0 ,2œÄ]. find sx(œâ).
solution . we know that the autocorrelation function is
rx(œÑ) =a2
2cos(œâ0œÑ)
=a2
2ejœâ0œÑ+e‚àíjœâ0œÑ
2
.
637chapter 10. random processes
by taking the fourier transform of both sides, we have
sx(œâ) =a2
22œÄŒ¥(œâ‚àíœâ0) + 2œÄŒ¥(œâ+œâ0)
2
=œÄa2
2[Œ¥(œâ‚àíœâ0) +Œ¥(œâ+œâ0)].
the result is shown in figure 10.18 .
-2 -1 0 1 2-0.500.5
rx()
00.511.52
-10 -5 0 5 10sx()
figure 10.18: example for rx(œÑ) =a2
2cos(œâ0œÑ), with a= 1andœâ0= 2œÄ.
practice exercise 10.6 . let sx(œâ) =n0
2rect(œâ
2w). find rx(œÑ).
solution . since sx(œâ) =f(rx(œÑ)), the inverse holds:
rx(œÑ) =n0
2w
œÄsinc(wœÑ).
this example shows what we call the bandlimited white noise . the power spectral
density sx(œâ) is uniform, meaning that it covers all frequencies (or wavelengths in
optics). it is called ‚Äúwhite noise‚Äù because white light is essentially a mixture of all
wavelengths.
the bandwidth of the power spectral density wdefines the zero crossings of
rx(œÑ). it is easy to show that when w‚Üí ‚àû ,rx(œÑ) converges to a delta function.
this happens when x(t) is i.i.d. gaussian. therefore, the pure gaussian noise random
process is also known as the white noise process . reshaping the i.i.d. gaussian noise
to an arbitrary power spectral density can be done by passing it through a linear filter,
as we will explain later.
-2 -1 0 1 2-0.500.511.52
rx()
-10 -5 0 5 10-0.500.511.5
sx()
figure 10.19: example for sx(œâ) =n0
2rect(œâ
2w), with n0= 2andw= 5.
63810.4. power spectral density
finding sx(œâ) from rx(œÑ) is straightforward, at least in principle. the more inter-
esting questions to ask are: (1) why do we need to learn about power spectral density? (2)
why do we need wss to define power spectral density?
how is power spectral density useful?
¬àpower spectral densities are useful when we pass a random process through some
linear operations, e.g., convolution, running average, or running difference.
¬àpower spectral densities are the fourier transforms of the autocorrelation func-
tions. fourier transforms are useful for speeding up computation and drawing
random samples from a given power spectral density.
a random process itself is not interesting until we process it; there are many ways to do
this. the most basic operation is to send the random process through a linear time-invariant
system, e.g., a convolution. convolution is equivalent to filtering the random process. for
example, if the input process contains noise, we can design a linear time-invariant filter to
denoise the random process. the power spectral density, which is the fourier transform
of the autocorrelation function, makes the filtering easier because everything can be done
in the spectral (fourier) domain. moreover, we can analyze the performance and quantify
the limit using standard results in fourier analysis. for some specialized problems such as
imaging through atmospheric turbulence, the distortions happen in the phase domain. this
can be simulated by drawing samples from the power spectral density, e.g., the kolmogorov
spectrum or the von k¬¥ arm¬¥ an spectrum. power spectral densities have many important
engineering applications.
why does the power spectral density require wide-sense stationarity?
¬àif a process is wss, then rxwill have a toeplitz structure.
¬àa toeplitz matrix is important. if you do eigendecomposition to a toeplitz ma-
trix, the eigenvectors are the fourier bases.
¬àso if rxis toeplitz, then you can diagonalize it using the fourier transform.
¬àtherefore, the power spectral density can be defined.
why does power spectral density require wss? this has to do with the toeplitz
structure of the autocorrelation function. to make our discussion easier let us discretize
the autocorrelation function rx(t1, t2) by considering rx[m, n]. (you can do a mental
calculation by converting t1to integer indices m, and t2ton. see any textbook on signals
and systems if you need help. this is called the ‚Äúdiscrete time signal‚Äù.) following the range
oft1andt2,rx[m, n] can be expressed as:
r=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞rx[0] rx[1] ¬∑¬∑¬∑rx[n‚àí1]
rx[1] rx[0] ¬∑¬∑¬∑rx[n‚àí2]
............
rx[n‚àí1]rx[n‚àí1]¬∑¬∑¬∑ rx[0]Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
where we used the fact that rx[m, n] =rx[m‚àín] for wss processes and rx[k] =rx[‚àík].
we call the resulting matrix rtheautocorrelation matrix , which is a discretized version
639chapter 10. random processes
of the autocorrelation function rx(t1, t2). looking at r, we again observe the toeplitz
structure. for example, figure 10.20 shows one toeplitz structure and one non-toeplitz
structure.
figure 10.20: we show two autocorrelation functions rx[m, n]on the left-hand side. the first autocor-
relation function comes from a wss process that has a toeplitz structure. the second autocorrelation
function does not have toeplitz structure. for the toeplitz matrix, we can diagonalize it using the
fourier transform. the eigenvalues are the power spectral density.
any toeplitz matrix rcan be diagonalized using the fourier transforms. that is, we
can write ras
r=fhŒªf,
where fis the (discrete) fourier transform matrix andŒªis a diagonal matrix. this can be
understood as the eigendecomposition of r. the important point here is that only toeplitz
matrices can be eigendecomposed using the fourier transforms; an arbitrary symmetric
matrix cannot. figure 10.20 illustrates this point. if your matrix is toeplitz, you can diago-
nalize it, and hence you can define the power spectral density, just as in the first example. if
your matrix is not toeplitz, then the power spectral density is undefined. to get the toeplitz
matrix, you must start with a wss process.
before moving on, we define cross power spectral densities , which will be useful in
some applications.
definition 10.9. thecross power spectral density between two random processes
x(t)andy(t)is
sx,y(œâ) =f(rx,y(œÑ))where rx,y(œÑ) =e[x(t+œÑ)y(t)],
sy,x(œâ) =f(ry,x(œÑ))where ry,x(œÑ) =e[y(t+œÑ)x(t)].(10.19)
remark . in general, sx,y(œâ)Ã∏=sy,x(œâ). rather, since rx,y(œÑ) =ry,x(‚àíœÑ), we have
sx,y(œâ) =sy,x(œâ).
10.4.2 origin of the power spectral density
to understand the power spectral density, it is crucial to understand where it comes from
and why it is the fourier transform of the autocorrelation function.
64010.4. power spectral density
we begin by assuming that x(t) is a wss random process with mean ¬µxand auto-
correlation rx(œÑ). we now consider the notion of power . consider a random process x(t).
the power within a period [ ‚àít, t] is
bpx=1
2tzt
‚àít|x(t)|2dt.
bpxdefines the power because the integration alone is the energy, and the normalization by
1/2tgives us the power. however, there are two problems. first, since x(t) is random, the
power bpxis also random. is there a way we can eliminate the randomness? second, tis
a finite period of time. it does not capture the entire process, and so we do not know the
power of the entire process.
a natural solution to these two problems is to consider
pxdef=e"
lim
t‚Üí‚àû1
2tzt
‚àít|x(t)|2dt#
. (10.20)
here, we take the limit of tto infinity so that we can compute the power of the entire
process. we also take the expectation to eliminate the randomness. therefore, pxcan be
regarded as the average power of the complete random process x(t).
next, we need one definition and one lemma. the definition defines sx(œâ), and the
lemma will link sx(œâ) with the power px.
definition 10.10. thepower spectral density (psd) of a wss process is defined as
sx(œâ) = lim
t‚Üí‚àûeh
|ext(œâ)|2i
2t, (10.21)
where
ext(œâ) =zt
‚àítx(t)e‚àíjœâtdt (10.22)
is the fourier transform of x(t)limited to [‚àít, t].
this definition is abstract, but in a nutshell, it simply considers everything in the fourier
domain. the ratio |ext(œâ)|2/2tis the power, but in the frequency domain. the reason is
that if x(t) is fourier transformable, then parseval‚Äôs theorem will hold. parseval‚Äôs theorem
states that energy in the original space is conserved in the fourier space. since the ratio
|ext(œâ)|2/2tis the energy divided by time, it is the power. however, this is still not enough
to help us understand power spectral density: we need a lemma.
lemma 10.2. define
pxdef=e"
lim
t‚Üí‚àû1
2tzt
‚àít|x(t)|2dt#
.
641chapter 10. random processes
then
px=1
2œÄz‚àû
‚àí‚àûsx(œâ)dœâ. (10.23)
the lemma has to be read together with the previous definition. if we can prove the lemma,
we know that by integrating sx(œâ) we will obtain the power. therefore, sx(œâ) can be
viewed as a density function , specifically the density function of the power. sx(œâ) is called
the power spectral density because everything is defined in the fourier domain. putting this
all together gives us ‚Äúpower spectral density‚Äù.
proof . first, we recall that pxis the expectation of the average power of x(t). let
xt(t) =x(t)‚àít‚â§t‚â§t,
0 otherwise .
it follows that integrating over ‚àí‚àûto‚àûis equivalent to
z‚àû
‚àí‚àû|xt(t)|2dt=zt
‚àít|x(t)|2dt.
by parseval‚Äôs theorem, energy is conserved in both the time and the frequency domain:
z‚àû
‚àí‚àû|xt(t)|2dt=1
2œÄz‚àû
‚àí‚àû|ext(œâ)|2dœâ.
therefore, pxsatisfies
px=e"
lim
t‚Üí‚àû1
2tzt
‚àít|x(t)|2dt#
=e
lim
t‚Üí‚àû1
2œÄ1
2tz‚àû
‚àí‚àû|ext(œâ)|2dœâ
=1
2œÄz‚àû
‚àí‚àûlim
t‚Üí‚àû1
2teh
|ext(œâ)|2i
| {z }
def=sx(œâ)dœâ.
‚ñ°
the power spectral densities are functions whose integrations give us the power. if we
want to determine the power of a random process, the einstein-wiener-khinchin theorem
(theorem 10.2) says that sx(œâ) is just the fourier transform of rx(œÑ):
sx(œâ) =z‚àû
‚àí‚àûrx(œÑ)e‚àíjœâœÑdœÑ=f(rx(œÑ)).
the proof of the einstein-wiener-khinchin theorem is quite intricate, so we defer
the proof to the appendix. the significance of the theorem is that it turns an abstract
quantity, the power spectral density, into a very easily computable quantity, namely the
fourier transform of the autocorrelation function. for now, we will happily use this theorem
because it saves us a great deal of trouble when we want to determine the power spectral
density from the first principles.
64210.5. wss process through lti systems
10.5 wss process through lti systems
random processes have limited usefulness until we can apply operations to them. in this
section we discuss how wss processes respond to a linear time-invariant (lti) system. this
technique is most useful in signal processing, communication, speech analysis, and imaging.
we will be brief here since you can find most of this information in any standard textbook
on signals and systems.
10.5.1 review of linear time-invariant systems
when we say a ‚Äúsystem‚Äù, we mean that there exists an input-output relationship as shown
infigure 10.21 .
figure 10.21: a system can be viewed as a black box that takes an input x(t)and turns it into an
output y(t).
linear time-invariant (lti) systems are the simplest systems we use in engineering
problems. an lti system has two properties.
¬àlinearity . linearity means that when two input random processes are added and
scaled , the output random processes will also be added and scaled in exactly the
same way. mathematically, linearity says that if x1(t)‚Üíy1(t) and x2(t)‚Üí
y2(t), then
ax1(t) +bx2(t)‚Üíay1(t) +by2(t).
¬àtime-invariant : time invariance means that if we shift the input random process
by a certain time period, the output will be shifted in the same way. mathemat-
ically, time invariance means that if x(t)‚Üíy(t), then
x(t+œÑ)‚Üíy(t+œÑ).
if a system is linear time-invariant, the input-to-output relation is given by convolution :
theconvolution between two functions x(t) and h(t) is defined as
y(t) =h(t)‚àóx(t) =z‚àû
‚àí‚àûh(œÑ)x(t‚àíœÑ)dœÑ,
643chapter 10. random processes
in which we call h(t) the system response or impulse response .
the function h(t) is called the impulse response because if x(t) =Œ¥(t), then according to
the convolution equation we have
y(t) =z‚àû
‚àí‚àûh(œÑ)Œ¥(t‚àíœÑ)dœÑ=h(t).
therefore, if we send an impulse to the system, the output will be h(t).
convolution is commutative, meaning that h(t)‚àóx(t) =x(t)‚àóh(t). written as inte-
grations, we have
z‚àû
‚àí‚àûh(œÑ)x(t‚àíœÑ)dœÑ=z‚àû
‚àí‚àûh(t‚àíœÑ)x(œÑ)dœÑ. (10.24)
for lti systems, y(t) can be determined through the fourier transforms.
thefourier transform of a (squared-integrable) function x(t) is
x(œâ) =f{x(t)}=z‚àû
‚àí‚àûx(œÑ)e‚àíjœâœÑdœÑ. (10.25)
a basic property of convolution is that convolution in the time domain is equivalent to
multiplication in the fourier domain . therefore
y(œâ) =h(œâ)x(œâ), (10.26)
where h(œâ) =f{h(t)}is the fourier transform of h(t), and y(œâ) =f(y(t)) is the fourier
transform of y(t).
in the rest of this section we study the pair of input and output random processes that
are defined as follows
¬àx(t) = input. it is a wss random process.
¬ày(t) = output. it is constructed by sending x(t) through an lti system with
impulse response h(t). therefore, y(t) =h(t)‚àóx(t).
10.5.2 mean and autocorrelation through lti systems
since x(t) is wss, the mean function of x(t) stays constant, i.e., ¬µx(t) =¬µx. the following
theorem gives the mean function of the output.
theorem 10.3. ifx(t)passes through an lti system to yield y(t), the mean function
ofy(t)is
e[y(t)] =¬µxz‚àû
‚àí‚àûh(œÑ)dœÑ. (10.27)
64410.5. wss process through lti systems
proof . suppose that y(t) =h(t)‚àóx(t). then,
¬µy(t) =e[y(t)] =ez‚àû
‚àí‚àûh(œÑ)x(t‚àíœÑ)dœÑ
=z‚àû
‚àí‚àûh(œÑ)e[x(t‚àíœÑ)]dœÑ
=z‚àû
‚àí‚àûh(œÑ)¬µxdœÑ=¬µxz‚àû
‚àí‚àûh(œÑ)dœÑ,
where the second to last equality is valid because x(t) is wss, so that e[x(t‚àíœÑ)] =¬µx.
‚ñ°
the theorem suggests that if the input x(t) has a constant mean, the output y(t)
should also have a constant mean. this should not be a surprise because if the system is
linear, a constant input will give a constant output.
example 10.15 . consider a wss random process x(t) such that each sample is an
i.i.d. gaussian random variable with zero mean and unit variance. we send this process
through an lti system with impulse response h(t), where
h(t) =(
10(1‚àí |t|),‚àí1‚â§t‚â§1,
0, otherwise .
the mean function of x(t) is¬µx(t) = 0, and that of y(t) is¬µy(t) = 0. figure 10.22
illustrates a numerical example, in which we see that the random processes x(t) and
y(t) have different shapes but the mean functions remain constant.
-10 -5 0 5 10-4-2024
x(t)
x(t)
y(t)
y(t)
-2 -1 0 1 2-0.0500.050.10.150.2
rx(t)
ry(t)
(a)¬µx(t) and ¬µy(t) (b) rx(t) and ry(t)
figure 10.22: when sending a wss random process through an lti system, the mean and the
autocorrelation functions are changed.
next, we derive the autocorrelation function of a random process when sent through
an lti system.
theorem 10.4. ifx(t)passes through an lti system to yield y(t), the autocorre-
645chapter 10. random processes
lation function ofy(t)is
ry(œÑ) =z‚àû
‚àí‚àûz‚àû
‚àí‚àûh(s)h(r)rx(œÑ+s‚àír)ds dr. (10.28)
proof . we start with the definition of y(t):
ry(œÑ) =e[y(t)y(t+œÑ)]
=ez‚àû
‚àí‚àûh(s)x(t‚àís)dsz‚àû
‚àí‚àûh(r)x(t+œÑ‚àír)dr
(a)=z‚àû
‚àí‚àûz‚àû
‚àí‚àûh(s)h(r)e[x(t‚àís)x(t+œÑ‚àír)ds dr ]
=z‚àû
‚àí‚àûz‚àû
‚àí‚àûh(s)h(r)rx(œÑ+s‚àír)ds dr,
where in (a) we assume that integration and expectation are interchangeable.
‚ñ°
a shorthand notation of the above formula is ry(t) = [h‚äõ(h‚àórx)](t), where ‚àódenotes
the convolution and ‚äõdenotes the correlation. figure 10.22 (b) shows the autocorrelation
functions rxandry. in this example rxis a delta function because for i.i.d. gaussian
noise the power spectral density is a constant. after convolving with the system response,
the autocorrelation ryhas a different shape.
10.5.3 power spectral density through lti systems
denoting the fourier transform of the impulse response by h(œâ) =f(h(t)), we derive the
power spectral density of the output.
theorem 10.5. ifx(t)passes through an lti system to yield y(t), the power spec-
tral density ofy(t)is
sy(œâ) =|h(œâ)|2sx(œâ). (10.29)
proof . by definition, the power spectral density sy(œâ) is the fourier transform of the
autocorrelation function ry(œâ). therefore,
sy(œâ) =z‚àû
‚àí‚àûry(œÑ)e‚àíjœâœÑdœÑ
=z‚àû
‚àí‚àûz‚àû
‚àí‚àûz‚àû
‚àí‚àûh(s)h(r)rx(œÑ+s‚àír)ds dre‚àíjœâœÑdœÑ.
letting u=œÑ+s‚àír, we have
sy(œâ) =z‚àû
‚àí‚àûz‚àû
‚àí‚àûz‚àû
‚àí‚àûh(s)h(r)rx(u)e‚àíjœâ(u‚àís+r)ds dr du
=z‚àû
‚àí‚àûh(s)ejœâsdsz‚àû
‚àí‚àûh(r)e‚àíjœârdrz‚àû
‚àí‚àûrx(u)e‚àíjœâudu
=h(œâ)h(œâ)sx(œâ),
64610.5. wss process through lti systems
where h(œâ) is the complex conjugate of h(œâ).
‚ñ°
it is tempting to think that since y(t) =h(t)‚àóx(t), the power spectral density should
also be sy(œâ) =h(œâ)x(œâ), but this is not true. the above result shows that we need an
additional complex conjugate h(œâ) because sy(œâ) is the power , which means the square
of the signal. note that rxis ‚Äúsquared‚Äù because we have convolved it with itself, and ry
is also squared. therefore, to match rxandry, the impulse response halso needs to be
squared in the fourier domain.
example 10.16 . a wss process x(t) has a correlation function
rx(œÑ) = sinc( œÄœÑ).
suppose that x(t) passes through an lti system with input/output relationship
2d2
dt2y(t) + 2d
dty(t) + 4y(t) = 3d2
dt2x(t)‚àí3d
dtx(t) + 6x(t).
find ry(œÑ).
solution : the sinc function has a fourier transform given by
sinc(wt)‚Üê‚Üí
fœÄ
wrectœâ
2w
.
therefore, the autocorrelation function is
rx(œÑ) = sinc( œÄœÑ)‚Üê‚Üí
fœÄ
œÄrectœâ
2œÄ
.
by taking the fourier transform on both sides, we have
sx(œâ) =(
1,‚àíœÄ‚â§œâ‚â§œÄ,
0, elsewhere .
the system response is found from the differential equation:
h(œâ) =3(jœâ)2‚àí3(jœâ) + 6
2(jœâ)2+ 2(jœâ) + 4
=3
(2‚àíœâ2)‚àíjœâ
2 [(2‚àíœâ2) +jœâ].
taking the magnitude square yields
|h(œâ)|2=3
(2‚àíœâ2)‚àíjœâ
2 [(2‚àíœâ2) +jœâ]3
(2‚àíœâ2) +jœâ
2 [(2‚àíœâ2)‚àíjœâ]
=9
4(2‚àíœâ2)2+œâ2
(2‚àíœâ2)2+œâ2=9
4.
647chapter 10. random processes
therefore, the output power spectral density is
sy(œâ) =|h(œâ)|2sx(œâ) =9
4sx(œâ).
taking the inverse fourier transform, we have
ry(œÑ) =9
4sinc(œÄœÑ).
example 10.17 . a random process x(t) has zero mean and rx(t, s) = min( t, s).
consider a new process y(t) =etx(e‚àí2t).
1. is y(t) wss?
2. suppose y(t) passes through a lti system to yield an output z(t) according to
d
dtz(t) + 2z(t) =d
dty(t) +y(t).
find rz(œÑ).
solution :
1. in order to verify whether y(t) is wss, we need to check the mean function and
the autocorrelation function. the mean function is
e[y(t)] =e
etx(e‚àí2t)
=ete
x(e‚àí2t)
.
since x(t) has zero mean, e[x(t)] = 0 for all t. this implies that if u=e‚àí2t,
thene[x(u)] = 0 because uis just another time instant. thus e[x(e‚àí2t)] = 0,
and hence e[y(t)] = 0.
the autocorrelation is
e[y(t+œÑ)y(t)] =eh
et+œÑx(e‚àí2(t+œÑ))etx(e‚àí2t)i
=e2t+œÑeh
x(e‚àí2(t+œÑ))x(e‚àí2t)i
=e2t+œÑrx(e‚àí2(t+œÑ), e‚àí2t).
substituting rx(t, s) = min( t, s), we have that
e2t+œÑrx(e‚àí2(t+œÑ), e‚àí2t) =e2t+œÑmin(e‚àí2(t+œÑ), e‚àí2t)
=e2t+œÑ
e‚àí2(t+œÑ), œÑ‚â•0
e‚àí2t, œÑ < 0
=e‚àíœÑ, œÑ‚â•0
eœÑ, œÑ < 0
=e‚àí|œÑ|.
64810.5. wss process through lti systems
sory(œÑ) =e‚àí|œÑ|. since ry(œÑ) is a function of œÑ,y(t) is wss.
2. the system response is given by
h(œâ) =1 +jœâ
2 +jœâ.
the magnitude is therefore
|h(œâ)|2=1 +œâ2
4 +œâ2.
hence, the output autocorrelation function is
ry(œÑ) =e‚àí|œÑ|‚Üê‚Üísy(œâ) =2
1 +œâ2,
and
sz(œâ) =|h(œâ)|2sy(œâ)
=1 +œâ2
4 +œâ22
1 +œâ2=2
4 +œâ2.
therefore
rz(œÑ) =1
2e‚àí2|œÑ|.
10.5.4 cross-correlation through lti systems
the above analyses are developed for the autocorrelation function. if we consider the cross-
correlation between two random processes, say x(t) and y(t), then the above results do not
hold. in this section, we discuss the cross-correlation through lti systems.
to begin with, we need to define wss for a pair of random processes.
definition 10.11. two random processes x(t)andy(t)arejointly wss if
1.x(t)is wss and y(t)is wss, and
2.rx,y(t1, t2) =e[x(t1)y(t2)]is a function of t1‚àít2.
ifx(t) and y(t) are jointly wss, we write
rx,y(t1, t2) =rx,y(œÑ)def=e[x(t+œÑ)y(œÑ)].
the definition of ‚Äújointly wss‚Äù is necessary here because rx,yis defined by xandy. just
knowing that x(t) and y(t) are wss does not allow one to say that rx,y(t1, t2) can be
written as the time difference.
if we flip the order of xandyto consider ry,x(œÑ) and not rx,y(œÑ), then we need
to flip the argument. the following lemma explains why.
649chapter 10. random processes
lemma 10.3. for any random processes x(t)andy(t), thecross-correlation rx,y(œÑ)
is related to ry,x(œÑ)as
rx,y(œÑ) =ry,x(‚àíœÑ). (10.30)
proof . recall the definition of ry,x(‚àíœÑ) =e[y(t‚àíœÑ)x(t)]. this can be simplified as
follows:
ry,x(‚àíœÑ) =e[y(t‚àíœÑ)x(t)]
=e[x(t)y(t‚àíœÑ)]
=e[x(t‚Ä≤+œÑ)y(t‚Ä≤)]
=rx,y(œÑ),
where we substituted t‚Ä≤=t‚àíœÑ.
‚ñ°
example 10.18 . let x(t) and n(t) be two independent wss random processes with
expectations e[x(t)] =¬µxande[n(t)] = 0, respectively. let y(t) =x(t) +n(t). we
want to show that x(t) and y(t) are jointly wss, and we want to find rx,y(œÑ).
solution . before we show the joint wss property of x(t) and y(t), we first show
thaty(t) is wss:
e[y(t)] =e[x(t) +n(t)] =¬µx.
ry(t1, t2) =e[(x(t1) +n(t1))(x(t2) +n(t2))]
=e[(x(t1)x(t2)] +e[(n(t1)n(t2)]
=rx(t1‚àít2) +rn(t1‚àít2).
thus, y(t) is wss.
to show that x(t) and y(t) are jointly wss, we need to check the cross-
correlation function:
rx,y(t1, t2) =e[x(t1)y(t2)]
=e[x(t1)(x(t2) +n(t2))]
=e[x(t1)(x(t2)] +e[x(t1)n(t2)]
=rx(t1, t2) +e[x(t1)]e[n(t2)]
=rx(t1, t2).
since rx,y(t1, t2) is a function of t1‚àít2, and since x(t) and y(t) are wss, x(t) and
y(t) must be jointly wss.
finally, to find rx,y(œÑ), we substitute œÑ=t1‚àít2and obtain rx,y(œÑ) =rx(œÑ).
knowing the definition of jointly wss, we consider the cross-correlation between x(t)
andy(t). note that here we are asking about the cross-correlation between the input and
the output of the same lti system, as illustrated in figure 10.23 . the pair x(t) and
y(t) =h(t)‚àóx(t) are special because y(t) is the convolved version of x(t).
65010.5. wss process through lti systems
figure 10.23: the source of the signals when defining rx(œÑ),rx,y(œÑ),ry,x(œÑ)andry(œÑ).
theorem 10.6. letx(t)andy(t)be jointly wss processes, and let y(t) =h(t)‚àó
x(t). then the cross-correlation ry,x(œÑ)is
ry,x(œÑ) =h(œÑ)‚àórx(œÑ). (10.31)
proof . recalling the definition of cross-correlation, we have
ry,x(œÑ) =e[y(t+œÑ)x(t)]
=e
x(t)z‚àû
‚àí‚àûx(t+œÑ‚àír)h(r)dr
=z‚àû
‚àí‚àûe[x(t)x(t+œÑ‚àír)]h(r)dr=z‚àû
‚àí‚àûrx(œÑ‚àír)h(r)dr,
which is the convolution ry,x(œÑ) =h(œÑ)‚àórx(œÑ).
‚ñ°
we next define the cross power spectral density of two jointly wss processes as the
fourier transform of the cross-correlation function.
definition 10.12. thecross power spectral density of two jointly wss processes
x(t)andy(t)is defined as
sx,y(œâ) =f[rx,y(œÑ)],
sy,x(œâ) =f[ry,x(œÑ)].
the relationship between sx,yandsy,xcan be seen from the following theorem.
theorem 10.7. for two jointly wss random processes x(t)andy(t), the cross
power spectral density satisfies the property that
sx,y(œâ) =sy,x(œâ), (10.32)
where (¬∑)denotes the complex conjugate.
651chapter 10. random processes
proof . since sx,y(œâ) =f[rx,y(œÑ)] by definition, it follows that
f[rx,y(œÑ)] =z‚àû
‚àí‚àûrx,y(œÑ)e‚àíjœâœÑdœÑ
=z‚àû
‚àí‚àûry,x(‚àíœÑ)e‚àíjœâœÑdœÑ=z‚àû
‚àí‚àûrx,y(œÑ‚Ä≤)ejœâœÑ‚Ä≤dœÑ‚Ä≤,
which is exactly the conjugate sy,x(œâ).
‚ñ°
when sending the random process through an lti system, the cross-correlation power
spectral density is given by the theorem below.
theorem 10.8. ifx(t)passes through an lti system to yield y(t), then the cross
power spectral density is
sy,x(œâ) =h(œâ)sx(œâ),
sx,y(œâ) =h(œâ)sx(œâ).
proof . by taking the fourier transform on ry,x(œÑ) we have that sy,x(œâ) =h(œâ)sx(œâ).
since rx,y(œÑ) =ry,x(‚àíœÑ), it holds that sx,y(œâ) =h(œâ)sx(œâ).
‚ñ°
example 10.19 . let x(t) be a wss random process with
rx(œÑ) =e‚àíœÑ2/2, h (œâ) =e‚àíœâ2/2.
find sx,y(œâ),rx,y(œÑ),sy(œâ) and ry(œÑ).
solution . first, by the fourier transform table we know that
sx(œâ) =‚àö
2œÄe‚àíœâ2/2.
since h(œâ) =e‚àíœâ2/2, we have
sx,y(œâ) =h(œâ)sx(œâ)
=‚àö
2œÄe‚àíœâ2.
the cross-correlation function is
rx,y(œâ) =f‚àí1h‚àö
2œÄe‚àíœâ2i
=1‚àö
2e‚àíœÑ2
4.
65210.6. optimal linear filter
the power spectral density of y(t) is
sy(œâ) =|h(œâ)|2sx(œâ)
=‚àö
2œÄe‚àí3œâ2
2.
therefore, the autocorrelation function of y(t) is
ry(œÑ) =f‚àí1h‚àö
2œÄe‚àí3œâ2
2i
=1‚àö
3e‚àíœÑ2/6.
10.6 optimal linear filter
in the previous sections, we have built many tools to analyze random processes. our next
goal is to apply these techniques. to that end, we will discuss optimal linear filter design ,
which is a set of estimation techniques for predicting and recovering information from a time
series.
10.6.1 discrete-time random processes
we begin by introducing some notations. in the previous sections, we have been using
continuous-time random processes to study statistics. in this section, we mainly focus on
discrete-time random processes. the shift from continuous-time to discrete-time is straight-
forward as far as the theories are concerned ‚Äî we switch the continuous-time index tto
a discrete-time index n. however, shifting to discrete-time random processes can simplify
many difficult problems because many discrete-time problems can be solved by matrices and
vectors. this will make the computations and implementations much easier. to make this
transition easier, we provide a few definitions and results without proof.
notations for discrete-time random processes
¬àwe denote the discrete-time indices by mandn, corresponding to the continuous-
time indices t1andt2, respectively.
¬àa discrete-time random process is denoted by x[n].
¬àits mean function and the autocorrelation function are
¬µx[n] =e[x[n]],
rx[m, n] =e[x[m]x[n]].
¬àwe say that x[n] is wss if ¬µx[n] = constant, and rx[m, n] is a function of
m‚àín.
653chapter 10. random processes
¬àifx[n] is wss, we write rx[m, n] as
rx[m, n] =rx[m‚àín] =rx[k],
where k=m‚àínis the interval.
¬àifx[n] is wss, we define the power spectral density as
sx(ejœâ) =f{rx[k]},
where sx(ejœâ) denotes the discrete-time fourier transform.
when a random process x[n] is sent through an lti system with an impulse response
h[n], the output is
y[n] =h[n]‚àóx[n] =‚àûx
k=‚àí‚àûh[k]x[n‚àík]. (10.33)
when a wss process x[n] passes through an lti system h[n] to yield an output y[n],
the auto- and cross-correlation function and power spectral densities are
¬àry[k] =e[y[n+k]y[n]],sy(ejœâ) =f{ry[k]}=|h(ejœâ)|2sx(ejœâ).
¬àrxy[k] =e[x[n+k]y[n]],sxy(ejœâ) =f{rxy[k]}=h(ejœâ)sx(ejœâ).
¬àry x[k] =e[y[n+k]x[n]],sy x(ejœâ) =f{ry x[k]}=h(ejœâ)sx(ejœâ).
10.6.2 problem formulation
the problem we study here is known as the optimal linear filter design . suppose that there
is a wss process x[n] that we want to process. for example, if x[n] is a corrupted version
of some clean time-series, we may want to remove the noise by filtering (also known as
averaging) x[n]. conceptualizing the denoising process as a linear time-invariant system
with an impulse response h[n], our goal is to determine the optimal h[n] such that the
estimated time series by[n] is as close to the true time series y[n] as possible.
referring to figure 10.24 , we refer to x[n] as the input function and to y[n] as the
target function .x[n] and y[n] are related according to the equation
y[n] =k‚àí1x
k=0h[k]x[n‚àík]
| {z }
by[n]+e[n], (10.34)
where e[n] is a noise random process to model the error. the linear part of the equation
is known as the prediction and is constructed by sending x[n] through the system. for
simplicity we assume that x[n] is wss. thus, it follows that y[n] is also wss. we may
also assume that we can estimate rx[k],ry x[k],rxy[k] and ry[k].
65410.6. optimal linear filter
figure 10.24: a schematic diagram illustrating the optimal linear filter problem: given an input function
x[n], we want to design a filter h[n]such that the prediction by[n]is close to the target function y[n].
example 10.20 . if we let k= 3, equation (10.34) gives us
y[n] =h[0]x[n] +h[1]x[n‚àí1] +h[2]x[n‚àí2] +e[n].
that is, the current sample y[n] is a linear combination of the previous samples x[n],
x[n‚àí1] and x[n‚àí2].
given x[n] and y[n], what would be the best guess of the impulse response h[n] so
that the prediction is as close to the true values as possible? from our discussions of linear
regression, we know that this is equivalent to solving the optimization problem
minimize
{h[k]}k‚àí1
k=0 
y[n]‚àík‚àí1x
k=0h[k]x[n‚àík]!2
. (10.35)
the choice of the squared error is more or less arbitrary, depending on how we want to
model e[n]. by using the square norm, we implicitly assume that the error is gaussian.
this may not be true, but it is commonly used because the squared norm is differentiable .
we will follow this tradition.
the challenge associated with the minimization is that in most of the practical set-
tings the random processes x[n] and y[n] are changing rapidly because they are random
processes. therefore, even if we solve the optimization problem, the estimates h[k] will be
random variables since we are solving a random equation. to eliminate this randomness, we
take the expectation over all the possible choices of x[n] and y[n], yielding
minimize
{h[k]}k‚àí1
k=0 
y[n]‚àík‚àí1x
k=0h[k]x[n‚àík]!2
,
‚áì
minimize
{h[k]}k‚àí1
k=0ex,yÔ£Æ
Ô£∞ 
y[n]‚àík‚àí1x
k=0h[k]x[n‚àík]!2Ô£π
Ô£ª.
the resulting impulse responses h[k], derived by solving the above minimization, is
known as the optimal linear filter . it is the best linear model for describing the input-
output relationships between x[n] and y[n].
655chapter 10. random processes
what is the optimal linear filter?
the optimal linear filter is the solution to the optimization problem
minimize
{h[k]}k‚àí1
k=0ex,yÔ£Æ
Ô£∞ 
y[n]‚àík‚àí1x
k=0h[k]x[n‚àík]!2Ô£π
Ô£ª. (10.36)
10.6.3 yule-walker equation
to solve the optimal linear filter problem, we first perform some (slightly tedious) algebra
to obtain the following results:
lemma 10.4. letby[n] =pk‚àí1
k=0h[k]x[n‚àík]be the prediction of y[n]. the squared-
norm error can be written as
ex,y
y[n]‚àíby[n]2
=ry[0]‚àí2k‚àí1x
k=0h[k]ry x[k] +k‚àí1x
k=0k‚àí1x
j=0h[k]h[j]rx[j‚àík]. (10.37)
thus we can express the error in terms of ry x[k],rx[k]andry[k].
proof . we expand the error as follows:
ex,y
y[n]‚àíby[n]2
=ey
(y[n])2
‚àí2ex,yh
y[n]by[n]i
+exh
(by[n])2i
.
the first term is the autocorrelation of y[n]:
ey
(y[n])2
=e[y[n+ 0]y[n]] =ry[0]. (10.38)
the second term is
ex,yh
y[n]by[n]i
=ex,y"
y[n]k‚àí1x
k=0h[k]x[n‚àík]#
=k‚àí1x
k=0h[k]ex,y[y[n]x[n‚àík]]
=k‚àí1x
k=0h[k]ry x[k]. (10.39)
65610.6. optimal linear filter
the third term is
exh
(by[n])2i
=exÔ£Æ
Ô£∞ k‚àí1x
k=0h[k]x[n‚àík]!Ô£´
Ô£≠k‚àí1x
j=0h[j]x[n‚àíj]Ô£∂
Ô£∏Ô£π
Ô£ª
=exÔ£Æ
Ô£∞k‚àí1x
k=0k‚àí1x
j=0h[k]h[j]x[n‚àík]x[n‚àíj]Ô£π
Ô£ª
=k‚àí1x
k=0k‚àí1x
j=0h[k]h[j]ex[x[n‚àík]x[n‚àíj]]
=k‚àí1x
k=0k‚àí1x
j=0h[k]h[j]rx[j‚àík]. (10.40)
this completes the proof.
‚ñ°
the significance of this theorem is that it allows us to write the error in terms of ry x[k],
rx[k] and ry[k]. as we have mentioned, while we can solve the randomized optimization
equation (10.35), the resulting solution will be a random vector depending on the particular
realizations x[n] and y[n]. switching from equation (10.35) to equation (10.36) eliminates
the randomness because we have taken the expectation. the resulting optimization according
to the theorem is also convenient. instead of seeking individual realizations, we only need
to know the overall statistical description of the data through ry x[k],rx[k] and ry[k].
these can be estimated through modeling or pseudorandom signals.
the solution to the optimal linear filter problem is summarized by the yule-walker
equation :
theorem 10.9. the solution {h[0], . . . , h [k‚àí1]}to the optimal linear filter problem
minimize
{h[k]}k‚àí1
k=0ex,yÔ£Æ
Ô£∞ 
y[n]‚àík‚àí1x
k=0h[k]x[n‚àík]!2Ô£π
Ô£ª (10.41)
is given by the following matrix equation:
Ô£´
Ô£¨Ô£¨Ô£¨Ô£≠ry x[0]
ry x[1]
...
ry x[k‚àí1]Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∏=Ô£´
Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠rx[0] rx[1] ¬∑¬∑¬∑ rx[k‚àí1]
rx[1] rx[0] ¬∑¬∑¬∑...
............
rx[k‚àí1]rx[k‚àí2]¬∑¬∑¬∑ rx[0]Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏Ô£´
Ô£¨Ô£¨Ô£¨Ô£≠h[0]
h[1]
...
h[k‚àí1]Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∏,(10.42)
which is known as the yule-walker equation.
therefore, by solving the simple linear problem given by the yule-walker equation, we will
find the optimal linear filter solution.
proof . since the error is a squared norm, the optimal solution is obtained by taking the
657chapter 10. random processes
derivative:
d
dh[i]ex,y
y[n]‚àíby[n]2
=d
dh[i]Ô£±
Ô£≤
Ô£≥ry[0]‚àí2k‚àí1x
k=0h[k]ry x[k] +k‚àí1x
k=0k‚àí1x
j=0h[k]h[j]rx[j‚àík]Ô£º
Ô£Ω
Ô£æ
= 0‚àí2ry x[i] + 2k‚àí1x
k=0h[k]rx[i‚àík],
in which the derivative of the last term is computed by noting that
d
dh[i]k‚àí1x
k=0k‚àí1x
j=0h[k]h[j]rx[j‚àík]
=d
dh[i]k‚àí1x
j=0h[j]2rx[0] +d
dh[i]k‚àí1x
k=0x
jÃ∏=kh[k]h[j]rx[j‚àík]
= 2k‚àí1x
k=0h[k]rx[i‚àík].
equating the derivative to zero yields
ry x[i] =k‚àí1x
k=0h[k]rx[i‚àík], i = 0, . . . , k ‚àí1,
and putting the above equations into the matrix-vector form we complete the proof.
‚ñ°
the matrix in the yule-walker equation is a toeplitz matrix, in which each row is
a shifted version of the preceding row. this matrix structure is a consequence of a wss
process so that the autocorrelation function is determined by the time difference kand not
by the starting and end times.
remark . if we take the derivative of the loss w.r.t. h[i], we have that
0 =d
dh[i]ex,y
y[n]‚àíby[n]2
=‚àí2eh
y[n]‚àíby[n]
x[n‚àíi]i
.
this condition is known as the orthogonality condition , as it says that the error y[n]‚àíby[n]
is orthogonal to the signal x[n‚àíi].
10.6.4 linear prediction
we now demonstrate how to use the yule-walker equation in modeling an autoregressive
process . the procedure in this simple example can be used in speech processing and time-
series forecasting.
suppose that we have a wss random process y[n]. we would like to predict the future
samples by using the most recent ksamples through an autoregressive model. since the
65810.6. optimal linear filter
model is linear, we can write
by[n] =kx
k=1h[k]y[n‚àík] +e[n]. (10.43)
in this model, we say that the predicted value by[n] is a linear combination of the past k
samples, albeit to approximation error e[n].
the problem we need to solve is
minimize
h[k]e
y[n]‚àíby[n]2
.
sinceby[n] is written in terms of the past samples of y[n] in this problem, in the yule-walker
equation we can replace xwith y. consequently, we can write the matrix equation from
Ô£´
Ô£¨Ô£¨Ô£¨Ô£≠ry x[0]
ry x[1]
...
ry x[k‚àí1]Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∏=Ô£´
Ô£¨Ô£¨Ô£¨Ô£≠rx[0] rx[1] ¬∑¬∑¬∑ rx[k‚àí1]
rx[1] rx[0] ¬∑¬∑¬∑ rx[k‚àí2]
............
rx[k‚àí1]rx[k‚àí2]¬∑¬∑¬∑ rx[0]Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∏Ô£´
Ô£¨Ô£¨Ô£¨Ô£≠h[0]
h[1]
...
h[k‚àí1]Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∏,
toÔ£´
Ô£¨Ô£¨Ô£¨Ô£≠ry[1]
ry[2]
...
ry[k]Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∏
|{z}
r=Ô£´
Ô£¨Ô£¨Ô£¨Ô£≠ry[0] ry[1] ¬∑¬∑¬∑ ry[k‚àí1]
ry[1] ry[0] ¬∑¬∑¬∑ ry[k‚àí2]
............
ry[k‚àí1]ry[k‚àí2]¬∑¬∑¬∑ ry[0]Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∏
| {z }
rÔ£´
Ô£¨Ô£¨Ô£¨Ô£≠h[0]
h[1]
...
h[k‚àí1]Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∏. (10.44)
on a computer, solving the yule-walker equation requires a few steps. first, we need
to estimate the correlation
ry[k] =e[y[n+k]y[n]]‚âà1
nnx
n=1y[n+k]y[n].
the averaging on the right-hand side is often done using xcorr in matlab or np.correlate
in python. a graphical illustration of the input and the autocorrelation function is shown
infigure 10.25 .
after we have found ry[n], we need to construct the yule-walker equation. for this
linear prediction problem, the left-hand side of the yule-walker equation is the vector r,
defined according to equation (10.44). the yule-walker equation also requires the matrix r.
thisrcan be constructed via the toeplitz matrix as
r= toeplitz
ry[0], ry[1], . . . , r y[k‚àí1]
.
in matlab, we can call toeplitz to construct the matrix. in python, the command is
lin.toeplitz .
to solve the yule-walker equation, we need to invert the matrix r. there are built-in
commands for such an operation. in matlab, the command is \(the backslash), whereas
in python the command is np.linalg.lstsq .
659chapter 10. random processes
0 50 100 150 200 250 300-0.2-0.100.10.2
y[n]
-300 -200 -100 0 100 200 300-0.2-0.100.10.20.30.4
ry[k]
(a)y[n] (b) ry[k]
figure 10.25: an example time-series and its autocorrelation function.
% matlab code to solve the yule walker equation
y = load(‚Äôdata_ch10.txt‚Äô);
k = 10;
n = 320;
y_corr = xcorr(y);
r = toeplitz(y_corr(n+[0:k-1]));
lhs = y_corr(n+[1:k]);
h = r\lhs;
# python code to solve the yule walker equation
y = np.loadtxt(‚Äô./data_ch10.txt‚Äô)
k = 10
n = 320
y_corr = np.correlate(y,y,mode=‚Äôfull‚Äô)
r = lin.toeplitz(y_corr[n-1:n+k-1]) #call scipy.linalg
lhs = y_corr[n:n+k]
h = np.linalg.lstsq(r,lhs,rcond = none)[0]
note that in both the matlab and python codes the toeplitz matrix rstarts with
the index n. this is because, as you can see from figure 10.25 , the origin of the autocor-
relation function is the middle index of the computed autocorrelation function. for r, the
starting index is n+ 1 because the vector starts with ry[1].
to predict the future samples, we recall the autoregressive model for this problem:
by[n] =k‚àí1x
k=0h[k]y[n‚àík].
therefore, given y[n‚àí1], y[n‚àí2], . . . , y [n‚àík], we can predict by[n]. then we insert this
predicted by[n] into the sequence and increment the estimation problem to the next time
index. by repeating the process, we will be able to predict the future samples of y[n].
66010.6. optimal linear filter
figure 10.26 illustrates the prediction results of the yule-walker equation. as you can
see, the predictions are reasonably meaningful since the patterns follow the trend.
0 50 100 150 200 250 300 350-0.3-0.2-0.100.10.2
prediction
input
figure 10.26: an example of the predictions made by the autoregressive model.
the matlab and python codes are shown below.
% matlab code to predict the samples
z = y(311:320);
yhat = zeros(340,1);
yhat(1:320) = y;
for t = 1:20
predict = z‚Äô*h;
z = [z(2:10); predict];
yhat(320+t) = predict;
end
plot(yhat, ‚Äôr‚Äô, ‚Äôlinewidth‚Äô, 3); hold on;
plot(y, ‚Äôk‚Äô, ‚Äôlinewidth‚Äô, 4);
# python code to predict the samples
z = y[310:320]
yhat = np.zeros((340,1))
yhat[0:320,0] = y
for t in range(20):
predict = np.inner(np.reshape(z,(1,10)),h)
z = np.concatenate((z[1:10], predict))
yhat[320+t,0] = predict
plt.plot(yhat,‚Äôr‚Äô)
plt.plot(y,‚Äôk‚Äô)
plt.show()
661chapter 10. random processes
10.6.5 wiener filter
in the previous formulation, we notice that the impulse response has a finite length. there
are, however, problems in which the impulse response is infinite. for example, a recur-
sive filter h[n] will be infinitely long. the extension from finite length to infinite length is
straightforward. we can model the problem as
y[n] =‚àûx
k=‚àí‚àûh[k]x[n‚àík] +e[n].
however, when h[n] is infinitely long the yule-walker equation does not hold because the
matrix rwill be infinitely large. nevertheless, the building block equation for yule-walker
is still valid:
ry x[i] =‚àûx
k=‚àí‚àûh[k]rx[i‚àík]. (10.45)
to maintain the spirit of the yule-walker equation while enabling computation, we
recognize that the infinite sum on the right-hand side is, in fact, a convolution . thus we
can take the (discrete-time) fourier transform of both sides to obtain
sy x(ejœâ) =h(ejœâ)sx(ejœâ). (10.46)
therefore, the corresponding optimal linear filter (in the fourier domain) is
h(ejœâ) =sy x(ejœâ)
sx(ejœâ), (10.47)
and
h[n] =f‚àí1sy x(e‚àíjœâ)
sx(e‚àíjœâ)
.
the filter obtained in this way is known as the wiener filter .
example 10.21 . (denoising ) suppose x[n] =y[n] +w[n], where w[n] is the noise
term that is independent of y[n], as shown in figure 10.27 .
figure 10.27: design of a wiener filter that takes an input function x[n]and outputs an estimate
by[n]that is close to the true function y[n].
now, given the input function x[n], can we construct the wiener filter h[n] such
that the predicted function by[n] is as close to y[n] as possible? the wiener filter for
this problem is also the optimal denoising filter.
66210.6. optimal linear filter
solution . the following correlation functions can easily be seen:
rx[k] =e[x[n+k]x[n]]
=e[(y[n+k] +w[n+k])(y[n] +w[n])]
=e[y[n+k]y[n]] +e[y[n+k]w[n]]
+e[w[n+k]y[n]] +e[w[n+k]w[n]]
=e[y[n+k]y[n]] + 0 + 0 + e[w[n+k]w[n]]
=ry[k] +rw[k].
similarly, we have
ry x[k] =e[y[n+k]x[n]]
=e[y[n](y[n+k] +w[n+k])] = ry[k].
consequently, the optimal linear filter is
h(ejœâ) =sy x(ejœâ)
sx(ejœâ)
=f{ry x[k]}
f{rx[k]}
=sy(ejœâ)
sy(ejœâ) +sw(ejœâ).
what is the wiener filter for a denoising problem?
¬àsuppose the corrupted function x[n] is related to the clean function y[n] through
x[n] =y[n] +w[n], for some noise function w[n].
¬àthe wiener filter is
h(ejœâ) =sy(ejœâ)
sy(ejœâ) +sw(ejœâ). (10.48)
¬àto perform the filtering, the denoised function by[n] is
by[n] =f‚àí1
h(ejœâ)x(ejœâ)	
.
figure 10.28 shows an example of applying the wiener filter to a noise removal prob-
lem. in this example we let w[n] be an i.i.d. gaussian process with standard deviation
œÉ= 0.05 and mean ¬µ= 0. the noisy samples of random process x[n] are defined as
x[n] =y[n]+w[n], where y[n] is the clean function. as you can see from figure 10.28 (a),
the wiener filter is able to denoise the function reasonably well.
the optimal linear filter used for this denoising task is infinitely long. this can be seen
infigure 10.28 (b), where the filter length is the same as the length of the observed time
series x[n]. ifx[n] is longer, the filter h[n] will also become longer. therefore, finite-length
approaches such as the yule-walker equation do not apply here.
663chapter 10. random processes
0 50 100 150 200 250 300-0.2-0.100.10.2
noisy input x[n]
wiener filtered yhat[n]
ground truth y[n]
-300 -200 -100 0 100 200 300-0.0500.050.10.150.20.25 h[n]
(a) noise removal by wiener filtering (b) wiener filter
figure 10.28: (a) applying a wiener filter to denoise a function. (b) the wiener filter used for the
denoising task.
the matlab / python codes used to generate figure 10.28 (a) are shown below.
the main commands here are scipy.fft andscipy.ifft , which are available in the scipy
library. the commands yhat = h.*fft(x, 639) in matlab execute the wiener filtering
step. here, we resample the function xto 639 samples so that it matches with the wiener
filter h. similar commands in python are h * fft(x, 639) .
% matlab code for wiener filtering
w = 0.05*randn(320,1);
x = y + w;
ry = xcorr(y);
rw = xcorr(w);
sy = fft(ry);
sw = fft(rw);
h = sy./(sy + sw);
yhat = h.*fft(x, 639);
yhat = real(ifft(yhat));
plot(x, ‚Äôlinewidth‚Äô, 4, ‚Äôcolor‚Äô, [0.7, 0.7, 0.7]); hold on;
plot(yhat(1:320), ‚Äôr‚Äô, ‚Äôlinewidth‚Äô, 2);
plot(y, ‚Äôk:‚Äô, ‚Äôlinewidth‚Äô, 2);
# python code for wiener filtering
from scipy.fft import fft, ifft
w = 0.05*np.random.randn(320)
x = y + w
ry = np.correlate(y,y,mode=‚Äôfull‚Äô)
rw = np.correlate(w,w,mode=‚Äôfull‚Äô)
sy = fft(ry)
sw = fft(rw)
h = sy / (sy+sw)
yhat = h * fft(x, 639)
66410.6. optimal linear filter
yhat = np.real(ifft(yhat))
plt.plot(x,color=‚Äôgray‚Äô)
plt.plot(yhat[0:320],‚Äôr‚Äô)
plt.plot(y,‚Äôk:‚Äô)
example 10.22 . (deconvolution ) suppose that the corrupted function is generated
according to a linear process given by
x[n] =‚àûx
‚Ñì=‚àí‚àûg[‚Ñì]y[n‚àí‚Ñì] +w[n],
where g[n] is the impulse response of some kind of degradation process and w[n] is
the gaussian noise term, as shown in figure 10.29 . find the optimal linear filter (i.e.,
the wiener filter) to estimate by[n].
figure 10.29: design of a wiener filter that takes an input function x[n]and outputs an estimate
by[n]that is close to the true function y[n].
solution . to construct the wiener filter, we first determine the cross-correlation func-
tion:
ry x[k] =e[y[n+k]x[n]] =e"
y[n+k]‚àûx
‚Ñì=‚àí‚àûg[‚Ñì]y[n‚àí‚Ñì] +w[n]#
.
using algebra, it follows that
e"
y[n+k]‚àûx
‚Ñì=‚àí‚àûg[‚Ñì]y[n‚àí‚Ñì] +w[n]#
=‚àûx
‚Ñì=‚àí‚àûg[‚Ñì]e[y[n+k]y[n‚àí‚Ñì]] +e[y[n+k]w[n]]
=‚àûx
‚Ñì=‚àí‚àûg[‚Ñì]ry[k+‚Ñì] + 0 = ( g‚äõry)[k],
which is the correlation between gandry. therefore, the cross power spectral density
sy x(ejœâ) is
sy x(ejœâ) =g(ejœâ)sy(ejœâ).
665chapter 10. random processes
the autocorrelation of this problem is
rx[k] =e[x[n+k]x[n]]
=e[((g‚àóy)[n+k] +w[n+k])((g‚àóy)[n] +w[n])]
=e[(g‚àóy)[n+k](g‚àóy)[n]] +e[w[n+k]w[n]]
= (g‚äõ(g‚àóry))[k] +rw[k],
where, according to the previous section, the first part is the correlation ‚äõfollowed by
a convolution ‚àó. therefore, the power spectral density of xis
sx(ejœâ) =|g(ejœâ)|2sy(ejœâ) +sw(ejœâ).
combining the results, the wiener filter is
h(ejœâ) =sy x(ejœâ)
sx(ejœâ)=g(ejœâ)sy(ejœâ)
|g(ejœâ)|2sy(ejœâ) +sw(ejœâ).
what is the wiener filter for a deconvolution problem?
¬àsuppose that the corrupted function x[n] is related to the clean function y[n]
through x[n] = (g‚àóy)[n] +w[n], for some degradation g[n] and noise w[n].
¬àthe wiener filter is
h(ejœâ) =g(ejœâ)sy(ejœâ)
|g(ejœâ)|2sy(ejœâ) +sw(ejœâ). (10.49)
¬àto perform the filtering, the estimated function by[n] is
by[n] =f‚àí1
h(ejœâ)x(ejœâ)	
.
as an example of the deconvolution problem, we show a wss function y[n] infig-
ure 10.30 . this clean function y[n] is constructed by passing an i.i.d. noise process through
an arbitrary lti system so that the wss property is guaranteed. given this y[n], we con-
struct a degradation process in which the impulse response is given by g[n]. in this example,
we assume that g[n] is a uniform function. we then add noise w[n] to the time series to
obtain the corrupted observation x[n]. the reconstruction by the wiener filter is shown in
figure 10.30 .
the matlab and python codes used to generate figure 10.30 are shown below.
% matlab code to solve the wiener deconvolution problem
load(‚Äôch10_wiener_deblur_data‚Äô);
g = ones(32,1)/32;
w = 0.02*randn(320,1);
x = conv(y,g,‚Äôsame‚Äô) + w;
ry = xcorr(y);
66610.6. optimal linear filter
50 100 150 200 250 300-0.8-0.6-0.4-0.200.20.40.6
noisy input x[n]
wiener filtered yhat[n]
ground truth y[n]
figure 10.30: reconstructing time series from degraded observations using a wiener filter.
rw = xcorr(w);
sy = fft(ry);
sw = fft(rw);
g = fft(g,639);
h = (conj(g).*sy)./(abs(g).^2.*sy + sw);
yhat = h.*fft(x, 639);
yhat = real(ifft(yhat));
figure;
plot(x, ‚Äôlinewidth‚Äô, 4, ‚Äôcolor‚Äô, [0.5, 0.5, 0.5]); hold on;
plot(16:320+15, yhat(1:320), ‚Äôr‚Äô, ‚Äôlinewidth‚Äô, 2);
plot(1:320, y, ‚Äôk:‚Äô, ‚Äôlinewidth‚Äô, 2);
# python code to solve the wiener deconvolution problem
y = np.loadtxt(‚Äô./ch10_wiener_deblur_data.txt‚Äô)
g = np.ones(64)/64
w = 0.02*np.random.randn(320)
x = np.convolve(y,g,mode=‚Äôsame‚Äô) + w
ry = np.correlate(y,y,mode=‚Äôfull‚Äô)
rw = np.correlate(w,w,mode=‚Äôfull‚Äô)
sy = fft(ry)
sw = fft(rw)
g = fft(g,639)
h = (np.conj(g)*sy)/( np.power(np.abs(g),2)*sy + sw )
yhat = h * fft(x, 639)
yhat = np.real(ifft(yhat))
plt.plot(x,color=‚Äôgray‚Äô)
667chapter 10. random processes
plt.plot(np.arange(32,320+32),yhat[0:320],‚Äôr‚Äô)
plt.plot(y,‚Äôk:‚Äô)
caveat to wiener filtering . in practice, the above wiener filter needs to be modified
because sy(ejœâ) and sw(ejœâ) cannot be estimated from the data via the temporal corre-
lation (as we did in the matlab/python programs). the reason is that we never have
access to y[n] and w[n]. in this case, one has to guess the power spectral densities sy(ejœâ)
andsw(ejœâ). the noise power sw(ejœâ) is usually not difficult to estimate. for example,
in the program we showed above, the noise power spectral density is sw = 0.02^2*320
(matlab), which is the noise standard deviation times the number of samples.
the signal sy(ejœâ) is often the hard part. in the absence of any knowledge about the
ground truth‚Äôs power spectral density, the wiener filter does not work. however, for certain
problems in which sy(ejœâ) can be predetermined by prior knowledge, the wiener filter is
guaranteed to be optimal ‚Äî optimal in the mean-squared-error sense over the entire time
axis.
wiener filter versus ridge regression . the wiener filter equation can be interpreted
as a ridge regression. denoting the forward observation model by
x=gy+w,
the corresponding ridge regression minimization is
by= argmin
y‚à•x‚àígy‚à•2+Œª‚à•y‚à•2
= (gtg+Œªi)‚àí1gtx.
ifgis a convolutional matrix, the above solution can be written in the fourier domain (by
using the fourier transform as the eigenvectors):
by(ejœâ) ="
g(ejœâ)
|g(ejœâ)|2+Œª#
| {z }
h(ejœâ)x(ejœâ).
comparing this ‚Äúoptimal linear filter‚Äù with the wiener filter, we observe that the wiener
filter has slightly more generality:
by(ejœâ) ="
g(ejœâ)sy(ejœâ)
|g(ejœâ)|2sy(ejœâ) +sw(ejœâ)#
x(ejœâ).
therefore, in the absence of sy(ejœâ) and assuming that sw(ejœâ) is a constant (e.g., for
gaussian noise), the wiener filter is exactly a ridge regression.
66810.7. summary
10.7 summary
random processes are very useful tools for analyzing random variables over time. in this
chapter, we have introduced some of the most basic mechanisms:
¬àstatistical versus temporal analysis : the statistical analysis of a random process
looks at the random process vertically . it treats x(t) as a random variable and studies
the randomness across different realizations. the temporal analysis is the horizontal
perspective. it treats x(t) as a function in time with a fixed random index. in general,
statistical average Ã∏= temporal average.
¬àmean function ¬µx(t): the mean function is the expectation of the random process.
at every time t, we take the expectation to obtain the expected value e[x(t)].
¬àautocorrelation function rx(t1, t2). this is the joint expectation of the random pro-
cess at two different time instants t1andt2. the corresponding values x(t1) and x(t2)
are two random variables, and so the joint expectation measures how correlated these
two variables are.
¬àwide-sense stationary (wss) : this is a special class of random processes in which
¬µx(t) is a constant and rx(t1, t2) is a function of t1‚àít2. when this happens, the auto-
correlation function (which is originally a 2d function) will have a toeplitz structure.
we write rx(t1, t2) asrx(œÑ), where œÑ=t1‚àít2.
¬àpower spectral density (psd) : this is the fourier transform of the autocorrelation
function rx(œÑ), according to the einstein-wiener-khinchin theorem. it is called the
power spectral density because we can integrate it in the fourier space to retrieve the
power. this provides us with some convenient computational tools for analyzing data.
¬àrandom process through a linear time-invariant (lti) system : this tells us how a
random process behaves after going through an lti system. the analysis can be done
at the realization level, where we look at each random process, or at the statistical
level, where we look at the autocorrelation function and the psd.
¬àoptimal linear filter : a set of techniques that can be used to retrieve signals by using
the statistical information of the data and the system. we introduced two specific
approaches: the yule-walker equation for a finite-length filter and the wiener filter
for an infinite-length filter. we demonstrated how these techniques could be applied
to forecast a time series and recover a time series from corrupted measurements.
while we have covered some of the most basic ideas in random processes, there are
also several topics we have not discussed. these include, but are not limited to: strictly
stationary process, a more restrictive class of random process than wss; poisson process,
a useful model for arrival analysis; markov chain, a discrete-time random process where
the current state only depends on the previous state. readers interested in these materials
should consult the references listed at the end of this chapter.
669chapter 10. random processes
10.8 appendix
the einstein-wiener-khinchin theorem
the einstein-wiener-khinchin theorem is a fundamental result. it states that for any wide-
sense stationary process, the power spectral density sx(œâ) is the fourier transform of the
autocorrelation function.
theorem 10.10 (the einstein-wiener-khinchin theorem ).for a wss random pro-
cessx(t),
sx(œâ) =f {rx(œÑ)}, (10.50)
whenever the fourier transform of rx(œÑ)exists.
proof . first, let‚Äôs recall the definition of sx(œâ):
sx(œâ)def= lim
t‚Üí‚àû1
2teh
|ext(œâ)|2i
. (10.51)
by expanding the expectation, we have
e[|ext(œâ)|2] =e" zt
‚àítx(t)e‚àíjœâtdt! zt
‚àítx(Œ∏)e‚àíjœâŒ∏dŒ∏!‚àó#
=zt
‚àítzt
‚àíte[x(t)x(Œ∏)]e‚àíjœâ(t‚àíŒ∏)dt dŒ∏ =zt
‚àítzt
‚àítrx(t‚àíŒ∏)e‚àíjœâ(t‚àíŒ∏)dt dŒ∏.
(10.52)
our next step is to analyze rx(t‚àíŒ∏). define
qx(v) =f {rx(œÑ)}. (10.53)
then, by inverse fourier transform
rx(œÑ) =1
2œÄz‚àû
‚àí‚àûqx(v)ejvœÑdv,
and therefore
rx(t‚àíŒ∏) =1
2œÄz‚àû
‚àí‚àûqx(v)ejv(t‚àíŒ∏)dv.
substituting this into equation (10.52) yields
e[|ext(œâ)|2=zt
‚àítzt
‚àít1
2œÄz‚àû
‚àí‚àûqx(v)ejv(t‚àíŒ∏)dv
e‚àíjœâ(t‚àíŒ∏)dt dŒ∏
=1
2œÄz‚àû
‚àí‚àûqx(v) zt
‚àítejt(v‚àíœâ)dt! zt
‚àítejŒ∏(œâ‚àív)dŒ∏!
dv.
67010.8. appendix
we now need to simplify the two inner integrals. recall by fourier pair that
rectt
t
f
‚Üê‚Üítsincœât
2
.
this implies that
zt
‚àítejt(v‚àíœâ)dt=zt
‚àíte‚àíj(œâ‚àív)tdt
=z‚àû
‚àí‚àûrect(t
2t)e‚àíj(œâ‚àív)tdt= 2tsinc(( œâ‚àív)t) = 2 tsin((œâ‚àív)t)
(œâ‚àív)t.
hence, we have
eh
|ext(œâ)|2i
=1
2œÄz‚àû
‚àí‚àûqx(v)
2tsin((œâ‚àív)t)
(œâ‚àív)t2
dv. (10.54)
and so
1
2te[|ext(œâ)|2=2t
2œÄz‚àû
‚àí‚àûqx(v)sin((œâ‚àív)t)
(œâ‚àív)t2
dv. (10.55)
ast‚Üí ‚àû (see lemma 10.5 below), we have
2tsin((œâ‚àív)t)
(œâ‚àív)t2
‚àí‚Üí 2œÄŒ¥(œâ‚àív).
therefore,
lim
t‚Üí‚àû1
2teh
|ext(œâ)|2i
=1
2œÄz‚àû
‚àí‚àûqx(v)"
lim
t‚Üí‚àû2tsin((œâ‚àív)t)
(œâ‚àív)t2#
dv
=z‚àû
‚àí‚àûqx(v)Œ¥(œâ‚àív)dv=qx(œâ).
since qx(œâ) =f[rx(œÑ)], we conclude that
sx(œâ) = lim
t‚Üí‚àû1
2te[|ext(œâ)|2] =qx(œâ) =f[rx(œÑ)].
lemma 10.5.
lim
t‚Üí‚àû1
2œÄz‚àû
‚àí‚àûqx(v)2tsin((œâ‚àív)t)
(œâ‚àív)t2
dv=qx(œâ). (10.56)
to prove this lemma, we first define Œ¥t(œâ) = 2 t(sin(œât)
œât)2. it is sufficient to show that
lim
t‚Üí‚àû1
2œÄz‚àû
‚àí‚àûqx(v)2tsin((œâ‚àív)t)
(œâ‚àív)t2
dv‚àíqx(œâ)‚Üí0 as t‚Üí ‚àû .(10.57)
we will proceed by demonstrating the following three facts about Œ¥t(œâ):
671chapter 10. random processes
1.
1
2œÄz‚àû
‚àí‚àûŒ¥t(œâ)dœâ= 1
.
2. for any ‚ñ≥>0, z
{œâ:|œâ|>‚ñ≥}Œ¥t(œâ)dœâ‚Üí0 as t‚Üí ‚àû
.
3. for any |œâ| ‚â• ‚ñ≥ >0, we have |Œ¥t(œâ)| ‚â§2
t‚ñ≥2.
proof of fact 1 .
1
2œÄz‚àû
‚àí‚àûŒ¥t(œâ)dœâ=1
2œÄz‚àû
‚àí‚àû2tsin(œât)
œât2
|{z}
sinc2(œât)dœâ.
note that
Œªt
4t
‚Üê‚Üí2tsinc2(œât).
therefore,
1
2œÄz‚àû
‚àí‚àû2tsinc2(œât)dœâ=1
2œÄz‚àû
‚àí‚àû2tsinc2(œât)ejœâ0dœâ
= Œª0
4t
= 1.
proof of fact 2 .Œ¥t(œâ) is symmetric, so, it is sufficient to check only one side:
z‚àû
‚ñ≥Œ¥t(œâ)dœâ=z‚àû
‚ñ≥2tsin(œât)
œât2
dœâ
=2t
t2z‚àû
‚ñ≥sin2(œât)
œâ2dœâ
‚â§2
tz‚àû
‚ñ≥1
œâ2dœâ |sin(.)|2‚â§1
=2
t
‚àí1
œâ‚àû
‚ñ≥=2
t‚ñ≥‚Üí0 as t‚Üí ‚àû .
proof of fact 3 .
|Œ¥t(œâ)|= 2tsin(œât)
œât2
‚â§2t1
(œât)2
=2
œâ2t‚â§2
t‚ñ≥2.
proof of lemma . consider qx(œâ). by property 1,
qx(œâ) =qx(œâ).1
2œÄz‚àû
‚àí‚àûŒ¥t(œâ‚àív)dv=1
2œÄz‚àû
‚àí‚àûqx(œâ)Œ¥t(œâ‚àív)dv.
67210.8. appendix
therefore,
1
2œÄz‚àû
‚àí‚àûqx(v)Œ¥t(œâ‚àív)dv‚àíqx(œâ)
=1
2œÄz‚àû
‚àí‚àûqx(v)Œ¥t(œâ‚àív)dv‚àí1
2œÄz‚àû
‚àí‚àûqx(œâ)Œ¥t(œâ‚àív)dv
=1
2œÄz‚àû
‚àí‚àû(qx(v)‚àíqx(œâ))Œ¥t(œâ‚àív)dv‚â§1
2œÄz‚àû
‚àí‚àûqx(v)‚àíqx(œâ)Œ¥t(œâ‚àív)dv.
for any œµ >0, let‚ñ≥be a constant such that
|œâ‚àív|<‚ñ≥ whenever |qx(v)‚àíqx(œâ)|< œµ.
then we can partition the above integral into
1
2œÄz‚àû
‚àí‚àûqx(œâ)‚àíqx(v)Œ¥t(œâ‚àív)dv=1
2œÄzœâ+‚ñ≥
œâ‚àí‚ñ≥qx(œâ)‚àíqx(v)Œ¥t(œâ‚àív)dv(1)
+1
2œÄz‚àû
œâ+‚ñ≥qx(œâ)‚àíqx(v)Œ¥t(œâ‚àív)dv (2)
+1
2œÄzœâ+‚ñ≥
‚àí‚àûqx(œâ)‚àíqx(v)Œ¥t(œâ‚àív)dv. (3)
partition (1) above can be evaluated as follows:
1
2œÄzœâ+‚ñ≥
œâ‚àí‚ñ≥qx(œâ)‚àíqx(v)Œ¥t(œâ‚àív)dv
‚â§1
2œÄzœâ+‚ñ≥
œâ‚àí‚ñ≥œµŒ¥t(œâ‚àív)dv
=œµ
2œÄzœâ+‚ñ≥
œâ‚àí‚ñ≥Œ¥t(œâ‚àív)dv
‚â§œµ
2œÄz‚àû
‚àí‚àûŒ¥t(œâ‚àív)dv=œµ,
where the last inequality holds because Œ¥t(œâ‚àív)‚â•0. since œµcan be arbitrarily small, the
only possibility for
1
2œÄzœâ+‚ñ≥
œâ‚àí‚ñ≥qx(œâ)‚àíqx(v)Œ¥t(œâ‚àív)dv
for all œµis that the integral is 0.
partition (2) above can be evaluated as follows:
1
2œÄz‚àû
œâ+‚ñ≥qx(œâ)‚àíqx(v)Œ¥t(œâ‚àív)dv
‚â§1
2œÄz‚àû
œâ+‚ñ≥ qx(œâ)+qx(v)
Œ¥t(œâ‚àív)dv
=qx(œâ)1
2œÄz‚àû
œâ+‚ñ≥Œ¥t(œâ‚àív)dv+1
2œÄz‚àû
œâ+‚ñ≥qx(v)Œ¥t(œâ‚àív)dv.
673chapter 10. random processes
by property 2,1
2œÄr‚àû
œâ+‚ñ≥Œ¥t(œâ‚àív)dv‚Üí0 ast‚Üí ‚àû . by property 3,
1
2œÄz‚àû
œâ+‚ñ≥qx(v)Œ¥t(œâ‚àív)dv‚â§1
2œÄ2
t‚ñ≥2z‚àû
œâ+‚ñ≥qx(v)dv
| {z }
<‚àûbecause qx(v)=f[rx(œÑ)]‚Üí0.
therefore, we conclude that
1
2œÄz‚àû
œâ+‚ñ≥qx(v)Œ¥t(œâ‚àív)dv‚Üí0 as t‚Üí ‚àû .
and hence (1), (2) and (3) all ‚Üí0 ast‚Üí ‚àû . so we have
lim
t‚Üí‚àû1
2œÄz‚àû
‚àí‚àûqx(v)2tsin((œâ‚àív)t)
(œâ‚àív)t2
dv‚àíqx(œâ)‚Üí0 as t‚Üí ‚àû ,
which completes the proof.
10.8.1 the mean-square ergodic theorem
the mean-square ergodic theorem states that for any wss random process, the statistical
average is the same as the temporal average . this provides an important tool in practice
because finding the statistical average is typically very difficult. with the mean ergodic
theorem, one can easily estimate the statistical average using the temporal average.
theorem 10.11 (mean-square ergodic theorem ).lety(t)be a wss process,
with mean e[y(t)] =mand autocorrelation function ry(œÑ). assume that the fourier
transform of ry(œÑ)exists. define
mtdef=1
2tzt
‚àíty(t)dt. (10.58)
thenehmt‚àím2i
‚Üí0ast‚Üí ‚àû .
proof of mean ergodic theorem . let x(t) =y(t)‚àím. it follows that
mt‚àím=1
2tzt
‚àíty(t)dt‚àím=1
2tzt
‚àítx(t)dt.
we define the finite-window approximation of x(t):
xt(t) =x(t),‚àít‚â§t‚â§t,
0, elsewhere .
then the difference mt‚àímcan be computed as
mt‚àím=1
2tzt
‚àítx(t)dt=1
2tz‚àû
‚àí‚àûx(t)e‚àíj0tdt=1
2text(œâ)
œâ=0=ext(0)
2t.
67410.9. references
taking the expectation of the squares yields
e
|mt‚àím|2
=ehext(0)2i
4t2.
recall from the einstein-wiener-khinchin theorem,
1
2tehext(œâ)2i
=1
2œÄz‚àû
‚àí‚àûsx(v)2tsin((œâ‚àív)t)
(œâ‚àív)t2
dv.
putting the limit t‚Üí ‚àû , if we have that
lim
t‚Üí‚àû1
2œÄz‚àû
‚àí‚àûsx(v)2tsin((œâ‚àív)t)
(œâ‚àív)t2
dv=sx(œâ),
then we will have
1
2tehext(œâ)2i
‚Üísx(œâ) and1
2tehext(0)2i
‚Üísx(0).
hence,
lim
t‚Üí‚àûehmt‚àím2i
= lim
t‚Üí‚àû1
2tehext(0)2i
= lim
t‚Üí‚àû1
2tsx(0) = 0 .
this completes the proof.
10.9 references
basic texts
the following textbooks are basic texts about random processes. they offer many comple-
mentary materials to our book. for example, we omitted the topics of straightly stationary
processes and memoryless properties. we have also omitted a few classical examples, such
as the random telegraph signal, the incremental independence of poisson processes, and
markov chains. these materials can be found in the texts below.
10-1 john a. gubner, probability and random processes for electrical and computer en-
gineers , cambridge university press, illustrated edition, 2006.
10-2 alberto leon-garcia, probability, statistics, and random processes for electrical en-
gineering , pearson, 3rd edition, 2007.
10-3 athanasios papoulis, s. unnikrishna pillai, probability, random variables and stochas-
tic processes , mcgraw-hill, 4th edition, 2012.
10-4 henry stark and john woods, probability and random processes with applications
to signal processing , prentice hall, 3rd edition, 2001.
675chapter 10. random processes
10-5 eugene wong and bruce hajek, stochastic processes in engineering systems , springer-
verlag, 1985.
10-6 bruce hajek, random processes for engineers , cambridge university press, 2015.
10-7 dimitri p. bertsekas and john n. tsitsiklis, introduction to probability , athena sci-
entific, 2nd edition, 2008.
10-8 robert g. gallager, stochastic processes: theory for applications , cambridge uni-
versity press, 1st edition, 2014.
signal and systems / fourier transforms
the following references are classic references on signal and systems.
10-9 alan oppenheim and ronald schafer, discrete-time signal processing , 2nd edition,
prentice hall 1999.
10-10 alan oppenheim and alan willsky, signals and systems , pearson, 2nd edition, 1996.
10-11 martin vetterli, jelena kovacevic, and vivek k. goyal, foundations of signal pro-
cessing , cambridge university press, 3rd edition, 2014.
10-12 todd k. moon and wynn c. stirling, mathematical methods and algorithms for signal
processing , prentice-hall, 2000.
engineering applications
10-13 john g. proakis and masoud salehi, communication systems engineering , pearson,
2nd edition, 2001.
10-14 rodger e. ziemer, william h. tranter, principles of communications , wiley, 7th
edition, 2014.
10-15 joseph w. goodman, statistical optics , wiley, 2015.
10.10 problems
exercise 1. (video solution)
consider the random process
x(t) = 2 acos(t) + (b‚àí1) sin( t),
where aandbare two independent random variables with e[a] =e[b] = 0, and e[a2] =
e[b2] = 1.
(a) find ¬µx(t).
(b) find rx(t1, t2).
67610.10. problems
(c) find cx(t1, t2).
exercise 2. (video solution)
letx[n] be a discrete-time random process with mean function mx[n] =e{x[n]}and
correlation function rx[n, m] =e{x[n]x[m]}. suppose that
y[n] =‚àûx
i=‚àí‚àûh[n‚àíi]x[i]. (10.59)
(a) find ¬µy[n].
(b) find rxy[n, m].
exercise 3. (video solution)
lety(t) =x(t)‚àíx(t‚àíd).
(a) find rx,y(œÑ) and sx,y(œâ).
(b) find ry(œÑ).
(c) find sy(œâ).
exercise 4. (video solution)
letx(t) be a zero-mean wss process with autocorrelation function rx(œÑ). let y(t) =
x(t) cos( œât+ Œ∏), where Œ∏ ‚àºuniform( ‚àíœÄ, œÄ) and Œ∏ is independent of the process x(t).
(a) find the autocorrelation function ry(œÑ).
(b) find the cross-correlation function of x(t) and y(t).
(c) is y(t) wss? why or why not?
exercise 5. (video solution)
a wss process x(t) with autocorrelation function
rx(œÑ) = 1 /(1 +œÑ2)
is passed through an lti system with impulse response
h(t) = 3 sin( œÄt)/(œÄt).
lety(t) be the system output. find sy(œâ) and sketch sy(œâ).
exercise 6. (video solution)
a white noise x(t) with power spectral density sx(œâ) =n0/2 is applied to a lowpass filter
h(t) with impulse response
h(t) =1
rce‚àít/rc, t > 0. (10.60)
find the followings.
677chapter 10. random processes
(a)sxy(œâ).
(b)rxy(œÑ).
(c)sy(œâ).
(d)ry(œÑ).
exercise 7. (video solution)
consider a wss process x(t) with autocorrelation function
rx(œÑ) = sinc( œÄœÑ).
the process is sent to an lti system with input-output relationship
2d2
dt2y(t) + 2d
dty(t) + 4y(t) = 3d2
dt2x(t)‚àí3d
dtx(t) + 6x(t).
find the autocorrelation function ry(œÑ).
exercise 8. (video solution)
given the functions a(t),b(t) and c(t), let
g(t,1) = a(t),
g(t,2) = b(t),
g(t,3) = c(t).
letx(t) = g(t, z), where zis a discrete random variable with pmf p[z= 1] = p1,
p[z= 2] = p2andp[z= 3] = p3. find, in terms of the p1,p2,p3,a(t),b(t) and c(t),
(a)¬µx(t).
(b)rx(t1, t2).
exercise 9.
in the previous problem, let a(t) =e‚àíŒª|t|,b(t) = sin( œÄt) and c(t) =‚àí1.
(a) choose p1,p2,p3so that x(t) is wss.
(b) choose p1,p2,p3so that x(t) is not wss.
exercise 10. (video solution)
find the autocorrelation function rx(œÑ) corresponding to each of the following power spec-
tral densities:
(a)Œ¥(œâ‚àíœâ0) +Œ¥(œâ+œâ0).
(b)e‚àíœâ2/2.
(c)e‚àí|œâ|.
67810.10. problems
exercise 11. (video solution)
a wss process x(t) with autocorrelation function rx(œÑ) =e‚àíœÑ2/(2œÉ2
t)is passed through
an lti system with transfer function h(œâ) =e‚àíœâ2/(2œÉ2
h). denote the system output by
y(t). find the followings.
(a)sxy(œâ).
(b)rxy(œÑ).
(c)sy(œâ).
(d)ry(œÑ).
exercise 12. (video solution)
a white noise x(t) with power spectral density sx(œâ) =n0/2 is applied to a lowpass filter
h(t) with
h(œâ) =(
1‚àíœâ2,if|œâ| ‚â§œÄ,
0, otherwise .
finde[|y(t)|2], where y(t) is the output of the filter.
exercise 13. (video solution)
letx(t) be a wss process with correlation function
rx(œÑ) =(
1‚àí |œÑ|,if‚àí1‚â§œÑ‚â§1,
0, otherwise .(10.61)
it is known that when x(t) is input to a system with transfer function h(œâ), the system
output y(t) has a correlation function
ry(œÑ) =sinœÄœÑ
œÄœÑ. (10.62)
find the transfer function h(œâ).
exercise 14.
consider the system
y(t) =e‚àítzt
‚àí‚àûeœÑx(œÑ)dœÑ.
assume that x(t) is zero-mean white noise with power spectral density sx(œâ) =n0/2.
find the followings:
(a)sxy(œâ).
(b)rxy(œÑ).
(c)sy(œâ).
(d)ry(œÑ).
679chapter 10. random processes
680chapter a
appendix
useful identities
1.‚àûp
k=0rk= 1 + r+r2+¬∑¬∑¬∑=1
1‚àír
2.np
k=1k= 1 + 2 + 3 + ¬∑¬∑¬∑+n=n(n+1)
2
3.ex=‚àûp
k=0xk
k!= 1 +x
1!+x2
2!+¬∑¬∑¬∑
4.‚àûp
k=1krk‚àí1= 1 + 2 r+ 3r2+¬∑¬∑¬∑=1
(1‚àír)2
5.np
k=1k2= 12+ 22+ 33+¬∑¬∑¬∑+n2=n3
3+n2
2+n
6
6. (a+b)n=np
k=0 n
k
akbn‚àík
common distributions
distribution pmf / pdf e[x] var[ x] mx(s)
bernoulli px(1) = pandpx(0) = 1 ‚àíp p p (1‚àíp) 1 ‚àíp+pes
binomial px(k) = n
k
pk(1‚àíp)n‚àíknp np (1‚àíp) (1 ‚àíp+pes)n
geometric px(k) =p(1‚àíp)k‚àí11
p1‚àíp
p2pes
1‚àí(1‚àíp)es
poisson px(k) =Œªke‚àíŒª
k!Œª Œª eŒª(es‚àí1)
gaussian fx(x) =1‚àö
2œÄœÉ2exp
‚àí(x‚àí¬µ)2
2œÉ2
¬µ œÉ2exp
¬µs+œÉ2s2
2
exponential fx(x) =Œªexp{‚àíŒªx}1
Œª1
Œª2Œª
Œª‚àís
uniform fx(x) =1
b‚àíaa+b
2(b‚àía)2
12esb‚àíesa
s(b‚àía)
681chapter a. appendix
sum of two random variables
x1 x2 sum x1+x2
bernoulli( p) bernoulli( p) binomial(2 , p)
binomial( n, p) binomial( m, p) binomial( m+n, p)
poisson( Œª1) poisson( Œª2) poisson( Œª1+Œª2)
exponential( Œª) exponential( Œª) erlang(2 , Œª)
gaussian( ¬µ1, œÉ2
1) gaussian( ¬µ2, œÉ2
2) gaussian( ¬µ1+¬µ2, œÉ2
1+œÉ2
2)
fourier transform table
f(œâ) =z‚àû
‚àí‚àûf(t)e‚àíjœâtdt.
f(t)‚Üê‚Üíf(œâ) f(t)‚Üê‚Üíf(œâ)
1. e‚àíatu(t)‚Üê‚Üí1
a+jœâ,a >0 10. sinc2wt
2
‚Üê‚Üí2œÄ
w‚àÜœâ
2w
2. eatu(‚àít)‚Üê‚Üí1
a‚àíjœâ,a >0 11. e‚àíatsin(œâ0t)u(t)‚Üê‚Üíœâ0
(a+jœâ)2+œâ2
0,a >0
3. e‚àía|t|‚Üê‚Üí2a
a2+œâ2,a >0 12. e‚àíatcos(œâ0t)u(t)‚Üê‚Üía+jœâ
(a+jœâ)2+œâ2
0,a >0
4.a2
a2+t2‚Üê‚ÜíœÄae‚àía|œâ|,a >0 13. expn
‚àít2
2œÉ2o
‚Üê‚Üí‚àö
2œÄœÉexpn
‚àíœÉ2œâ2
2o
5. te‚àíatu(t)‚Üê‚Üí1
(a+jœâ)2,a >0 14. Œ¥(t)‚Üê‚Üí1
6.tne‚àíatu(t)‚Üê‚Üín!
(a+jœâ)n+1,a >0 15. 1 ‚Üê‚Üí2œÄŒ¥(œâ)
7. rectt
œÑ
‚Üê‚ÜíœÑsincœâœÑ
2
16. Œ¥(t‚àít0)‚Üê‚Üíe‚àíjwt0
8. sinc( wt)‚Üê‚ÜíœÄ
wrectœâ
2w
17. ejœâ0t‚Üê‚Üí2œÄŒ¥(œâ‚àíœâ0)
9. ‚àÜt
œÑ
‚Üê‚ÜíœÑ
2sinc2œâœÑ
4
18. f(t)ejœâ0t‚Üê‚Üíf(œâ‚àíœâ0)
some definitions:
sinc(t) =sin(t)
t
rect(t) =(
1,‚àí0.5‚â§t‚â§0.5,
0, otherwise .
‚àÜ(t) =(
1‚àí2|t|,‚àí0.5‚â§t‚â§0.5,
0, otherwise .
682basic trigonometric identities
ejŒ∏= cos Œ∏+jsinŒ∏
sin 2Œ∏= 2 sin Œ∏cosŒ∏
cos 2Œ∏= 2 cos2Œ∏‚àí1
cosacosb=1
2(cos(a+b) + cos( a‚àíb))
sinasinb=‚àí1
2(cos(a+b)‚àícos(a‚àíb))
sinacosb=1
2(sin(a+b) + sin( a‚àíb))
cosasinb=1
2(sin(a+b)‚àísin(a‚àíb))
cos(a+b) = cos acosb‚àísinasinb
cos(a‚àíb) = cos acosb+ sin asinb
sin(a+b) = sin acosb+ cos asinb
sin(a‚àíb) = sin acosb‚àícosasinb
683index
absolutely integrable, 183
almost sure convergence, 362
autocorrelation function
2d visualization, 622
interpretation, 623, 633
lti system, 644
properties, 632
temporal average, 635
definition, 618
matlab and python, 625
autocovariance function
definition, 618
relation to autocorrelation function, 628
autoregressive model, 406, 658
linear prediction, 658
matlab and python, 407, 659
prediction, 660
toeplitz, 659
yule-walker equation, 658
basel problem, 5
basis functions, 405
bayes‚Äô theorem, 89
conditional probability, 81
law of total probability, 90
bayesian, 43
bernoulli random variable
definition, 137
matlab and python, 137
maximum variance, 140
properties, 138
bias-variance
average predictor, 433
matlab and python, 434
noise-free case, 430
noisy case, 433
trade off, 429
binomial random variable
alternative definition, 148definition, 143
matlab and python, 144
properties, 146
binomial series, 6
binomial theorem, 6
proof, 9
birthday paradox, 31, 321
bootstrapping, 559
bootstrapped distribution, 562
confidence interval, 559
definition, 559
distribution of samples, 560
interpretation, 564
matlab and python, 565
procedure, 562
standard error, 565
when to use, 560
cauchy distribution, 331, 360
cauchy-schwarz inequality, 261, 335
central limit theorem, 323, 366, 372, 381
berry-esseen theorem, 375
examples, 376
interpretation, 375
limitations, 378
proof, 374
characteristic function, 329
alternative definition, 329
fourier transform, 330
chebyshev‚Äôs inequality, 341
proof, 342
chernoff‚Äôs bound, 343
compare with chebyshev, 344
chernoff, herman, 343
combination, 34
concave function, 336
conditional distribution
conditional expectation, 275
conditional pdf, 271
684index
conditional pmf, 267
conditional probability, 81
bayes‚Äô theorem, 89
definition, 81
independence, 85
properties, 84
ratio, 81
confidence interval, 541
bootstrapping, 559
critical value, 552
definition, 546
distribution of estimator, 544
estimator, 543
examples, 547
how to construct, 548
interpretation, 545
margin of error, 552
matlab and python, 550
number of samples, 553
properties, 551
standard error, 551
student‚Äôs t-distribution, 554
conjugate prior, 513
convergence in distribution, 367
convergence in probability, 356
convex function, 336
convex optimization
cvxpy, 451
convolution, 220, 639
correlation, 639
filtering, 639
correlation, 633
autocorrelation function, 618
autocovariance function, 618
cross-correlation function, 649
convolution, 639
correlation coefficient
matlab and python, 265
properties, 263
definition, 263
cosine angle, 26
covariance, 261
covariance matrix, 289
independent, 289
cross power spectral density, 651
cross-correlation function
cross-covariance function, 629
definition, 629examples, 650
through lti systems, 649
cross-covariance function, 629
cross-correlation function, 629
cumulative distribution function
continuous, 186
discrete, 121
left- and right-continuous, 190
matlab and python, 186
properties, 188
delta function, 178
discrete cosine transform (dct), 23
eigenvalues and eigenvectors, 295
gaussian, 296
matlab and python, 296
erdÀù os-r¬¥ enyi graph, 140
matlab and python, 480
even functions, 15
event, 61
event space, 61
expectation, 104
continuous, 180
properties, 130, 182
transformation, 182
center of mass, 127
discrete, 125
existence, 130, 183
exponential random variables
definition, 205
matlab and python, 205
origin, 207, 209
properties, 206
exponential series, 12
field, 64
œÉ-field, 65
borel œÉ-field, 65
fourier transform, 644
table, 330
characteristic function, 330
frequentist, 43
fundamental theorem of calculus, 17
chain rule, 19
proof, 18
gaussian random variables
cdf, 214
685index
definition, 211
matlab and python, 212
origin, 220
properties, 212
standard gaussian, 213
geometric random variable
definition, 149
matlab and python, 150
properties, 151
geometric sequence
finite, 4
infinite, 4
geometric series, 3
finite, 4
infinite, 4
harmonic series, 5
histogram, 2, 113
hoeffding‚Äôs inequality, 348
hoeffding lemma, 348
proof, 348
hypothesis testing
p-value test, 567, 571
t-test, 574
z-test, 574
alternative hypothesis, 566
critical level, 569
critical-value test, 567
definition, 566
matlab and python, 568
null hypothesis, 566
impulse response, 643
independence, 85
conditional probability, 88
versus disjoint, 86
independent
random variables, 251
independent and identically distributed (i.i.d.),
253
indicator function, 182
inner product, 24
matlab and python, 24
jensen‚Äôs inequality, 336
proof, 338
joint distribution
definition, 241
joint cdf, 255joint pdf, 247
joint pmf, 245
joint expectation, 257
cosine angle, 258
kurtosis, 216
matlab and python, 217
laplace transform, 324
law of large numbers, 323, 351, 381
strong law of large numbers, 360
weak law of large numbers, 354
learning curve, 427
matlab and python, 427
legendre polynomial, 403
matlab and python, 404
likelihood, 466, 468, 503
log-likelihood, 470
linear algebra
basis vector, 23
representation, 23
span, 22
standard basis vector, 22
linear combination, 21
linear model, 21
linear prediction, 658
linear programming, 414
linear regression
matlab and python, 30
linear time-invariant (lti)
convolution, 639
definition, 643
system, 643
marginal distribution, 250
markov‚Äôs inequality, 339
proof, 339
tight, 341
matrix calculus, 28
maximum-a-posteriori (map), 502
choosing prior, 505
conjugate prior, 513
map versus lasso, 519
map versus ml, 504
map versus regression, 517
map versus ridge, 518
posterior, 503, 511
prior, 503
solution, 506
686index
maximum-likelihood
1d gaussian, 484
consistent estimator, 494
estimation, 468
estimator, 491
high-dimensional gaussian, 486
image reconstruction, 481
independent observations, 469
invariance principle, 500
matlab and python, 472
number of training samples, 475
poisson, 485
regression versus ml, 487
social networks, 478
unbiased estimator, 492
visualization, 471
mean, 199
mean function
lti system, 644
definition, 618
matlab and python, 621
mean squared error (mse), 520, 522
measure, 68
almost surely, 73
finite sets, 68
intervals, 68
lebesgue integration, 71
measure zero sets, 71
definition, 72
examples, 72
regions, 68
size, 69
median, 196
minimum mean-square estimation (mmse),
520
conditional expectation, 523
gaussian, 529
minimum-norm least squares, 411
mode, 198
model selection, 165
moment, 133
continuous case, 184
moment-generating function, 322, 324
common distributions, 326
derivative, 325
existence, 331
sum of random variables, 327
multidimensional gaussian, 290matlab and python, 291
covariance, 293
transformation, 293
whitening, 299
neyman-pearson test, 577
decision rule, 582
likelihood ratio, 584
rejection zone, 578
likelihood ratio test, 578
norm, 24, 26
‚Ñì1, 27
‚Ñì‚àû, 27
matlab and python, 26
weighted, 27
normalization property, 112
odd functions, 15
open and closed intervals, 45
optimal linear filter, 653
deconvolution, 665
denoising, 662
orthogonality condition, 658
wiener filter, 661
yule-walker equation, 656
input function, 654
prediction, 654
target function, 654
orthogonality condition, 658
overdetermined system, 409
overfitting, 418
factors, 420
lasso, 454
linear analysis, 425
source, 429
parameter estimation, 165, 465
pascal triangle, 8
pascal‚Äôs identity, 7
performance guarantee
average case, 321
worst case, 321
permutation, 33
poisson random variable
applications, 154
definition, 152
origin, 157
photon arrivals, 161
poisson approximation of binomial, 159
687index
properties, 155
matlab and python, 152
positive semi-definite, 297
posterior, 466, 503
power spectral density, 636
einstein-wiener-khinchin theorem, 636
through lti systems, 646
cross power spectral density, 640, 651
eigendecomposition, 639
fourier transform, 640
origin, 640
wide-sense stationary, 639
pr (precision-recall) curve
definition, 601
matlab and python, 603
precision, 601
recall, 601
principal-component analysis, 303
limitations, 311
main idea, 303
matlab and python, 306
prior, 466, 503
probability, 43, 45
measure of a set, 43
probability axioms, 74
additivity, 75
corollaries, 77
countable additivity, 75
measure, 76
non-negativity, 75
normalization, 75
probability density function, 172
definition, 175
discrete cases, 178
properties, 174
intuition, 172
per unit length, 173
probability inequality, 323, 333
probability law, 66
definition, 66
examples, 66
measure, 67
probability mass function, 104, 110
probability space
(œâ,f,p), 58
rademacher random variable, 140
random number generator, 229random process
discrete time, 653
definition, 612
example
random amplitude, 612
random phase, 613
function, 612
independent, 629
index, 612
sample space, 614
statistical average, 614
temporal average, 614
uncorrelated, 630
random variable, 104, 105
function of, 223
transformation of, 223
random vector, 286
expectation, 288
independent, 286
regression, 391, 394
loss, 394
matlab and python, 400
outliers, 412
prediction model, 394
solution, 397
linear model, 395
outliers, 417
squared error, 396
regularization, 440
lasso, 449
matlab and python, 442
parameter, 445
ridge, 440
sparse solution, 449
robust linear regression, 412
matlab and python, 416
linear programming, 414
roc
comparing performance, 597
computation, 592
definition, 589
matlab and python, 593
properties, 591
receiver operating characteristic, 589
sample average, 320, 351
sample space, 59
continuous outcomes, 59
688index
counterexamples, 61
discrete outcomes, 59
examples, 59
exclusive, 61
exhaustive, 61
functions, 59
set, 45
associative, 56
commutative, 56
complement, 52
countable, 45
de morgan‚Äôs law, 57
difference, 53
disjoint, 54
distributive, 56
empty set, 48
finite, 45
improper subset, 47
infinite, 45
intersection, 50
finite, 50
infinite, 51
of functions, 46
partition, 55
proper subset, 47
subset, 47
uncountable, 45
union, 48
finite, 48
infinite, 49
universal set, 48
simplex method, 414
skewness, 216
matlab and python, 217
statistic, 320
student‚Äôs t-distribution
definition, 554
degrees of freedom, 555
matlab and python, 556
relation to gaussian, 555
sum of random variables, 280
bernoulli, 327
binomial, 328
gaussian, 283, 329
poisson, 328
common distributions, 282
convolution, 281
symmetric matrices, 296taylor approximation, 11
first-order, 11
second-order, 11
exponential, 12
logarithmic, 13
testing error, 420
analysis, 424
testing set, 420
three prisoners problem, 92
toeplitz, 407, 630
training error, 420
analysis, 421
training set, 420
type 1 error
definition, 579
false alarm, 580
false positive, 579
power of test, 581
type 2 error
definition, 579
false negative, 579
miss, 580
underdetermined system, 409
uniform random variables, 202
matlab and python, 203
union bound, 333
validation, 165
variance, 134
properties, 135
continuous case, 184
white noise, 638
wide-sense stationary, 630
jointly, 649
wiener filter, 661
deconvolution, 665
definition, 661
denoising, 662
matlab and python, 661
power spectral density, 662
recursive filter, 661
yule-walker equation, 656
matlab and python, 659
689