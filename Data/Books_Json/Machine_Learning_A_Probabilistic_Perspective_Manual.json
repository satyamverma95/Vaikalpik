{
  "1": {
    "Title": "Introduction",
    "Sub Topics": {
      "1.1": {
        "Title": "Machine learning: what and why?",
        "Sub Topics": {
          "1.1.1": {
            "Title": "Types of machine learning"
          }
        }
      },
      "1.2": {
        "Title": "Supervised learning",
        "Sub Topics": {
          "1.2.1": {
            "Title": "Classification"
          },
          "1.2.2": {
            "Title": "Regression"
          }
        }
      },
      "1.3": {
        "Title": "Unsupervised learning",
        "Sub Topics": {
          "1.3.1": {
            "Title": "Discovering clusters"
          },
          "1.3.2": {
            "Title": "Discovering latent factors"
          },
          "1.3.3": {
            "Title": "Discovering graph structure"
          },
          "1.3.4": {
            "Title": "Matrix completion"
          }
        }
      },
      "1.4": {
        "Title": "Some basic concepts in machine learning",
        "Sub Topics": {
          "1.4.1": {
            "Title": "Parametric vs non-parametric models"
          },
          "1.4.2": {
            "Title": "A simple non-parametric classifier: K-nearest neighbors"
          },
          "1.4.3": {
            "Title": "The curse of dimensionality"
          },
          "1.4.4": {
            "Title": "Parametric models for classification and regression"
          },
          "1.4.5": {
            "Title": "Linear regression"
          },
          "1.4.6": {
            "Title": "Logistic regression"
          },
          "1.4.7": {
            "Title": "Overfitting"
          },
          "1.4.8": {
            "Title": "Model selection"
          },
          "1.4.9": {
            "Title": "No free lunch theorem"
          }
        }
      }
    }
  },
  "2": {
    "Title": "Probability",
    "Sub Topics": {
      "2.1": {
        "Title": "Introduction",
        "Sub Topics": {}
      },
      "2.2": {
        "Title": "A brief review of probability theory",
        "Sub Topics": {
          "2.2.1": {
            "Title": "Discrete random variables"
          },
          "2.2.2": {
            "Title": "Fundamental rules"
          },
          "2.2.3": {
            "Title": "Bayes rule"
          },
          "2.2.4": {
            "Title": "Independence and conditional independence"
          },
          "2.2.5": {
            "Title": "Continuous random variables"
          },
          "2.2.6": {
            "Title": "Quantiles"
          },
          "2.2.7": {
            "Title": "Mean and variance"
          }
        }
      },
      "2.3": {
        "Title": "Some common discrete distributions",
        "Sub Topics": {
          "2.3.1": {
            "Title": "The binomial and Bernoulli distributions"
          },
          "2.3.2": {
            "Title": "The multinomial and multinoulli distributions"
          },
          "2.3.3": {
            "Title": "The Poisson distribution"
          },
          "2.3.4": {
            "Title": "The empirical distribution"
          }
        }
      },
      "2.4": {
        "Title": "Some common continuous distributions",
        "Sub Topics": {
          "2.4.1": {
            "Title": "Gaussian (normal) distribution"
          },
          "2.4.2": {
            "Title": "Degenerate pdf"
          },
          "2.4.3": {
            "Title": "The Laplace distribution"
          },
          "2.4.4": {
            "Title": "The gamma distribution"
          },
          "2.4.5": {
            "Title": "The beta distribution"
          },
          "2.4.6": {
            "Title": "Pareto distribution"
          }
        }
      },
      "2.5": {
        "Title": "Joint probability distributions",
        "Sub Topics": {
          "2.5.1": {
            "Title": "Covariance and correlation"
          },
          "2.5.2": {
            "Title": "The multivariate Gaussian"
          },
          "2.5.3": {
            "Title": "Multivariate Student t distribution"
          },
          "2.5.4": {
            "Title": "Dirichlet distribution"
          }
        }
      },
      "2.6": {
        "Title": "Transformations of random variables",
        "Sub Topics": {
          "2.6.1": {
            "Title": "Linear transformations"
          },
          "2.6.2": {
            "Title": "General transformations"
          },
          "2.6.3": {
            "Title": "Central limit theorem"
          }
        }
      },
      "2.7": {
        "Title": "Monte Carlo approximation",
        "Sub Topics": {
          "2.7.1": {
            "Title": "Example: change of variables, the MC way"
          },
          "2.7.2": {
            "Title": "Example: estimating \u03c0 by Monte Carlo integration"
          },
          "2.7.3": {
            "Title": "Accuracy of Monte Carlo approximation"
          }
        }
      },
      "2.8": {
        "Title": "Information theory",
        "Sub Topics": {
          "2.8.1": {
            "Title": "Entropy"
          },
          "2.8.2": {
            "Title": "KL divergence 57283 Mutual information"
          },
          "2.8.3": {
            "Title": "Mutual information"
          }
        }
      }
    }
  },
  "3": {
    "Title": "Generative models for discrete data",
    "Sub Topics": {
      "3.1": {
        "Title": "Introduction",
        "Sub Topics": {}
      },
      "3.2": {
        "Title": "Bayesian concept learning",
        "Sub Topics": {
          "3.2.1": {
            "Title": "Likelihood"
          },
          "3.2.2": {
            "Title": "Prior"
          },
          "3.2.3": {
            "Title": "Posterior"
          },
          "3.2.4": {
            "Title": "Posterior predictive distribution"
          },
          "3.2.5": {
            "Title": "A more complex prior"
          }
        }
      },
      "3.3": {
        "Title": "The beta-binomial model",
        "Sub Topics": {
          "3.3.1": {
            "Title": "Likelihood"
          },
          "3.3.2": {
            "Title": "Prior"
          },
          "3.3.3": {
            "Title": "Posterior"
          },
          "3.3.4": {
            "Title": "Posterior predictive distribution"
          }
        }
      },
      "3.4": {
        "Title": "The Dirichlet-multinomial model",
        "Sub Topics": {
          "3.4.1": {
            "Title": "Likelihood 79342 Prior 79343 Posterior 79344 Posterior predictive"
          }
        }
      },
      "3.5": {
        "Title": "Naive Bayes classifiers",
        "Sub Topics": {
          "3.5.1": {
            "Title": "Model fitting"
          },
          "3.5.2": {
            "Title": "Using the model for prediction"
          },
          "3.5.3": {
            "Title": "The log-sum-exp trick"
          },
          "3.5.4": {
            "Title": "Feature selection using mutual information"
          },
          "3.5.5": {
            "Title": "Classifying documents using bag of words"
          }
        }
      }
    }
  },
  "4": {
    "Title": "Gaussian models",
    "Sub Topics": {
      "4.1": {
        "Title": "Introduction",
        "Sub Topics": {
          "4.1.1": {
            "Title": "Notation"
          },
          "4.1.2": {
            "Title": "Basics"
          },
          "4.1.3": {
            "Title": "MLE for an MVN"
          },
          "4.1.4": {
            "Title": "Maximum entropy derivation of the Gaussian *"
          }
        }
      },
      "4.2": {
        "Title": "Gaussian discriminant analysis",
        "Sub Topics": {
          "4.2.1": {
            "Title": "Quadratic discriminant analysis (QDA) 102422 Linear discriminant analysis (LDA) 103423 Two-class LDA 104424 MLE for discriminant analysis 106425 Strategies for preventing overfitting 106426 Regularized LDA * 107427 Diagonal LDA"
          },
          "4.2.8": {
            "Title": "Nearest shrunken centroids classifier *"
          }
        }
      },
      "4.3": {
        "Title": "Inference in jointly Gaussian distributions",
        "Sub Topics": {
          "4.3.1": {
            "Title": "Statement of the result 111432 Examples 111433 Information form 115434 Proof of the result *"
          }
        }
      },
      "4.4": {
        "Title": "Linear Gaussian systems",
        "Sub Topics": {
          "4.4.1": {
            "Title": "Statement of the result 119"
          },
          "4.4.2": {
            "Title": "Examples 120"
          },
          "4.4.3": {
            "Title": "Proof of the result * 124"
          }
        }
      },
      "4.5": {
        "Title": "Digression: The Wishart distribution *",
        "Sub Topics": {
          "4.5.1": {
            "Title": "Inverse Wishart distribution 126"
          },
          "4.5.2": {
            "Title": "Visualizing the Wishart distribution * 127"
          }
        }
      },
      "4.6": {
        "Title": "Inferring the parameters of an MVN",
        "Sub Topics": {
          "4.6.1": {
            "Title": "Posterior distribution of \u03bc 128"
          },
          "4.6.2": {
            "Title": "Posterior distribution of \u03a3 * 128"
          },
          "4.6.3": {
            "Title": "Posterior distribution of \u03bc and \u03a3 * 132"
          },
          "4.6.4": {
            "Title": "Sensor fusion with unknown precisions * 138"
          }
        }
      }
    }
  },
  "5": {
    "Title": "Bayesian statistics",
    "Sub Topics": {
      "5.1": {
        "Title": "Introduction 149"
      },
      "5.2": {
        "Title": "Summarizing posterior distributions 149",
        "Sub Topics": {
          "5.2.1": {
            "Title": "MAP estimation 149"
          },
          "5.2.2": {
            "Title": "Credible intervals 152"
          },
          "5.2.3": {
            "Title": "Inference for a difference in proportions 154"
          }
        }
      },
      "5.3": {
        "Title": "Bayesian model selection 155",
        "Sub Topics": {
          "5.3.1": {
            "Title": "Bayesian Occam\u2019s razor 156"
          },
          "5.3.2": {
            "Title": "Computing the marginal likelihood (evidence) 158"
          },
          "5.3.3": {
            "Title": "Bayes factors 163"
          },
          "5.3.4": {
            "Title": "Jeffreys-Lindley paradox * 164"
          }
        }
      },
      "5.4": {
        "Title": "Priors 165",
        "Sub Topics": {
          "5.4.1": {
            "Title": "Uninformative priors 165"
          },
          "5.4.2": {
            "Title": "Jeffreys priors * 166"
          },
          "5.4.3": {
            "Title": "Robust priors 168"
          },
          "5.4.4": {
            "Title": "Mixtures of conjugate priors 168"
          }
        }
      },
      "5.5": {
        "Title": "Hierarchical Bayes 171",
        "Sub Topics": {
          "5.5.1": {
            "Title": "Example: modeling related cancer rates 171"
          }
        }
      },
      "5.6": {
        "Title": "Empirical Bayes 172",
        "Sub Topics": {
          "5.6.1": {
            "Title": "Example: beta-binomial model 173"
          },
          "5.6.2": {
            "Title": "Example: Gaussian-Gaussian model 173"
          }
        }
      },
      "5.7": {
        "Title": "Bayesian decision theory 176",
        "Sub Topics": {
          "5.7.1": {
            "Title": "Bayes estimators for common loss functions 177"
          },
          "5.7.2": {
            "Title": "The false positive vs false negative tradeoff 180"
          },
          "5.7.3": {
            "Title": "Other topics * 184"
          }
        }
      }
    }
  },
  "6": {
    "Title": "Frequentist statistics",
    "Sub Topics": {
      "6.1": {
        "Title": "Introduction"
      },
      "6.2": {
        "Title": "Sampling distribution of an estimator",
        "Sub Topics": {
          "6.2.1": {
            "Title": "Bootstrap"
          },
          "6.2.2": {
            "Title": "Large sample theory for the MLE *"
          }
        }
      },
      "6.3": {
        "Title": "Frequentist decision theory",
        "Sub Topics": {
          "6.3.1": {
            "Title": "Bayes risk"
          },
          "6.3.2": {
            "Title": "Minimax risk"
          },
          "6.3.3": {
            "Title": "Admissible estimators"
          }
        }
      },
      "6.4": {
        "Title": "Desirable properties of estimators",
        "Sub Topics": {
          "6.4.1": {
            "Title": "Consistent estimators"
          },
          "6.4.2": {
            "Title": "Unbiased estimators"
          },
          "6.4.3": {
            "Title": "Minimum variance estimators"
          },
          "6.4.4": {
            "Title": "The bias-variance tradeoff"
          }
        }
      },
      "6.5": {
        "Title": "Empirical risk minimization",
        "Sub Topics": {
          "6.5.1": {
            "Title": "Regularized risk minimization"
          },
          "6.5.2": {
            "Title": "Structural risk minimization"
          },
          "6.5.3": {
            "Title": "Estimating the risk using cross validation"
          },
          "6.5.4": {
            "Title": "Upper bounding the risk using statistical learning theory *"
          },
          "6.5.5": {
            "Title": "Surrogate loss functions"
          }
        }
      },
      "6.6": {
        "Title": "Pathologies of frequentist statistics *",
        "Sub Topics": {
          "6.6.1": {
            "Title": "Counter-intuitive behavior of confidence intervals"
          },
          "6.6.2": {
            "Title": "p-values considered harmful"
          },
          "6.6.3": {
            "Title": "The likelihood principle"
          },
          "6.6.4": {
            "Title": "Why isn\u2019t everyone a Bayesian?"
          }
        }
      }
    }
  },
  "7": {
    "Title": "Linear regression",
    "Sub Topics": {
      "7.1": {
        "Title": "Introduction"
      },
      "7.2": {
        "Title": "Model specification"
      },
      "7.3": {
        "Title": "Maximum likelihood estimation (least squares)",
        "Sub Topics": {
          "7.3.1": {
            "Title": "Derivation of the MLE"
          },
          "7.3.2": {
            "Title": "Geometric interpretation"
          },
          "7.3.3": {
            "Title": "Convexity"
          }
        }
      },
      "7.4": {
        "Title": "Robust linear regression *"
      },
      "7.5": {
        "Title": "Ridge regression",
        "Sub Topics": {
          "7.5.1": {
            "Title": "Basic idea"
          },
          "7.5.2": {
            "Title": "Numerically stable computation *"
          },
          "7.5.3": {
            "Title": "Connection with PCA *"
          },
          "7.5.4": {
            "Title": "Regularization effects of big data"
          }
        }
      },
      "7.6": {
        "Title": "Bayesian linear regression",
        "Sub Topics": {
          "7.6.1": {
            "Title": "Computing the posterior"
          },
          "7.6.2": {
            "Title": "Computing the posterior predictive"
          },
          "7.6.3": {
            "Title": "Bayesian inference when \u03c32 is unknown *"
          },
          "7.6.4": {
            "Title": "EB for linear regression (evidence procedure)"
          }
        }
      }
    }
  },
  "8": {
    "Title": "Logistic regression",
    "Sub Topics": {
      "8.1": {
        "Title": "Introduction"
      },
      "8.2": {
        "Title": "Model specification"
      },
      "8.3": {
        "Title": "Model fitting",
        "Sub Topics": {
          "8.3.1": {
            "Title": "MLE"
          },
          "8.3.2": {
            "Title": "Steepest descent"
          },
          "8.3.3": {
            "Title": "Newton\u2019s method"
          },
          "8.3.4": {
            "Title": "Iteratively reweighted least squares (IRLS)"
          },
          "8.3.5": {
            "Title": "Quasi-Newton (variable metric) methods"
          },
          "8.3.6": {
            "Title": "L2 regularization"
          },
          "8.3.7": {
            "Title": "Multi-class logistic regression"
          }
        }
      },
      "8.4": {
        "Title": "Bayesian logistic regression",
        "Sub Topics": {
          "8.4.1": {
            "Title": "Laplace approximation"
          },
          "8.4.2": {
            "Title": "Derivation of the BIC"
          },
          "8.4.3": {
            "Title": "Gaussian approximation for logistic regression"
          },
          "8.4.4": {
            "Title": "Approximating the posterior predictive"
          },
          "8.4.5": {
            "Title": "Residual analysis (outlier detection) *"
          }
        }
      },
      "8.5": {
        "Title": "Online learning and stochastic optimization",
        "Sub Topics": {
          "8.5.1": {
            "Title": "Online learning and regret minimization"
          },
          "8.5.2": {
            "Title": "Stochastic optimization and risk minimization"
          },
          "8.5.3": {
            "Title": "The LMS algorithm"
          },
          "8.5.4": {
            "Title": "The perceptron algorithm"
          },
          "8.5.5": {
            "Title": "A Bayesian view"
          }
        }
      },
      "8.6": {
        "Title": "Generative vs discriminative classifiers",
        "Sub Topics": {
          "8.6.1": {
            "Title": "Pros and cons of each approach"
          },
          "8.6.2": {
            "Title": "Dealing with missing data"
          },
          "8.6.3": {
            "Title": "Fisher\u2019s linear discriminant analysis (FLDA) *"
          }
        }
      }
    }
  },
  "9": {
    "Title": "Generalized linear models and the exponential family",
    "Sub Topics": {
      "9.1": {
        "Title": "Introduction"
      },
      "9.2": {
        "Title": "The exponential family",
        "Sub Topics": {
          "9.2.1": {
            "Title": "Definition"
          },
          "9.2.2": {
            "Title": "Examples"
          },
          "9.2.3": {
            "Title": "Log partition function"
          },
          "9.2.4": {
            "Title": "MLE for the exponential family"
          },
          "9.2.5": {
            "Title": "Bayes for the exponential family *"
          },
          "9.2.6": {
            "Title": "Maximum entropy derivation of the exponential family *"
          }
        }
      },
      "9.3": {
        "Title": "Generalized linear models (GLMs)",
        "Sub Topics": {
          "9.3.1": {
            "Title": "Basics"
          },
          "9.3.2": {
            "Title": "ML and MAP estimation"
          },
          "9.3.3": {
            "Title": "Bayesian inference"
          }
        }
      },
      "9.4": {
        "Title": "Probit regression",
        "Sub Topics": {
          "9.4.1": {
            "Title": "ML/MAP estimation using gradient-based optimization"
          },
          "9.4.2": {
            "Title": "Latent variable interpretation"
          },
          "9.4.3": {
            "Title": "Ordinal probit regression *"
          },
          "9.4.4": {
            "Title": "Multinomial probit models *"
          }
        }
      },
      "9.5": {
        "Title": "Multi-task learning",
        "Sub Topics": {
          "9.5.1": {
            "Title": "Hierarchical Bayes for multi-task learning"
          },
          "9.5.2": {
            "Title": "Application to personalized email spam filtering"
          },
          "9.5.3": {
            "Title": "Application to domain adaptation"
          },
          "9.5.4": {
            "Title": "Other kinds of prior"
          }
        }
      },
      "9.6": {
        "Title": "Generalized linear mixed models *",
        "Sub Topics": {
          "9.6.1": {
            "Title": "Example: semi-parametric GLMMs for medical data"
          },
          "9.6.2": {
            "Title": "Computational issues"
          }
        }
      },
      "9.7": {
        "Title": "Learning to rank *",
        "Sub Topics": {
          "9.7.1": {
            "Title": "The pointwise approach"
          },
          "9.7.2": {
            "Title": "The pairwise approach"
          },
          "9.7.3": {
            "Title": "The listwise approach"
          },
          "9.7.4": {
            "Title": "Loss functions for ranking"
          }
        }
      }
    }
  },
  "10": {
    "Title": "Directed graphical models (Bayes nets)",
    "Sub Topics": {
      "10.1": {
        "Title": "Introduction",
        "Sub Topics": {
          "10.1.1": {
            "Title": "Chain rule"
          },
          "10.1.2": {
            "Title": "Conditional independence"
          },
          "10.1.3": {
            "Title": "Graphical models"
          },
          "10.1.4": {
            "Title": "Graph terminology"
          },
          "10.1.5": {
            "Title": "Directed graphical models"
          }
        }
      },
      "10.2": {
        "Title": "Examples",
        "Sub Topics": {
          "10.2.1": {
            "Title": "Naive Bayes classifiers"
          },
          "10.2.2": {
            "Title": "Markov and hidden Markov models"
          },
          "10.2.3": {
            "Title": "Medical diagnosis"
          },
          "10.2.4": {
            "Title": "Genetic linkage analysis *"
          },
          "10.2.5": {
            "Title": "Directed Gaussian graphical models *"
          }
        }
      },
      "10.3": {
        "Title": "Inference"
      },
      "10.4": {
        "Title": "Learning",
        "Sub Topics": {
          "10.4.1": {
            "Title": "Plate notation"
          },
          "10.4.2": {
            "Title": "Learning from complete data"
          },
          "10.4.3": {
            "Title": "Learning with missing and/or latent variables"
          }
        }
      },
      "10.5": {
        "Title": "Conditional independence properties of DGMs",
        "Sub Topics": {
          "10.5.1": {
            "Title": "d-separation and the Bayes Ball algorithm (global Markov properties)"
          },
          "10.5.2": {
            "Title": "Other Markov properties of DGMs"
          },
          "10.5.3": {
            "Title": "Markov blanket and full conditionals"
          }
        }
      },
      "10.6": {
        "Title": "Influence (decision) diagrams *"
      }
    }
  },
  "11": {
    "Title": "Mixture models and the EM algorithm",
    "Sub Topics": {
      "11.1": {
        "Title": "Latent variable models"
      },
      "11.2": {
        "Title": "Mixture models",
        "Sub Topics": {
          "11.2.1": {
            "Title": "Mixtures of Gaussians"
          },
          "11.2.2": {
            "Title": "Mixture of multinoullis"
          },
          "11.2.3": {
            "Title": "Using mixture models for clustering"
          },
          "11.2.4": {
            "Title": "Mixtures of experts"
          }
        }
      },
      "11.3": {
        "Title": "Parameter estimation for mixture models",
        "Sub Topics": {
          "11.3.1": {
            "Title": "Unidentifiability"
          },
          "11.3.2": {
            "Title": "Computing a MAP estimate is non-convex"
          }
        }
      },
      "11.4": {
        "Title": "The EM algorithm",
        "Sub Topics": {
          "11.4.1": {
            "Title": "Basic idea"
          },
          "11.4.2": {
            "Title": "EM for GMMs"
          },
          "11.4.3": {
            "Title": "EM for mixture of experts"
          },
          "11.4.4": {
            "Title": "EM for DGMs with hidden variables"
          },
          "11.4.5": {
            "Title": "EM for the Student distribution *"
          },
          "11.4.6": {
            "Title": "EM for probit regression *"
          },
          "11.4.7": {
            "Title": "Theoretical basis for EM *"
          },
          "11.4.8": {
            "Title": "Online EM"
          },
          "11.4.9": {
            "Title": "Other EM variants *"
          }
        }
      },
      "11.5": {
        "Title": "Model selection for latent variable models",
        "Sub Topics": {
          "11.5.1": {
            "Title": "Model selection for probabilistic models"
          },
          "11.5.2": {
            "Title": "Model selection for non-probabilistic methods"
          }
        }
      },
      "11.6": {
        "Title": "Fitting models with missing data",
        "Sub Topics": {
          "11.6.1": {
            "Title": "EM for the MLE of an MVN with missing data"
          }
        }
      }
    }
  },
  "12": {
    "Title": "Latent linear models",
    "Sub Topics": {
      "12.1": {
        "Title": "Factor analysis",
        "Sub Topics": {
          "12.1.1": {
            "Title": "FA is a low rank parameterization of an MVN 381"
          },
          "12.1.2": {
            "Title": "Inference of the latent factors 382"
          },
          "12.1.3": {
            "Title": "Unidentifiability 383"
          },
          "12.1.4": {
            "Title": "Mixtures of factor analysers 385"
          },
          "12.1.5": {
            "Title": "EM for factor analysis models 386"
          },
          "12.1.6": {
            "Title": "Fitting FA models with missing data 387"
          }
        }
      },
      "12.2": {
        "Title": "Principal components analysis (PCA)",
        "Sub Topics": {
          "12.2.1": {
            "Title": "Classical PCA: statement of the theorem 387"
          },
          "12.2.2": {
            "Title": "Proof * 389"
          },
          "12.2.3": {
            "Title": "Singular value decomposition (SVD) 392"
          },
          "12.2.4": {
            "Title": "Probabilistic PCA 395"
          },
          "12.2.5": {
            "Title": "EM algorithm for PCA 396"
          }
        }
      },
      "12.3": {
        "Title": "Choosing the number of latent dimensions",
        "Sub Topics": {
          "12.3.1": {
            "Title": "Model selection for FA/PPCA 398"
          },
          "12.3.2": {
            "Title": "Model selection for PCA 399"
          }
        }
      },
      "12.4": {
        "Title": "PCA for categorical data",
        "Sub Topics": {
          "12.4.1": {
            "Title": "Statement of the result 402"
          }
        }
      },
      "12.5": {
        "Title": "PCA for paired and multi-view data",
        "Sub Topics": {
          "12.5.1": {
            "Title": "Supervised PCA (latent factor regression) 405"
          },
          "12.5.2": {
            "Title": "Partial least squares 406"
          },
          "12.5.3": {
            "Title": "Canonical correlation analysis 407"
          }
        }
      },
      "12.6": {
        "Title": "Independent Component Analysis (ICA)",
        "Sub Topics": {
          "12.6.1": {
            "Title": "Maximum likelihood estimation 410"
          },
          "12.6.2": {
            "Title": "The FastICA algorithm 411"
          },
          "12.6.3": {
            "Title": "Using EM 414"
          },
          "12.6.4": {
            "Title": "Other estimation principles * 415"
          }
        }
      }
    }
  },
  "13": {
    "Title": "Sparse linear models",
    "Sub Topics": {
      "13.1": {
        "Title": "Introduction"
      },
      "13.2": {
        "Title": "Bayesian variable selection",
        "Sub Topics": {
          "13.2.1": {
            "Title": "The spike and slab model"
          },
          "13.2.2": {
            "Title": "From the Bernoulli-Gaussian model to L0 regularization"
          },
          "13.2.3": {
            "Title": "Algorithms"
          }
        }
      },
      "13.3": {
        "Title": "L1 regularization: basics",
        "Sub Topics": {
          "13.3.1": {
            "Title": "Why does L1 regularization yield sparse solutions?"
          },
          "13.3.2": {
            "Title": "Optimality conditions for lasso"
          },
          "13.3.3": {
            "Title": "Comparison of least squares, lasso, ridge and subset selection"
          },
          "13.3.4": {
            "Title": "Regularization path"
          },
          "13.3.5": {
            "Title": "Model selection"
          },
          "13.3.6": {
            "Title": "Bayesian inference for linear models with Laplace priors"
          }
        }
      },
      "13.4": {
        "Title": "L1 regularization: algorithms",
        "Sub Topics": {
          "13.4.1": {
            "Title": "Coordinate descent"
          },
          "13.4.2": {
            "Title": "LARS and other homotopy methods"
          },
          "13.4.3": {
            "Title": "Proximal and gradient projection methods"
          },
          "13.4.4": {
            "Title": "EM for lasso"
          }
        }
      },
      "13.5": {
        "Title": "L1 regularization: extensions",
        "Sub Topics": {
          "13.5.1": {
            "Title": "Group Lasso"
          },
          "13.5.2": {
            "Title": "Fused lasso"
          },
          "13.5.3": {
            "Title": "Elastic net (ridge and lasso combined)"
          }
        }
      },
      "13.6": {
        "Title": "Non-convex regularizers",
        "Sub Topics": {
          "13.6.1": {
            "Title": "Bridge regression"
          },
          "13.6.2": {
            "Title": "Hierarchical adaptive lasso"
          },
          "13.6.3": {
            "Title": "Other hierarchical priors"
          }
        }
      },
      "13.7": {
        "Title": "Automatic relevance determination (ARD)/sparse Bayesian learning (SBL)",
        "Sub Topics": {
          "13.7.1": {
            "Title": "ARD for linear regression"
          },
          "13.7.2": {
            "Title": "Whence sparsity?"
          },
          "13.7.3": {
            "Title": "Connection to MAP estimation"
          },
          "13.7.4": {
            "Title": "Algorithms for ARD *"
          },
          "13.7.5": {
            "Title": "ARD for logistic regression"
          }
        }
      },
      "13.8": {
        "Title": "Sparse coding *",
        "Sub Topics": {
          "13.8.1": {
            "Title": "Learning a sparse coding dictionary"
          },
          "13.8.2": {
            "Title": "Results of dictionary learning from image patches"
          },
          "13.8.3": {
            "Title": "Compressed sensing"
          },
          "13.8.4": {
            "Title": "Image inpainting and denoising"
          }
        }
      }
    }
  },
  "14": {
    "Title": "Kernel Methods",
    "Sub Topics": {
      "14.1": {
        "Title": "Introduction"
      },
      "14.2": {
        "Title": "Kernel functions",
        "Sub Topics": {
          "14.2.1": {
            "Title": "RBF kernels"
          },
          "14.2.2": {
            "Title": "Kernels for comparing documents"
          },
          "14.2.3": {
            "Title": "Mercer (positive definite) kernels"
          },
          "14.2.4": {
            "Title": "Linear kernels"
          },
          "14.2.5": {
            "Title": "Matern kernels"
          },
          "14.2.6": {
            "Title": "String kernels"
          },
          "14.2.7": {
            "Title": "Pyramid match kernels"
          },
          "14.2.8": {
            "Title": "Kernels derived from probabilistic generative models"
          }
        }
      },
      "14.3": {
        "Title": "Using kernels inside GLMs",
        "Sub Topics": {
          "14.3.1": {
            "Title": "Kernel machines"
          },
          "14.3.2": {
            "Title": "L1VMs, RVMs, and other sparse vector machines"
          }
        }
      },
      "14.4": {
        "Title": "The kernel trick",
        "Sub Topics": {
          "14.4.1": {
            "Title": "Kernelized nearest neighbor classification"
          },
          "14.4.2": {
            "Title": "Kernelized K-medoids clustering"
          },
          "14.4.3": {
            "Title": "Kernelized ridge regression"
          },
          "14.4.4": {
            "Title": "Kernel PCA"
          }
        }
      },
      "14.5": {
        "Title": "Support vector machines (SVMs)",
        "Sub Topics": {
          "14.5.1": {
            "Title": "SVMs for regression"
          },
          "14.5.2": {
            "Title": "SVMs for classification"
          },
          "14.5.3": {
            "Title": "Choosing C"
          },
          "14.5.4": {
            "Title": "Summary of key points"
          },
          "14.5.5": {
            "Title": "A probabilistic interpretation of SVMs"
          }
        }
      },
      "14.6": {
        "Title": "Comparison of discriminative kernel methods"
      },
      "14.7": {
        "Title": "Kernels for building generative models",
        "Sub Topics": {
          "14.7.1": {
            "Title": "Smoothing kernels"
          },
          "14.7.2": {
            "Title": "Kernel density estimation (KDE)"
          },
          "14.7.3": {
            "Title": "From KDE to KNN"
          },
          "14.7.4": {
            "Title": "Kernel regression"
          },
          "14.7.5": {
            "Title": "Locally weighted regression"
          }
        }
      }
    }
  },
  "15": {
    "Title": "Gaussian processes",
    "Sub Topics": {
      "15.1": {
        "Title": "Introduction 515"
      },
      "15.2": {
        "Title": "GPs for regression",
        "Sub Topics": {
          "15.2.1": {
            "Title": "Predictions using noise-free observations 517"
          },
          "15.2.2": {
            "Title": "Predictions using noisy observations 518"
          },
          "15.2.3": {
            "Title": "Effect of the kernel parameters 519"
          },
          "15.2.4": {
            "Title": "Estimating the kernel parameters 521"
          },
          "15.2.5": {
            "Title": "Computational and numerical issues * 524"
          },
          "15.2.6": {
            "Title": "Semi-parametric GPs * 524"
          }
        }
      },
      "15.3": {
        "Title": "GPs meet GLMs",
        "Sub Topics": {
          "15.3.1": {
            "Title": "Binary classification 525"
          },
          "15.3.2": {
            "Title": "Multi-class classification 528"
          },
          "15.3.3": {
            "Title": "GPs for Poisson regression 531"
          }
        }
      },
      "15.4": {
        "Title": "Connection with other methods",
        "Sub Topics": {
          "15.4.1": {
            "Title": "Linear models compared to GPs 532"
          },
          "15.4.2": {
            "Title": "Linear smoothers compared to GPs 533"
          },
          "15.4.3": {
            "Title": "SVMs compared to GPs 534"
          },
          "15.4.4": {
            "Title": "L1VM and RVMs compared to GPs 534"
          },
          "15.4.5": {
            "Title": "Neural networks compared to GPs 535"
          },
          "15.4.6": {
            "Title": "Smoothing splines compared to GPs * 536"
          },
          "15.4.7": {
            "Title": "RKHS methods compared to GPs * 538"
          }
        }
      },
      "15.5": {
        "Title": "GP latent variable model 540"
      },
      "15.6": {
        "Title": "Approximation methods for large datasets 542"
      }
    }
  },
  "16": {
    "Title": "Ensemble Methods",
    "Sub Topics": {
      "16.1": {
        "Title": "Introduction",
        "Sub Topics": {}
      },
      "16.2": {
        "Title": "Classification and regression trees (CART)",
        "Sub Topics": {
          "16.2.1": {
            "Title": "Basics"
          },
          "16.2.2": {
            "Title": "Growing a tree"
          },
          "16.2.3": {
            "Title": "Pruning a tree"
          },
          "16.2.4": {
            "Title": "Pros and cons of trees"
          },
          "16.2.5": {
            "Title": "Random forests"
          },
          "16.2.6": {
            "Title": "CART compared to hierarchical mixture of experts *"
          }
        }
      },
      "16.3": {
        "Title": "Generalized additive models",
        "Sub Topics": {
          "16.3.1": {
            "Title": "Backfitting"
          },
          "16.3.2": {
            "Title": "Computational efficiency"
          },
          "16.3.3": {
            "Title": "Multivariate adaptive regression splines (MARS)"
          }
        }
      },
      "16.4": {
        "Title": "Boosting",
        "Sub Topics": {
          "16.4.1": {
            "Title": "Forward stagewise additive modeling"
          },
          "16.4.2": {
            "Title": "L2boosting"
          },
          "16.4.3": {
            "Title": "AdaBoost"
          },
          "16.4.4": {
            "Title": "LogitBoost"
          },
          "16.4.5": {
            "Title": "Boosting as functional gradient descent"
          },
          "16.4.6": {
            "Title": "Sparse boosting"
          },
          "16.4.7": {
            "Title": "Multivariate adaptive regression trees (MART)"
          },
          "16.4.8": {
            "Title": "Why does boosting work so well?"
          },
          "16.4.9": {
            "Title": "A Bayesian view"
          }
        }
      },
      "16.5": {
        "Title": "Feedforward neural networks (multilayer perceptrons)",
        "Sub Topics": {
          "16.5.1": {
            "Title": "Convolutional neural networks"
          },
          "16.5.2": {
            "Title": "Other kinds of neural networks"
          },
          "16.5.3": {
            "Title": "A brief history of the field"
          },
          "16.5.4": {
            "Title": "The backpropagation algorithm"
          },
          "16.5.5": {
            "Title": "Identifiability"
          },
          "16.5.6": {
            "Title": "Regularization"
          },
          "16.5.7": {
            "Title": "Bayesian inference *"
          }
        }
      },
      "16.6": {
        "Title": "Ensemble learning",
        "Sub Topics": {
          "16.6.1": {
            "Title": "Stacking"
          },
          "16.6.2": {
            "Title": "Error-correcting output codes"
          },
          "16.6.3": {
            "Title": "Ensemble learning is not equivalent to Bayes model averaging"
          }
        }
      },
      "16.7": {
        "Title": "Experimental comparison",
        "Sub Topics": {
          "16.7.1": {
            "Title": "Low-dimensional features"
          },
          "16.7.2": {
            "Title": "High-dimensional features"
          }
        }
      },
      "16.8": {
        "Title": "Interpreting black-box models",
        "Sub Topics": {}
      }
    }
  },
  "17": {
    "Title": "Markov and hidden Markov models",
    "Sub Topics": {
      "17.1": {
        "Title": "Introduction"
      },
      "17.2": {
        "Title": "Markov models",
        "Sub Topics": {
          "17.2.1": {
            "Title": "Transition matrix"
          },
          "17.2.2": {
            "Title": "Application: Language modeling"
          },
          "17.2.3": {
            "Title": "Stationary distribution of a Markov chain *"
          },
          "17.2.4": {
            "Title": "Application: Google\u2019s PageRank algorithm for web page ranking *"
          }
        }
      },
      "17.3": {
        "Title": "Hidden Markov models",
        "Sub Topics": {
          "17.3.1": {
            "Title": "Applications of HMMs"
          }
        }
      },
      "17.4": {
        "Title": "Inference in HMMs",
        "Sub Topics": {
          "17.4.1": {
            "Title": "Types of inference problems for temporal models"
          },
          "17.4.2": {
            "Title": "The forwards algorithm"
          },
          "17.4.3": {
            "Title": "The forwards-backwards algorithm"
          },
          "17.4.4": {
            "Title": "The Viterbi algorithm"
          },
          "17.4.5": {
            "Title": "Forwards filtering, backwards sampling"
          }
        }
      },
      "17.5": {
        "Title": "Learning for HMMs",
        "Sub Topics": {
          "17.5.1": {
            "Title": "Training with fully observed data"
          },
          "17.5.2": {
            "Title": "EM for HMMs (the Baum-Welch algorithm)"
          },
          "17.5.3": {
            "Title": "Bayesian methods for \u201cfitting\u201d HMMs *"
          },
          "17.5.4": {
            "Title": "Discriminative training"
          },
          "17.5.5": {
            "Title": "Model selection"
          }
        }
      },
      "17.6": {
        "Title": "Generalizations of HMMs",
        "Sub Topics": {
          "17.6.1": {
            "Title": "Variable duration (semi-Markov) HMMs"
          },
          "17.6.2": {
            "Title": "Hierarchical HMMs"
          },
          "17.6.3": {
            "Title": "Input-output HMMs"
          },
          "17.6.4": {
            "Title": "Auto-regressive and buried HMMs"
          },
          "17.6.5": {
            "Title": "Factorial HMM"
          },
          "17.6.6": {
            "Title": "Coupled HMM and the influence model"
          },
          "17.6.7": {
            "Title": "Dynamic Bayesian networks (DBNs)"
          }
        }
      }
    }
  },
  "18": {
    "Title": "State space models",
    "Sub Topics": {
      "18.1": {
        "Title": "Introduction"
      },
      "18.2": {
        "Title": "Applications of SSMs",
        "Sub Topics": {
          "18.2.1": {
            "Title": "SSMs for object tracking"
          },
          "18.2.2": {
            "Title": "Robotic SLAM"
          },
          "18.2.3": {
            "Title": "Online parameter learning using recursive least squares"
          },
          "18.2.4": {
            "Title": "SSM for time series forecasting *"
          }
        }
      },
      "18.3": {
        "Title": "Inference in LG-SSM",
        "Sub Topics": {
          "18.3.1": {
            "Title": "The Kalman filtering algorithm"
          },
          "18.3.2": {
            "Title": "The Kalman smoothing algorithm"
          }
        }
      },
      "18.4": {
        "Title": "Learning for LG-SSM",
        "Sub Topics": {
          "18.4.1": {
            "Title": "Identifiability and numerical stability"
          },
          "18.4.2": {
            "Title": "Training with fully observed data"
          },
          "18.4.3": {
            "Title": "EM for LG-SSM"
          },
          "18.4.4": {
            "Title": "Subspace methods"
          },
          "18.4.5": {
            "Title": "Bayesian methods for 'fitting' LG-SSMs"
          }
        }
      },
      "18.5": {
        "Title": "Approximate online inference for non-linear, non-Gaussian SSMs",
        "Sub Topics": {
          "18.5.1": {
            "Title": "Extended Kalman filter (EKF)"
          },
          "18.5.2": {
            "Title": "Unscented Kalman filter (UKF)"
          },
          "18.5.3": {
            "Title": "Assumed density filtering (ADF)"
          }
        }
      },
      "18.6": {
        "Title": "Hybrid discrete/continuous SSMs",
        "Sub Topics": {
          "18.6.1": {
            "Title": "Inference"
          },
          "18.6.2": {
            "Title": "Application: data association and multi-target tracking"
          },
          "18.6.3": {
            "Title": "Application: fault diagnosis"
          },
          "18.6.4": {
            "Title": "Application: econometric forecasting"
          }
        }
      }
    }
  },
  "19": {
    "Title": "Undirected graphical models (Markov random fields)",
    "Sub Topics": {
      "19.1": {
        "Title": "Introduction"
      },
      "19.2": {
        "Title": "Conditional independence properties of UGMs",
        "Sub Topics": {
          "19.2.1": {
            "Title": "Key properties"
          },
          "19.2.2": {
            "Title": "An undirected alternative to d-separation"
          },
          "19.2.3": {
            "Title": "Comparing directed and undirected graphical models"
          }
        }
      },
      "19.3": {
        "Title": "Parameterization of MRFs",
        "Sub Topics": {
          "19.3.1": {
            "Title": "The Hammersley-Clifford theorem"
          },
          "19.3.2": {
            "Title": "Representing potential functions"
          }
        }
      },
      "19.4": {
        "Title": "Examples of MRFs",
        "Sub Topics": {
          "19.4.1": {
            "Title": "Ising model"
          },
          "19.4.2": {
            "Title": "Hopfield networks"
          },
          "19.4.3": {
            "Title": "Potts model"
          },
          "19.4.4": {
            "Title": "Gaussian MRFs"
          },
          "19.4.5": {
            "Title": "Markov logic networks *"
          }
        }
      },
      "19.5": {
        "Title": "Learning",
        "Sub Topics": {
          "19.5.1": {
            "Title": "Training maxent models using gradient methods"
          },
          "19.5.2": {
            "Title": "Training partially observed maxent models"
          },
          "19.5.3": {
            "Title": "Approximate methods for computing the MLEs of MRFs"
          },
          "19.5.4": {
            "Title": "Pseudo likelihood"
          },
          "19.5.5": {
            "Title": "Stochastic maximum likelihood"
          },
          "19.5.6": {
            "Title": "Feature induction for maxent models *"
          },
          "19.5.7": {
            "Title": "Iterative proportional fitting (IPF) *"
          }
        }
      },
      "19.6": {
        "Title": "Conditional random fields (CRFs)",
        "Sub Topics": {
          "19.6.1": {
            "Title": "Chain-structured CRFs, MEMMs and the label-bias problem"
          },
          "19.6.2": {
            "Title": "Applications of CRFs"
          },
          "19.6.3": {
            "Title": "CRF training"
          }
        }
      },
      "19.7": {
        "Title": "Structural SVMs",
        "Sub Topics": {
          "19.7.1": {
            "Title": "SSVMs: a probabilistic view"
          },
          "19.7.2": {
            "Title": "SSVMs: a non-probabilistic view"
          },
          "19.7.3": {
            "Title": "Cutting plane methods for fitting SSVMs"
          },
          "19.7.4": {
            "Title": "Online algorithms for fitting SSVMs"
          },
          "19.7.5": {
            "Title": "Latent structural SVMs"
          }
        }
      }
    }
  },
  "20": {
    "Title": "Exact inference for graphical models",
    "Sub Topics": {
      "20.1": {
        "Title": "Introduction"
      },
      "20.2": {
        "Title": "Belief propagation for trees",
        "Sub Topics": {
          "20.2.1": {
            "Title": "Serial protocol"
          },
          "20.2.2": {
            "Title": "Parallel protocol"
          },
          "20.2.3": {
            "Title": "Gaussian BP *"
          },
          "20.2.4": {
            "Title": "Other BP variants *"
          }
        }
      },
      "20.3": {
        "Title": "The variable elimination algorithm",
        "Sub Topics": {
          "20.3.1": {
            "Title": "The generalized distributive law *"
          },
          "20.3.2": {
            "Title": "Computational complexity of VE"
          },
          "20.3.3": {
            "Title": "A weakness of VE"
          }
        }
      },
      "20.4": {
        "Title": "The junction tree algorithm *",
        "Sub Topics": {
          "20.4.1": {
            "Title": "Creating a junction tree"
          },
          "20.4.2": {
            "Title": "Message passing on a junction tree"
          },
          "20.4.3": {
            "Title": "Computational complexity of JTA"
          },
          "20.4.4": {
            "Title": "JTA generalizations *"
          }
        }
      },
      "20.5": {
        "Title": "Computational intractability of exact inference in the worst case",
        "Sub Topics": {
          "20.5.1": {
            "Title": "Approximate inference"
          }
        }
      }
    }
  },
  "21": {
    "Title": "Variational inference",
    "Sub Topics": {
      "21.1": {
        "Title": "Introduction 731"
      },
      "21.2": {
        "Title": "Variational inference 732",
        "Sub Topics": {
          "21.2.1": {
            "Title": "Alternative interpretations of the variational objective 733"
          },
          "21.2.2": {
            "Title": "Forward or reverse KL? * 733"
          }
        }
      },
      "21.3": {
        "Title": "The mean field method 735",
        "Sub Topics": {
          "21.3.1": {
            "Title": "Derivation of the mean field update equations 736"
          },
          "21.3.2": {
            "Title": "Example: mean field for the Ising model 737"
          }
        }
      },
      "21.4": {
        "Title": "Structured mean field * 739",
        "Sub Topics": {
          "21.4.1": {
            "Title": "Example: factorial HMM 740"
          }
        }
      },
      "21.5": {
        "Title": "Variational Bayes 742",
        "Sub Topics": {
          "21.5.1": {
            "Title": "Example: VB for a univariate Gaussian 742"
          },
          "21.5.2": {
            "Title": "Example: VB for linear regression 746"
          }
        }
      },
      "21.6": {
        "Title": "Variational Bayes EM 749",
        "Sub Topics": {
          "21.6.1": {
            "Title": "Example: VBEM for mixtures of Gaussians * 750"
          }
        }
      },
      "21.7": {
        "Title": "Variational message passing and VIBES 756"
      },
      "21.8": {
        "Title": "Local variational bounds * 756",
        "Sub Topics": {
          "21.8.1": {
            "Title": "Motivating applications 756"
          },
          "21.8.2": {
            "Title": "Bohning\u2019s quadratic bound to the log-sum-exp function 758"
          },
          "21.8.3": {
            "Title": "Bounds for the sigmoid function 760"
          },
          "21.8.4": {
            "Title": "Other bounds and approximations to the log-sum-exp function * 762"
          },
          "21.8.5": {
            "Title": "Variational inference based on upper bounds 763"
          }
        }
      }
    }
  },
  "22": {
    "Title": "Variational Inference",
    "Sub Topics": {
      "22.1": {
        "Title": "Introduction 767"
      },
      "22.2": {
        "Title": "Loopy belief propagation: algorithmic issues 767",
        "Sub Topics": {
          "22.2.1": {
            "Title": "A brief history 767"
          },
          "22.2.2": {
            "Title": "LBP on pairwise models 768"
          },
          "22.2.3": {
            "Title": "LBP on a factor graph 769"
          },
          "22.2.4": {
            "Title": "Convergence 771"
          },
          "22.2.5": {
            "Title": "Accuracy of LBP 774"
          },
          "22.2.6": {
            "Title": "Other speedup tricks for LBP * 775"
          }
        }
      },
      "22.3": {
        "Title": "Loopy belief propagation: theoretical issues * 776",
        "Sub Topics": {
          "22.3.1": {
            "Title": "UGMs represented in exponential family form 776"
          },
          "22.3.2": {
            "Title": "The marginal polytope 777"
          },
          "22.3.3": {
            "Title": "Exact inference as a variational optimization problem 778"
          },
          "22.3.4": {
            "Title": "Mean field as a variational optimization problem 779"
          },
          "22.3.5": {
            "Title": "LBP as a variational optimization problem 779"
          },
          "22.3.6": {
            "Title": "Loopy BP vs mean field 783"
          }
        }
      },
      "22.4": {
        "Title": "Extensions of belief propagation * 783",
        "Sub Topics": {
          "22.4.1": {
            "Title": "Generalized belief propagation 783"
          },
          "22.4.2": {
            "Title": "Convex belief propagation 785"
          }
        }
      },
      "22.5": {
        "Title": "Expectation propagation 787",
        "Sub Topics": {
          "22.5.1": {
            "Title": "EP as a variational inference problem 788"
          },
          "22.5.2": {
            "Title": "Optimizing the EP objective using moment matching 789"
          },
          "22.5.3": {
            "Title": "EP for the clutter problem 791"
          },
          "22.5.4": {
            "Title": "LBP is a special case of EP 792"
          },
          "22.5.5": {
            "Title": "Ranking players using TrueSkill 793"
          },
          "22.5.6": {
            "Title": "Other applications of EP 799"
          }
        }
      },
      "22.6": {
        "Title": "MAP state estimation 799",
        "Sub Topics": {
          "22.6.1": {
            "Title": "Linear programming relaxation 799"
          },
          "22.6.2": {
            "Title": "Max-product belief propagation 800"
          },
          "22.6.3": {
            "Title": "Graphcuts 801"
          },
          "22.6.4": {
            "Title": "Experimental comparison of graphcuts and BP 804"
          },
          "22.6.5": {
            "Title": "Dual decomposition 806"
          }
        }
      }
    }
  },
  "23": {
    "Title": "Monte Carlo inference",
    "Sub Topics": {
      "23.1": {
        "Title": "Introduction"
      },
      "23.2": {
        "Title": "Sampling from standard distributions",
        "Sub Topics": {
          "23.2.1": {
            "Title": "Using the cdf"
          },
          "23.2.2": {
            "Title": "Sampling from a Gaussian (Box-Muller method)"
          }
        }
      },
      "23.3": {
        "Title": "Rejection sampling",
        "Sub Topics": {
          "23.3.1": {
            "Title": "Basic idea"
          },
          "23.3.2": {
            "Title": "Example"
          },
          "23.3.3": {
            "Title": "Application to Bayesian statistics"
          },
          "23.3.4": {
            "Title": "Adaptive rejection sampling"
          },
          "23.3.5": {
            "Title": "Rejection sampling in high dimensions"
          }
        }
      },
      "23.4": {
        "Title": "Importance sampling",
        "Sub Topics": {
          "23.4.1": {
            "Title": "Basic idea"
          },
          "23.4.2": {
            "Title": "Handling unnormalized distributions"
          },
          "23.4.3": {
            "Title": "Importance sampling for a DGM: likelihood weighting"
          },
          "23.4.4": {
            "Title": "Sampling importance resampling (SIR)"
          }
        }
      },
      "23.5": {
        "Title": "Particle filtering",
        "Sub Topics": {
          "23.5.1": {
            "Title": "Sequential importance sampling"
          },
          "23.5.2": {
            "Title": "The degeneracy problem"
          },
          "23.5.3": {
            "Title": "The resampling step"
          },
          "23.5.4": {
            "Title": "The proposal distribution"
          },
          "23.5.5": {
            "Title": "Application: robot localization"
          },
          "23.5.6": {
            "Title": "Application: visual object tracking"
          },
          "23.5.7": {
            "Title": "Application: time series forecasting"
          }
        }
      },
      "23.6": {
        "Title": "Rao-Blackwellised particle filtering (RBPF)",
        "Sub Topics": {
          "23.6.1": {
            "Title": "RBPF for switching LG-SSMs"
          },
          "23.6.2": {
            "Title": "Application: tracking a maneuvering target"
          },
          "23.6.3": {
            "Title": "Application: Fast SLAM"
          }
        }
      }
    }
  },
  "24": {
    "Title": "Markov chain Monte Carlo (MCMC) inference",
    "Sub Topics": {
      "24.1": {
        "Title": "Introduction"
      },
      "24.2": {
        "Title": "Gibbs sampling",
        "Sub Topics": {
          "24.2.1": {
            "Title": "Basic idea"
          },
          "24.2.2": {
            "Title": "Example: Gibbs sampling for the Ising model"
          },
          "24.2.3": {
            "Title": "Example: Gibbs sampling for inferring the parameters of a GMM"
          },
          "24.2.4": {
            "Title": "Collapsed Gibbs sampling *"
          },
          "24.2.5": {
            "Title": "Gibbs sampling for hierarchical GLMs"
          },
          "24.2.6": {
            "Title": "BUGS and JAGS"
          },
          "24.2.7": {
            "Title": "The Imputation Posterior (IP) algorithm"
          },
          "24.2.8": {
            "Title": "Blocking Gibbs sampling"
          }
        }
      },
      "24.3": {
        "Title": "Metropolis Hastings algorithm",
        "Sub Topics": {
          "24.3.1": {
            "Title": "Basic idea"
          },
          "24.3.2": {
            "Title": "Gibbs sampling is a special case of MH"
          },
          "24.3.3": {
            "Title": "Proposal distributions"
          },
          "24.3.4": {
            "Title": "Adaptive MCMC"
          },
          "24.3.5": {
            "Title": "Initialization and mode hopping"
          },
          "24.3.6": {
            "Title": "Why MH works *"
          },
          "24.3.7": {
            "Title": "Reversible jump (trans-dimensional) MCMC *"
          }
        }
      },
      "24.4": {
        "Title": "Speed and accuracy of MCMC",
        "Sub Topics": {
          "24.4.1": {
            "Title": "The burn-in phase"
          },
          "24.4.2": {
            "Title": "Mixing rates of Markov chains *"
          },
          "24.4.3": {
            "Title": "Practical convergence diagnostics"
          },
          "24.4.4": {
            "Title": "Accuracy of MCMC"
          },
          "24.4.5": {
            "Title": "How many chains?"
          }
        }
      },
      "24.5": {
        "Title": "Auxiliary variable MCMC *",
        "Sub Topics": {
          "24.5.1": {
            "Title": "Auxiliary variable sampling for logistic regression"
          },
          "24.5.2": {
            "Title": "Slice sampling"
          },
          "24.5.3": {
            "Title": "Swendsen Wang"
          },
          "24.5.4": {
            "Title": "Hybrid/Hamiltonian MCMC *"
          }
        }
      },
      "24.6": {
        "Title": "Annealing methods",
        "Sub Topics": {
          "24.6.1": {
            "Title": "Simulated annealing"
          },
          "24.6.2": {
            "Title": "Annealed importance sampling"
          },
          "24.6.3": {
            "Title": "Parallel tempering"
          }
        }
      },
      "24.7": {
        "Title": "Approximating the marginal likelihood",
        "Sub Topics": {
          "24.7.1": {
            "Title": "The candidate method"
          },
          "24.7.2": {
            "Title": "Harmonic mean estimate"
          },
          "24.7.3": {
            "Title": "Annealed importance sampling"
          }
        }
      }
    }
  },
  "25": {
    "Title": "Clustering",
    "Sub Topics": {
      "25.1": {
        "Title": "Introduction",
        "Sub Topics": {
          "25.1.1": {
            "Title": "Measuring (dis)similarity"
          },
          "25.1.2": {
            "Title": "Evaluating the output of clustering methods *"
          }
        }
      },
      "25.2": {
        "Title": "Dirichlet process mixture models",
        "Sub Topics": {
          "25.2.1": {
            "Title": "From finite to infinite mixture models"
          },
          "25.2.2": {
            "Title": "The Dirichlet process"
          },
          "25.2.3": {
            "Title": "Applying Dirichlet processes to mixture modeling"
          },
          "25.2.4": {
            "Title": "Fitting a DP mixture model"
          }
        }
      },
      "25.3": {
        "Title": "Affinity propagation",
        "Sub Topics": {}
      },
      "25.4": {
        "Title": "Spectral clustering",
        "Sub Topics": {
          "25.4.1": {
            "Title": "Graph Laplacian"
          },
          "25.4.2": {
            "Title": "Normalized graph Laplacian"
          },
          "25.4.3": {
            "Title": "Example"
          }
        }
      },
      "25.5": {
        "Title": "Hierarchical clustering",
        "Sub Topics": {
          "25.5.1": {
            "Title": "Agglomerative clustering"
          },
          "25.5.2": {
            "Title": "Divisive clustering"
          },
          "25.5.3": {
            "Title": "Choosing the number of clusters"
          },
          "25.5.4": {
            "Title": "Bayesian hierarchical clustering"
          }
        }
      },
      "25.6": {
        "Title": "Clustering datapoints and features",
        "Sub Topics": {
          "25.6.1": {
            "Title": "Biclustering"
          },
          "25.6.2": {
            "Title": "Multi-view clustering"
          }
        }
      }
    }
  },
  "26": {
    "Title": "Graphical model structure learning",
    "Sub Topics": {
      "26.1": {
        "Title": "Introduction"
      },
      "26.2": {
        "Title": "Structure learning for knowledge discovery",
        "Sub Topics": {
          "26.2.1": {
            "Title": "Relevance networks"
          },
          "26.2.2": {
            "Title": "Dependency networks"
          }
        }
      },
      "26.3": {
        "Title": "Learning tree structures",
        "Sub Topics": {
          "26.3.1": {
            "Title": "Directed or undirected tree?"
          },
          "26.3.2": {
            "Title": "Chow-Liu algorithm for finding the ML tree structure"
          },
          "26.3.3": {
            "Title": "Finding the MAP forest"
          },
          "26.3.4": {
            "Title": "Mixtures of trees"
          }
        }
      },
      "26.4": {
        "Title": "Learning DAG structures",
        "Sub Topics": {
          "26.4.1": {
            "Title": "Markov equivalence"
          },
          "26.4.2": {
            "Title": "Exact structural inference"
          },
          "26.4.3": {
            "Title": "Scaling up to larger graphs"
          }
        }
      },
      "26.5": {
        "Title": "Learning DAG structure with latent variables",
        "Sub Topics": {
          "26.5.1": {
            "Title": "Approximating the marginal likelihood when we have missing data"
          },
          "26.5.2": {
            "Title": "Structural EM"
          },
          "26.5.3": {
            "Title": "Discovering hidden variables"
          },
          "26.5.4": {
            "Title": "Case study: Google\u2019s Rephil"
          },
          "26.5.5": {
            "Title": "Structural equation models *"
          }
        }
      },
      "26.6": {
        "Title": "Learning causal DAGs",
        "Sub Topics": {
          "26.6.1": {
            "Title": "Causal interpretation of DAGs"
          },
          "26.6.2": {
            "Title": "Using causal DAGs to resolve Simpson\u2019s paradox"
          },
          "26.6.3": {
            "Title": "Learning causal DAG structures"
          }
        }
      },
      "26.7": {
        "Title": "Learning undirected Gaussian graphical models",
        "Sub Topics": {
          "26.7.1": {
            "Title": "MLE for a GGM"
          },
          "26.7.2": {
            "Title": "Graphical lasso"
          },
          "26.7.3": {
            "Title": "Bayesian inference for GGM structure *"
          },
          "26.7.4": {
            "Title": "Handling non-Gaussian data using copulas *"
          }
        }
      },
      "26.8": {
        "Title": "Learning undirected discrete graphical models",
        "Sub Topics": {
          "26.8.1": {
            "Title": "Graphical lasso for MRFs/CRFs"
          },
          "26.8.2": {
            "Title": "Thin junction trees"
          }
        }
      }
    }
  },
  "27": {
    "Title": "Latent variable models for discrete data",
    "Sub Topics": {
      "27.1": {
        "Title": "Introduction"
      },
      "27.2": {
        "Title": "Distributed state LVMs for discrete data",
        "Sub Topics": {
          "27.2.1": {
            "Title": "Mixture models"
          },
          "27.2.2": {
            "Title": "Exponential family PCA"
          },
          "27.2.3": {
            "Title": "LDA and mPCA"
          },
          "27.2.4": {
            "Title": "GaP model and non-negative matrix factorization"
          }
        }
      },
      "27.3": {
        "Title": "Latent Dirichlet allocation (LDA)",
        "Sub Topics": {
          "27.3.1": {
            "Title": "Basics"
          },
          "27.3.2": {
            "Title": "Unsupervised discovery of topics"
          },
          "27.3.3": {
            "Title": "Quantitatively evaluating LDA as a language model"
          },
          "27.3.4": {
            "Title": "Fitting using (collapsed) Gibbs sampling"
          },
          "27.3.5": {
            "Title": "Example"
          },
          "27.3.6": {
            "Title": "Fitting using batch variational inference"
          },
          "27.3.7": {
            "Title": "Fitting using online variational inference"
          },
          "27.3.8": {
            "Title": "Determining the number of topics"
          }
        }
      },
      "27.4": {
        "Title": "Extensions of LDA",
        "Sub Topics": {
          "27.4.1": {
            "Title": "Correlated topic model"
          },
          "27.4.2": {
            "Title": "Dynamic topic model"
          },
          "27.4.3": {
            "Title": "LDA-HMM"
          },
          "27.4.4": {
            "Title": "Supervised LDA"
          }
        }
      },
      "27.5": {
        "Title": "LVMs for graph-structured data",
        "Sub Topics": {
          "27.5.1": {
            "Title": "Stochastic block model"
          },
          "27.5.2": {
            "Title": "Mixed membership stochastic block model"
          },
          "27.5.3": {
            "Title": "Relational topic model"
          }
        }
      },
      "27.6": {
        "Title": "LVMs for relational data",
        "Sub Topics": {
          "27.6.1": {
            "Title": "Infinite relational model"
          },
          "27.6.2": {
            "Title": "Probabilistic matrix factorization for collaborative filtering"
          }
        }
      },
      "27.7": {
        "Title": "Restricted Boltzmann machines (RBMs)",
        "Sub Topics": {
          "27.7.1": {
            "Title": "Varieties of RBMs"
          },
          "27.7.2": {
            "Title": "Learning RBMs"
          },
          "27.7.3": {
            "Title": "Applications of RBMs"
          }
        }
      }
    }
  },
  "28": {
    "Title": "Deep learning",
    "Sub Topics": {
      "28.1": {
        "Title": "Introduction"
      },
      "28.2": {
        "Title": "Deep generative models",
        "Sub Topics": {
          "28.2.1": {
            "Title": "Deep directed networks"
          },
          "28.2.2": {
            "Title": "Deep Boltzmann machines"
          },
          "28.2.3": {
            "Title": "Deep belief networks"
          },
          "28.2.4": {
            "Title": "Greedy layer-wise learning of DBNs"
          }
        }
      },
      "28.3": {
        "Title": "Deep neural networks",
        "Sub Topics": {
          "28.3.1": {
            "Title": "Deep multi-layer perceptrons"
          },
          "28.3.2": {
            "Title": "Deep auto-encoders"
          },
          "28.3.3": {
            "Title": "Stacked denoising auto-encoders"
          }
        }
      },
      "28.4": {
        "Title": "Applications of deep networks",
        "Sub Topics": {
          "28.4.1": {
            "Title": "Handwritten digit classification using DBNs"
          },
          "28.4.2": {
            "Title": "Data visualization and feature discovery using deep auto-encoders"
          },
          "28.4.3": {
            "Title": "Information retrieval using deep auto-encoders (semantic hashing)"
          },
          "28.4.4": {
            "Title": "Learning audio features using 1d convolutional DBNs"
          },
          "28.4.5": {
            "Title": "Learning image features using 2d convolutional DBNs"
          }
        }
      }
    }
  }
}