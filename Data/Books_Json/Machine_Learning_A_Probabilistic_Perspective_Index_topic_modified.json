{
  "Title": "Machine Learning",
  "Sub Topics":{
  "1": {
    "Title": "Introduction to Machine Learning",
    "Sub Topics": {
      "1.1": {
        "Title": "Machine learning: what and why?",
        "Sub Topics": {
          "1.1.1": {
            "Title": "Types of machine learning"
          }
        }
      },
      "1.2": {
        "Title": "Supervised learning",
        "Sub Topics": {
          "1.2.1": {
            "Title": "Classification"
          },
          "1.2.2": {
            "Title": "Regression"
          }
        }
      },
      "1.3": {
        "Title": "Unsupervised learning",
        "Sub Topics": {
          "1.3.1": {
            "Title": "Discovering clusters"
          },
          "1.3.2": {
            "Title": "Discovering latent factors"
          },
          "1.3.3": {
            "Title": "Discovering graph structure"
          },
          "1.3.4": {
            "Title": "Matrix completion"
          }
        }
      },
      "1.4": {
        "Title": "Some basic concepts in machine learning",
        "Sub Topics": {
          "1.4.1": {
            "Title": "Parametric vs non-parametric models"
          },
          "1.4.2": {
            "Title": "A simple non-parametric classifier: K-nearest neighbors"
          },
          "1.4.3": {
            "Title": "The curse of dimensionality"
          },
          "1.4.4": {
            "Title": "Parametric models for classification and regression"
          },
          "1.4.5": {
            "Title": "Linear regression (Basics)"
          },
          "1.4.6": {
            "Title": "Logistic regression (Basics)"
          },
          "1.4.7": {
            "Title": "Overfitting"
          },
          "1.4.8": {
            "Title": "Model selection (Basics)"
          },
          "1.4.9": {
            "Title": "No free lunch theorem"
          }
        }
      }
    }
  },
  "2": {
    "Title": "Probability",
    "Sub Topics": {
      "2.1": {
        "Title": "Introduction to Probability",
        "Sub Topics": {}
      },
      "2.2": {
        "Title": "A brief review of probability theory",
        "Sub Topics": {
          "2.2.1": {
            "Title": "Discrete random variables"
          },
          "2.2.2": {
            "Title": "Fundamental rules"
          },
          "2.2.3": {
            "Title": "Bayes rule"
          },
          "2.2.4": {
            "Title": "Independence and conditional independence"
          },
          "2.2.5": {
            "Title": "Continuous random variables"
          },
          "2.2.6": {
            "Title": "Quantiles"
          },
          "2.2.7": {
            "Title": "Mean and variance"
          }
        }
      },
      "2.3": {
        "Title": "Some common discrete distributions",
        "Sub Topics": {
          "2.3.1": {
            "Title": "The binomial and Bernoulli distributions"
          },
          "2.3.2": {
            "Title": "The multinomial and multinoulli distributions"
          },
          "2.3.3": {
            "Title": "The Poisson distribution"
          },
          "2.3.4": {
            "Title": "The empirical distribution"
          }
        }
      },
      "2.4": {
        "Title": "Some common continuous distributions",
        "Sub Topics": {
          "2.4.1": {
            "Title": "Gaussian (normal) distribution"
          },
          "2.4.2": {
            "Title": "Degenerate pdf"
          },
          "2.4.3": {
            "Title": "The Laplace distribution"
          },
          "2.4.4": {
            "Title": "The gamma distribution"
          },
          "2.4.5": {
            "Title": "The beta distribution"
          },
          "2.4.6": {
            "Title": "Pareto distribution"
          }
        }
      },
      "2.5": {
        "Title": "Joint probability distributions",
        "Sub Topics": {
          "2.5.1": {
            "Title": "Covariance and correlation"
          },
          "2.5.2": {
            "Title": "The multivariate Gaussian"
          },
          "2.5.3": {
            "Title": "Multivariate Student t distribution"
          },
          "2.5.4": {
            "Title": "Dirichlet distribution"
          }
        }
      },
      "2.6": {
        "Title": "Transformations of random variables",
        "Sub Topics": {
          "2.6.1": {
            "Title": "Linear transformations"
          },
          "2.6.2": {
            "Title": "General transformations"
          },
          "2.6.3": {
            "Title": "Central limit theorem"
          }
        }
      },
      "2.7": {
        "Title": "Monte Carlo approximation",
        "Sub Topics": {
          "2.7.1": {
            "Title": "Example: change of variables, the MC way"
          },
          "2.7.2": {
            "Title": "Example: estimating pi by Monte Carlo integration"
          },
          "2.7.3": {
            "Title": "Accuracy of Monte Carlo approximation"
          }
        }
      },
      "2.8": {
        "Title": "Information theory",
        "Sub Topics": {
          "2.8.1": {
            "Title": "Entropy"
          },
          "2.8.2": {
            "Title": "KL divergence"
          },
          "2.8.3": {
            "Title": "Mutual information"
          }
        }
      }
    }
  },
  "3": {
    "Title": "Generative models for discrete data",
    "Sub Topics": {
      "3.1": {
        "Title": "Introductio to Generative models",
        "Sub Topics": {}
      },
      "3.2": {
        "Title": "Bayesian concept learning",
        "Sub Topics": {
          "3.2.1": {
            "Title": "Likelihood (Bayesian concept learning)"
          },
          "3.2.2": {
            "Title": "Prior (Bayesian concept learning)"
          },
          "3.2.3": {
            "Title": "Posterior (Bayesian concept learning)"
          },
          "3.2.4": {
            "Title": "Posterior predictive distribution (Bayesian concept learning)"
          },
          "3.2.5": {
            "Title": "A more complex prior"
          }
        }
      },
      "3.3": {
        "Title": "The beta-binomial model",
        "Sub Topics": {
          "3.3.1": {
            "Title": "Likelihood (beta-binomial model)"
          },
          "3.3.2": {
            "Title": "Prior (beta-binomial model)"
          },
          "3.3.3": {
            "Title": "Posterior (beta-binomial model)"
          },
          "3.3.4": {
            "Title": "Posterior predictive distribution (beta-binomial model)"
          }
        }
      },
      "3.4": {
        "Title": "The Dirichlet-multinomial model",
        "Sub Topics": {
          "3.4.1": {
            "Title": "Likelihood (Dirichlet-multinomial model)"
          },
          "3.4.2": {
            "Title": "Prior (Dirichlet-multinomial model)"
          },
          "3.4.3": {
            "Title": "Posterior (Dirichlet-multinomial model)"
          },
          "3.4.4": {
            "Title": "Posterior predictive (Dirichlet-multinomial model)"
          }
        }
      },
      "3.5": {
        "Title": "Naive Bayes classifiers",
        "Sub Topics": {
          "3.5.1": {
            "Title": "Model fitting (Naive Bayes classifiers)"
          },
          "3.5.2": {
            "Title": "Using the model for prediction"
          },
          "3.5.3": {
            "Title": "The log-sum-exp trick"
          },
          "3.5.4": {
            "Title": "Feature selection using mutual information"
          },
          "3.5.5": {
            "Title": "Classifying documents using bag of words"
          }
        }
      }
    }
  },
  "4": {
    "Title": "Gaussian models",
    "Sub Topics": {
      "4.1": {
        "Title": "Introduction to Gaussian models",
        "Sub Topics": {
          "4.1.1": {
            "Title": "Notation"
          },
          "4.1.2": {
            "Title": "Basics of Gaussian Models"
          },
          "4.1.3": {
            "Title": "MLE for an MVN"
          },
          "4.1.4": {
            "Title": "Maximum entropy derivation of the Gaussian"
          }
        }
      },
      "4.2": {
        "Title": "Gaussian discriminant analysis",
        "Sub Topics": {
          "4.2.1": {
            "Title": "Quadratic discriminant analysis (QDA)"
          },
          "4.2.2": {
            "Title": "Linear discriminant analysis (LDA)"
          },
          "4.2.3": {
            "Title": "Two-class LDA"
          },
          "4.2.4": {
            "Title": "MLE for discriminant analysis"
          },
          "4.2.5": {
            "Title": "Strategies for preventing overfitting"
          },
          "4.2.6": {
            "Title": "Regularized LDA"
          },
          "4.2.7": {
            "Title": "Diagonal LDA"
          },
          "4.2.8": {
            "Title": "Nearest shrunken centroids classifier"
          }
        }
      },
      "4.3": {
        "Title": "Inference in jointly Gaussian distributions",
        "Sub Topics": {
          "4.3.1": {
            "Title": "Statement of the result (jointly Gaussian distributions)"
          },
          "4.3.2": {
            "Title": "Examples of jointly Gaussian distributions"
          },
          "4.3.3": {
            "Title": "Information form"
          },
          "4.3.4": {
            "Title": "Proof of the result of jointly Gaussian distributions"
          }
        }
      },
      "4.4": {
        "Title": "Linear Gaussian systems",
        "Sub Topics": {
          "4.4.1": {
            "Title": "Statement of the result (Linear Gaussian systems)"
          },
          "4.4.2": {
            "Title": "Examples of Linear Gaussian systems"
          },
          "4.4.3": {
            "Title": "Proof of the result of Linear Gaussian systems"
          }
        }
      },
      "4.5": {
        "Title": "Digression: The Wishart distribution",
        "Sub Topics": {
          "4.5.1": {
            "Title": "Inverse Wishart distribution"
          },
          "4.5.2": {
            "Title": "Visualizing the Wishart distribution"
          }
        }
      },
      "4.6": {
        "Title": "Inferring the parameters of an MVN",
        "Sub Topics": {
          "4.6.1": {
            "Title": "Posterior distribution of mu"
          },
          "4.6.2": {
            "Title": "Posterior distribution of sigma"
          },
          "4.6.3": {
            "Title": "Posterior distribution of mu and sigma"
          },
          "4.6.4": {
            "Title": "Sensor fusion with unknown precisions"
          }
        }
      }
    }
  },
  "5": {
    "Title": "Bayesian statistics",
    "Sub Topics": {
      "5.1": {
        "Title": "Introduction to Bayesian statistics",
        "Sub Topics": {}
      },
      "5.2": {
        "Title": "Summarizing posterior distributions",
        "Sub Topics": {
          "5.2.1": {
            "Title": "MAP estimation"
          },
          "5.2.2": {
            "Title": "Credible intervals"
          },
          "5.2.3": {
            "Title": "Inference for a difference in proportions"
          }
        }
      },
      "5.3": {
        "Title": "Bayesian model selection",
        "Sub Topics": {
          "5.3.1": {
            "Title": "Bayesian Occam's razor"
          },
          "5.3.2": {
            "Title": "Computing the marginal likelihood (evidence)"
          },
          "5.3.3": {
            "Title": "Bayes factors"
          },
          "5.3.4": {
            "Title": "Jeffreys-Lindley paradox"
          }
        }
      },
      "5.4": {
        "Title": "Priors (Bayesian Statistics)",
        "Sub Topics": {
          "5.4.1": {
            "Title": "Uninformative priors"
          },
          "5.4.2": {
            "Title": "Jeffreys priors"
          },
          "5.4.3": {
            "Title": "Robust priors"
          },
          "5.4.4": {
            "Title": "Mixtures of conjugate priors"
          }
        }
      },
      "5.5": {
        "Title": "Hierarchical Bayes",
        "Sub Topics": {
          "5.5.1": {
            "Title": "Example: modeling related cancer rates"
          }
        }
      },
      "5.6": {
        "Title": "Empirical Bayes",
        "Sub Topics": {
          "5.6.1": {
            "Title": "Example: beta-binomial model"
          },
          "5.6.2": {
            "Title": "Example: Gaussian-Gaussian model"
          }
        }
      },
      "5.7": {
        "Title": "Bayesian decision theory",
        "Sub Topics": {
          "5.7.1": {
            "Title": "Bayes estimators for common loss functions"
          },
          "5.7.2": {
            "Title": "The false positive vs false negative tradeoff"
          },
          "5.7.3": {
            "Title": "Other topics"
          }
        }
      }
    }
  },
  "6": {
    "Title": "Frequentist statistics",
    "Sub Topics": {
      "6.1": {
        "Title": "Introduction to Frequentist statistics",
        "Sub Topics": {}
      },
      "6.2": {
        "Title": "Sampling distribution of an estimator",
        "Sub Topics": {
          "6.2.1": {
            "Title": "Bootstrap"
          },
          "6.2.2": {
            "Title": "Large sample theory for the MLE"
          }
        }
      },
      "6.3": {
        "Title": "Frequentist decision theory",
        "Sub Topics": {
          "6.3.1": {
            "Title": "Bayes risk"
          },
          "6.3.2": {
            "Title": "Minimax risk"
          },
          "6.3.3": {
            "Title": "Admissible estimators"
          }
        }
      },
      "6.4": {
        "Title": "Desirable properties of estimators",
        "Sub Topics": {
          "6.4.1": {
            "Title": "Consistent estimators"
          },
          "6.4.2": {
            "Title": "Unbiased estimators"
          },
          "6.4.3": {
            "Title": "Minimum variance estimators"
          },
          "6.4.4": {
            "Title": "The bias-variance tradeoff"
          }
        }
      },
      "6.5": {
        "Title": "Empirical risk minimization",
        "Sub Topics": {
          "6.5.1": {
            "Title": "Regularized risk minimization"
          },
          "6.5.2": {
            "Title": "Structural risk minimization"
          },
          "6.5.3": {
            "Title": "Estimating the risk using cross validation"
          },
          "6.5.4": {
            "Title": "Upper bounding the risk using statistical learning theory"
          },
          "6.5.5": {
            "Title": "Surrogate loss functions"
          }
        }
      },
      "6.6": {
        "Title": "Pathologies of frequentist statistics",
        "Sub Topics": {
          "6.6.1": {
            "Title": "Counter-intuitive behavior of confidence intervals"
          },
          "6.6.2": {
            "Title": "p-values considered harmful"
          },
          "6.6.3": {
            "Title": "The likelihood principle"
          },
          "6.6.4": {
            "Title": "Why isn't everyone a Bayesian?"
          }
        }
      }
    }
  },
  "7": {
    "Title": "Linear regression",
    "Sub Topics": {
      "7.1": {
        "Title": "Introduction to Linear regression",
        "Sub Topics": {}
      },
      "7.2": {
        "Title": "Model specification (Linear regression)",
        "Sub Topics": {}
      },
      "7.3": {
        "Title": "Maximum likelihood estimation (least squares)",
        "Sub Topics": {
          "7.3.1": {
            "Title": "Derivation of the MLE"
          },
          "7.3.2": {
            "Title": "Geometric interpretation"
          },
          "7.3.3": {
            "Title": "Convexity"
          }
        }
      },
      "7.4": {
        "Title": "Robust linear regression",
        "Sub Topics": {}
      },
      "7.5": {
        "Title": "Ridge regression",
        "Sub Topics": {
          "7.5.1": {
            "Title": "Basic idea (Ridge regression)"
          },
          "7.5.2": {
            "Title": "Numerically stable computation"
          },
          "7.5.3": {
            "Title": "Connection with PCA"
          },
          "7.5.4": {
            "Title": "Regularization effects of big data"
          }
        }
      },
      "7.6": {
        "Title": "Bayesian linear regression",
        "Sub Topics": {
          "7.6.1": {
            "Title": "Computing the posterior"
          },
          "7.6.2": {
            "Title": "Computing the posterior predictive"
          },
          "7.6.3": {
            "Title": "Bayesian inference when sigma square is unknown"
          },
          "7.6.4": {
            "Title": "EB for linear regression (evidence procedure)"
          }
        }
      }
    }
  },
  "8": {
    "Title": "Logistic regression",
    "Sub Topics": {
      "8.1": {
        "Title": "Introduction to Logistic regression",
        "Sub Topics": {}
      },
      "8.2": {
        "Title": "Model specification (Logistic regression)",
        "Sub Topics": {}
      },
      "8.3": {
        "Title": "Model fitting (Logistic regression)",
        "Sub Topics": {
          "8.3.1": {
            "Title": "MLE"
          },
          "8.3.2": {
            "Title": "Steepest descent"
          },
          "8.3.3": {
            "Title": "Newton's method"
          },
          "8.3.4": {
            "Title": "Iteratively reweighted least squares (IRLS)"
          },
          "8.3.5": {
            "Title": "Quasi-Newton (variable metric) methods"
          },
          "8.3.6": {
            "Title": "L2 regularization"
          },
          "8.3.7": {
            "Title": "Multi-class logistic regression"
          }
        }
      },
      "8.4": {
        "Title": "Bayesian logistic regression",
        "Sub Topics": {
          "8.4.1": {
            "Title": "Laplace approximation"
          },
          "8.4.2": {
            "Title": "Derivation of the BIC"
          },
          "8.4.3": {
            "Title": "Gaussian approximation for logistic regression"
          },
          "8.4.4": {
            "Title": "Approximating the posterior predictive"
          },
          "8.4.5": {
            "Title": "Residual analysis (outlier detection)"
          }
        }
      },
      "8.5": {
        "Title": "Online learning and stochastic optimization",
        "Sub Topics": {
          "8.5.1": {
            "Title": "Online learning and regret minimization"
          },
          "8.5.2": {
            "Title": "Stochastic optimization and risk minimization"
          },
          "8.5.3": {
            "Title": "The LMS algorithm"
          },
          "8.5.4": {
            "Title": "The perceptron algorithm"
          },
          "8.5.5": {
            "Title": "A Bayesian view"
          }
        }
      },
      "8.6": {
        "Title": "Generative vs discriminative classifiers",
        "Sub Topics": {
          "8.6.1": {
            "Title": "Pros and cons of each approach"
          },
          "8.6.2": {
            "Title": "Dealing with missing data"
          },
          "8.6.3": {
            "Title": "Fisher's linear discriminant analysis (FLDA)"
          }
        }
      }
    }
  },
  "9": {
    "Title": "Generalized linear models and the exponential family",
    "Sub Topics": {
      "9.1": {
        "Title": "Introduction to Generalized linear models",
        "Sub Topics": {}
      },
      "9.2": {
        "Title": "The exponential family",
        "Sub Topics": {
          "9.2.1": {
            "Title": "Definition of the exponential family"
          },
          "9.2.2": {
            "Title": "Examples of the exponential family"
          },
          "9.2.3": {
            "Title": "Log partition function"
          },
          "9.2.4": {
            "Title": "MLE for the exponential family"
          },
          "9.2.5": {
            "Title": "Bayes for the exponential family"
          },
          "9.2.6": {
            "Title": "Maximum entropy derivation of the exponential family"
          }
        }
      },
      "9.3": {
        "Title": "Generalized linear models (GLMs)",
        "Sub Topics": {
          "9.3.1": {
            "Title": "Basics of Generalized linear models"
          },
          "9.3.2": {
            "Title": "ML and MAP estimation"
          },
          "9.3.3": {
            "Title": "Bayesian inference (GLMs)"
          }
        }
      },
      "9.4": {
        "Title": "Probit regression",
        "Sub Topics": {
          "9.4.1": {
            "Title": "ML/MAP estimation using gradient-based optimization"
          },
          "9.4.2": {
            "Title": "Latent variable interpretation"
          },
          "9.4.3": {
            "Title": "Ordinal probit regression"
          },
          "9.4.4": {
            "Title": "Multinomial probit models"
          }
        }
      },
      "9.5": {
        "Title": "Multi-task learning",
        "Sub Topics": {
          "9.5.1": {
            "Title": "Hierarchical Bayes for multi-task learning"
          },
          "9.5.2": {
            "Title": "Application to personalized email spam filtering"
          },
          "9.5.3": {
            "Title": "Application to domain adaptation"
          },
          "9.5.4": {
            "Title": "Other kinds of prior"
          }
        }
      },
      "9.6": {
        "Title": "Generalized linear mixed models",
        "Sub Topics": {
          "9.6.1": {
            "Title": "Example: semi-parametric GLMMs for medical data"
          },
          "9.6.2": {
            "Title": "Computational issues"
          }
        }
      },
      "9.7": {
        "Title": "Learning to rank",
        "Sub Topics": {
          "9.7.1": {
            "Title": "The pointwise approach"
          },
          "9.7.2": {
            "Title": "The pairwise approach"
          },
          "9.7.3": {
            "Title": "The listwise approach"
          },
          "9.7.4": {
            "Title": "Loss functions for ranking"
          }
        }
      }
    }
  },
  "10": {
    "Title": "Directed graphical models (Bayes nets)",
    "Sub Topics": {
      "10.1": {
        "Title": "Introduction to Directed graphical models",
        "Sub Topics": {
          "10.1.1": {
            "Title": "Chain rule"
          },
          "10.1.2": {
            "Title": "Conditional independence"
          },
          "10.1.3": {
            "Title": "Graphical models"
          },
          "10.1.4": {
            "Title": "Graph terminology"
          },
          "10.1.5": {
            "Title": "Directed graphical models"
          }
        }
      },
      "10.2": {
        "Title": "Examples of Directed graphical models",
        "Sub Topics": {
          "10.2.1": {
            "Title": "Naive Bayes classifiers (Directed graphical models)"
          },
          "10.2.2": {
            "Title": "Markov and hidden Markov models (Directed graphical models)"
          },
          "10.2.3": {
            "Title": "Medical diagnosis"
          },
          "10.2.4": {
            "Title": "Genetic linkage analysis"
          },
          "10.2.5": {
            "Title": "Directed Gaussian graphical models"
          }
        }
      },
      "10.3": {
        "Title": "Inference (Directed graphical models)",
        "Sub Topics": {}
      },
      "10.4": {
        "Title": "Learning (Directed graphical models)",
        "Sub Topics": {
          "10.4.1": {
            "Title": "Plate notation"
          },
          "10.4.2": {
            "Title": "Learning from complete data"
          },
          "10.4.3": {
            "Title": "Learning with missing and/or latent variables"
          }
        }
      },
      "10.5": {
        "Title": "Conditional independence properties of DGMs",
        "Sub Topics": {
          "10.5.1": {
            "Title": "d-separation and the Bayes Ball algorithm (global Markov properties)"
          },
          "10.5.2": {
            "Title": "Other Markov properties of DGMs"
          },
          "10.5.3": {
            "Title": "Markov blanket and full conditionals"
          }
        }
      },
      "10.6": {
        "Title": "Influence (decision) diagrams",
        "Sub Topics": {}
      }
    }
  },
  "11": {
    "Title": "Mixture models and the EM algorithm",
    "Sub Topics": {
      "11.1": {
        "Title": "Latent variable models",
        "Sub Topics": {}
      },
      "11.2": {
        "Title": "Mixture models",
        "Sub Topics": {
          "11.2.1": {
            "Title": "Mixtures of Gaussians"
          },
          "11.2.2": {
            "Title": "Mixture of multinoullis"
          },
          "11.2.3": {
            "Title": "Using mixture models for clustering"
          },
          "11.2.4": {
            "Title": "Mixtures of experts"
          }
        }
      },
      "11.3": {
        "Title": "Parameter estimation for mixture models",
        "Sub Topics": {
          "11.3.1": {
            "Title": "Unidentifiability in Parameter estimation"
          },
          "11.3.2": {
            "Title": "Computing a MAP estimate is non-convex"
          }
        }
      },
      "11.4": {
        "Title": "The EM algorithm",
        "Sub Topics": {
          "11.4.1": {
            "Title": "Basic idea behind EM algorithm"
          },
          "11.4.2": {
            "Title": "EM for GMMs"
          },
          "11.4.3": {
            "Title": "EM for mixture of experts"
          },
          "11.4.4": {
            "Title": "EM for DGMs with hidden variables"
          },
          "11.4.5": {
            "Title": "EM for the Student distribution"
          },
          "11.4.6": {
            "Title": "EM for probit regression"
          },
          "11.4.7": {
            "Title": "Theoretical basis for EM"
          },
          "11.4.8": {
            "Title": "Online EM"
          },
          "11.4.9": {
            "Title": "Other EM variants"
          }
        }
      },
      "11.5": {
        "Title": "Model selection for latent variable models",
        "Sub Topics": {
          "11.5.1": {
            "Title": "Model selection for probabilistic models"
          },
          "11.5.2": {
            "Title": "Model selection for non-probabilistic methods"
          }
        }
      },
      "11.6": {
        "Title": "Fitting models with missing data",
        "Sub Topics": {
          "11.6.1": {
            "Title": "EM for the MLE of an MVN with missing data"
          }
        }
      }
    }
  },
  "12": {
    "Title": "Latent linear models",
    "Sub Topics": {
      "12.1": {
        "Title": "Factor analysis",
        "Sub Topics": {
          "12.1.1": {
            "Title": "FA is a low rank parameterization of an MVN"
          },
          "12.1.2": {
            "Title": "Inference of the latent factors"
          },
          "12.1.3": {
            "Title": "Unidentifiability in Latent linear models"
          },
          "12.1.4": {
            "Title": "Mixtures of factor analysers"
          },
          "12.1.5": {
            "Title": "EM for factor analysis models"
          },
          "12.1.6": {
            "Title": "Fitting FA models with missing data"
          }
        }
      },
      "12.2": {
        "Title": "Principal components analysis (PCA)",
        "Sub Topics": {
          "12.2.1": {
            "Title": "Classical PCA: statement of the theorem"
          },
          "12.2.2": {
            "Title": "Proof of PCA"
          },
          "12.2.3": {
            "Title": "Singular value decomposition (SVD)"
          },
          "12.2.4": {
            "Title": "Probabilistic PCA"
          },
          "12.2.5": {
            "Title": "EM algorithm for PCA"
          }
        }
      },
      "12.3": {
        "Title": "Choosing the number of latent dimensions",
        "Sub Topics": {
          "12.3.1": {
            "Title": "Model selection for FA/PPCA"
          },
          "12.3.2": {
            "Title": "Model selection for PCA"
          }
        }
      },
      "12.4": {
        "Title": "PCA for categorical data",
        "Sub Topics": {
          "12.4.1": {
            "Title": "Statement of the result"
          }
        }
      },
      "12.5": {
        "Title": "PCA for paired and multi-view data",
        "Sub Topics": {
          "12.5.1": {
            "Title": "Supervised PCA (latent factor regression)"
          },
          "12.5.2": {
            "Title": "Partial least squares"
          },
          "12.5.3": {
            "Title": "Canonical correlation analysis"
          }
        }
      },
      "12.6": {
        "Title": "Independent Component Analysis (ICA)",
        "Sub Topics": {
          "12.6.1": {
            "Title": "Maximum likelihood estimation"
          },
          "12.6.2": {
            "Title": "The FastICA algorithm"
          },
          "12.6.3": {
            "Title": "Using EM for ICA"
          },
          "12.6.4": {
            "Title": "Other estimation principles"
          }
        }
      }
    }
  },
  "13": {
    "Title": "Sparse linear models",
    "Sub Topics": {
      "13.1": {
        "Title": "Introduction to Sparse linear models",
        "Sub Topics": {}
      },
      "13.2": {
        "Title": "Bayesian variable selection",
        "Sub Topics": {
          "13.2.1": {
            "Title": "The spike and slab model"
          },
          "13.2.2": {
            "Title": "From the Bernoulli-Gaussian model to L0 regularization"
          },
          "13.2.3": {
            "Title": "Algorithms (Bayesian variable selection)"
          }
        }
      },
      "13.3": {
        "Title": "L1 regularization: basics",
        "Sub Topics": {
          "13.3.1": {
            "Title": "Why does L1 regularization yield sparse solutions?"
          },
          "13.3.2": {
            "Title": "Optimality conditions for lasso"
          },
          "13.3.3": {
            "Title": "Comparison of least squares, lasso, ridge and subset selection"
          },
          "13.3.4": {
            "Title": "Regularization path L1"
          },
          "13.3.5": {
            "Title": "Model selection L1"
          },
          "13.3.6": {
            "Title": "Bayesian inference for linear models with Laplace priors"
          }
        }
      },
      "13.4": {
        "Title": "L1 regularization: algorithms",
        "Sub Topics": {
          "13.4.1": {
            "Title": "Coordinate descent"
          },
          "13.4.2": {
            "Title": "LARS and other homotopy methods"
          },
          "13.4.3": {
            "Title": "Proximal and gradient projection methods"
          },
          "13.4.4": {
            "Title": "EM for lasso"
          }
        }
      },
      "13.5": {
        "Title": "L1 regularization: extensions",
        "Sub Topics": {
          "13.5.1": {
            "Title": "Group Lasso"
          },
          "13.5.2": {
            "Title": "Fused lasso"
          },
          "13.5.3": {
            "Title": "Elastic net (ridge and lasso combined)"
          }
        }
      },
      "13.6": {
        "Title": "Non-convex regularizers",
        "Sub Topics": {
          "13.6.1": {
            "Title": "Bridge regression"
          },
          "13.6.2": {
            "Title": "Hierarchical adaptive lasso"
          },
          "13.6.3": {
            "Title": "Other hierarchical priors"
          }
        }
      },
      "13.7": {
        "Title": "Automatic relevance determination (ARD)/sparse Bayesian learning (SBL)",
        "Sub Topics": {
          "13.7.1": {
            "Title": "ARD for linear regression"
          },
          "13.7.2": {
            "Title": "Whence sparsity?"
          },
          "13.7.3": {
            "Title": "Connection to MAP estimation"
          },
          "13.7.4": {
            "Title": "Algorithms for ARD"
          },
          "13.7.5": {
            "Title": "ARD for logistic regression"
          }
        }
      },
      "13.8": {
        "Title": "Sparse coding",
        "Sub Topics": {
          "13.8.1": {
            "Title": "Learning a sparse coding dictionary"
          },
          "13.8.2": {
            "Title": "Results of dictionary learning from image patches"
          },
          "13.8.3": {
            "Title": "Compressed sensing"
          },
          "13.8.4": {
            "Title": "Image inpainting and denoising"
          }
        }
      }
    }
  },
  "14": {
    "Title": "Kernel Methods",
    "Sub Topics": {
      "14.1": {
        "Title": "Introduction to Kernel Methods",
        "Sub Topics": {}
      },
      "14.2": {
        "Title": "Kernel functions",
        "Sub Topics": {
          "14.2.1": {
            "Title": "RBF kernels"
          },
          "14.2.2": {
            "Title": "Kernels for comparing documents"
          },
          "14.2.3": {
            "Title": "Mercer (positive definite) kernels"
          },
          "14.2.4": {
            "Title": "Linear kernels"
          },
          "14.2.5": {
            "Title": "Matern kernels"
          },
          "14.2.6": {
            "Title": "String kernels"
          },
          "14.2.7": {
            "Title": "Pyramid match kernels"
          },
          "14.2.8": {
            "Title": "Kernels derived from probabilistic generative models"
          }
        }
      },
      "14.3": {
        "Title": "Using kernels inside GLMs",
        "Sub Topics": {
          "14.3.1": {
            "Title": "Kernel machines"
          },
          "14.3.2": {
            "Title": "L1VMs, RVMs, and other sparse vector machines"
          }
        }
      },
      "14.4": {
        "Title": "The kernel trick",
        "Sub Topics": {
          "14.4.1": {
            "Title": "Kernelized nearest neighbor classification"
          },
          "14.4.2": {
            "Title": "Kernelized K-medoids clustering"
          },
          "14.4.3": {
            "Title": "Kernelized ridge regression"
          },
          "14.4.4": {
            "Title": "Kernel PCA"
          }
        }
      },
      "14.5": {
        "Title": "Support vector machines (SVMs)",
        "Sub Topics": {
          "14.5.1": {
            "Title": "SVMs for regression"
          },
          "14.5.2": {
            "Title": "SVMs for classification"
          },
          "14.5.3": {
            "Title": "Choosing C"
          },
          "14.5.4": {
            "Title": "Summary of key points"
          },
          "14.5.5": {
            "Title": "A probabilistic interpretation of SVMs"
          }
        }
      },
      "14.6": {
        "Title": "Comparison of discriminative kernel methods",
        "Sub Topics": {}
      },
      "14.7": {
        "Title": "Kernels for building generative models",
        "Sub Topics": {
          "14.7.1": {
            "Title": "Smoothing kernels"
          },
          "14.7.2": {
            "Title": "Kernel density estimation (KDE)"
          },
          "14.7.3": {
            "Title": "From KDE to KNN"
          },
          "14.7.4": {
            "Title": "Kernel regression"
          },
          "14.7.5": {
            "Title": "Locally weighted regression"
          }
        }
      }
    }
  },
  "15": {
    "Title": "Gaussian processes",
    "Sub Topics": {
      "15.1": {
        "Title": "Introduction to Gaussian processes",
        "Sub Topics": {}
      },
      "15.2": {
        "Title": "GPs for regression",
        "Sub Topics": {
          "15.2.1": {
            "Title": "Predictions using noise-free observations"
          },
          "15.2.2": {
            "Title": "Predictions using noisy observations"
          },
          "15.2.3": {
            "Title": "Effect of the kernel parameters"
          },
          "15.2.4": {
            "Title": "Estimating the kernel parameters"
          },
          "15.2.5": {
            "Title": "Computational and numerical issues"
          },
          "15.2.6": {
            "Title": "Semi-parametric GPs"
          }
        }
      },
      "15.3": {
        "Title": "GPs meet GLMs",
        "Sub Topics": {
          "15.3.1": {
            "Title": "Binary classification"
          },
          "15.3.2": {
            "Title": "Multi-class classification"
          },
          "15.3.3": {
            "Title": "GPs for Poisson regression"
          }
        }
      },
      "15.4": {
        "Title": "Connection with other methods",
        "Sub Topics": {
          "15.4.1": {
            "Title": "Linear models compared to GPs"
          },
          "15.4.2": {
            "Title": "Linear smoothers compared to GPs"
          },
          "15.4.3": {
            "Title": "SVMs compared to GPs"
          },
          "15.4.4": {
            "Title": "L1VM and RVMs compared to GPs"
          },
          "15.4.5": {
            "Title": "Neural networks compared to GPs"
          },
          "15.4.6": {
            "Title": "Smoothing splines compared to GPs"
          },
          "15.4.7": {
            "Title": "RKHS methods compared to GPs"
          }
        }
      },
      "15.5": {
        "Title": "GP latent variable model",
        "Sub Topics": {}
      },
      "15.6": {
        "Title": "Approximation methods for large datasets",
        "Sub Topics": {}
      }
    }
  },
  "16": {
    "Title": "Ensemble Methods",
    "Sub Topics": {
      "16.1": {
        "Title": "Introduction to Ensemble Methods",
        "Sub Topics": {}
      },
      "16.2": {
        "Title": "Classification and regression trees (CART)",
        "Sub Topics": {
          "16.2.1": {
            "Title": "Basics of CART"
          },
          "16.2.2": {
            "Title": "Growing a tree"
          },
          "16.2.3": {
            "Title": "Pruning a tree"
          },
          "16.2.4": {
            "Title": "Pros and cons of trees"
          },
          "16.2.5": {
            "Title": "Random forests"
          },
          "16.2.6": {
            "Title": "CART compared to hierarchical mixture of experts"
          }
        }
      },
      "16.3": {
        "Title": "Generalized additive models",
        "Sub Topics": {
          "16.3.1": {
            "Title": "Backfitting"
          },
          "16.3.2": {
            "Title": "Computational efficiency"
          },
          "16.3.3": {
            "Title": "Multivariate adaptive regression splines (MARS)"
          }
        }
      },
      "16.4": {
        "Title": "Boosting",
        "Sub Topics": {
          "16.4.1": {
            "Title": "Forward stagewise additive modeling"
          },
          "16.4.2": {
            "Title": "L2boosting"
          },
          "16.4.3": {
            "Title": "AdaBoost"
          },
          "16.4.4": {
            "Title": "LogitBoost"
          },
          "16.4.5": {
            "Title": "Boosting as functional gradient descent"
          },
          "16.4.6": {
            "Title": "Sparse boosting"
          },
          "16.4.7": {
            "Title": "Multivariate adaptive regression trees (MART)"
          },
          "16.4.8": {
            "Title": "Why does boosting work so well?"
          },
          "16.4.9": {
            "Title": "A Bayesian view to Boosting"
          }
        }
      },
      "16.5": {
        "Title": "Feedforward neural networks (multilayer perceptrons)",
        "Sub Topics": {
          "16.5.1": {
            "Title": "Convolutional neural networks"
          },
          "16.5.2": {
            "Title": "Other kinds of neural networks"
          },
          "16.5.3": {
            "Title": "A brief history of the field"
          },
          "16.5.4": {
            "Title": "The backpropagation algorithm"
          },
          "16.5.5": {
            "Title": "Identifiability"
          },
          "16.5.6": {
            "Title": "Regularization (multilayer perceptrons)"
          },
          "16.5.7": {
            "Title": "Bayesian inference (multilayer perceptrons)"
          }
        }
      },
      "16.6": {
        "Title": "Ensemble learning",
        "Sub Topics": {
          "16.6.1": {
            "Title": "Stacking"
          },
          "16.6.2": {
            "Title": "Error-correcting output codes"
          },
          "16.6.3": {
            "Title": "Ensemble learning is not equivalent to Bayes model averaging"
          }
        }
      },
      "16.7": {
        "Title": "Experimental comparison",
        "Sub Topics": {
          "16.7.1": {
            "Title": "Low-dimensional features"
          },
          "16.7.2": {
            "Title": "High-dimensional features"
          }
        }
      },
      "16.8": {
        "Title": "Interpreting black-box models",
        "Sub Topics": {}
      }
    }
  },
  "17": {
    "Title": "Markov and hidden Markov models",
    "Sub Topics": {
      "17.1": {
        "Title": "Introduction to Markov and hidden Markov models",
        "Sub Topics": {}
      },
      "17.2": {
        "Title": "Markov models",
        "Sub Topics": {
          "17.2.1": {
            "Title": "Transition matrix"
          },
          "17.2.2": {
            "Title": "Application: Language modeling"
          },
          "17.2.3": {
            "Title": "Stationary distribution of a Markov chain"
          },
          "17.2.4": {
            "Title": "Application: Google's PageRank algorithm for web page ranking"
          }
        }
      },
      "17.3": {
        "Title": "Hidden Markov models",
        "Sub Topics": {
          "17.3.1": {
            "Title": "Applications of HMMs"
          }
        }
      },
      "17.4": {
        "Title": "Inference in HMMs",
        "Sub Topics": {
          "17.4.1": {
            "Title": "Types of inference problems for temporal models"
          },
          "17.4.2": {
            "Title": "The forwards algorithm"
          },
          "17.4.3": {
            "Title": "The forwards-backwards algorithm"
          },
          "17.4.4": {
            "Title": "The Viterbi algorithm"
          },
          "17.4.5": {
            "Title": "Forwards filtering, backwards sampling"
          }
        }
      },
      "17.5": {
        "Title": "Learning for HMMs",
        "Sub Topics": {
          "17.5.1": {
            "Title": "Training with fully observed data HMMs"
          },
          "17.5.2": {
            "Title": "EM for HMMs (the Baum-Welch algorithm)"
          },
          "17.5.3": {
            "Title": "Bayesian methods for fitting HMMs"
          },
          "17.5.4": {
            "Title": "Discriminative training for HMMs"
          },
          "17.5.5": {
            "Title": "Model selection for HMMs"
          }
        }
      },
      "17.6": {
        "Title": "Generalizations of HMMs",
        "Sub Topics": {
          "17.6.1": {
            "Title": "Variable duration (semi-Markov) HMMs"
          },
          "17.6.2": {
            "Title": "Hierarchical HMMs"
          },
          "17.6.3": {
            "Title": "Input-output HMMs"
          },
          "17.6.4": {
            "Title": "Auto-regressive and buried HMMs"
          },
          "17.6.5": {
            "Title": "Factorial HMM"
          },
          "17.6.6": {
            "Title": "Coupled HMM and the influence model"
          },
          "17.6.7": {
            "Title": "Dynamic Bayesian networks (DBNs)"
          }
        }
      }
    }
  },
  "18": {
    "Title": "State space models",
    "Sub Topics": {
      "18.1": {
        "Title": "Introduction to State space models",
        "Sub Topics": {}
      },
      "18.2": {
        "Title": "Applications of SSMs",
        "Sub Topics": {
          "18.2.1": {
            "Title": "SSMs for object tracking"
          },
          "18.2.2": {
            "Title": "Robotic SLAM"
          },
          "18.2.3": {
            "Title": "Online parameter learning using recursive least squares"
          },
          "18.2.4": {
            "Title": "SSM for time series forecasting"
          }
        }
      },
      "18.3": {
        "Title": "Inference in LG-SSM",
        "Sub Topics": {
          "18.3.1": {
            "Title": "The Kalman filtering algorithm"
          },
          "18.3.2": {
            "Title": "The Kalman smoothing algorithm"
          }
        }
      },
      "18.4": {
        "Title": "Learning for LG-SSM",
        "Sub Topics": {
          "18.4.1": {
            "Title": "Identifiability and numerical stability"
          },
          "18.4.2": {
            "Title": "Training with fully observed data LG-SSM"
          },
          "18.4.3": {
            "Title": "EM for LG-SSM"
          },
          "18.4.4": {
            "Title": "Subspace methods"
          },
          "18.4.5": {
            "Title": "Bayesian methods for 'fitting' LG-SSMs"
          }
        }
      },
      "18.5": {
        "Title": "Approximate online inference for non-linear, non-Gaussian SSMs",
        "Sub Topics": {
          "18.5.1": {
            "Title": "Extended Kalman filter (EKF)"
          },
          "18.5.2": {
            "Title": "Unscented Kalman filter (UKF)"
          },
          "18.5.3": {
            "Title": "Assumed density filtering (ADF)"
          }
        }
      },
      "18.6": {
        "Title": "Hybrid discrete/continuous SSMs",
        "Sub Topics": {
          "18.6.1": {
            "Title": "Inference (Hybrid discrete/continuous SSMs)"
          },
          "18.6.2": {
            "Title": "Application: data association and multi-target tracking"
          },
          "18.6.3": {
            "Title": "Application: fault diagnosis"
          },
          "18.6.4": {
            "Title": "Application: econometric forecasting"
          }
        }
      }
    }
  },
  "19": {
    "Title": "Undirected graphical models (Markov random fields)",
    "Sub Topics": {
      "19.1": {
        "Title": "Introduction to Markov random fields",
        "Sub Topics": {}
      },
      "19.2": {
        "Title": "Conditional independence properties of UGMs",
        "Sub Topics": {
          "19.2.1": {
            "Title": "Key properties (UGMs)"
          },
          "19.2.2": {
            "Title": "An undirected alternative to d-separation"
          },
          "19.2.3": {
            "Title": "Comparing directed and undirected graphical models"
          }
        }
      },
      "19.3": {
        "Title": "Parameterization of MRFs",
        "Sub Topics": {
          "19.3.1": {
            "Title": "The Hammersley-Clifford theorem"
          },
          "19.3.2": {
            "Title": "Representing potential functions"
          }
        }
      },
      "19.4": {
        "Title": "Examples of MRFs",
        "Sub Topics": {
          "19.4.1": {
            "Title": "Ising model"
          },
          "19.4.2": {
            "Title": "Hopfield networks"
          },
          "19.4.3": {
            "Title": "Potts model"
          },
          "19.4.4": {
            "Title": "Gaussian MRFs"
          },
          "19.4.5": {
            "Title": "Markov logic networks"
          }
        }
      },
      "19.5": {
        "Title": "Learning (Markov random fields)",
        "Sub Topics": {
          "19.5.1": {
            "Title": "Training maxent models using gradient methods"
          },
          "19.5.2": {
            "Title": "Training partially observed maxent models"
          },
          "19.5.3": {
            "Title": "Approximate methods for computing the MLEs of MRFs"
          },
          "19.5.4": {
            "Title": "Pseudo likelihood"
          },
          "19.5.5": {
            "Title": "Stochastic maximum likelihood"
          },
          "19.5.6": {
            "Title": "Feature induction for maxent models"
          },
          "19.5.7": {
            "Title": "Iterative proportional fitting (IPF)"
          }
        }
      },
      "19.6": {
        "Title": "Conditional random fields (CRFs)",
        "Sub Topics": {
          "19.6.1": {
            "Title": "Chain-structured CRFs, MEMMs and the label-bias problem"
          },
          "19.6.2": {
            "Title": "Applications of CRFs"
          },
          "19.6.3": {
            "Title": "CRF training"
          }
        }
      },
      "19.7": {
        "Title": "Structural SVMs",
        "Sub Topics": {
          "19.7.1": {
            "Title": "SSVMs: a probabilistic view"
          },
          "19.7.2": {
            "Title": "SSVMs: a non-probabilistic view"
          },
          "19.7.3": {
            "Title": "Cutting plane methods for fitting SSVMs"
          },
          "19.7.4": {
            "Title": "Online algorithms for fitting SSVMs"
          },
          "19.7.5": {
            "Title": "Latent structural SVMs"
          }
        }
      }
    }
  },
  "20": {
    "Title": "Exact inference for graphical models",
    "Sub Topics": {
      "20.1": {
        "Title": "Introduction to Exact inference for graphical models",
        "Sub Topics": {}
      },
      "20.2": {
        "Title": "Belief propagation for trees",
        "Sub Topics": {
          "20.2.1": {
            "Title": "Serial protocol"
          },
          "20.2.2": {
            "Title": "Parallel protocol"
          },
          "20.2.3": {
            "Title": "Gaussian BP"
          },
          "20.2.4": {
            "Title": "Other BP variants"
          }
        }
      },
      "20.3": {
        "Title": "The variable elimination algorithm",
        "Sub Topics": {
          "20.3.1": {
            "Title": "The generalized distributive law"
          },
          "20.3.2": {
            "Title": "Computational complexity of VE"
          },
          "20.3.3": {
            "Title": "A weakness of VE"
          }
        }
      },
      "20.4": {
        "Title": "The junction tree algorithm",
        "Sub Topics": {
          "20.4.1": {
            "Title": "Creating a junction tree"
          },
          "20.4.2": {
            "Title": "Message passing on a junction tree"
          },
          "20.4.3": {
            "Title": "Computational complexity of JTA"
          },
          "20.4.4": {
            "Title": "JTA generalizations"
          }
        }
      },
      "20.5": {
        "Title": "Computational intractability of exact inference in the worst case",
        "Sub Topics": {
          "20.5.1": {
            "Title": "Approximate inference"
          }
        }
      }
    }
  },
  "21": {
    "Title": "Variational inference",
    "Sub Topics": {
      "21.1": {
        "Title": "Introduction to Variational inference",
        "Sub Topics": {}
      },
      "21.2": {
        "Title": "Variational inference explained",
        "Sub Topics": {
          "21.2.1": {
            "Title": "Alternative interpretations of the variational objective"
          },
          "21.2.2": {
            "Title": "Forward or reverse KL?"
          }
        }
      },
      "21.3": {
        "Title": "The mean field method",
        "Sub Topics": {
          "21.3.1": {
            "Title": "Derivation of the mean field update equations"
          },
          "21.3.2": {
            "Title": "Example: mean field for the Ising model"
          }
        }
      },
      "21.4": {
        "Title": "Structured mean field",
        "Sub Topics": {
          "21.4.1": {
            "Title": "Example: factorial HMM"
          }
        }
      },
      "21.5": {
        "Title": "Variational Bayes",
        "Sub Topics": {
          "21.5.1": {
            "Title": "Example: VB for a univariate Gaussian"
          },
          "21.5.2": {
            "Title": "Example: VB for linear regression"
          }
        }
      },
      "21.6": {
        "Title": "Variational Bayes EM",
        "Sub Topics": {
          "21.6.1": {
            "Title": "Example: VBEM for mixtures of Gaussians"
          }
        }
      },
      "21.7": {
        "Title": "Variational message passing and VIBES",
        "Sub Topics": {}
      },
      "21.8": {
        "Title": "Local variational bounds",
        "Sub Topics": {
          "21.8.1": {
            "Title": "Motivating applications of Local variational bounds"
          },
          "21.8.2": {
            "Title": "Bohning's quadratic bound to the log-sum-exp function"
          },
          "21.8.3": {
            "Title": "Bounds for the sigmoid function"
          },
          "21.8.4": {
            "Title": "Other bounds and approximations to the log-sum-exp function"
          },
          "21.8.5": {
            "Title": "Variational inference based on upper bounds"
          }
        }
      }
    }
  },
  "22": {
    "Title": "Variational Inference",
    "Sub Topics": {
      "22.1": {
        "Title": "Introduction to Variational Inference",
        "Sub Topics": {}
      },
      "22.2": {
        "Title": "Loopy belief propagation: algorithmic issues",
        "Sub Topics": {
          "22.2.1": {
            "Title": "A brief history on algorithmic issues"
          },
          "22.2.2": {
            "Title": "LBP on pairwise models"
          },
          "22.2.3": {
            "Title": "LBP on a factor graph"
          },
          "22.2.4": {
            "Title": "Convergence"
          },
          "22.2.5": {
            "Title": "Accuracy of LBP"
          },
          "22.2.6": {
            "Title": "Other speedup tricks for LBP"
          }
        }
      },
      "22.3": {
        "Title": "Loopy belief propagation: theoretical issues",
        "Sub Topics": {
          "22.3.1": {
            "Title": "UGMs represented in exponential family form"
          },
          "22.3.2": {
            "Title": "The marginal polytope"
          },
          "22.3.3": {
            "Title": "Exact inference as a variational optimization problem"
          },
          "22.3.4": {
            "Title": "Mean field as a variational optimization problem"
          },
          "22.3.5": {
            "Title": "LBP as a variational optimization problem"
          },
          "22.3.6": {
            "Title": "Loopy BP vs mean field"
          }
        }
      },
      "22.4": {
        "Title": "Extensions of belief propagation",
        "Sub Topics": {
          "22.4.1": {
            "Title": "Generalized belief propagation"
          },
          "22.4.2": {
            "Title": "Convex belief propagation"
          }
        }
      },
      "22.5": {
        "Title": "Expectation propagation",
        "Sub Topics": {
          "22.5.1": {
            "Title": "EP as a variational inference problem"
          },
          "22.5.2": {
            "Title": "Optimizing the EP objective using moment matching"
          },
          "22.5.3": {
            "Title": "EP for the clutter problem"
          },
          "22.5.4": {
            "Title": "LBP is a special case of EP"
          },
          "22.5.5": {
            "Title": "Ranking players using TrueSkill"
          },
          "22.5.6": {
            "Title": "Other applications of EP"
          }
        }
      },
      "22.6": {
        "Title": "MAP state estimation",
        "Sub Topics": {
          "22.6.1": {
            "Title": "Linear programming relaxation"
          },
          "22.6.2": {
            "Title": "Max-product belief propagation"
          },
          "22.6.3": {
            "Title": "Graphcuts"
          },
          "22.6.4": {
            "Title": "Experimental comparison of graphcuts and BP"
          },
          "22.6.5": {
            "Title": "Dual decomposition"
          }
        }
      }
    }
  },
  "23": {
    "Title": "Monte Carlo inference",
    "Sub Topics": {
      "23.1": {
        "Title": "Introduction to Monte Carlo inference",
        "Sub Topics": {}
      },
      "23.2": {
        "Title": "Sampling from standard distributions",
        "Sub Topics": {
          "23.2.1": {
            "Title": "Using the cdf"
          },
          "23.2.2": {
            "Title": "Sampling from a Gaussian (Box-Muller method)"
          }
        }
      },
      "23.3": {
        "Title": "Rejection sampling",
        "Sub Topics": {
          "23.3.1": {
            "Title": "Basic idea (Rejection sampling)"
          },
          "23.3.2": {
            "Title": "Example of Rejection sampling"
          },
          "23.3.3": {
            "Title": "Application to Bayesian statistics"
          },
          "23.3.4": {
            "Title": "Adaptive rejection sampling"
          },
          "23.3.5": {
            "Title": "Rejection sampling in high dimensions"
          }
        }
      },
      "23.4": {
        "Title": "Importance sampling",
        "Sub Topics": {
          "23.4.1": {
            "Title": "Basic idea (Importance sampling)"
          },
          "23.4.2": {
            "Title": "Handling unnormalized distributions"
          },
          "23.4.3": {
            "Title": "Importance sampling for a DGM: likelihood weighting"
          },
          "23.4.4": {
            "Title": "Sampling importance resampling (SIR)"
          }
        }
      },
      "23.5": {
        "Title": "Particle filtering",
        "Sub Topics": {
          "23.5.1": {
            "Title": "Sequential importance sampling"
          },
          "23.5.2": {
            "Title": "The degeneracy problem"
          },
          "23.5.3": {
            "Title": "The resampling step"
          },
          "23.5.4": {
            "Title": "The proposal distribution"
          },
          "23.5.5": {
            "Title": "Application: robot localization"
          },
          "23.5.6": {
            "Title": "Application: visual object tracking"
          },
          "23.5.7": {
            "Title": "Application: time series forecasting"
          }
        }
      },
      "23.6": {
        "Title": "Rao-Blackwellised particle filtering (RBPF)",
        "Sub Topics": {
          "23.6.1": {
            "Title": "RBPF for switching LG-SSMs"
          },
          "23.6.2": {
            "Title": "Application: tracking a maneuvering target"
          },
          "23.6.3": {
            "Title": "Application: Fast SLAM"
          }
        }
      }
    }
  },
  "24": {
    "Title": "Markov chain Monte Carlo (MCMC) inference",
    "Sub Topics": {
      "24.1": {
        "Title": "Introductio to Markov chain Monte Carlo",
        "Sub Topics": {}
      },
      "24.2": {
        "Title": "Gibbs sampling",
        "Sub Topics": {
          "24.2.1": {
            "Title": "Basic idea (Gibbs sampling)"
          },
          "24.2.2": {
            "Title": "Example: Gibbs sampling for the Ising model"
          },
          "24.2.3": {
            "Title": "Example: Gibbs sampling for inferring the parameters of a GMM"
          },
          "24.2.4": {
            "Title": "Collapsed Gibbs sampling"
          },
          "24.2.5": {
            "Title": "Gibbs sampling for hierarchical GLMs"
          },
          "24.2.6": {
            "Title": "BUGS and JAGS"
          },
          "24.2.7": {
            "Title": "The Imputation Posterior (IP) algorithm"
          },
          "24.2.8": {
            "Title": "Blocking Gibbs sampling"
          }
        }
      },
      "24.3": {
        "Title": "Metropolis Hastings algorithm",
        "Sub Topics": {
          "24.3.1": {
            "Title": "Basic idea (Metropolis Hastings algorithm)"
          },
          "24.3.2": {
            "Title": "Gibbs sampling is a special case of MH"
          },
          "24.3.3": {
            "Title": "Proposal distributions"
          },
          "24.3.4": {
            "Title": "Adaptive MCMC"
          },
          "24.3.5": {
            "Title": "Initialization and mode hopping"
          },
          "24.3.6": {
            "Title": "Why MH works"
          },
          "24.3.7": {
            "Title": "Reversible jump (trans-dimensional) MCMC"
          }
        }
      },
      "24.4": {
        "Title": "Speed and accuracy of MCMC",
        "Sub Topics": {
          "24.4.1": {
            "Title": "The burn-in phase"
          },
          "24.4.2": {
            "Title": "Mixing rates of Markov chains"
          },
          "24.4.3": {
            "Title": "Practical convergence diagnostics"
          },
          "24.4.4": {
            "Title": "Accuracy of MCMC"
          },
          "24.4.5": {
            "Title": "How many chains?"
          }
        }
      },
      "24.5": {
        "Title": "Auxiliary variable MCMC",
        "Sub Topics": {
          "24.5.1": {
            "Title": "Auxiliary variable sampling for logistic regression"
          },
          "24.5.2": {
            "Title": "Slice sampling"
          },
          "24.5.3": {
            "Title": "Swendsen Wang"
          },
          "24.5.4": {
            "Title": "Hybrid/Hamiltonian MCMC"
          }
        }
      },
      "24.6": {
        "Title": "Annealing methods",
        "Sub Topics": {
          "24.6.1": {
            "Title": "Simulated annealing"
          },
          "24.6.2": {
            "Title": "Annealed importance sampling (Annealing methods)"
          },
          "24.6.3": {
            "Title": "Parallel tempering"
          }
        }
      },
      "24.7": {
        "Title": "Approximating the marginal likelihood",
        "Sub Topics": {
          "24.7.1": {
            "Title": "The candidate method"
          },
          "24.7.2": {
            "Title": "Harmonic mean estimate"
          },
          "24.7.3": {
            "Title": "Annealed importance sampling (marginal likelihood)"
          }
        }
      }
    }
  },
  "25": {
    "Title": "Clustering",
    "Sub Topics": {
      "25.1": {
        "Title": "Introduction to Clustering",
        "Sub Topics": {
          "25.1.1": {
            "Title": "Measuring (dis)similarity"
          },
          "25.1.2": {
            "Title": "Evaluating the output of clustering methods"
          }
        }
      },
      "25.2": {
        "Title": "Dirichlet process mixture models",
        "Sub Topics": {
          "25.2.1": {
            "Title": "From finite to infinite mixture models"
          },
          "25.2.2": {
            "Title": "The Dirichlet process"
          },
          "25.2.3": {
            "Title": "Applying Dirichlet processes to mixture modeling"
          },
          "25.2.4": {
            "Title": "Fitting a DP mixture model"
          }
        }
      },
      "25.3": {
        "Title": "Affinity propagation",
        "Sub Topics": {}
      },
      "25.4": {
        "Title": "Spectral clustering",
        "Sub Topics": {
          "25.4.1": {
            "Title": "Graph Laplacian"
          },
          "25.4.2": {
            "Title": "Normalized graph Laplacian"
          },
          "25.4.3": {
            "Title": "Example (Spectral clustering)"
          }
        }
      },
      "25.5": {
        "Title": "Hierarchical clustering",
        "Sub Topics": {
          "25.5.1": {
            "Title": "Agglomerative clustering"
          },
          "25.5.2": {
            "Title": "Divisive clustering"
          },
          "25.5.3": {
            "Title": "Choosing the number of clusters"
          },
          "25.5.4": {
            "Title": "Bayesian hierarchical clustering"
          }
        }
      },
      "25.6": {
        "Title": "Clustering datapoints and features",
        "Sub Topics": {
          "25.6.1": {
            "Title": "Biclustering"
          },
          "25.6.2": {
            "Title": "Multi-view clustering"
          }
        }
      }
    }
  },
  "26": {
    "Title": "Graphical model structure learning",
    "Sub Topics": {
      "26.1": {
        "Title": "Introduction to Graphical model structure learning",
        "Sub Topics": {}
      },
      "26.2": {
        "Title": "Structure learning for knowledge discovery",
        "Sub Topics": {
          "26.2.1": {
            "Title": "Relevance networks"
          },
          "26.2.2": {
            "Title": "Dependency networks"
          }
        }
      },
      "26.3": {
        "Title": "Learning tree structures",
        "Sub Topics": {
          "26.3.1": {
            "Title": "Directed or undirected tree?"
          },
          "26.3.2": {
            "Title": "Chow-Liu algorithm for finding the ML tree structure"
          },
          "26.3.3": {
            "Title": "Finding the MAP forest"
          },
          "26.3.4": {
            "Title": "Mixtures of trees"
          }
        }
      },
      "26.4": {
        "Title": "Learning DAG structures",
        "Sub Topics": {
          "26.4.1": {
            "Title": "Markov equivalence"
          },
          "26.4.2": {
            "Title": "Exact structural inference"
          },
          "26.4.3": {
            "Title": "Scaling up to larger graphs"
          }
        }
      },
      "26.5": {
        "Title": "Learning DAG structure with latent variables",
        "Sub Topics": {
          "26.5.1": {
            "Title": "Approximating the marginal likelihood when we have missing data"
          },
          "26.5.2": {
            "Title": "Structural EM"
          },
          "26.5.3": {
            "Title": "Discovering hidden variables"
          },
          "26.5.4": {
            "Title": "Case study: Google's Rephil"
          },
          "26.5.5": {
            "Title": "Structural equation models"
          }
        }
      },
      "26.6": {
        "Title": "Learning causal DAGs",
        "Sub Topics": {
          "26.6.1": {
            "Title": "Causal interpretation of DAGs"
          },
          "26.6.2": {
            "Title": "Using causal DAGs to resolve Simpson's paradox"
          },
          "26.6.3": {
            "Title": "Learning causal DAG structures"
          }
        }
      },
      "26.7": {
        "Title": "Learning undirected Gaussian graphical models",
        "Sub Topics": {
          "26.7.1": {
            "Title": "MLE for a GGM"
          },
          "26.7.2": {
            "Title": "Graphical lasso"
          },
          "26.7.3": {
            "Title": "Bayesian inference for GGM structure"
          },
          "26.7.4": {
            "Title": "Handling non-Gaussian data using copulas"
          }
        }
      },
      "26.8": {
        "Title": "Learning undirected discrete graphical models",
        "Sub Topics": {
          "26.8.1": {
            "Title": "Graphical lasso for MRFs/CRFs"
          },
          "26.8.2": {
            "Title": "Thin junction trees"
          }
        }
      }
    }
  },
  "27": {
    "Title": "Latent variable models for discrete data",
    "Sub Topics": {
      "27.1": {
        "Title": "Introduction to Latent variable models for discrete data",
        "Sub Topics": {}
      },
      "27.2": {
        "Title": "Distributed state LVMs for discrete data",
        "Sub Topics": {
          "27.2.1": {
            "Title": "Mixture models (Distributed state LVMs)"
          },
          "27.2.2": {
            "Title": "Exponential family PCA"
          },
          "27.2.3": {
            "Title": "LDA and mPCA"
          },
          "27.2.4": {
            "Title": "GaP model and non-negative matrix factorization"
          }
        }
      },
      "27.3": {
        "Title": "Latent Dirichlet allocation (LDA)",
        "Sub Topics": {
          "27.3.1": {
            "Title": "Basics of Latent Dirichlet allocation"
          },
          "27.3.2": {
            "Title": "Unsupervised discovery of topics"
          },
          "27.3.3": {
            "Title": "Quantitatively evaluating LDA as a language model"
          },
          "27.3.4": {
            "Title": "Fitting using (collapsed) Gibbs sampling"
          },
          "27.3.5": {
            "Title": "Example (Latent Dirichlet allocation)"
          },
          "27.3.6": {
            "Title": "Fitting using batch variational inference"
          },
          "27.3.7": {
            "Title": "Fitting using online variational inference"
          },
          "27.3.8": {
            "Title": "Determining the number of topics"
          }
        }
      },
      "27.4": {
        "Title": "Extensions of LDA",
        "Sub Topics": {
          "27.4.1": {
            "Title": "Correlated topic model"
          },
          "27.4.2": {
            "Title": "Dynamic topic model"
          },
          "27.4.3": {
            "Title": "LDA-HMM"
          },
          "27.4.4": {
            "Title": "Supervised LDA"
          }
        }
      },
      "27.5": {
        "Title": "LVMs for graph-structured data",
        "Sub Topics": {
          "27.5.1": {
            "Title": "Stochastic block model"
          },
          "27.5.2": {
            "Title": "Mixed membership stochastic block model"
          },
          "27.5.3": {
            "Title": "Relational topic model"
          }
        }
      },
      "27.6": {
        "Title": "LVMs for relational data",
        "Sub Topics": {
          "27.6.1": {
            "Title": "Infinite relational model"
          },
          "27.6.2": {
            "Title": "Probabilistic matrix factorization for collaborative filtering"
          }
        }
      },
      "27.7": {
        "Title": "Restricted Boltzmann machines (RBMs)",
        "Sub Topics": {
          "27.7.1": {
            "Title": "Varieties of RBMs"
          },
          "27.7.2": {
            "Title": "Learning RBMs"
          },
          "27.7.3": {
            "Title": "Applications of RBMs"
          }
        }
      }
    }
  },
  "28": {
    "Title": "Deep learning",
    "Sub Topics": {
      "28.1": {
        "Title": "Introduction Deep learning",
        "Sub Topics": {}
      },
      "28.2": {
        "Title": "Deep generative models",
        "Sub Topics": {
          "28.2.1": {
            "Title": "Deep directed networks"
          },
          "28.2.2": {
            "Title": "Deep Boltzmann machines"
          },
          "28.2.3": {
            "Title": "Deep belief networks"
          },
          "28.2.4": {
            "Title": "Greedy layer-wise learning of DBNs"
          }
        }
      },
      "28.3": {
        "Title": "Deep neural networks",
        "Sub Topics": {
          "28.3.1": {
            "Title": "Deep multi-layer perceptrons"
          },
          "28.3.2": {
            "Title": "Deep auto-encoders"
          },
          "28.3.3": {
            "Title": "Stacked denoising auto-encoders"
          }
        }
      },
      "28.4": {
        "Title": "Applications of deep networks",
        "Sub Topics": {
          "28.4.1": {
            "Title": "Handwritten digit classification using DBNs"
          },
          "28.4.2": {
            "Title": "Data visualization and feature discovery using deep auto-encoders"
          },
          "28.4.3": {
            "Title": "Information retrieval using deep auto-encoders (semantic hashing)"
          },
          "28.4.4": {
            "Title": "Learning audio features using 1d convolutional DBNs"
          },
          "28.4.5": {
            "Title": "Learning image features using 2d convolutional DBNs"
          }
        }
      }
    }
  }
}
}